<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <script>
        var papers = [{"id": "2306.03832", "date": "Tue, 6 Jun 2023 16:20:44 GMT", "title": "Sequential Principal-Agent Problems with Communication: Efficient\n Computation and Learning\n", "authors": ["Jiarui Gan", "Rupak Majumdar", "Debmalya Mandal", "Goran Radanovic\n"], "categories": ["cs.GT", "cs.LG", "cs.MA\n"], "abstract": "We study a sequential decision making problem between a principal and an agent with incomplete information on both sides. In this model, the principal and the agent interact in a stochastic environment, and each is privy to observations about the state not available to the other. The principal has the power of commitment, both to elicit information from the agent and to provide signals about her own information. The principal and the agent communicate their signals to each other, and select their actions independently based on this communication. Each player receives a payoff based on the state and their joint actions, and the environment moves to a new state. The interaction continues over a finite time horizon, and both players act to optimize their own total payoffs over the horizon. Our model encompasses as special cases stochastic games of incomplete information and POMDPs, as well as sequential Bayesian persuasion and mechanism design problems. We study both computation of optimal policies and learning in our setting. While the general problems are computationally intractable, we study algorithmic solutions under a conditional independence assumption on the underlying state-observation distributions. We present an polynomial-time algorithm to compute the principal's optimal policy up to an additive approximation. Additionally, we show an efficient learning algorithm in the case where the transition probabilities are not known beforehand. The algorithm guarantees sublinear regret for both players.", "link": "https://arxiv.org/abs/2306.03832"}, {"id": "2306.03163", "date": "Mon, 5 Jun 2023 18:17:37 GMT", "title": "How Can We Train Deep Learning Models Across Clouds and Continents? An\n Experimental Study\n", "authors": ["Alexander Isenko", "Ruben Mayer", "Hans-Arno Jacobsen\n"], "categories": ["cs.LG", "cs.DC", "cs.NI", "cs.PF\nComments:", "Currently", "in", "review.", "Artifacts", "and", "Code:\n", "https://github.com/cirquit/hivemind-multi-cloud\nACM-class:", "I.2.11;", "C.2.4;", "C.4;", "D.2.8\n"], "abstract": "Training deep learning models in the cloud or on dedicated hardware is expensive. A more cost-efficient option are hyperscale clouds offering spot instances, a cheap but ephemeral alternative to on-demand resources. As spot instance availability can change depending on the time of day, continent, and cloud provider, it could be more cost-efficient to distribute resources over the world. Still, it has not been investigated whether geo-distributed, data-parallel spot deep learning training could be a more cost-efficient alternative to centralized training. This paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV and NLP models. To expand the current training options further, we compare the scalability potential for hybrid-cloud scenarios by adding cloud resources to on-premise hardware to improve training throughput. Finally, we show how leveraging spot instance pricing enables a new cost-efficient way to train models with multiple cheap VMs, trumping both more centralized and powerful hardware and even on-demand cloud offerings at competitive prices.", "link": "https://arxiv.org/abs/2306.03163"}, {"id": "2306.03186", "date": "Mon, 5 Jun 2023 18:56:48 GMT", "title": "Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement\n Learning\n", "authors": ["Sam Lobel and Akhil Bagaria and George Konidaris\n"], "categories": ["cs.LG", "cs.AI\nComments:", "11", "pages", "(+9", "appendix).", "Published", "as", "a", "conference", "paper", "at", "ICML", "2023.\n", "Code", "available", "at", "https://github.com/samlobel/CFN/\n"], "abstract": "We propose a new method for count-based exploration in high-dimensional state spaces. Unlike previous work which relies on density models, we show that counts can be derived by averaging samples from the Rademacher distribution (or coin flips). This insight is used to set up a simple supervised learning objective which, when optimized, yields a state's visitation count. We show that our method is significantly more effective at deducing ground-truth visitation counts than previous work; when used as an exploration bonus for a model-free reinforcement learning algorithm, it outperforms existing approaches on most of 9 challenging exploration tasks, including the Atari game Montezuma's Revenge.", "link": "https://arxiv.org/abs/2306.03186"}, {"id": "2306.03228", "date": "Mon, 5 Jun 2023 20:22:05 GMT", "title": "Discovering Novel Biological Traits From Images Using Phylogeny-Guided\n Neural Networks\n", "authors": ["Mohannad Elhamod", "Mridul Khurana", "Harish Babu Manogaran", "Josef C.\n Uyeda", "Meghan A. Balk", "Wasila Dahdul", "Yasin Bak{\\i}\\c{s}", "Henry L. Bart Jr.,\n Paula M. Mabee", "Hilmar Lapp", "James P. Balhoff", "Caleb Charpentier", "David\n Carlyn", "Wei-Lun Chao", "Charles V. Stewart", "Daniel I. Rubenstein", "Tanya\n Berger-Wolf", "Anuj Karpatne\n"], "categories": ["cs.LG", "cs.CV", "eess.IV\n"], "abstract": "Discovering evolutionary traits that are heritable across species on the tree of life (also referred to as a phylogenetic tree) is of great interest to biologists to understand how organisms diversify and evolve. However, the measurement of traits is often a subjective and labor-intensive process, making trait discovery a highly label-scarce problem. We present a novel approach for discovering evolutionary traits directly from images without relying on trait labels. Our proposed approach, Phylo-NN, encodes the image of an organism into a sequence of quantized feature vectors -- or codes -- where different segments of the sequence capture evolutionary signals at varying ancestry levels in the phylogeny. We demonstrate the effectiveness of our approach in producing biologically meaningful results in a number of downstream tasks including species image generation and species-to-species image translation, using fish species as a target example.", "link": "https://arxiv.org/abs/2306.03228"}, {"id": "2306.03235", "date": "Mon, 5 Jun 2023 20:40:05 GMT", "title": "Information Flow Control in Machine Learning through Modular Model\n Architecture\n", "authors": ["Trishita Tiwari", "Suchin Gururangan", "Chuan Guo", "Weizhe Hua", "Sanjay\n Kariyappa", "Udit Gupta", "Wenjie Xiong", "Kiwan Maeng", "Hsien-Hsin S. Lee", "G.\n Edward Suh\n"], "categories": ["cs.LG", "cs.CR\n"], "abstract": "In today's machine learning (ML) models, any part of the training data can affect its output. This lack of control for information flow from training data to model output is a major obstacle in training models on sensitive data when access control only allows individual users to access a subset of data. To enable secure machine learning for access controlled data, we propose the notion of information flow control for machine learning, and develop a secure Transformer-based language model based on the Mixture-of-Experts (MoE) architecture. The secure MoE architecture controls information flow by limiting the influence of training data from each security domain to a single expert module, and only enabling a subset of experts at inference time based on an access control policy. The evaluation using a large corpus of text data shows that the proposed MoE architecture has minimal (1.9%) performance overhead and can significantly improve model accuracy (up to 37%) by enabling training on access-controlled data.", "link": "https://arxiv.org/abs/2306.03235"}, {"id": "2306.03249", "date": "Mon, 5 Jun 2023 21:08:34 GMT", "title": "Probabilistic Unrolling: Scalable, Inverse-Free Maximum Likelihood\n Estimation for Latent Gaussian Models\n", "authors": ["Alexander Lin", "Bahareh Tolooshams", "Yves Atchad\\'e", "Demba Ba\n"], "categories": ["cs.LG", "eess.SP", "stat.CO\nComments:", "29", "pages,", "4", "figures\nJournal-ref:", "International", "Conference", "on", "Machine", "Learning,", "2023\n"], "abstract": "Latent Gaussian models have a rich history in statistics and machine learning, with applications ranging from factor analysis to compressed sensing to time series analysis. The classical method for maximizing the likelihood of these models is the expectation-maximization (EM) algorithm. For problems with high-dimensional latent variables and large datasets, EM scales poorly because it needs to invert as many large covariance matrices as the number of data points. We introduce probabilistic unrolling, a method that combines Monte Carlo sampling with iterative linear solvers to circumvent matrix inversion. Our theoretical analyses reveal that unrolling and backpropagation through the iterations of the solver can accelerate gradient estimation for maximum likelihood estimation. In experiments on simulated and real data, we demonstrate that probabilistic unrolling learns latent Gaussian models up to an order of magnitude faster than gradient EM, with minimal losses in model performance.", "link": "https://arxiv.org/abs/2306.03249"}, {"id": "2306.03256", "date": "Mon, 5 Jun 2023 21:17:48 GMT", "title": "Explaining and Adapting Graph Conditional Shift\n", "authors": ["Qi Zhu", "Yizhu Jiao", "Natalia Ponomareva", "Jiawei Han", "Bryan Perozzi\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "Graph Neural Networks (GNNs) have shown remarkable performance on graph-structured data. However, recent empirical studies suggest that GNNs are very susceptible to distribution shift. There is still significant ambiguity about why graph-based models seem more vulnerable to these shifts. In this work we provide a thorough theoretical analysis on it by quantifying the magnitude of conditional shift between the input features and the output label. Our findings show that both graph heterophily and model architecture exacerbate conditional shifts, leading to performance degradation. To address this, we propose an approach that involves estimating and minimizing the conditional shift for unsupervised domain adaptation on graphs. In our controlled synthetic experiments, our algorithm demonstrates robustness towards distribution shift, resulting in up to 10% absolute ROC AUC improvement versus the second-best algorithm. Furthermore, comprehensive experiments on both node classification and graph classification show its robust performance under various distribution shifts.", "link": "https://arxiv.org/abs/2306.03256"}, {"id": "2306.03262", "date": "Mon, 5 Jun 2023 21:26:12 GMT", "title": "Has the Machine Learning Review Process Become More Arbitrary as the\n Field Has Grown? The NeurIPS 2021 Consistency Experiment\n", "authors": ["Alina Beygelzimer", "Yann N. Dauphin", "Percy Liang", "Jennifer Wortman\n Vaughan\n"], "categories": ["cs.LG", "cs.DL\n"], "abstract": "We present the NeurIPS 2021 consistency experiment, a larger-scale variant of the 2014 NeurIPS experiment in which 10% of conference submissions were reviewed by two independent committees to quantify the randomness in the review process. We observe that the two committees disagree on their accept/reject recommendations for 23% of the papers and that, consistent with the results from 2014, approximately half of the list of accepted papers would change if the review process were randomly rerun. Our analysis suggests that making the conference more selective would increase the arbitrariness of the process. Taken together with previous research, our results highlight the inherent difficulty of objectively measuring the quality of research, and suggest that authors should not be excessively discouraged by rejected work.", "link": "https://arxiv.org/abs/2306.03262"}, {"id": "2306.03266", "date": "Mon, 5 Jun 2023 21:35:32 GMT", "title": "Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking\n Folklore Weisfeiler-Lehman\n", "authors": ["Jiarui Feng", "Lecheng Kong", "Hao Liu", "Dacheng Tao", "Fuhai Li", "Muhan\n Zhang", "Yixin Chen\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k, t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^2)$ in $(k, t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors instead of all nodes, thereby greatly expanding the design space of $k$-FWL. Combining these two modifications results in a flexible and powerful framework $(k, t)$-FWL+. We demonstrate $(k, t)$-FWL+ can implement most existing models with matching expressiveness. We then introduce an instance of $(k,t)$-FWL+ called Neighborhood$^2$-FWL (N$^2$-FWL), which is practically and theoretically sound. We prove that N$^2$-FWL is no less powerful than 3-WL, can encode many substructures while only requiring $O(n^2)$ space. Finally, we design its neural version named N$^2$-GNN and evaluate its performance on various tasks. N$^2$-GNN achieves superior performance on almost all tasks, with record-breaking results on ZINC-Subset (0.059) and ZINC-Full (0.013), outperforming previous state-of-the-art results by 10.6% and 40.9%, respectively.", "link": "https://arxiv.org/abs/2306.03266"}, {"id": "2306.03273", "date": "Mon, 5 Jun 2023 21:45:23 GMT", "title": "Under-Counted Tensor Completion with Neural Incorporation of Attributes\n", "authors": ["Shahana Ibrahim", "Xiao Fu", "Rebecca Hutchinson", "Eugene Seo\n"], "categories": ["cs.LG", "eess.SP\nComments:", "33", "pages,", "5", "figures,", "ICML", "2023\n"], "abstract": "Systematic under-counting effects are observed in data collected across many disciplines, e.g., epidemiology and ecology. Under-counted tensor completion (UC-TC) is well-motivated for many data analytics tasks, e.g., inferring the case numbers of infectious diseases at unobserved locations from under-counted case numbers in neighboring regions. However, existing methods for similar problems often lack supports in theory, making it hard to understand the underlying principles and conditions beyond empirical successes. In this work, a low-rank Poisson tensor model with an expressive unknown nonlinear side information extractor is proposed for under-counted multi-aspect data. A joint low-rank tensor completion and neural network learning algorithm is designed to recover the model. Moreover, the UC-TC formulation is supported by theoretical analysis showing that the fully counted entries of the tensor and each entry's under-counting probability can be provably recovered from partial observations -- under reasonable conditions. To our best knowledge, the result is the first to offer theoretical supports for under-counted multi-aspect data completion. Simulations and real-data experiments corroborate the theoretical claims.", "link": "https://arxiv.org/abs/2306.03273"}, {"id": "2306.03284", "date": "Mon, 5 Jun 2023 22:09:06 GMT", "title": "Optimizing Sampling Patterns for Compressed Sensing MRI with Diffusion\n Generative Models\n", "authors": ["Sriram Ravula", "Brett Levac", "Ajil Jalal", "Jonathan I. Tamir", "Alexandros\n G. Dimakis\n"], "categories": ["cs.LG", "eess.IV\n"], "abstract": "Diffusion-based generative models have been used as powerful priors for magnetic resonance imaging (MRI) reconstruction. We present a learning method to optimize sub-sampling patterns for compressed sensing multi-coil MRI that leverages pre-trained diffusion generative models. Crucially, during training we use a single-step reconstruction based on the posterior mean estimate given by the diffusion model and the MRI measurement process. Experiments across varying anatomies, acceleration factors, and pattern types show that sampling operators learned with our method lead to competitive, and in the case of 2D patterns, improved reconstructions compared to baseline patterns. Our method requires as few as five training images to learn effective sampling patterns.", "link": "https://arxiv.org/abs/2306.03284"}, {"id": "2306.03286", "date": "Mon, 5 Jun 2023 22:15:39 GMT", "title": "Survival Instinct in Offline Reinforcement Learning\n", "authors": ["Anqi Li", "Dipendra Misra", "Andrey Kolobov", "Ching-An Cheng\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with \"wrong\" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of pessimism in offline RL algorithms and a certain bias implicit in common data collection practices. As we prove in this work, pessimism endows the agent with a \"survival instinct\", i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival policies. Formally, given a reward class -- which may not even contain the true reward -- we identify conditions on the training data distribution that enable offline RL to learn a near-optimal and safe policy from any reward within the class. We argue that the survival instinct should be taken into account when interpreting results from existing offline RL benchmarks and when creating future ones. Our empirical and theoretical results suggest a new paradigm for RL, whereby an agent is \"nudged\" to learn a desirable behavior with imperfect reward but purposely biased data coverage.", "link": "https://arxiv.org/abs/2306.03286"}, {"id": "2306.03288", "date": "Mon, 5 Jun 2023 22:21:26 GMT", "title": "Deep Learning From Crowdsourced Labels: Coupled Cross-entropy\n Minimization, Identifiability, and Regularization\n", "authors": ["Shahana Ibrahim", "Tri Nguyen", "Xiao Fu\n"], "categories": ["cs.LG", "eess.SP", "stat.ML\nComments:", "39", "pages,", "5", "figures,", "ICLR", "2023\n"], "abstract": "Using noisy crowdsourced labels from multiple annotators, a deep learning-based end-to-end (E2E) system aims to learn the label correction mechanism and the neural classifier simultaneously. To this end, many E2E systems concatenate the neural classifier with multiple annotator-specific ``label confusion'' layers and co-train the two parts in a parameter-coupled manner. The formulated coupled cross-entropy minimization (CCEM)-type criteria are intuitive and work well in practice. Nonetheless, theoretical understanding of the CCEM criterion has been limited. The contribution of this work is twofold: First, performance guarantees of the CCEM criterion are presented. Our analysis reveals for the first time that the CCEM can indeed correctly identify the annotators' confusion characteristics and the desired ``ground-truth'' neural classifier under realistic conditions, e.g., when only incomplete annotator labeling and finite samples are available. Second, based on the insights learned from our analysis, two regularized variants of the CCEM are proposed. The regularization terms provably enhance the identifiability of the target model parameters in various more challenging cases. A series of synthetic and real data experiments are presented to showcase the effectiveness of our approach.", "link": "https://arxiv.org/abs/2306.03288"}, {"id": "2306.03291", "date": "Mon, 5 Jun 2023 22:25:28 GMT", "title": "Switching Autoregressive Low-rank Tensor Models\n", "authors": ["Hyun Dong Lee", "Andrew Warrington", "Joshua I. Glaser", "Scott W. Linderman\n"], "categories": ["cs.LG", "stat.ME", "stat.ML\n"], "abstract": "An important problem in time-series analysis is modeling systems with time-varying dynamics. Probabilistic models with joint continuous and discrete latent states offer interpretable, efficient, and experimentally useful descriptions of such data. Commonly used models include autoregressive hidden Markov models (ARHMMs) and switching linear dynamical systems (SLDSs), each with its own advantages and disadvantages. ARHMMs permit exact inference and easy parameter estimation, but are parameter intensive when modeling long dependencies, and hence are prone to overfitting. In contrast, SLDSs can capture long-range dependencies in a parameter efficient way through Markovian latent dynamics, but present an intractable likelihood and a challenging parameter estimation task. In this paper, we propose switching autoregressive low-rank tensor (SALT) models, which retain the advantages of both approaches while ameliorating the weaknesses. SALT parameterizes the tensor of an ARHMM with a low-rank factorization to control the number of parameters and allow longer range dependencies without overfitting. We prove theoretical and discuss practical connections between SALT, linear dynamical systems, and SLDSs. We empirically demonstrate quantitative advantages of SALT models on a range of simulated and real prediction tasks, including behavioral and neural datasets. Furthermore, the learned low-rank tensor provides novel insights into temporal dependencies within each discrete state.", "link": "https://arxiv.org/abs/2306.03291"}, {"id": "2306.03301", "date": "Mon, 5 Jun 2023 23:03:03 GMT", "title": "Estimating Conditional Mutual Information for Dynamic Feature Selection\n", "authors": ["Soham Gadgil", "Ian Covert", "Su-In Lee\n"], "categories": ["cs.LG", "cs.IT", "math.IT\n"], "abstract": "Dynamic feature selection, where we sequentially query features to make accurate predictions with a minimal budget, is a promising paradigm to reduce feature acquisition costs and provide transparency into the prediction process. The problem is challenging, however, as it requires both making predictions with arbitrary feature sets and learning a policy to identify the most valuable selections. Here, we take an information-theoretic perspective and prioritize features based on their mutual information with the response variable. The main challenge is learning this selection policy, and we design a straightforward new modeling approach that estimates the mutual information in a discriminative rather than generative fashion. Building on our learning approach, we introduce several further improvements: allowing variable feature budgets across samples, enabling non-uniform costs between features, incorporating prior information, and exploring modern architectures to handle partial input information. We find that our method provides consistent gains over recent state-of-the-art methods across a variety of datasets.", "link": "https://arxiv.org/abs/2306.03301"}, {"id": "2306.03311", "date": "Mon, 5 Jun 2023 23:38:31 GMT", "title": "Learning Embeddings for Sequential Tasks Using Population of Agents\n", "authors": ["Mridul Mahajan", "Georgios Tzannetos", "Goran Radanovic", "Adish Singla\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "We present an information-theoretic framework to learn fixed-dimensional embeddings for tasks in reinforcement learning. We leverage the idea that two tasks are similar to each other if observing an agent's performance on one task reduces our uncertainty about its performance on the other. This intuition is captured by our information-theoretic criterion which uses a diverse population of agents to measure similarity between tasks in sequential decision-making settings. In addition to qualitative assessment, we empirically demonstrate the effectiveness of our techniques based on task embeddings by quantitative comparisons against strong baselines on two application scenarios: predicting an agent's performance on a test task by observing its performance on a small quiz of tasks, and selecting tasks with desired characteristics from a given set of options.", "link": "https://arxiv.org/abs/2306.03311"}, {"id": "2306.03329", "date": "Tue, 6 Jun 2023 00:42:36 GMT", "title": "AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca\n for Predicting Antigen-Antibody Interactions\n", "authors": ["Hirofumi Tsuruta", "Hiroyuki Yamazaki", "Ryota Maeda", "Ryotaro Tamura,\n Jennifer N. Wei", "Zelda Mariet", "Poomarin Phloyphisut", "Hidetoshi Shimokawa,\n Joseph R. Ledsam", "Lucy Colwell", "Akihiro Imura\n"], "categories": ["cs.LG", "q-bio.QM\n"], "abstract": "Antibodies have become an important class of therapeutic agents to treat human diseases. To accelerate therapeutic antibody discovery, computational methods, especially machine learning, have attracted considerable interest for predicting specific interactions between antibody candidates and target antigens such as viruses and bacteria. However, the publicly available datasets in existing works have notable limitations, such as small sizes and the lack of non-binding samples and exact amino acid sequences. To overcome these limitations, we have developed AVIDa-hIL6, a large-scale dataset for predicting antigen-antibody interactions in the variable domain of heavy chain of heavy chain antibodies (VHHs), produced from an alpaca immunized with the human interleukin-6 (IL-6) protein, as antigens. By leveraging the simple structure of VHHs, which facilitates identification of full-length amino acid sequences by DNA sequencing technology, AVIDa-hIL6 contains 573,891 antigen-VHH pairs with amino acid sequences. All the antigen-VHH pairs have reliable labels for binding or non-binding, as generated by a novel labeling method. Furthermore, via introduction of artificial mutations, AVIDa-hIL6 contains 30 different mutants in addition to wild-type IL-6 protein. This characteristic provides opportunities to develop machine learning models for predicting changes in antibody binding by antigen mutations. We report experimental benchmark results on AVIDa-hIL6 by using neural network-based baseline models. The results indicate that the existing models have potential, but further research is needed to generalize them to predict effective antibodies against unknown mutants. The dataset is available at https://avida-hil6.cognanous.com.", "link": "https://arxiv.org/abs/2306.03329"}, {"id": "2306.03346", "date": "Tue, 6 Jun 2023 01:36:56 GMT", "title": "Stabilizing Contrastive RL: Techniques for Offline Goal Reaching\n", "authors": ["Chongyi Zheng", "Benjamin Eysenbach", "Homer Walke", "Patrick Yin", "Kuan\n Fang", "Ruslan Salakhutdinov", "Sergey Levine\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.", "link": "https://arxiv.org/abs/2306.03346"}, {"id": "2306.03355", "date": "Tue, 6 Jun 2023 02:13:27 GMT", "title": "BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision,\n Language, and Graphs\n", "authors": ["Zhen Yang", "Tinglin Huang", "Ming Ding", "Yuxiao Dong", "Rex Ying", "Yukuo Cen,\n Yangliao Geng", "and Jie Tang\n"], "categories": ["cs.LG", "cs.CL", "cs.CV\nComments:", "17", "pages,", "16", "figures\nJournal-ref:", "KDD2023\n"], "abstract": "In-Batch contrastive learning is a state-of-the-art self-supervised method that brings semantically-similar instances close while pushing dissimilar instances apart within a mini-batch. Its key to success is the negative sharing strategy, in which every instance serves as a negative for the others within the mini-batch. Recent studies aim to improve performance by sampling hard negatives \\textit{within the current mini-batch}, whose quality is bounded by the mini-batch itself. In this work, we propose to improve contrastive learning by sampling mini-batches from the input data. We present BatchSampler\\footnote{The code is available at \\url{https://github.com/THUDM/BatchSampler}} to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances. To make each mini-batch have fewer false negatives, we design the proximity graph of randomly-selected instances. To form the mini-batch, we leverage random walk with restart on the proximity graph to help sample hard-to-distinguish instances. BatchSampler is a simple and general technique that can be directly plugged into existing contrastive learning models in vision, language, and graphs. Extensive experiments on datasets of three modalities show that BatchSampler can consistently improve the performance of powerful contrastive models, as shown by significant improvements of SimCLR on ImageNet-100, SimCSE on STS (language), and GraphCL and MVGRL on graph datasets.", "link": "https://arxiv.org/abs/2306.03355"}, {"id": "2306.03362", "date": "Tue, 6 Jun 2023 02:29:40 GMT", "title": "Boosting Offline Reinforcement Learning with Action Preference Query\n", "authors": ["Qisen Yang", "Shenzhi Wang", "Matthieu Gaetan Lin", "Shiji Song", "Gao Huang\n"], "categories": ["cs.LG", "cs.AI\nComments:", "International", "Conference", "on", "Machine", "Learning", "2023\n"], "abstract": "Training practical agents usually involve offline and online reinforcement learning (RL) to balance the policy's performance and interaction costs. In particular, online fine-tuning has become a commonly used method to correct the erroneous estimates of out-of-distribution data learned in the offline training phase. However, even limited online interactions can be inaccessible or catastrophic for high-stake scenarios like healthcare and autonomous driving. In this work, we introduce an interaction-free training scheme dubbed Offline-with-Action-Preferences (OAP). The main insight is that, compared to online fine-tuning, querying the preferences between pre-collected and learned actions can be equally or even more helpful to the erroneous estimate problem. By adaptively encouraging or suppressing policy constraint according to action preferences, OAP could distinguish overestimation from beneficial policy improvement and thus attains a more accurate evaluation of unseen data. Theoretically, we prove a lower bound of the behavior policy's performance improvement brought by OAP. Moreover, comprehensive experiments on the D4RL benchmark and state-of-the-art algorithms demonstrate that OAP yields higher (29% on average) scores, especially on challenging AntMaze tasks (98% higher).", "link": "https://arxiv.org/abs/2306.03362"}, {"id": "2306.03364", "date": "Tue, 6 Jun 2023 02:38:01 GMT", "title": "Learning Representations on the Unit Sphere: Application to Online\n Continual Learning\n", "authors": ["Nicolas Michel", "Giovanni Chierchia", "Romain Negrel", "Jean-Fran\\c{c}ois\n Bercher\n"], "categories": ["cs.LG", "cs.CV\nComments:", "16", "pages,", "4", "figures,", "under", "review\n"], "abstract": "We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We derive loss functions for the von Mises-Fisher distribution and the angular Gaussian distribution, both designed for modeling symmetric directional data. A noteworthy feature of our approach is that the learned representations are pushed toward fixed directions, allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch sizes while being computationally efficient. We demonstrate with extensive experiments that the proposed method outperforms the current state-of-the-art methods on both standard evaluation scenarios and realistic scenarios with blurry task boundaries. For reproducibility, we use the same training pipeline for every compared method and share the code at https://t.ly/SQTj.", "link": "https://arxiv.org/abs/2306.03364"}, {"id": "2306.03401", "date": "Tue, 6 Jun 2023 04:32:10 GMT", "title": "A Lightweight Method for Tackling Unknown Participation Probabilities in\n Federated Averaging\n", "authors": ["Shiqiang Wang", "Mingyue Ji\n"], "categories": ["cs.LG", "cs.DC", "cs.IT", "math.IT", "math.OC", "stat.ML\n"], "abstract": "In federated learning (FL), clients usually have diverse participation probabilities that are unknown a priori, which can significantly harm the performance of FL if not handled properly. Existing works aiming at addressing this problem are usually based on global variance reduction, which requires a substantial amount of additional memory in a multiplicative factor equal to the total number of clients. An important open problem is to find a lightweight method for FL in the presence of clients with unknown participation rates. In this paper, we address this problem by adapting the aggregation weights in federated averaging (FedAvg) based on the participation history of each client. We first show that, with heterogeneous participation probabilities, FedAvg with non-optimal aggregation weights can diverge from the optimal solution of the original FL objective, indicating the need of finding optimal aggregation weights. However, it is difficult to compute the optimal weights when the participation probabilities are unknown. To address this problem, we present a new algorithm called FedAU, which improves FedAvg by adaptively weighting the client updates based on online estimates of the optimal weights without knowing the probabilities of client participation. We provide a theoretical convergence analysis of FedAU using a novel methodology to connect the estimation error and convergence. Our theoretical results reveal important and interesting insights, while showing that FedAU converges to an optimal solution of the original objective and has desirable properties such as linear speedup. Our experimental results also verify the advantage of FedAU over baseline methods.", "link": "https://arxiv.org/abs/2306.03401"}, {"id": "2306.03434", "date": "Tue, 6 Jun 2023 06:22:42 GMT", "title": "Learning-Based Heuristic for Combinatorial Optimization of the Minimum\n Dominating Set Problem using Graph Convolutional Networks\n", "authors": ["Abihith Kothapalli", "Mudassir Shabbir", "Xenofon Koutsoukos\n"], "categories": ["cs.LG", "cs.DM\n"], "abstract": "A dominating set of a graph $\\mathcal{G=(V, E)}$ is a subset of vertices $S\\subseteq\\mathcal{V}$ such that every vertex $v\\in \\mathcal{V} \\setminus S$ outside the dominating set is adjacent to a vertex $u\\in S$ within the set. The minimum dominating set problem seeks to find a dominating set of minimum cardinality and is a well-established NP-hard combinatorial optimization problem. We propose a novel learning-based heuristic approach to compute solutions for the minimum dominating set problem using graph convolutional networks. We conduct an extensive experimental evaluation of the proposed method on a combination of randomly generated graphs and real-world graph datasets. Our results indicate that the proposed learning-based approach can outperform a classical greedy approximation algorithm. Furthermore, we demonstrate the generalization capability of the graph convolutional network across datasets and its ability to scale to graphs of higher order than those on which it was trained. Finally, we utilize the proposed learning-based heuristic in an iterative greedy algorithm, achieving state-of-the-art performance in the computation of dominating sets.", "link": "https://arxiv.org/abs/2306.03434"}, {"id": "2306.03435", "date": "Tue, 6 Jun 2023 06:23:38 GMT", "title": "On the Role of Attention in Prompt-tuning\n", "authors": ["Samet Oymak", "Ankit Singh Rawat", "Mahdi Soltanolkotabi", "Christos\n Thrampoulidis\n"], "categories": ["cs.LG", "cs.CL", "stat.ML\nComments:", "Published", "at", "ICML", "2023\n"], "abstract": "Prompt-tuning is an emerging strategy to adapt large language models (LLM) to downstream tasks by learning a (soft-)prompt parameter from data. Despite its success in LLMs, there is limited theoretical understanding of the power of prompt-tuning and the role of the attention mechanism in prompting. In this work, we explore prompt-tuning for one-layer attention architectures and study contextual mixture-models where each input token belongs to a context-relevant or -irrelevant set. We isolate the role of prompt-tuning through a self-contained prompt-attention model. Our contributions are as follows: (1) We show that softmax-prompt-attention is provably more expressive than softmax-self-attention and linear-prompt-attention under our contextual data model. (2) We analyze the initial trajectory of gradient descent and show that it learns the prompt and prediction head with near-optimal sample complexity and demonstrate how prompt can provably attend to sparse context-relevant tokens. (3) Assuming a known prompt but an unknown prediction head, we characterize the exact finite sample performance of prompt-attention which reveals the fundamental performance limits and the precise benefit of the context information. We also provide experiments that verify our theoretical insights on real datasets and demonstrate how prompt-tuning enables the model to attend to context-relevant information.", "link": "https://arxiv.org/abs/2306.03435"}, {"id": "2306.03447", "date": "Tue, 6 Jun 2023 07:00:24 GMT", "title": "GRAFENNE: Learning on Graphs with Heterogeneous and Dynamic Feature Sets\n", "authors": ["Shubham Gupta", "Sahil Manchanda", "Sayan Ranu", "Srikanta Bedathur\n"], "categories": ["cs.LG", "cs.AI\nComments:", "17", "pages,", "4", "figures", "and", "9", "tables.", "Accepted", "in", "ICML", "2023,", "DOI", "will", "be\n", "updated", "once", "it", "is", "available\n"], "abstract": "Graph neural networks (GNNs), in general, are built on the assumption of a static set of features characterizing each node in a graph. This assumption is often violated in practice. Existing methods partly address this issue through feature imputation. However, these techniques (i) assume uniformity of feature set across nodes, (ii) are transductive by nature, and (iii) fail to work when features are added or removed over time. In this work, we address these limitations through a novel GNN framework called GRAFENNE. GRAFENNE performs a novel allotropic transformation on the original graph, wherein the nodes and features are decoupled through a bipartite encoding. Through a carefully chosen message passing framework on the allotropic transformation, we make the model parameter size independent of the number of features and thereby inductive to both unseen nodes and features. We prove that GRAFENNE is at least as expressive as any of the existing message-passing GNNs in terms of Weisfeiler-Leman tests, and therefore, the additional inductivity to unseen features does not come at the cost of expressivity. In addition, as demonstrated over four real-world graphs, GRAFENNE empowers the underlying GNN with high empirical efficacy and the ability to learn in continual fashion over streaming feature sets.", "link": "https://arxiv.org/abs/2306.03447"}, {"id": "2306.03460", "date": "Tue, 6 Jun 2023 07:28:49 GMT", "title": "Natural Language Commanding via Program Synthesis\n", "authors": ["Apurva Gandhi", "Thong Q. Nguyen", "Huitian Jiao", "Robert Steen", "Ameya\n Bhatawdekar\n"], "categories": ["cs.LG", "cs.CL", "cs.HC\n"], "abstract": "We present Semantic Interpreter, a natural language-friendly AI system for productivity software such as Microsoft Office that leverages large language models (LLMs) to execute user intent across application features. While LLMs are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations. We therefore introduce the Office Domain Specific Language (ODSL), a concise, high-level language specialized for performing actions in and interacting with entities in Office applications. Semantic Interpreter leverages an Analysis-Retrieval prompt construction method with LLMs for program synthesis, translating natural language user utterances to ODSL programs that can be transpiled to application APIs and then executed. We focus our discussion primarily on a research exploration for Microsoft PowerPoint.", "link": "https://arxiv.org/abs/2306.03460"}, {"id": "2306.03480", "date": "Tue, 6 Jun 2023 08:03:18 GMT", "title": "GSHOT: Few-shot Generative Modeling of Labeled Graphs\n", "authors": ["Sahil Manchanda", "Shubham Gupta", "Sayan Ranu", "Srikanta Bedathur\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Deep graph generative modeling has gained enormous attraction in recent years due to its impressive ability to directly learn the underlying hidden graph distribution. Despite their initial success, these techniques, like much of the existing deep generative methods, require a large number of training samples to learn a good model. Unfortunately, large number of training samples may not always be available in scenarios such as drug discovery for rare diseases. At the same time, recent advances in few-shot learning have opened door to applications where available training data is limited. In this work, we introduce the hitherto unexplored paradigm of few-shot graph generative modeling. Towards this, we develop GSHOT, a meta-learning based framework for few-shot labeled graph generative modeling. GSHOT learns to transfer meta-knowledge from similar auxiliary graph datasets. Utilizing these prior experiences, GSHOT quickly adapts to an unseen graph dataset through self-paced fine-tuning. Through extensive experiments on datasets from diverse domains having limited training samples, we establish that GSHOT generates graphs of superior fidelity compared to existing baselines.", "link": "https://arxiv.org/abs/2306.03480"}, {"id": "2306.03506", "date": "Tue, 6 Jun 2023 08:52:44 GMT", "title": "Subgraph Networks Based Contrastive Learning\n", "authors": ["Jinhuan Wang", "Jiafei Shao", "Zeyu Wang", "Shanqing Yu", "Qi Xuan", "Xiaoniu\n Yang\n"], "categories": ["cs.LG", "cs.AI\nComments:", "12", "pages,", "6", "figures\n"], "abstract": "Graph contrastive learning (GCL), as a self-supervised learning method, can solve the problem of annotated data scarcity. It mines explicit features in unannotated graphs to generate favorable graph representations for downstream tasks. Most existing GCL methods focus on the design of graph augmentation strategies and mutual information estimation operations. Graph augmentation produces augmented views by graph perturbations. These views preserve a locally similar structure and exploit explicit features. However, these methods have not considered the interaction existing in subgraphs. To explore the impact of substructure interactions on graph representations, we propose a novel framework called subgraph network-based contrastive learning (SGNCL). SGNCL applies a subgraph network generation strategy to produce augmented views. This strategy converts the original graph into an Edge-to-Node mapping network with both topological and attribute features. The single-shot augmented view is a first-order subgraph network that mines the interaction between nodes, node-edge, and edges. In addition, we also investigate the impact of the second-order subgraph augmentation on mining graph structure interactions, and further, propose a contrastive objective that fuses the first-order and second-order subgraph information. We compare SGNCL with classical and state-of-the-art graph contrastive learning methods on multiple benchmark datasets of different domains. Extensive experiments show that SGNCL achieves competitive or better performance (top three) on all datasets in unsupervised learning settings. Furthermore, SGNCL achieves the best average gain of 6.9\\% in transfer learning compared to the best method. Finally, experiments also demonstrate that mining substructure interactions have positive implications for graph contrastive learning.", "link": "https://arxiv.org/abs/2306.03506"}, {"id": "2306.03521", "date": "Tue, 6 Jun 2023 09:12:49 GMT", "title": "Machine learning in and out of equilibrium\n", "authors": ["Shishir Adhikari", "Alkan Kabak\\c{c}{\\i}o\\u{g}lu", "Alexander Strang,\n Deniz Yuret", "Michael Hinczewski\n"], "categories": ["cs.LG", "cond-mat.stat-mech\nComments:", "24", "pages,", "6", "figures\n"], "abstract": "The algorithms used to train neural networks, like stochastic gradient descent (SGD), have close parallels to natural processes that navigate a high-dimensional parameter space -- for example protein folding or evolution. Our study uses a Fokker-Planck approach, adapted from statistical physics, to explore these parallels in a single, unified framework. We focus in particular on the stationary state of the system in the long-time limit, which in conventional SGD is out of equilibrium, exhibiting persistent currents in the space of network parameters. As in its physical analogues, the current is associated with an entropy production rate for any given training trajectory. The stationary distribution of these rates obeys the integral and detailed fluctuation theorems -- nonequilibrium generalizations of the second law of thermodynamics. We validate these relations in two numerical examples, a nonlinear regression network and MNIST digit classification. While the fluctuation theorems are universal, there are other aspects of the stationary state that are highly sensitive to the training details. Surprisingly, the effective loss landscape and diffusion matrix that determine the shape of the stationary distribution vary depending on the simple choice of minibatching done with or without replacement. We can take advantage of this nonequilibrium sensitivity to engineer an equilibrium stationary state for a particular application: sampling from a posterior distribution of network weights in Bayesian machine learning. We propose a new variation of stochastic gradient Langevin dynamics (SGLD) that harnesses without replacement minibatching. In an example system where the posterior is exactly known, this SGWORLD algorithm outperforms SGLD, converging to the posterior orders of magnitude faster as a function of the learning rate.", "link": "https://arxiv.org/abs/2306.03521"}, {"id": "2306.03522", "date": "Tue, 6 Jun 2023 09:14:05 GMT", "title": "A Functional Data Perspective and Baseline On Multi-Layer\n Out-of-Distribution Detection\n", "authors": ["Eduardo Dadalto", "Pierre Colombo", "Guillaume Staerman", "Nathan Noiry", "and\n Pablo Piantanida\n"], "categories": ["cs.LG", "cs.CV", "stat.ML\n"], "abstract": "A key feature of out-of-distribution (OOD) detection is to exploit a trained neural network by extracting statistical patterns and relationships through the multi-layer classifier to detect shifts in the expected input data distribution. Despite achieving solid results, several state-of-the-art methods rely on the penultimate or last layer outputs only, leaving behind valuable information for OOD detection. Methods that explore the multiple layers either require a special architecture or a supervised objective to do so. This work adopts an original approach based on a functional view of the network that exploits the sample's trajectories through the various layers and their statistical dependencies. It goes beyond multivariate features aggregation and introduces a baseline rooted in functional anomaly detection. In this new framework, OOD detection translates into detecting samples whose trajectories differ from the typical behavior characterized by the training set. We validate our method and empirically demonstrate its effectiveness in OOD detection compared to strong state-of-the-art baselines on computer vision benchmarks.", "link": "https://arxiv.org/abs/2306.03522"}, {"id": "2306.03534", "date": "Tue, 6 Jun 2023 09:34:11 GMT", "title": "Continual Learning in Linear Classification on Separable Data\n", "authors": ["Itay Evron", "Edward Moroshko", "Gon Buzaglo", "Maroun Khriesh", "Badea\n Marjieh", "Nathan Srebro", "Daniel Soudry\n"], "categories": ["cs.LG", "cs.NA", "math.NA\n"], "abstract": "We analyze continual learning on a sequence of separable linear classification tasks with binary labels. We show theoretically that learning with weak regularization reduces to solving a sequential max-margin problem, corresponding to a special case of the Projection Onto Convex Sets (POCS) framework. We then develop upper bounds on the forgetting and other quantities of interest under various settings with recurring tasks, including cyclic and random orderings of tasks. We discuss several practical implications to popular training practices like regularization scheduling and weighting. We point out several theoretical differences between our continual classification setting and a recently studied continual regression setting.", "link": "https://arxiv.org/abs/2306.03534"}, {"id": "2306.03536", "date": "Tue, 6 Jun 2023 09:35:29 GMT", "title": "On Pitfalls of Test-Time Adaptation\n", "authors": ["Hao Zhao", "Yuejiang Liu", "Alexandre Alahi", "Tao Lin\n"], "categories": ["cs.LG", "cs.AI\nComments:", "Accepted", "at", "ICML", "2023\n"], "abstract": "Test-Time Adaptation (TTA) has recently emerged as a promising approach for tackling the robustness challenge under distribution shifts. However, the lack of consistent settings and systematic studies in prior literature hinders thorough assessments of existing methods. To address this issue, we present TTAB, a test-time adaptation benchmark that encompasses ten state-of-the-art algorithms, a diverse array of distribution shifts, and two evaluation protocols. Through extensive experiments, our benchmark reveals three common pitfalls in prior efforts. First, selecting appropriate hyper-parameters, especially for model selection, is exceedingly difficult due to online batch dependency. Second, the effectiveness of TTA varies greatly depending on the quality and properties of the model being adapted. Third, even under optimal algorithmic conditions, none of the existing methods are capable of addressing all common types of distribution shifts. Our findings underscore the need for future research in the field to conduct rigorous evaluations on a broader set of models and shifts, and to re-examine the assumptions behind the empirical success of TTA. Our code is available at \\url{https://github.com/lins-lab/ttab}.", "link": "https://arxiv.org/abs/2306.03536"}, {"id": "2306.03548", "date": "Tue, 6 Jun 2023 09:50:38 GMT", "title": "Learning Dynamical Systems from Noisy Data with Inverse-Explicit\n Integrators\n", "authors": ["H\\r{a}kon Noren", "S{\\o}lve Eidnes and Elena Celledoni\n"], "categories": ["cs.LG", "cs.NA", "math.NA\nComments:", "23", "pages,", "10", "figures\n"], "abstract": "We introduce the mean inverse integrator (MII), a novel approach to increase the accuracy when training neural networks to approximate vector fields of dynamical systems from noisy data. This method can be used to average multiple trajectories obtained by numerical integrators such as Runge-Kutta methods. We show that the class of mono-implicit Runge-Kutta methods (MIRK) has particular advantages when used in connection with MII. When training vector field approximations, explicit expressions for the loss functions are obtained when inserting the training data in the MIRK formulae, unlocking symmetric and high-order integrators that would otherwise be implicit for initial value problems. The combined approach of applying MIRK within MII yields a significantly lower error compared to the plain use of the numerical integrator without averaging the trajectories. This is demonstrated with experiments using data from several (chaotic) Hamiltonian systems. Additionally, we perform a sensitivity analysis of the loss functions under normally distributed perturbations, supporting the favorable performance of MII.", "link": "https://arxiv.org/abs/2306.03548"}, {"id": "2306.03552", "date": "Tue, 6 Jun 2023 10:06:09 GMT", "title": "State Regularized Policy Optimization on Data with Dynamics Shift\n", "authors": ["Zhenghai Xue", "Qingpeng Cai", "Shuchang Liu", "Dong Zheng", "Peng Jiang", "Kun\n Gai", "Bo An\n"], "categories": ["cs.LG", "cs.AI\nComments:", "Preprint.", "Under", "Review\n"], "abstract": "In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \\textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\\textbf{S}tate \\textbf{R}egularized \\textbf{P}olicy \\textbf{O}ptimization) algorithm. To conduct theoretical analyses, the intuition of similar environment structures is characterized by the notion of homomorphous MDPs. We then demonstrate a lower-bound performance guarantee on policies regularized by the stationary state distribution. In practice, SRPO can be an add-on module to context-based algorithms in both online and offline RL settings. Experimental results show that SRPO can make several context-based algorithms far more data efficient and significantly improve their overall performance.", "link": "https://arxiv.org/abs/2306.03552"}, {"id": "2306.03561", "date": "Tue, 6 Jun 2023 10:25:10 GMT", "title": "CIN++: Enhancing Topological Message Passing\n", "authors": ["Lorenzo Giusti", "Teodora Reu", "Francesco Ceccarelli", "Cristian Bodnar,\n Pietro Li\\`o\n"], "categories": ["cs.LG", "cs.AI\nComments:", "21", "pages,", "9", "figures\n"], "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in learning from graph-structured data. However, they face significant limitations in expressive power, struggling with long-range interactions and lacking a principled approach to modeling higher-order structures and group interactions. Cellular Isomorphism Networks (CINs) recently addressed most of these challenges with a message passing scheme based on cell complexes. Despite their advantages, CINs make use only of boundary and upper messages which do not consider a direct interaction between the rings present in the underlying complex. Accounting for these interactions might be crucial for learning representations of many real-world complex phenomena such as the dynamics of supramolecular assemblies, neural activity within the brain, and gene regulation processes. In this work, we propose CIN++, an enhancement of the topological message passing scheme introduced in CINs. Our message passing scheme accounts for the aforementioned limitations by letting the cells to receive also lower messages within each layer. By providing a more comprehensive representation of higher-order and long-range interactions, our enhanced topological message passing scheme achieves state-of-the-art results on large-scale and long-range chemistry benchmarks.", "link": "https://arxiv.org/abs/2306.03561"}, {"id": "2306.03566", "date": "Tue, 6 Jun 2023 10:34:03 GMT", "title": "Memory-Based Dual Gaussian Processes for Sequential Learning\n", "authors": ["Paul E. Chang", "Prakhar Verma", "S.T. John", "Arno Solin", "Mohammad Emtiyaz\n Khan\n"], "categories": ["cs.LG", "stat.ML\nComments:", "International", "Conference", "on", "Machine", "Learning", "(ICML)", "2023\n"], "abstract": "Sequential learning with Gaussian processes (GPs) is challenging when access to past data is limited, for example, in continual and active learning. In such cases, errors can accumulate over time due to inaccuracies in the posterior, hyperparameters, and inducing points, making accurate learning challenging. Here, we present a method to keep all such errors in check using the recently proposed dual sparse variational GP. Our method enables accurate inference for generic likelihoods and improves learning by actively building and updating a memory of past data. We demonstrate its effectiveness in several applications involving Bayesian optimization, active learning, and continual learning.", "link": "https://arxiv.org/abs/2306.03566"}, {"id": "2306.03589", "date": "Tue, 6 Jun 2023 11:15:53 GMT", "title": "How does over-squashing affect the power of GNNs?\n", "authors": ["Francesco Di Giovanni", "T. Konstantin Rusch", "Michael M. Bronstein,\n Andreea Deac", "Marc Lackenby", "Siddhartha Mishra", "Petar Veli\\v{c}kovi\\'c\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be large enough, depending on properties of the input graph structure, such as commute times. For many relevant scenarios, our analysis results in impossibility statements in practice, showing that over-squashing hinders the expressive power of MPNNs. We validate our theoretical findings through extensive controlled experiments and ablation studies.", "link": "https://arxiv.org/abs/2306.03589"}, {"id": "2306.03626", "date": "Tue, 6 Jun 2023 12:27:54 GMT", "title": "Understanding Progressive Training Through the Framework of Randomized\n Coordinate Descent\n", "authors": ["Rafa{\\l} Szlendak", "Elnur Gasanov", "Peter Richt\\'arik\n"], "categories": ["cs.LG", "math.OC\n"], "abstract": "We propose a Randomized Progressive Training algorithm (RPT) -- a stochastic proxy for the well-known Progressive Training method (PT) (Karras et al., 2017). Originally designed to train GANs (Goodfellow et al., 2014), PT was proposed as a heuristic, with no convergence analysis even for the simplest objective functions. On the contrary, to the best of our knowledge, RPT is the first PT-type algorithm with rigorous and sound theoretical guarantees for general smooth objective functions. We cast our method into the established framework of Randomized Coordinate Descent (RCD) (Nesterov, 2012; Richt\\'arik & Tak\\'a\\v{c}, 2014), for which (as a by-product of our investigations) we also propose a novel, simple and general convergence analysis encapsulating strongly-convex, convex and nonconvex objectives. We then use this framework to establish a convergence theory for RPT. Finally, we validate the effectiveness of our method through extensive computational experiments.", "link": "https://arxiv.org/abs/2306.03626"}, {"id": "2306.03638", "date": "Sun, 4 Jun 2023 11:31:41 GMT", "title": "Provable convergence guarantees for black-box variational inference\n", "authors": ["Justin Domke", "Guillaume Garrigos and Robert Gower\n"], "categories": ["cs.LG", "math.OC", "stat.ML\nComments:", "32", "pages\n"], "abstract": "While black-box variational inference is widely used, there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs-namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides the first rigorous guarantee that black-box variational inference converges for realistic inference problems.", "link": "https://arxiv.org/abs/2306.03638"}, {"id": "2306.03646", "date": "Tue, 6 Jun 2023 13:00:47 GMT", "title": "Dance Generation by Sound Symbolic Words\n", "authors": ["Miki Okamura", "Naruya Kondo", "Tatsuki Fushimi Maki Sakamoto and Yoichi\n Ochiai\n"], "categories": ["cs.LG", "cs.HC", "cs.SD", "eess.AS\n"], "abstract": "This study introduces a novel approach to generate dance motions using onomatopoeia as input, with the aim of enhancing creativity and diversity in dance generation. Unlike text and music, onomatopoeia conveys rhythm and meaning through abstract word expressions without constraints on expression and without need for specialized knowledge. We adapt the AI Choreographer framework and employ the Sakamoto system, a feature extraction method for onomatopoeia focusing on phonemes and syllables. Additionally, we present a new dataset of 40 onomatopoeia-dance motion pairs collected through a user survey. Our results demonstrate that the proposed method enables more intuitive dance generation and can create dance motions using sound-symbolic words from a variety of languages, including those without onomatopoeia. This highlights the potential for diverse dance creation across different languages and cultures, accessible to a wider audience. Qualitative samples from our model can be found at: https://sites.google.com/view/onomatopoeia-dance/home/.", "link": "https://arxiv.org/abs/2306.03646"}, {"id": "2306.03648", "date": "Tue, 6 Jun 2023 13:04:05 GMT", "title": "Supervised Knowledge May Hurt Novel Class Discovery Performance\n", "authors": ["Ziyun Li", "Jona Otholt", "Ben Dai", "Di Hu", "Christoph Meinel", "Haojin Yang\n"], "categories": ["cs.LG", "cs.CV\nComments:", "TMLR", "2023", "accepted", "paper.", "arXiv", "admin", "note:", "substantial", "text", "overlap\n", "with", "arXiv:2209.09120\n"], "abstract": "Novel class discovery (NCD) aims to infer novel categories in an unlabeled dataset by leveraging prior knowledge of a labeled set comprising disjoint but related classes. Given that most existing literature focuses primarily on utilizing supervised knowledge from a labeled set at the methodology level, this paper considers the question: Is supervised knowledge always helpful at different levels of semantic relevance? To proceed, we first establish a novel metric, so-called transfer flow, to measure the semantic similarity between labeled/unlabeled datasets. To show the validity of the proposed metric, we build up a large-scale benchmark with various degrees of semantic similarities between labeled/unlabeled datasets on ImageNet by leveraging its hierarchical class structure. The results based on the proposed benchmark show that the proposed transfer flow is in line with the hierarchical class structure; and that NCD performance is consistent with the semantic similarities (measured by the proposed metric). Next, by using the proposed transfer flow, we conduct various empirical experiments with different levels of semantic similarity, yielding that supervised knowledge may hurt NCD performance. Specifically, using supervised information from a low-similarity labeled set may lead to a suboptimal result as compared to using pure self-supervised knowledge. These results reveal the inadequacy of the existing NCD literature which usually assumes that supervised knowledge is beneficial. Finally, we develop a pseudo-version of the transfer flow as a practical reference to decide if supervised knowledge should be used in NCD. Its effectiveness is supported by our empirical studies, which show that the pseudo transfer flow (with or without supervised knowledge) is consistent with the corresponding accuracy based on various datasets. Code is released at https://github.com/J-L-O/SK-Hurt-NCD", "link": "https://arxiv.org/abs/2306.03648"}, {"id": "2306.03655", "date": "Tue, 6 Jun 2023 13:15:01 GMT", "title": "Online Learning under Adversarial Nonlinear Constraints\n", "authors": ["Pavel Kolev", "Georg Martius", "Michael Muehlebach\n"], "categories": ["cs.LG", "math.OC\n"], "abstract": "In many applications, learning systems are required to process continuous non-stationary data streams. We study this problem in an online learning framework and propose an algorithm that can deal with adversarial time-varying and nonlinear constraints. As we show in our work, the algorithm called Constraint Violation Velocity Projection (CVV-Pro) achieves $\\sqrt{T}$ regret and converges to the feasible set at a rate of $1/\\sqrt{T}$, despite the fact that the feasible set is slowly time-varying and a priori unknown to the learner. CVV-Pro only relies on local sparse linear approximations of the feasible set and therefore avoids optimizing over the entire set at each iteration, which is in sharp contrast to projected gradients or Frank-Wolfe methods. We also empirically evaluate our algorithm on two-player games, where the players are subjected to a shared constraint.", "link": "https://arxiv.org/abs/2306.03655"}, {"id": "2306.03698", "date": "Tue, 6 Jun 2023 14:12:23 GMT", "title": "Fine-grained Expressivity of Graph Neural Networks\n", "authors": ["Jan B\\\"oker", "Ron Levie", "Ningyuan Huang", "Soledad Villar", "Christopher\n Morris\n"], "categories": ["cs.LG", "cs.DM", "cs.NE\n"], "abstract": "Numerous recent works have analyzed the expressive power of message-passing graph neural networks (MPNNs), primarily utilizing combinatorial techniques such as the $1$-dimensional Weisfeiler-Leman test ($1$-WL) for the graph isomorphism problem. However, the graph isomorphism objective is inherently binary, not giving insights into the degree of similarity between two given graphs. This work resolves this issue by considering continuous extensions of both $1$-WL and MPNNs to graphons. Concretely, we show that the continuous variant of $1$-WL delivers an accurate topological characterization of the expressive power of MPNNs on graphons, revealing which graphs these networks can distinguish and the level of difficulty in separating them. We identify the finest topology where MPNNs separate points and prove a universal approximation theorem. Consequently, we provide a theoretical framework for graph and graphon similarity combining various topological variants of classical characterizations of the $1$-WL. In particular, we characterize the expressive power of MPNNs in terms of the tree distance, which is a graph distance based on the concepts of fractional isomorphisms, and substructure counts via tree homomorphisms, showing that these concepts have the same expressive power as the $1$-WL and MPNNs on graphons. Empirically, we validate our theoretical findings by showing that randomly initialized MPNNs, without training, exhibit competitive performance compared to their trained counterparts. Moreover, we evaluate different MPNN architectures based on their ability to preserve graph distances, highlighting the significance of our continuous $1$-WL test in understanding MPNNs' expressivity.", "link": "https://arxiv.org/abs/2306.03698"}, {"id": "2306.03702", "date": "Tue, 6 Jun 2023 14:15:29 GMT", "title": "Bayesian post-hoc regularization of random forests\n", "authors": ["Bastian Pfeifer\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Random Forests are powerful ensemble learning algorithms widely used in various machine learning tasks. However, they have a tendency to overfit noisy or irrelevant features, which can result in decreased generalization performance. Post-hoc regularization techniques aim to mitigate this issue by modifying the structure of the learned ensemble after its training. Here, we propose Bayesian post-hoc regularization to leverage the reliable patterns captured by leaf nodes closer to the root, while potentially reducing the impact of more specific and potentially noisy leaf nodes deeper in the tree. This approach allows for a form of pruning that does not alter the general structure of the trees but rather adjusts the influence of leaf nodes based on their proximity to the root node. We have evaluated the performance of our method on various machine learning data sets. Our approach demonstrates competitive performance with the state-of-the-art methods and, in certain cases, surpasses them in terms of predictive accuracy and generalization.", "link": "https://arxiv.org/abs/2306.03702"}, {"id": "2306.03726", "date": "Tue, 6 Jun 2023 14:45:24 GMT", "title": "Exploring Model Dynamics for Accumulative Poisoning Discovery\n", "authors": ["Jianing Zhu", "Xiawei Guo", "Jiangchao Yao", "Chao Du", "Li He", "Shuo Yuan,\n Tongliang Liu", "Liang Wang", "Bo Han\n"], "categories": ["cs.LG", "cs.CR\nComments:", "accepted", "by", "ICML", "2023\n"], "abstract": "Adversarial poisoning attacks pose huge threats to various machine learning applications. Especially, the recent accumulative poisoning attacks show that it is possible to achieve irreparable harm on models via a sequence of imperceptible attacks followed by a trigger batch. Due to the limited data-level discrepancy in real-time data streaming, current defensive methods are indiscriminate in handling the poison and clean samples. In this paper, we dive into the perspective of model dynamics and propose a novel information measure, namely, Memorization Discrepancy, to explore the defense via the model-level information. By implicitly transferring the changes in the data manipulation to that in the model outputs, Memorization Discrepancy can discover the imperceptible poison samples based on their distinct dynamics from the clean samples. We thoroughly explore its properties and propose Discrepancy-aware Sample Correction (DSC) to defend against accumulative poisoning attacks. Extensive experiments comprehensively characterized Memorization Discrepancy and verified its effectiveness. The code is publicly available at: https://github.com/tmlr-group/Memorization-Discrepancy.", "link": "https://arxiv.org/abs/2306.03726"}, {"id": "2306.03770", "date": "Tue, 6 Jun 2023 15:31:05 GMT", "title": "Graph Classification Gaussian Processes via Spectral Features\n", "authors": ["Felix L. Opolka", "Yin-Cong Zhi", "Pietro Li\\`o", "Xiaowen Dong\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "Graph classification aims to categorise graphs based on their structure and node attributes. In this work, we propose to tackle this task using tools from graph signal processing by deriving spectral features, which we then use to design two variants of Gaussian process models for graph classification. The first variant uses spectral features based on the distribution of energy of a node feature signal over the spectrum of the graph. We show that even such a simple approach, having no learned parameters, can yield competitive performance compared to strong neural network and graph kernel baselines. A second, more sophisticated variant is designed to capture multi-scale and localised patterns in the graph by learning spectral graph wavelet filters, obtaining improved performance on synthetic and real-world data sets. Finally, we show that both models produce well calibrated uncertainty estimates, enabling reliable decision making based on the model predictions.", "link": "https://arxiv.org/abs/2306.03770"}, {"id": "2306.03801", "date": "Tue, 6 Jun 2023 15:45:07 GMT", "title": "Stable Vectorization of Multiparameter Persistent Homology using Signed\n Barcodes as Measures\n", "authors": ["David Loiseaux", "Luis Scoccola", "Mathieu Carri\\`ere", "Magnus Bakke\n Botnan", "Steve Oudot\n"], "categories": ["cs.LG", "cs.CG", "math.AT", "stat.ML\nComments:", "23", "pages,", "3", "figures,", "8", "tables\n"], "abstract": "Persistent homology (PH) provides topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. Most applications of PH focus on the one-parameter case -- where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest -- and there is now a wide array of methods enabling the use of one-parameter PH descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a Hilbert space. Although the multiparameter PH (MPH) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for MPH descriptors has so far limited the available options for the stable vectorization of MPH. In this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes -- a recent family of MPH descriptors -- as signed measures leads to natural extensions of vectorization strategies from one parameter to multiple parameters. The resulting feature vectors are easy to define and to compute, and provably stable. While, as a proof of concept, we focus on simple choices of signed barcodes and vectorizations, we already see notable performance improvements when comparing our feature vectors to state-of-the-art topology-based methods on various types of data.", "link": "https://arxiv.org/abs/2306.03801"}, {"id": "2306.03819", "date": "Tue, 6 Jun 2023 16:07:24 GMT", "title": "LEACE: Perfect linear concept erasure in closed form\n", "authors": ["Nora Belrose", "David Schneider-Joseph", "Shauli Ravfogel", "Ryan Cotterell,\n Edward Raff", "Stella Biderman\n"], "categories": ["cs.LG", "cs.CL", "cs.CY\n"], "abstract": "Concept erasure aims to remove specified features from a representation. It can be used to improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). In this paper, we introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while inflicting the least possible damage to the representation. We apply LEACE to large language models with a novel procedure called \"concept scrubbing,\" which erases target concept information from every layer in the network. We demonstrate the usefulness of our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.", "link": "https://arxiv.org/abs/2306.03819"}, {"id": "2306.03831", "date": "Tue, 6 Jun 2023 16:16:05 GMT", "title": "GEO-Bench: Toward Foundation Models for Earth Monitoring\n", "authors": ["Alexandre Lacoste", "Nils Lehmann", "Pau Rodriguez", "Evan David Sherwin,\n Hannah Kerner", "Bj\\\"orn L\\\"utjens", "Jeremy Andrew Irvin", "David Dao", "Hamed\n Alemohammad", "Alexandre Drouin", "Mehmet Gunturkun", "Gabriel Huang", "David\n Vazquez", "Dava Newman", "Yoshua Bengio", "Stefano Ermon", "Xiao Xiang Zhu\n"], "categories": ["cs.LG", "cs.CV\n"], "abstract": "Recent progress in self-supervision has shown that pre-training large neural networks on vast amounts of unsupervised data can lead to substantial increases in generalization to downstream tasks. Such models, recently coined foundation models, have been transformational to the field of natural language processing. Variants have also been proposed for image data, but their applicability to remote sensing tasks is limited. To stimulate the development of foundation models for Earth monitoring, we propose a benchmark comprised of six classification and six segmentation tasks, which were carefully curated and adapted to be both relevant to the field and well-suited for model evaluation. We accompany this benchmark with a robust methodology for evaluating models and reporting aggregated results to enable a reliable assessment of progress. Finally, we report results for 20 baselines to gain information about the performance of existing models. We believe that this benchmark will be a driver of progress across a variety of Earth monitoring tasks.", "link": "https://arxiv.org/abs/2306.03831"}, {"id": "2306.03834", "date": "Tue, 6 Jun 2023 16:24:27 GMT", "title": "MTS2Graph: Interpretable Multivariate Time Series Classification with\n Temporal Evolving Graphs\n", "authors": ["Raneen Younis", "Abdul Hakmeh", "and Zahra Ahmadi\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Conventional time series classification approaches based on bags of patterns or shapelets face significant challenges in dealing with a vast amount of feature candidates from high-dimensional multivariate data. In contrast, deep neural networks can learn low-dimensional features efficiently, and in particular, Convolutional Neural Networks (CNN) have shown promising results in classifying Multivariate Time Series (MTS) data. A key factor in the success of deep neural networks is this astonishing expressive power. However, this power comes at the cost of complex, black-boxed models, conflicting with the goals of building reliable and human-understandable models. An essential criterion in understanding such predictive deep models involves quantifying the contribution of time-varying input variables to the classification. Hence, in this work, we introduce a new framework for interpreting multivariate time series data by extracting and clustering the input representative patterns that highly activate CNN neurons. This way, we identify each signal's role and dependencies, considering all possible combinations of signals in the MTS input. Then, we construct a graph that captures the temporal relationship between the extracted patterns for each layer. An effective graph merging strategy finds the connection of each node to the previous layer's nodes. Finally, a graph embedding algorithm generates new representations of the created interpretable time-series features. To evaluate the performance of our proposed framework, we run extensive experiments on eight datasets of the UCR/UEA archive, along with HAR and PAM datasets. The experiments indicate the benefit of our time-aware graph-based representation in MTS classification while enriching them with more interpretability.", "link": "https://arxiv.org/abs/2306.03834"}, {"id": "2306.03838", "date": "Tue, 6 Jun 2023 16:27:17 GMT", "title": "Spherical Fourier Neural Operators: Learning Stable Dynamics on the\n Sphere\n", "authors": ["Boris Bonev", "Thorsten Kurth", "Christian Hundt", "Jaideep Pathak,\n Maximilian Baust", "Karthik Kashinath", "Anima Anandkumar\n"], "categories": ["cs.LG", "cs.NA", "math.NA", "physics.ao-ph", "physics.comp-ph\n"], "abstract": "Fourier Neural Operators (FNOs) have proven to be an efficient and effective method for resolution-independent operator learning in a broad variety of application areas across scientific machine learning. A key reason for their success is their ability to accurately model long-range dependencies in spatio-temporal data by learning global convolutions in a computationally efficient manner. To this end, FNOs rely on the discrete Fourier transform (DFT), however, DFTs cause visual and spectral artifacts as well as pronounced dissipation when learning operators in spherical coordinates since they incorrectly assume a flat geometry. To overcome this limitation, we generalize FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators on spherical geometries. We apply SFNOs to forecasting atmospheric dynamics, and demonstrate stable auto\\-regressive rollouts for a year of simulated time (1,460 steps), while retaining physically plausible dynamics. The SFNO has important implications for machine learning-based simulation of climate dynamics that could eventually help accelerate our response to climate change.", "link": "https://arxiv.org/abs/2306.03838"}, {"id": "2306.03885", "date": "Fri, 19 May 2023 06:38:29 GMT", "title": "Three-way Imbalanced Learning based on Fuzzy Twin SVM\n", "authors": ["Wanting Cai", "Mingjie Cai", "Qingguo Li", "Qiong Liu\n"], "categories": ["cs.LG", "cs.IT", "math.IT\n"], "abstract": "Three-way decision (3WD) is a powerful tool for granular computing to deal with uncertain data, commonly used in information systems, decision-making, and medical care. Three-way decision gets much research in traditional rough set models. However, three-way decision is rarely combined with the currently popular field of machine learning to expand its research. In this paper, three-way decision is connected with SVM, a standard binary classification model in machine learning, for solving imbalanced classification problems that SVM needs to improve. A new three-way fuzzy membership function and a new fuzzy twin support vector machine with three-way membership (TWFTSVM) are proposed. The new three-way fuzzy membership function is defined to increase the certainty of uncertain data in both input space and feature space, which assigns higher fuzzy membership to minority samples compared with majority samples. To evaluate the effectiveness of the proposed model, comparative experiments are designed for forty-seven different datasets with varying imbalance ratios. In addition, datasets with different imbalance ratios are derived from the same dataset to further assess the proposed model's performance. The results show that the proposed model significantly outperforms other traditional SVM-based methods.", "link": "https://arxiv.org/abs/2306.03885"}, {"id": "2306.03897", "date": "Sun, 4 Jun 2023 15:03:39 GMT", "title": "DANSE: Data-driven Non-linear State Estimation of Model-free Process in\n Unsupervised Learning Setup\n", "authors": ["Anubhab Ghosh", "Antoine Honor\\'e and Saikat Chatterjee\n"], "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.SP\nComments:", "12", "pages,", "The", "paper", "is", "under", "review\n"], "abstract": "We address the tasks of Bayesian state estimation and forecasting for a model-free process in an unsupervised learning setup. In the article, we propose DANSE -- a Data-driven Nonlinear State Estimation method. DANSE provides a closed-form posterior of the state of the model-free process, given linear measurements of the state. In addition, it provides a closed-form posterior for forecasting. A data-driven recurrent neural network (RNN) is used in DANSE to provide the parameters of a prior of the state. The prior depends on the past measurements as input, and then we find the closed-form posterior of the state using the current measurement as input. The data-driven RNN captures the underlying non-linear dynamics of the model-free process. The training of DANSE, mainly learning the parameters of the RNN, is executed using an unsupervised learning approach. In unsupervised learning, we have access to a training dataset comprising only a set of measurement data trajectories, but we do not have any access to the state trajectories. Therefore, DANSE does not have access to state information in the training data and can not use supervised learning. Using simulated linear and non-linear process models (Lorenz attractor and Chen attractor), we evaluate the unsupervised learning-based DANSE. We show that the proposed DANSE, without knowledge of the process model and without supervised learning, provides a competitive performance against model-driven methods, such as the Kalman filter (KF), extended KF (EKF), unscented KF (UKF), and a recently proposed hybrid method called KalmanNet.", "link": "https://arxiv.org/abs/2306.03897"}, {"id": "2306.03105", "date": "Sat, 3 Jun 2023 06:06:27 GMT", "title": "Data driven localized wave solution of the Fokas-Lenells equation using\n modified PINN\n", "authors": ["Gautam Kumar Saharia", "Sagardeep Talukdar", "Riki Dutta and Sudipta Nandy\n"], "categories": ["nlin.PS", "cs.LG", "nlin.SI\nComments:", "14", "pages\n"], "abstract": "We investigate data driven localized wave solutions of the Fokas-Lenells equation by using physics informed neural network(PINN). We improve basic PINN by incorporating control parameters into the residual loss function. We also add conserve quantity as another loss term to modify the PINN. Using modified PINN we obtain the data driven bright soliton and dark soliton solutions of Fokas-Lenells equation. Conserved quantities informed loss function achieve more accuracy in terms of relative L2 error between predicted and exact soliton solutions. We hope that the present investigation would be useful to study the applications of deep learning in nonlinear optics and other branches of nonlinear physics. Source codes are available at https://github.com/gautamksaharia/Fokas-Lenells", "link": "https://arxiv.org/abs/2306.03105"}, {"id": "2306.03109", "date": "Mon, 5 Jun 2023 04:34:54 GMT", "title": "Machine Learning Force Fields with Data Cost Aware Training\n", "authors": ["Alexander Bukharin", "Tianyi Liu", "Shengjie Wang", "Simiao Zuo", "Weihao Gao,\n Wen Yan", "Tuo Zhao\n"], "categories": ["q-bio.QM", "cs.LG", "physics.chem-ph\n"], "abstract": "Machine learning force fields (MLFF) have been proposed to accelerate molecular dynamics (MD) simulation, which finds widespread applications in chemistry and biomedical research. Even for the most data-efficient MLFFs, reaching chemical accuracy can require hundreds of frames of force and energy labels generated by expensive quantum mechanical algorithms, which may scale as $O(n^3)$ to $O(n^7)$, with $n$ proportional to the number of basis functions. To address this issue, we propose a multi-stage computational framework -- ASTEROID, which lowers the data cost of MLFFs by leveraging a combination of cheap inaccurate data and expensive accurate data. The motivation behind ASTEROID is that inaccurate data, though incurring large bias, can help capture the sophisticated structures of the underlying force field. Therefore, we first train a MLFF model on a large amount of inaccurate training data, employing a bias-aware loss function to prevent the model from overfitting tahe potential bias of this data. We then fine-tune the obtained model using a small amount of accurate training data, which preserves the knowledge learned from the inaccurate training data while significantly improving the model's accuracy. Moreover, we propose a variant of ASTEROID based on score matching for the setting where the inaccurate training data are unlabeled. Extensive experiments on MD datasets and downstream tasks validate the efficacy of ASTEROID. Our code and data are available at https://github.com/abukharin3/asteroid.", "link": "https://arxiv.org/abs/2306.03109"}, {"id": "2306.03111", "date": "Mon, 5 Jun 2023 08:23:46 GMT", "title": "Bootstrapped Training of Score-Conditioned Generator for Offline Design\n of Biological Sequences\n", "authors": ["Minsu Kim", "Federico Berto", "Sungsoo Ahn", "Jinkyoo Park\n"], "categories": ["q-bio.QM", "cs.LG", "stat.ML\nComments:", "18", "pages,", "5", "figures\n"], "abstract": "We study the problem of optimizing biological sequences, e.g., proteins, DNA, and RNA, to maximize a black-box score function that is only evaluated in an offline dataset. We propose a novel solution, bootstrapped training of score-conditioned generator (BootGen) algorithm. Our algorithm repeats a two-stage process. In the first stage, our algorithm trains the biological sequence generator with rank-based weights to enhance the accuracy of sequence generation based on high scores. The subsequent stage involves bootstrapping, which augments the training dataset with self-generated data labeled by a proxy score function. Our key idea is to align the score-based generation with a proxy score function, which distills the knowledge of the proxy score function to the generator. After training, we aggregate samples from multiple bootstrapped generators and proxies to produce a diverse design. Extensive experiments show that our method outperforms competitive baselines on biological sequential design tasks. We provide reproducible source code: \\href{https://github.com/kaist-silab/bootgen}{https://github.com/kaist-silab/bootgen}.", "link": "https://arxiv.org/abs/2306.03111"}, {"id": "2306.03117", "date": "Mon, 5 Jun 2023 15:19:06 GMT", "title": "Score-based Enhanced Sampling for Protein Molecular Dynamics\n", "authors": ["Jiarui Lu", "Bozitao Zhong", "Jian Tang\n"], "categories": ["q-bio.QM", "cs.LG", "q-bio.BM\nComments:", "Under", "review\n"], "abstract": "The dynamic nature of proteins is crucial for determining their biological functions and properties, and molecular dynamics (MD) simulations stand as a predominant tool to study such phenomena. By utilizing empirically derived force fields, MD simulations explore the conformational space through numerically evolving the system along MD trajectories. However, the high-energy barrier of the force fields can hamper the exploration of MD, resulting in inadequately sampled ensemble. In this paper, we propose leveraging score-based generative models (SGMs) trained on general protein structures to perform protein conformational sampling to complement traditional MD simulations. We argue that SGMs can provide a novel framework as an alternative to traditional enhanced sampling methods by learning multi-level score functions, which directly sample a diversity-controllable ensemble of conformations. We demonstrate the effectiveness of our approach on several benchmark systems by comparing the results with long MD trajectories and state-of-the-art generative structure prediction models. Our framework provides new insights that SGMs have the potential to serve as an efficient and simulation-free methods to study protein dynamics.", "link": "https://arxiv.org/abs/2306.03117"}, {"id": "2306.03143", "date": "Mon, 5 Jun 2023 18:00:07 GMT", "title": "Machine learning feature discovery of spinon Fermi surface\n", "authors": ["Kevin Zhang", "Shi Feng", "Yuri D. Lensky", "Nandini Trivedi", "Eun-Ah Kim\n"], "categories": ["cond-mat.str-el", "cond-mat.dis-nn", "cs.LG", "quant-ph\nComments:", "8", "pages", "+", "8", "pages", "supplemental\n"], "abstract": "With rapid progress in simulation of strongly interacting quantum Hamiltonians, the challenge in characterizing unknown phases becomes a bottleneck for scientific progress. We demonstrate that a Quantum-Classical hybrid approach (QuCl) of mining the projective snapshots with interpretable classical machine learning, can unveil new signatures of seemingly featureless quantum states. The Kitaev-Heisenberg model on a honeycomb lattice with bond-dependent frustrated interactions presents an ideal system to test QuCl. The model hosts a wealth of quantum spin liquid states: gapped and gapless $\\mathbb{Z}_2$ spin liquids, and a chiral spin liquid (CSL) phase in a small external magnetic field. Recently, various simulations have found a new intermediate gapless phase (IGP), sandwiched between the CSL and a partially polarized phase, launching a debate over its elusive nature. We reveal signatures of phases in the model by contrasting two phases pairwise using an interpretable neural network, the correlator convolutional neural network (CCNN). We train the CCNN with a labeled collection of sampled projective measurements and reveal signatures of each phase through regularization path analysis. We show that QuCl reproduces known features of established spin liquid phases and ordered phases. Most significantly, we identify a signature motif of the field-induced IGP in the spin channel perpendicular to the field direction, which we interpret as a signature of Friedel oscillations of gapless spinons forming a Fermi surface. Our predictions can guide future experimental searches for $U(1)$ spin liquids.", "link": "https://arxiv.org/abs/2306.03143"}, {"id": "2306.03202", "date": "Mon, 5 Jun 2023 19:22:02 GMT", "title": "Nonlinear Distributionally Robust Optimization\n", "authors": ["Mohammed Rayyan Sheriff and Peyman Mohajerin Esfahani\n"], "categories": ["stat.ML", "cs.LG", "math.OC\n"], "abstract": "This article focuses on a class of distributionally robust optimization (DRO) problems where, unlike the growing body of the literature, the objective function is potentially non-linear in the distribution. Existing methods to optimize nonlinear functions in probability space use the Frechet derivatives, which present both theoretical and computational challenges. Motivated by this, we propose an alternative notion for the derivative and corresponding smoothness based on Gateaux (G)-derivative for generic risk measures. These concepts are explained via three running risk measure examples of variance, entropic risk, and risk on finite support sets. We then propose a G-derivative based Frank-Wolfe~(FW) algorithm for generic non-linear optimization problems in probability spaces and establish its convergence under the proposed notion of smoothness in a completely norm-independent manner. We use the set-up of the FW algorithm to devise a methodology to compute a saddle point of the non-linear DRO problem. Finally, for the minimum variance portfolio selection problem we analyze the regularity conditions and compute the FW-oracle in various settings, and validate the theoretical results numerically.", "link": "https://arxiv.org/abs/2306.03202"}, {"id": "2306.03257", "date": "Mon, 5 Jun 2023 21:19:37 GMT", "title": "Generating Private Synthetic Data with Genetic Algorithms\n", "authors": ["Terrance Liu", "Jingwu Tang", "Giuseppe Vietri", "Zhiwei Steven Wu\n"], "categories": ["cs.OH", "cs.CR", "cs.LG", "cs.NE\n"], "abstract": "We study the problem of efficiently generating differentially private synthetic data that approximate the statistical properties of an underlying sensitive dataset. In recent years, there has been a growing line of work that approaches this problem using first-order optimization techniques. However, such techniques are restricted to optimizing differentiable objectives only, severely limiting the types of analyses that can be conducted. For example, first-order mechanisms have been primarily successful in approximating statistical queries only in the form of marginals for discrete data domains. In some cases, one can circumvent such issues by relaxing the task's objective to maintain differentiability. However, even when possible, these approaches impose a fundamental limitation in which modifications to the minimization problem become additional sources of error. Therefore, we propose Private-GSD, a private genetic algorithm based on zeroth-order optimization heuristics that do not require modifying the original objective. As a result, it avoids the aforementioned limitations of first-order optimization. We empirically evaluate Private-GSD against baseline algorithms on data derived from the American Community Survey across a variety of statistics--otherwise known as statistical queries--both for discrete and real-valued attributes. We show that Private-GSD outperforms the state-of-the-art methods on non-differential queries while matching accuracy in approximating differentiable ones.", "link": "https://arxiv.org/abs/2306.03257"}, {"id": "2306.03303", "date": "Mon, 5 Jun 2023 23:06:32 GMT", "title": "Global universal approximation of functional input maps on weighted\n spaces\n", "authors": ["Christa Cuchiero", "Philipp Schmocker", "Josef Teichmann\n"], "categories": ["stat.ML", "cs.LG", "math.FA", "math.PR", "q-fin.MF\nComments:", "57", "pages,", "4", "figures\nMSC-class:", "26A16,", "26E20,", "41A65,", "41A81,", "46E40,", "60L10,", "68T07\n"], "abstract": "We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gaussian processes. This paves the way towards uncertainty quantification for signature kernel regression.", "link": "https://arxiv.org/abs/2306.03303"}, {"id": "2306.03335", "date": "Tue, 6 Jun 2023 01:13:18 GMT", "title": "Unraveling Projection Heads in Contrastive Learning: Insights from\n Expansion and Shrinkage\n", "authors": ["Yu Gui", "Cong Ma", "Yiqiao Zhong\n"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH\n"], "abstract": "We investigate the role of projection heads, also known as projectors, within the encoder-projector framework (e.g., SimCLR) used in contrastive learning. We aim to demystify the observed phenomenon where representations learned before projectors outperform those learned after -- measured using the downstream linear classification accuracy, even when the projectors themselves are linear. In this paper, we make two significant contributions towards this aim. Firstly, through empirical and theoretical analysis, we identify two crucial effects -- expansion and shrinkage -- induced by the contrastive loss on the projectors. In essence, contrastive loss either expands or shrinks the signal direction in the representations learned by an encoder, depending on factors such as the augmentation strength, the temperature used in contrastive loss, etc. Secondly, drawing inspiration from the expansion and shrinkage phenomenon, we propose a family of linear transformations to accurately model the projector's behavior. This enables us to precisely characterize the downstream linear classification accuracy in the high-dimensional asymptotic limit. Our findings reveal that linear projectors operating in the shrinkage (or expansion) regime hinder (or improve) the downstream classification accuracy. This provides the first theoretical explanation as to why (linear) projectors impact the downstream performance of learned representations. Our theoretical findings are further corroborated by extensive experiments on both synthetic data and real image data.", "link": "https://arxiv.org/abs/2306.03335"}, {"id": "2306.03398", "date": "Tue, 6 Jun 2023 04:28:12 GMT", "title": "Minimum intrinsic dimension scaling for entropic optimal transport\n", "authors": ["Austin J. Stromme\n"], "categories": ["math.ST", "cs.LG", "math.PR", "stat.TH\nComments:", "53", "pages\n"], "abstract": "Motivated by the manifold hypothesis, which states that data with a high extrinsic dimension may yet have a low intrinsic dimension, we develop refined statistical bounds for entropic optimal transport that are sensitive to the intrinsic dimension of the data. Our bounds involve a robust notion of intrinsic dimension, measured at only a single distance scale depending on the regularization parameter, and show that it is only the minimum of these single-scale intrinsic dimensions which governs the rate of convergence. We call this the Minimum Intrinsic Dimension scaling (MID scaling) phenomenon, and establish MID scaling with no assumptions on the data distributions so long as the cost is bounded and Lipschitz, and for various entropic optimal transport quantities beyond just values, with stronger analogs when one distribution is supported on a manifold. Our results significantly advance the theoretical state of the art by showing that MID scaling is a generic phenomenon, and provide the first rigorous interpretation of the statistical effect of entropic regularization as a distance scale.", "link": "https://arxiv.org/abs/2306.03398"}, {"id": "2306.03466", "date": "Tue, 6 Jun 2023 07:36:47 GMT", "title": "Convergent Bregman Plug-and-Play Image Restoration for Poisson Inverse\n Problems\n", "authors": ["Samuel Hurault", "Ulugbek Kamilov", "Arthur Leclaire", "Nicolas Papadakis\n"], "categories": ["eess.IV", "cs.LG", "math.OC\n"], "abstract": "Plug-and-Play (PnP) methods are efficient iterative algorithms for solving ill-posed image inverse problems. PnP methods are obtained by using deep Gaussian denoisers instead of the proximal operator or the gradient-descent step within proximal algorithms. Current PnP schemes rely on data-fidelity terms that have either Lipschitz gradients or closed-form proximal operators, which is not applicable to Poisson inverse problems. Based on the observation that the Gaussian noise is not the adequate noise model in this setting, we propose to generalize PnP using theBregman Proximal Gradient (BPG) method. BPG replaces the Euclidean distance with a Bregman divergence that can better capture the smoothness properties of the problem. We introduce the Bregman Score Denoiser specifically parametrized and trained for the new Bregman geometry and prove that it corresponds to the proximal operator of a nonconvex potential. We propose two PnP algorithms based on the Bregman Score Denoiser for solving Poisson inverse problems. Extending the convergence results of BPG in the nonconvex settings, we show that the proposed methods converge, targeting stationary points of an explicit global functional. Experimental evaluations conducted on various Poisson inverse problems validate the convergence results and showcase effective restoration performance.", "link": "https://arxiv.org/abs/2306.03466"}, {"id": "2306.03625", "date": "Tue, 6 Jun 2023 12:22:20 GMT", "title": "Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy\n Learning\n", "authors": ["Kwangho Kim and Jos\\'e R. Zubizarreta\n"], "categories": ["stat.ME", "cs.LG", "stat.ML\nJournal-ref:", "Proceedings", "of", "the", "40", "th", "International", "Conference", "on", "Machine\n", "Learning,", "Honolulu,", "Hawaii,", "USA.", "PMLR", "202,", "2023\n"], "abstract": "We propose a simple and general framework for nonparametric estimation of heterogeneous treatment effects under fairness constraints. Under standard regularity conditions, we show that the resulting estimators possess the double robustness property. We use this framework to characterize the trade-off between fairness and the maximum welfare achievable by the optimal policy. We evaluate the methods in a simulation study and illustrate them in a real-world case study.", "link": "https://arxiv.org/abs/2306.03625"}, {"id": "2306.03718", "date": "Tue, 6 Jun 2023 14:28:57 GMT", "title": "Emotion-Conditioned Melody Harmonization with Hierarchical Variational\n Autoencoder\n", "authors": ["Shulei Ji and Xinyu Yang\n"], "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS\nComments:", "Accepted", "by", "IEEE", "SMC", "2023\n"], "abstract": "Existing melody harmonization models have made great progress in improving the quality of generated harmonies, but most of them ignored the emotions beneath the music. Meanwhile, the variability of harmonies generated by previous methods is insufficient. To solve these problems, we propose a novel LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the influence of emotional conditions on melody harmonization, while improving the quality of generated harmonies and capturing the abundant variability of chord progressions. Specifically, LHVAE incorporates latent variables and emotional conditions at different levels (piece- and bar-level) to model the global and local music properties. Additionally, we introduce an attention-based melody context vector at each step to better learn the correspondence between melodies and harmonies. Experimental results of the objective evaluation show that our proposed model outperforms other LSTM-based models. Through subjective evaluation, we conclude that only altering the chords hardly changes the overall emotion of the music. The qualitative analysis demonstrates the ability of our model to generate variable harmonies.", "link": "https://arxiv.org/abs/2306.03718"}, {"id": "2306.03773", "date": "Thu, 1 Jun 2023 11:42:34 GMT", "title": "Some voices are too common: Building fair speech recognition systems\n using the Common Voice dataset\n", "authors": ["Lucas Maison", "Yannick Est\\`eve\n"], "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD\nComments:", "5", "pages,", "3", "figures.", "Accepted", "to", "Interspeech", "2023\n"], "abstract": "Automatic speech recognition (ASR) systems become increasingly efficient thanks to new advances in neural network training like self-supervised learning. However, they are known to be unfair toward certain groups, for instance, people speaking with an accent. In this work, we use the French Common Voice dataset to quantify the biases of a pre-trained wav2vec~2.0 model toward several demographic groups. By fine-tuning the pre-trained model on a variety of fixed-size, carefully crafted training sets, we demonstrate the importance of speaker diversity. We also run an in-depth analysis of the Common Voice corpus and identify important shortcomings that should be taken into account by users of this dataset.", "link": "https://arxiv.org/abs/2306.03773"}, {"id": "2306.03110", "date": "Mon, 5 Jun 2023 05:11:03 GMT", "title": "SwinRDM: Integrate SwinRNN with Diffusion Model towards High-Resolution\n and High-Quality Weather Forecasting\n", "authors": ["Lei Chen", "Fei Du", "Yuan Hu", "Fan Wang", "Zhibin Wang\n"], "categories": ["cs.AI", "cs.CV", "physics.ao-ph\nDOI:", "10.48448/zn7f-fc64\n"], "abstract": "Data-driven medium-range weather forecasting has attracted much attention in recent years. However, the forecasting accuracy at high resolution is unsatisfactory currently. Pursuing high-resolution and high-quality weather forecasting, we develop a data-driven model SwinRDM which integrates an improved version of SwinRNN with a diffusion model. SwinRDM performs predictions at 0.25-degree resolution and achieves superior forecasting accuracy to IFS (Integrated Forecast System), the state-of-the-art operational NWP model, on representative atmospheric variables including 500 hPa geopotential (Z500), 850 hPa temperature (T850), 2-m temperature (T2M), and total precipitation (TP), at lead times of up to 5 days. We propose to leverage a two-step strategy to achieve high-resolution predictions at 0.25-degree considering the trade-off between computation memory and forecasting accuracy. Recurrent predictions for future atmospheric fields are firstly performed at 1.40625-degree resolution, and then a diffusion-based super-resolution model is leveraged to recover the high spatial resolution and finer-scale atmospheric details. SwinRDM pushes forward the performance and potential of data-driven models for a large margin towards operational applications.", "link": "https://arxiv.org/abs/2306.03110"}, {"id": "2306.03197", "date": "Mon, 5 Jun 2023 19:16:37 GMT", "title": "AutoScrum: Automating Project Planning Using Large Language Models\n", "authors": ["Martin Schroder\n"], "categories": ["cs.AI", "cs.CL\nComments:", "25", "pages,", "3", "figures,", "demo:", "https://github.com/autoscrum/autoscrum\n"], "abstract": "Recent advancements in the field of large language models have made it possible to use language models for advanced reasoning. In this paper we leverage this ability for designing complex project plans based only on knowing the current state and the desired state. Two approaches are demonstrated - a scrum based approach and a shortcut plan approach. The scrum based approach executes an automated process of requirements gathering, user story mapping, feature identification, task decomposition and finally generates questions and search terms for seeking out domain specific information to assist with task completion. The shortcut approach looks at most recent snapshot of the current and desired state and generates the next most reasonable task to do in order to get to the desired state as quickly as possible. In this paper we automate everything using a novel concept of \"Language Programs\". These are programs written in natural language designed to process input data through the language model. Guidance language is used for all LLM programs. All demo source code for this paper is available at https://github.com/autoscrum/autoscrum", "link": "https://arxiv.org/abs/2306.03197"}, {"id": "2306.03293", "date": "Mon, 5 Jun 2023 22:38:21 GMT", "title": "Towards Fairness in Personalized Ads Using Impression Variance Aware\n Reinforcement Learning\n", "authors": ["Aditya Srinivas Timmaraju", "Mehdi Mashayekhi", "Mingliang Chen", "Qi Zeng,\n Quintin Fettes", "Wesley Cheung", "Yihan Xiao", "Manojkumar Rangasamy Kannadasan,\n Pushkar Tripathi", "Sean Gahagan", "Miranda Boge", "Rob Roudani\n"], "categories": ["cs.AI", "cs.CY\nComments:", "11", "pages,", "7", "figure,", "KDD", "2023\nDOI:", "10.1145/3580305.3599916\n"], "abstract": "Variances in ad impression outcomes across demographic groups are increasingly considered to be potentially indicative of algorithmic bias in personalized ads systems. While there are many definitions of fairness that could be applicable in the context of personalized systems, we present a framework which we call the Variance Reduction System (VRS) for achieving more equitable outcomes in Meta's ads systems. VRS seeks to achieve a distribution of impressions with respect to selected protected class (PC) attributes that more closely aligns the demographics of an ad's eligible audience (a function of advertiser targeting criteria) with the audience who sees that ad, in a privacy-preserving manner. We first define metrics to quantify fairness gaps in terms of ad impression variances with respect to PC attributes including gender and estimated race. We then present the VRS for re-ranking ads in an impression variance-aware manner. We evaluate VRS via extensive simulations over different parameter choices and study the effect of the VRS on the chosen fairness metric. We finally present online A/B testing results from applying VRS to Meta's ads systems, concluding with a discussion of future work. We have deployed the VRS to all users in the US for housing ads, resulting in significant improvement in our fairness metric. VRS is the first large-scale deployed framework for pursuing fairness for multiple PC attributes in online advertising.", "link": "https://arxiv.org/abs/2306.03293"}, {"id": "2306.03375", "date": "Tue, 6 Jun 2023 03:29:47 GMT", "title": "Identifying Shared Decodable Concepts in the Human Brain Using\n Image-Language Foundation Models\n", "authors": ["Cory Efird", "Alex Murphy", "Joel Zylberberg", "Alona Fyshe\n"], "categories": ["cs.AI", "cs.CV\nComments:", "Under", "review\n"], "abstract": "We introduce a method that takes advantage of high-quality pretrained multimodal representations to explore fine-grained semantic networks in the human brain. Previous studies have documented evidence of functional localization in the brain, with different anatomical regions preferentially activating for different types of sensory input. Many such localized structures are known, including the fusiform face area and parahippocampal place area. This raises the question of whether additional brain regions (or conjunctions of brain regions) are also specialized for other important semantic concepts. To identify such brain regions, we developed a data-driven approach to uncover visual concepts that are decodable from a massive functional magnetic resonance imaging (fMRI) dataset. Our analysis is broadly split into three sections. First, a fully connected neural network is trained to map brain responses to the outputs of an image-language foundation model, CLIP (Radford et al., 2021). Subsequently, a contrastive-learning dimensionality reduction method reveals the brain-decodable components of CLIP space. In the final section of our analysis, we localize shared decodable concepts in the brain using a voxel-masking optimization method to produce a shared decodable concept (SDC) space. The accuracy of our procedure is validated by comparing it to previous localization experiments that identify regions for faces, bodies, and places. In addition to these concepts, whose corresponding brain regions were already known, we localize novel concept representations which are shared across participants to other areas of the human brain. We also demonstrate how this method can be used to inspect fine-grained semantic networks for individual participants. We envisage that this extensible method can also be adapted to explore other questions at the intersection of AI and neuroscience.", "link": "https://arxiv.org/abs/2306.03375"}, {"id": "2306.03408", "date": "Tue, 6 Jun 2023 05:11:58 GMT", "title": "Agents Explore the Environment Beyond Good Actions to Improve Their\n Model for Better Decisions\n", "authors": ["Matthias Unverzagt\n"], "categories": ["cs.AI", "cs.LG\nComments:", "Submitted", "to", "NeurIPS", "2023\n"], "abstract": "Improving the decision-making capabilities of agents is a key challenge on the road to artificial intelligence. To improve the planning skills needed to make good decisions, MuZero's agent combines prediction by a network model and planning by a tree search using the predictions. MuZero's learning process can fail when predictions are poor but planning requires them. We use this as an impetus to get the agent to explore parts of the decision tree in the environment that it otherwise would not explore. The agent achieves this, first by normal planning to come up with an improved policy. Second, it randomly deviates from this policy at the beginning of each training episode. And third, it switches back to the improved policy at a random time step to experience the rewards from the environment associated with the improved policy, which is the basis for learning the correct value expectation. The simple board game Tic-Tac-Toe is used to illustrate how this approach can improve the agent's decision-making ability. The source code, written entirely in Java, is available at https://github.com/enpasos/muzero.", "link": "https://arxiv.org/abs/2306.03408"}, {"id": "2306.03409", "date": "Tue, 6 Jun 2023 05:13:29 GMT", "title": "Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum\n Weight Base Problems\n", "authors": ["Anh Viet Do", "Aneta Neumann", "Frank Neumann", "Andrew M. Sutton\n"], "categories": ["cs.AI", "cs.DS", "cs.NE\nComments:", "12", "pages\n"], "abstract": "We study the multi-objective minimum weight base problem, an abstraction of classical NP-hard combinatorial problems such as the multi-objective minimum spanning tree problem. We prove some important properties of the convex hull of the non-dominated front, such as its approximation quality and an upper bound on the number of extreme points. Using these properties, we give the first run-time analysis of the MOEA/D algorithm for this problem, an evolutionary algorithm that effectively optimizes by decomposing the objectives into single-objective components. We show that the MOEA/D, given an appropriate decomposition setting, finds all extreme points within expected fixed-parameter polynomial time in the oracle model, the parameter being the number of objectives. Experiments are conducted on random bi-objective minimum spanning tree instances, and the results agree with our theoretical findings. Furthermore, compared with a previously studied evolutionary algorithm for the problem GSEMO, MOEA/D finds all extreme points much faster across all instances.", "link": "https://arxiv.org/abs/2306.03409"}, {"id": "2306.03532", "date": "Tue, 6 Jun 2023 09:30:48 GMT", "title": "A Belief Model for Conflicting and Uncertain Evidence -- Connecting\n Dempster-Shafer Theory and the Topology of Evidence\n", "authors": ["Daira Pinto Prieto", "Ronald de Haan", "Ayb\\\"uke \\\"Ozg\\\"un\n"], "categories": ["cs.AI", "cs.MA\nComments:", "To", "appear", "in", "the", "proceedings", "of", "KR", "2023\n"], "abstract": "One problem to solve in the context of information fusion, decision-making, and other artificial intelligence challenges is to compute justified beliefs based on evidence. In real-life examples, this evidence may be inconsistent, incomplete, or uncertain, making the problem of evidence fusion highly non-trivial. In this paper, we propose a new model for measuring degrees of beliefs based on possibly inconsistent, incomplete, and uncertain evidence, by combining tools from Dempster-Shafer Theory and Topological Models of Evidence. Our belief model is more general than the aforementioned approaches in two important ways: (1) it can reproduce them when appropriate constraints are imposed, and, more notably, (2) it is flexible enough to compute beliefs according to various standards that represent agents' evidential demands. The latter novelty allows the users of our model to employ it to compute an agent's (possibly) distinct degrees of belief, based on the same evidence, in situations when, e.g, the agent prioritizes avoiding false negatives and when it prioritizes avoiding false positives. Finally, we show that computing degrees of belief with this model is #P-complete in general.", "link": "https://arxiv.org/abs/2306.03532"}, {"id": "2306.03551", "date": "Tue, 6 Jun 2023 09:57:04 GMT", "title": "Scalable Concept Extraction in Industry 4.0\n", "authors": ["Andr\\'es Felipe Posada-Moreno", "Kai M\\\"uller", "Florian Brillowski,\n Friedrich Solowjow", "Thomas Gries", "Sebastian Trimpe\n"], "categories": ["cs.AI", "cs.CV", "cs.LG\n"], "abstract": "The industry 4.0 is leveraging digital technologies and machine learning techniques to connect and optimize manufacturing processes. Central to this idea is the ability to transform raw data into human understandable knowledge for reliable data-driven decision-making. Convolutional Neural Networks (CNNs) have been instrumental in processing image data, yet, their ``black box'' nature complicates the understanding of their prediction process. In this context, recent advances in the field of eXplainable Artificial Intelligence (XAI) have proposed the extraction and localization of concepts, or which visual cues intervene on the prediction process of CNNs. This paper tackles the application of concept extraction (CE) methods to industry 4.0 scenarios. To this end, we modify a recently developed technique, ``Extracting Concepts with Local Aggregated Descriptors'' (ECLAD), improving its scalability. Specifically, we propose a novel procedure for calculating concept importance, utilizing a wrapper function designed for CNNs. This process is aimed at decreasing the number of times each image needs to be evaluated. Subsequently, we demonstrate the potential of CE methods, by applying them in three industrial use cases. We selected three representative use cases in the context of quality control for material design (tailored textiles), manufacturing (carbon fiber reinforcement), and maintenance (photovoltaic module inspection). In these examples, CE was able to successfully extract and locate concepts directly related to each task. This is, the visual cues related to each concept, coincided with what human experts would use to perform the task themselves, even when the visual cues were entangled between multiple classes. Through empirical results, we show that CE can be applied for understanding CNNs in an industrial context, giving useful insights that can relate to domain knowledge.", "link": "https://arxiv.org/abs/2306.03551"}, {"id": "2306.03659", "date": "Tue, 6 Jun 2023 13:22:54 GMT", "title": "Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing\n Semantics with MASCHInE\n", "authors": ["Nicolas Hubert", "Heiko Paulheim", "Pierre Monnin", "Armelle Brun", "Davy\n Monticolo\n"], "categories": ["cs.AI", "cs.LG\n"], "abstract": "Knowledge graph embedding models (KGEMs) have gained considerable traction in recent years. These models learn a vector representation of knowledge graph entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning versatile KGEs is desirable as it makes them useful for a broad range of tasks. However, KGEMs are usually trained for a specific task, which makes their embeddings task-dependent. In parallel, the widespread assumption that KGEMs actually create a semantic representation of the underlying entities and relations (e.g., project similar entities closer than dissimilar ones) has been challenged. In this work, we design heuristics for generating protographs -- small, modified versions of a KG that leverage schema-based information. The learnt protograph-based embeddings are meant to encapsulate the semantics of a KG, and can be leveraged in learning KGEs that, in turn, also better capture semantics. Extensive experiments on various evaluation benchmarks demonstrate the soundness of this approach, which we call Modular and Agnostic SCHema-based Integration of protograph Embeddings (MASCHInE). In particular, MASCHInE helps produce more versatile KGEs that yield substantially better performance for entity clustering and node classification tasks. For link prediction, using MASCHInE has little impact on rank-based performance but increases the number of semantically valid predictions.", "link": "https://arxiv.org/abs/2306.03659"}, {"id": "2306.03717", "date": "Tue, 6 Jun 2023 14:27:03 GMT", "title": "Description Logics with Abstraction and Refinement\n", "authors": ["Carsten Lutz", "Lukas Schulze\n"], "categories": ["cs.AI", "cs.LO\nComments:", "25", "pages,", "Long", "version", "of", "paper", "accepted", "at", "KR", "2023\n"], "abstract": "Ontologies often require knowledge representation on multiple levels of abstraction, but description logics (DLs) are not well-equipped for supporting this. We propose an extension of DLs in which abstraction levels are first-class citizens and which provides explicit operators for the abstraction and refinement of concepts and roles across multiple abstraction levels, based on conjunctive queries. We prove that reasoning in the resulting family of DLs is decidable while several seemingly harmless variations turn out to be undecidable. We also pinpoint the precise complexity of our logics and several relevant fragments.", "link": "https://arxiv.org/abs/2306.03717"}, {"id": "2306.03753", "date": "Tue, 6 Jun 2023 15:13:58 GMT", "title": "Newly Formed Cities: an AI Curation\n", "authors": ["Dario Negueruela del Castillo", "Ludovica Schaerf", "Pepe Ballesteros,\n Iacopo Neri", "Valentine Bernasconi\n"], "categories": ["cs.AI", "cs.CV\n"], "abstract": "Art curatorial processes are characterized by the presentation of a collection of artworks in a knowledgeable way. Machine processes are characterized by their capacity to manage and analyze large amounts of data. This paper envisages machine curation and audience interaction as a means to explore the implications of contemporary AI models for the curatorial world. This project was developed for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city of Helsinki through the lens of machine perception. We use visual-textual models to place artworks currently hosted inside the museum in outdoor public spaces of the city, assigning fictional coordinates based on similarity scores. Synthetic 360{\\deg} art panoramas are generated using diffusion-based models to propose a machinic visual style guided by the artworks. The result of this project will be virtually presented as a web-based installation, where such a re-contextualization allows the navigation of an alternative version of the city while exploring its artistic heritage. Finally, we discuss our contributions to machine curation and the ethical implications that such a process entails. The web-based installation is available at this link: http://newlyformedcity.com/.", "link": "https://arxiv.org/abs/2306.03753"}, {"id": "2306.03795", "date": "Tue, 6 Jun 2023 15:40:27 GMT", "title": "AI-Supported Assessment of Load Safety\n", "authors": ["Julius Sch\\\"oning and Niklas Kruse\n"], "categories": ["cs.AI", "cs.HC", "cs.LG\nComments:", "9", "pages,", "4", "figures,", "2", "tables\n"], "abstract": "Load safety assessment and compliance is an essential step in the corporate process of every logistics service provider. In 2020, a total of 11,371 police checks of trucks were carried out, during which 9.6% (1091) violations against the load safety regulations were detected. For a logistic service provider, every load safety violation results in height fines and damage to reputation. An assessment of load safety supported by artificial intelligence (AI) will reduce the risk of accidents by unsecured loads and fines during safety assessments. This work shows how photos of the load, taken by the truck driver or the loadmaster after the loading process, can be used to assess load safety. By a trained two-stage artificial neural network (ANN), these photos are classified into three different classes I) cargo loaded safely, II) cargo loaded unsafely, and III) unusable image. By applying several architectures of convolutional neural networks (CNN), it can be shown that it is possible to distinguish between unusable and usable images for cargo safety assessment. This distinction is quite crucial since the truck driver and the loadmaster sometimes provide photos without the essential image features like the case structure of the truck and the whole cargo. A human operator or another ANN will then assess the load safety within the second stage.", "link": "https://arxiv.org/abs/2306.03795"}, {"id": "2306.03842", "date": "Tue, 6 Jun 2023 16:29:31 GMT", "title": "Remarks on Utility in Repeated Bets\n", "authors": ["Nimrod Megiddo\n"], "categories": ["cs.AI", "math.PR\n"], "abstract": "The use of von Neumann -- Morgenstern utility is examined in the context of multiple choices between lotteries. Different conclusions are reached if the choices are simultaneous or sequential. It is demonstrated that utility cannot be additive.", "link": "https://arxiv.org/abs/2306.03842"}, {"id": "2306.03849", "date": "Tue, 6 Jun 2023 16:39:58 GMT", "title": "Considering Human Factors in Risk Maps for Robust and Foresighted Driver\n Warning\n", "authors": ["Tim Puphal", "Ryohei Hirano", "Malte Probst", "Raphael Wenzel and Akihito\n Kimata\n"], "categories": ["cs.AI", "cs.HC\nReport-no:", "Accepted", "at", "IEEE", "ROMAN", "2023\n"], "abstract": "Driver support systems that include human states in the support process is an active research field. Many recent approaches allow, for example, to sense the driver's drowsiness or awareness of the driving situation. However, so far, this rich information has not been utilized much for improving the effectiveness of support systems. In this paper, we therefore propose a warning system that uses human states in the form of driver errors and can warn users in some cases of upcoming risks several seconds earlier than the state of the art systems not considering human factors. The system consists of a behavior planner Risk Maps which directly changes its prediction of the surrounding driving situation based on the sensed driver errors. By checking if this driver's behavior plan is objectively safe, a more robust and foresighted driver warning is achieved. In different simulations of a dynamic lane change and intersection scenarios, we show how the driver's behavior plan can become unsafe, given the estimate of driver errors, and experimentally validate the advantages of considering human factors.", "link": "https://arxiv.org/abs/2306.03849"}, {"id": "2306.03874", "date": "Tue, 6 Jun 2023 17:21:21 GMT", "title": "Embracing Background Knowledge in the Analysis of Actual Causality: An\n Answer Set Programming Approach\n", "authors": ["Michael Gelfond", "Jorge Fandinno and Evgenii Balai\n"], "categories": ["cs.AI", "cs.LO\nComments:", "Under", "consideration", "for", "publication", "in", "Theory", "and", "Practice", "of", "Logic\n", "Programming\n"], "abstract": "This paper presents a rich knowledge representation language aimed at formalizing causal knowledge. This language is used for accurately and directly formalizing common benchmark examples from the literature of actual causality. A definition of cause is presented and used to analyze the actual causes of changes with respect to sequences of actions representing those examples.", "link": "https://arxiv.org/abs/2306.03874"}, {"id": "2306.03901", "date": "Tue, 6 Jun 2023 17:58:24 GMT", "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory\n", "authors": ["Chenxu Hu", "Jie Fu", "Chenzhuang Du", "Simian Luo", "Junbo Zhao", "Hang Zhao\n"], "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG\n"], "abstract": "Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .", "link": "https://arxiv.org/abs/2306.03901"}, {"id": "2306.03400", "date": "Tue, 6 Jun 2023 04:30:18 GMT", "title": "G-CAME: Gaussian-Class Activation Mapping Explainer for Object Detectors\n", "authors": ["Quoc Khanh Nguyen", "Truong Thanh Hung Nguyen", "Vo Thanh Khang Nguyen,\n Van Binh Truong", "Quoc Hung Cao\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\nComments:", "10", "figures\n"], "abstract": "Nowadays, deep neural networks for object detection in images are very prevalent. However, due to the complexity of these networks, users find it hard to understand why these objects are detected by models. We proposed Gaussian Class Activation Mapping Explainer (G-CAME), which generates a saliency map as the explanation for object detection models. G-CAME can be considered a CAM-based method that uses the activation maps of selected layers combined with the Gaussian kernel to highlight the important regions in the image for the predicted box. Compared with other Region-based methods, G-CAME can transcend time constraints as it takes a very short time to explain an object. We also evaluated our method qualitatively and quantitatively with YOLOX on the MS-COCO 2017 dataset and guided to apply G-CAME into the two-stage Faster-RCNN model.", "link": "https://arxiv.org/abs/2306.03400"}, {"id": "2306.03414", "date": "Tue, 6 Jun 2023 05:26:26 GMT", "title": "DreamSparse: Escaping from Plato's Cave with 2D Diffusion Model Given\n Sparse Views\n", "authors": ["Paul Yoo", "Jiaxian Guo", "Yutaka Matsuo", "Shixiang Shane Gu\n"], "categories": ["cs.CV", "cs.AI", "cs.GR\n"], "abstract": "Synthesizing novel view images from a few views is a challenging but practical problem. Existing methods often struggle with producing high-quality results or necessitate per-object optimization in such few-view settings due to the insufficient information provided. In this work, we explore leveraging the strong 2D priors in pre-trained diffusion models for synthesizing novel view images. 2D diffusion models, nevertheless, lack 3D awareness, leading to distorted image synthesis and compromising the identity. To address these problems, we propose DreamSparse, a framework that enables the frozen pre-trained diffusion model to generate geometry and identity-consistent novel view image. Specifically, DreamSparse incorporates a geometry module designed to capture 3D features from sparse views as a 3D prior. Subsequently, a spatial guidance model is introduced to convert these 3D feature maps into spatial information for the generative process. This information is then used to guide the pre-trained diffusion model, enabling it to generate geometrically consistent images without tuning it. Leveraging the strong image priors in the pre-trained diffusion models, DreamSparse is capable of synthesizing high-quality novel views for both object and scene-level images and generalising to open-set images. Experimental results demonstrate that our framework can effectively synthesize novel view images from sparse views and outperforms baselines in both trained and open-set category images. More results can be found on our project page: https://sites.google.com/view/dreamsparse-webpage.", "link": "https://arxiv.org/abs/2306.03414"}, {"id": "2306.03102", "date": "Fri, 2 Jun 2023 06:28:21 GMT", "title": "ChatGPT is a Remarkable Tool -- For Experts\n", "authors": ["Amos Azaria", "Rina Azoulay", "Shulamit Reches\n"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY\n"], "abstract": "This paper investigates the capabilities of ChatGPT as an automated assistant in diverse domains, including scientific writing, mathematics, education, programming, and healthcare. We explore the potential of ChatGPT to enhance productivity, streamline problem-solving processes, and improve writing style. Furthermore, we highlight the potential risks associated with excessive reliance on ChatGPT in these fields. These limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation. We outline areas and objectives where ChatGPT proves beneficial, applications where it should be used judiciously, and scenarios where its reliability may be limited. In light of observed limitations, and given that the tool's fundamental errors may pose a special challenge for non-experts, ChatGPT should be used with a strategic methodology. By drawing from comprehensive experimental studies, we offer methods and flow charts for effectively using ChatGPT. Our recommendations emphasize iterative interaction with ChatGPT and independent verification of its outputs. Considering the importance of utilizing ChatGPT judiciously and with expertise, we recommend its usage for experts who are well-versed in the respective domains.", "link": "https://arxiv.org/abs/2306.03102"}, {"id": "2306.03104", "date": "Sat, 3 Jun 2023 00:56:34 GMT", "title": "Guided scenarios with simulated expert personae: a remarkable strategy\n to perform cognitive work\n", "authors": ["David Van Buren\n"], "categories": ["cs.HC", "cs.AI", "quant-ph\nComments:", "11", "pages,", "3", "figures,", "1", "listing\nMSC-class:", "68T01,", "68T20,", "68T35,", "68T37,", "81V80\nACM-class:", "I.2.0;", "I.2.1;", "I.2.3;", "I.2.8\n"], "abstract": "Large language models (LLMs) trained on a substantial corpus of human knowledge and literature productively work with a large array of facts from that corpus. Surprisingly, they are also able to re-create the behaviors of personae that are captured within the corpus. By forming teams of simulated personae, supplying contexts that set the stage, and providing gentle prompts, one can move through scenarios that elicit expert behavior to perform meaningful cognitive work. The power of this strategy is demonstrated with two examples, one attacking factuality of LLM responses and the other reproducing a very recently published result in quantum optics.", "link": "https://arxiv.org/abs/2306.03104"}, {"id": "2306.03115", "date": "Mon, 5 Jun 2023 13:13:19 GMT", "title": "AutoExp: A multidisciplinary, multi-sensor framework to evaluate human\n activities in self-driving cars\n", "authors": ["Carlos Crispim-Junior", "Romain Guesdon", "Christophe Jallais", "Florent\n Laroche", "Stephanie Souche-Le Corvec", "Laure Tougne Rodet\n"], "categories": ["cs.HC", "cs.AI", "cs.LG\nComments:", "This", "paper", "is", "currently", "under", "review", "by", "the", "26th", "IEEE", "International\n", "Conference", "on", "Intelligent", "Transportation", "Systems", "(ITSC", "2023)\n"], "abstract": "The adoption of self-driving cars will certainly revolutionize our lives, even though they may take more time to become fully autonomous than initially predicted. The first vehicles are already present in certain cities of the world, as part of experimental robot-taxi services. However, most existing studies focus on the navigation part of such vehicles. We currently miss methods, datasets, and studies to assess the in-cabin human component of the adoption of such technology in real-world conditions. This paper proposes an experimental framework to study the activities of occupants of self-driving cars using a multidisciplinary approach (computer vision associated with human and social sciences), particularly non-driving related activities. The framework is composed of an experimentation scenario, and a data acquisition module. We seek firstly to capture real-world data about the usage of the vehicle in the nearest possible, real-world conditions, and secondly to create a dataset containing in-cabin human activities to foster the development and evaluation of computer vision algorithms. The acquisition module records multiple views of the front seats of the vehicle (Intel RGB-D and GoPro cameras); in addition to survey data about the internal states and attitudes of participants towards this type of vehicle before, during, and after the experimentation. We evaluated the proposed framework with the realization of real-world experimentation with 30 participants (1 hour each) to study the acceptance of SDCs of SAE level 4.", "link": "https://arxiv.org/abs/2306.03115"}, {"id": "2306.03116", "date": "Mon, 5 Jun 2023 13:43:29 GMT", "title": "Transferring Annotator- and Instance-dependent Transition Matrix for\n Learning from Crowds\n", "authors": ["Shikun Li", "Xiaobo Xia", "Jiankang Deng", "Shiming Ge", "Tongliang Liu\n"], "categories": ["cs.HC", "cs.AI", "cs.LG\n"], "abstract": "Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \\textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling generality, we parameterize AIDTM with deep neural networks. To alleviate the modeling challenge, we suppose every annotator shares its noise pattern with similar annotators, and estimate AIDTM via \\textit{knowledge transfer}. We hence first model the mixture of noise patterns by all annotators, and then transfer this modeling to individual annotators. Furthermore, considering that the transfer from the mixture of noise patterns to individuals may cause two annotators with highly different noise generations to perturb each other, we employ the knowledge transfer between identified neighboring annotators to calibrate the modeling. Experiments confirm the superiority of the proposed approach on synthetic and real-world crowd-sourcing data. Source codes will be released.", "link": "https://arxiv.org/abs/2306.03116"}, {"id": "2306.03195", "date": "Mon, 5 Jun 2023 19:13:44 GMT", "title": "Lumos in the Night Sky: AI-enabled Visual Tool for Exploring Night-Time\n Light Patterns\n", "authors": ["Jakob Hederich", "Shreya Ghosh", "Zeyu He and Prasenjit Mitra\n"], "categories": ["cs.HC", "cs.AI", "cs.IR", "cs.LG\nComments:", "5", "pages,", "3", "figures.", "Accepted", "in", "ECML", "PKDD", "Demo", "track\n"], "abstract": "We introduce NightPulse, an interactive tool for Night-time light (NTL) data visualization and analytics, which enables researchers and stakeholders to explore and analyze NTL data with a user-friendly platform. Powered by efficient system architecture, NightPulse supports image segmentation, clustering, and change pattern detection to identify urban development and sprawl patterns. It captures temporal trends of NTL and semantics of cities, answering questions about demographic factors, city boundaries, and unusual differences.", "link": "https://arxiv.org/abs/2306.03195"}, {"id": "2306.03233", "date": "Tue, 30 May 2023 17:04:39 GMT", "title": "Unified Information Dynamic Analysis of Quantum Decision-Making and\n Search Algorithms: Computational Intelligence Measure\n", "authors": ["Sergey V. Ulyanov", "Fabio Ghisi", "Ichiro Kurawaki and Viktor S. Ulyanov\n"], "categories": ["quant-ph", "cs.AI", "cs.RO\nComments:", "arXiv", "admin", "note:", "substantial", "text", "overlap", "with\n", "arXiv:quant-ph/0102094", "by", "other", "authors;", "text", "overlap", "with\n", "arXiv:quant-ph/0112105,", "arXiv:quant-ph/9802065", "by", "other", "authors\nMSC-class:", "81-08,", "81-05,", "68T40,", "93C85\n"], "abstract": "There are important algorithms built upon a mixture of basic techniques described; for example, the Fast Fourier Transform (FFT) employs both Divide-and-Conquer and Transform-and-Conquer techniques. In this article, the evolution of a quantum algorithm (QA) is examined from an information theory viewpoint. The complex vector entering the quantum algorithmic gate - QAG is considered as an information source both from the classical and the quantum level. The analysis of the classical and quantum information flow in Deutsch-Jozsa, Shor and Grover algorithms is used. It is shown that QAG, based on superposition of states, quantum entanglement and interference, when acting on the input vector, stores information into the system state, minimizing the gap between classical Shannon entropy and quantum von Neumann entropy. Minimizing of the gap between Shannon and von Neumann entropies is considered as a termination criterion of QA computational intelligence measure.", "link": "https://arxiv.org/abs/2306.03233"}, {"id": "2306.03411", "date": "Tue, 6 Jun 2023 05:18:21 GMT", "title": "Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search\n", "authors": ["Zhiyu Chen", "Jason Choi", "Besnik Fetahu", "Oleg Rokhlenko", "Shervin Malmasi\n"], "categories": ["cs.CL", "cs.AI", "cs.IR\nComments:", "ACL", "2023", "Industry", "Track\n"], "abstract": "Customers interacting with product search engines are increasingly formulating information-seeking queries. Frequently Asked Question (FAQ) retrieval aims to retrieve common question-answer pairs for a user query with question intent. Integrating FAQ retrieval in product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Determining when an FAQ entry can satisfy a user's information need within product search, without disrupting their shopping experience, represents an important challenge. We propose an intent-aware FAQ retrieval system consisting of (1) an intent classifier that predicts when a user's information need can be answered by an FAQ; (2) a reformulation model that rewrites a query into a natural question. Offline evaluation demonstrates that our approach improves Hit@1 by 13% on retrieving ground-truth FAQs, while reducing latency by 95% compared to baseline systems. These improvements are further validated by real user feedback, where 71% of displayed FAQs on top of product search results received explicit positive user feedback. Overall, our findings show promising directions for integrating FAQ retrieval into product search at scale.", "link": "https://arxiv.org/abs/2306.03411"}, {"id": "2306.03502", "date": "Tue, 6 Jun 2023 08:41:02 GMT", "title": "Russo-Ukrainian War: Prediction and explanation of Twitter suspension\n", "authors": ["Alexander Shevtsov", "Despoina Antonakaki", "Ioannis Lamprou", "Ioannis\n Kontogiorgakis", "Polyvios Pratikakis", "Sotiris Ioannidis\n"], "categories": ["cs.SI", "cs.AI", "cs.LG\n"], "abstract": "On 24 February 2022, Russia invaded Ukraine, starting what is now known as the Russo-Ukrainian War, initiating an online discourse on social media. Twitter as one of the most popular SNs, with an open and democratic character, enables a transparent discussion among its large user base. Unfortunately, this often leads to Twitter's policy violations, propaganda, abusive actions, civil integrity violation, and consequently to user accounts' suspension and deletion. This study focuses on the Twitter suspension mechanism and the analysis of shared content and features of the user accounts that may lead to this. Toward this goal, we have obtained a dataset containing 107.7M tweets, originating from 9.8 million users, using Twitter API. We extract the categories of shared content of the suspended accounts and explain their characteristics, through the extraction of text embeddings in junction with cosine similarity clustering. Our results reveal scam campaigns taking advantage of trending topics regarding the Russia-Ukrainian conflict for Bitcoin and Ethereum fraud, spam, and advertisement campaigns. Additionally, we apply a machine learning methodology including a SHapley Additive explainability model to understand and explain how user accounts get suspended.", "link": "https://arxiv.org/abs/2306.03502"}, {"id": "2306.03503", "date": "Tue, 6 Jun 2023 08:47:42 GMT", "title": "Applying Standards to Advance Upstream & Downstream Ethics in Large\n Language Models\n", "authors": ["Jose Berengueres and Marybeth Sandell\n"], "categories": ["cs.CY", "cs.AI", "cs.CL\nComments:", "8", "pages,", "4", "tables,", "2", "figures\nACM-class:", "K.4.1;", "I.2.0\n"], "abstract": "This paper explores how AI-owners can develop safeguards for AI-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries. It delves into the current state of ethical awareness on Large Language Models (LLMs). By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified. A comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices. The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation. Drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling LLM-generated content. Finally, potential conflicts of interest between dataset curation at upstream and ethical benchmarking downstream are highlighted to underscore the need for a broader evaluation beyond mere output. This study prompts a nuanced conversation around ethical implications in this rapidly evolving field of content generation.", "link": "https://arxiv.org/abs/2306.03503"}, {"id": "2306.03509", "date": "Tue, 6 Jun 2023 08:54:49 GMT", "title": "Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive\n Bias\n", "authors": ["Ziyue Jiang", "Yi Ren", "Zhenhui Ye", "Jinglin Liu", "Chen Zhang", "Qian Yang,\n Shengpeng Ji", "Rongjie Huang", "Chunfeng Wang", "Xiang Yin", "Zejun Ma", "Zhou Zhao\n"], "categories": ["eess.AS", "cs.AI", "cs.SD\n"], "abstract": "Scaling text-to-speech to a large and wild dataset has been proven to be highly effective in achieving timbre and speech style generalization, particularly in zero-shot TTS. However, previous works usually encode speech into latent using audio codec and use autoregressive language models or diffusion models to generate it, which ignores the intrinsic nature of speech and may lead to inferior or uncontrollable results. We argue that speech can be decomposed into several attributes (e.g., content, timbre, prosody, and phase) and each of them should be modeled using a module with appropriate inductive biases. From this perspective, we carefully design a novel and large zero-shot TTS system called Mega-TTS, which is trained with large-scale wild data and models different attributes in different ways: 1) Instead of using latent encoded by audio codec as the intermediate feature, we still choose spectrogram as it separates the phase and other attributes very well. Phase can be appropriately constructed by the GAN-based vocoder and does not need to be modeled by the language model. 2) We model the timbre using global vectors since timbre is a global attribute that changes slowly over time. 3) We further use a VQGAN-based acoustic model to generate the spectrogram and a latent code language model to fit the distribution of prosody, since prosody changes quickly over time in a sentence, and language models can capture both local and long-range dependencies. We scale Mega-TTS to multi-domain datasets with 20K hours of speech and evaluate its performance on unseen speakers. Experimental results demonstrate that Mega-TTS surpasses state-of-the-art TTS systems on zero-shot TTS, speech editing, and cross-lingual TTS tasks, with superior naturalness, robustness, and speaker similarity due to the proper inductive bias of each module. Audio samples are available at https://mega-tts.github.io/demo-page.", "link": "https://arxiv.org/abs/2306.03509"}, {"id": "2306.03523", "date": "Tue, 6 Jun 2023 09:17:56 GMT", "title": "Inconsistency Handling in Prioritized Databases with Universal\n Constraints: Complexity Analysis and Links with Active Integrity Constraints\n", "authors": ["Meghyn Bienvenu and Camille Bourgaux\n"], "categories": ["cs.DB", "cs.AI", "cs.LO\nComments:", "This", "is", "an", "extended", "version", "of", "a", "paper", "appearing", "at", "the", "20th\n", "International", "Conference", "on", "Principles", "of", "Knowledge", "Representation", "and\n", "Reasoning", "(KR", "2023).", "28", "pages\n"], "abstract": "This paper revisits the problem of repairing and querying inconsistent databases equipped with universal constraints. We adopt symmetric difference repairs, in which both deletions and additions of facts can be used to restore consistency, and suppose that preferred repair actions are specified via a binary priority relation over (negated) facts. Our first contribution is to show how existing notions of optimal repairs, defined for simpler denial constraints and repairs solely based on fact deletion, can be suitably extended to our richer setting. We next study the computational properties of the resulting repair notions, in particular, the data complexity of repair checking and inconsistency-tolerant query answering. Finally, we clarify the relationship between optimal repairs of prioritized databases and repair notions introduced in the framework of active integrity constraints. In particular, we show that Pareto-optimal repairs in our setting correspond to founded, grounded and justified repairs w.r.t. the active integrity constraints obtained by translating the prioritized database. Our study also yields useful insights into the behavior of active integrity constraints.", "link": "https://arxiv.org/abs/2306.03523"}, {"id": "2306.03572", "date": "Tue, 6 Jun 2023 10:42:40 GMT", "title": "Range-Restricted Interpolation through Clausal Tableaux\n", "authors": ["Christoph Wernhard\n"], "categories": ["cs.LO", "cs.AI", "cs.DB\n"], "abstract": "We show how variations of range-restriction and also the Horn property can be passed from inputs to outputs of Craig interpolation in first-order logic. The proof system is clausal tableaux, which stems from first-order ATP. Our results are induced by a restriction of the clausal tableau structure, which can be achieved in general by a proof transformation, also if the source proof is by resolution/paramodulation. Primarily addressed applications are query synthesis and reformulation with interpolation. Our methodical approach combines operations on proof structures with the immediate perspective of feasible implementation through incorporating highly optimized first-order provers.", "link": "https://arxiv.org/abs/2306.03572"}, {"id": "2306.03723", "date": "Tue, 6 Jun 2023 14:41:30 GMT", "title": "Financial Numeric Extreme Labelling: A Dataset and Benchmarking for XBRL\n Tagging\n", "authors": ["Soumya Sharma", "Subhendu Khatuya", "Manjunath Hegde", "Afreen Shaikh.\n Koustuv Dasgupta", "Pawan Goyal", "Niloy Ganguly\n"], "categories": ["cs.CL", "cs.AI", "cs.CE\nComments:", "Accepted", "to", "ACL'23", "Findings", "Paper\n"], "abstract": "The U.S. Securities and Exchange Commission (SEC) mandates all public companies to file periodic financial statements that should contain numerals annotated with a particular label from a taxonomy. In this paper, we formulate the task of automating the assignment of a label to a particular numeral span in a sentence from an extremely large label set. Towards this task, we release a dataset, Financial Numeric Extreme Labelling (FNXL), annotated with 2,794 labels. We benchmark the performance of the FNXL dataset by formulating the task as (a) a sequence labelling problem and (b) a pipeline with span extraction followed by Extreme Classification. Although the two approaches perform comparably, the pipeline solution provides a slight edge for the least frequent labels.", "link": "https://arxiv.org/abs/2306.03723"}, {"id": "2306.03823", "date": "Thu, 25 May 2023 17:35:57 GMT", "title": "Transformative Effects of ChatGPT on Modern Education: Emerging Era of\n AI Chatbots\n", "authors": ["Sukhpal Singh Gill", "Minxian Xu", "Panos Patros", "Huaming Wu", "Rupinder\n Kaur", "Kamalpreet Kaur", "Stephanie Fuller", "Manmeet Singh", "Priyansh Arora", "Ajith\n Kumar Parlikad", "Vlado Stankovski", "Ajith Abraham", "Soumya K. Ghosh", "Hanan\n Lutfiyya", "Salil S. Kanhere", "Rami Bahsoon", "Omer Rana", "Schahram Dustdar", "Rizos\n Sakellariou", "Steve Uhlig", "Rajkumar Buyya\n"], "categories": ["cs.CY", "cs.AI", "cs.CL\nComments:", "Preprint", "submitted", "to", "IoTCPS", "Elsevier", "(2023)\n"], "abstract": "ChatGPT, an AI-based chatbot, was released to provide coherent and useful replies based on analysis of large volumes of data. In this article, leading scientists, researchers and engineers discuss the transformative effects of ChatGPT on modern education. This research seeks to improve our knowledge of ChatGPT capabilities and its use in the education sector, identifying potential concerns and challenges. Our preliminary evaluation concludes that ChatGPT performed differently in each subject area including finance, coding and maths. While ChatGPT has the ability to help educators by creating instructional content, offering suggestions and acting as an online educator to learners by answering questions and promoting group work, there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential. The often reported hallucinations within Generative AI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential. What ChatGPT lacks is a stochastic measure to help provide sincere and sensitive communication with its users. Academic regulations and evaluation practices used in educational institutions need to be updated, should ChatGPT be used as a tool in education. To address the transformative effects of ChatGPT on the learning environment, educating teachers and students alike about its capabilities and limitations will be crucial.", "link": "https://arxiv.org/abs/2306.03823"}, {"id": "2306.03872", "date": "Tue, 6 Jun 2023 17:18:56 GMT", "title": "Deductive Verification of Chain-of-Thought Reasoning\n", "authors": ["Zhan Ling", "Yunhao Fang", "Xuanlin Li", "Zhiao Huang", "Mingu Lee", "Roland\n Memisevic and Hao Su\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\n"], "abstract": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify_cot.", "link": "https://arxiv.org/abs/2306.03872"}, {"id": "2306.03902", "date": "Tue, 6 Jun 2023 17:58:44 GMT", "title": "Utterance Classification with Logical Neural Network: Explainable AI for\n Mental Disorder Diagnosis\n", "authors": ["Yeldar Toleubay", "Don Joven Agravante", "Daiki Kimura", "Baihan Lin,\n Djallel Bouneffouf", "Michiaki Tatsubori\n"], "categories": ["cs.CL", "cs.AI", "cs.LO", "q-bio.NC\nComments:", "ACL", "2023\n"], "abstract": "In response to the global challenge of mental health problems, we proposes a Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis of mental disorders. Due to the lack of effective therapy coverage for mental disorders, there is a need for an AI solution that can assist therapists with the diagnosis. However, current Neural Network models lack explainability and may not be trusted by therapists. The LNN is a Recurrent Neural Network architecture that combines the learning capabilities of neural networks with the reasoning capabilities of classical logic-based AI. The proposed system uses input predicates from clinical interviews to output a mental disorder class, and different predicate pruning techniques are used to achieve scalability and higher scores. In addition, we provide an insight extraction method to aid therapists with their diagnosis. The proposed system addresses the lack of explainability of current Neural Network models and provides a more trustworthy solution for mental disorder diagnosis.", "link": "https://arxiv.org/abs/2306.03902"}, {"id": "2306.03175", "date": "Mon, 5 Jun 2023 18:32:53 GMT", "title": "Infusing Lattice Symmetry Priors in Attention Mechanisms for\n Sample-Efficient Abstract Geometric Reasoning\n", "authors": ["Mattia Atzeni", "Mrinmaya Sachan", "Andreas Loukas\n"], "categories": ["cs.AI", "cs.LG", "stat.ML\nComments:", "Accepted", "for", "publication", "at", "the", "International", "Conference", "on", "Machine\n", "Learning,", "ICML", "2023\n"], "abstract": "The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer requires 2 orders of magnitude fewer data than standard attention and transformers. Moreover, our results on ARC and LARC tasks that incorporate geometric priors provide preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.", "link": "https://arxiv.org/abs/2306.03175"}, {"id": "2306.03314", "date": "Mon, 5 Jun 2023 23:55:37 GMT", "title": "Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM\n Agents\n", "authors": ["Yashar Talebirad and Amirhossein Nadiri\n"], "categories": ["cs.AI", "cs.LG", "cs.MA\n"], "abstract": "In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the \"Gorilla\" model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of LLMs through collaboration and knowledge exchange among intelligent agents.", "link": "https://arxiv.org/abs/2306.03314"}, {"id": "2306.03403", "date": "Tue, 6 Jun 2023 04:49:51 GMT", "title": "SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic\n Segmentation\n", "authors": ["Xuewei Li", "Tao Wu", "Zhongang Qi", "Gaoang Wang", "Ying Shan", "Xi Li\n"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM\nComments:", "Accepted", "by", "IJCAI", "2023\n"], "abstract": "As an important and challenging problem in computer vision, PAnoramic Semantic Segmentation (PASS) gives complete scene perception based on an ultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic image input focus on solving image distortions but lack consideration of the 3D properties of original $360^{\\circ}$ data. Therefore, their performance will drop a lot when inputting panoramic images with the 3D disturbance. To be more robust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge. Specifically, a spherical geometry-aware framework is proposed for PASS. It includes three modules, i.e., spherical geometry-aware image projection, spherical deformable patch embedding, and a panorama-aware loss, which takes input images with 3D disturbance into account, adds a spherical geometry-aware constraint on the existing deformable patch embedding, and indicates the pixel density of original $360^{\\circ}$ data, respectively. Experimental results on Stanford2D3D Panoramic datasets show that SGAT4PASS significantly improves performance and robustness, with approximately a 2% increase in mIoU, and when small 3D disturbances occur in the data, the stability of our performance is improved by an order of magnitude. Our code and supplementary material are available at https://github.com/TencentARC/SGAT4PASS.", "link": "https://arxiv.org/abs/2306.03403"}, {"id": "2306.03679", "date": "Tue, 6 Jun 2023 13:41:37 GMT", "title": "Human-imperceptible, Machine-recognizable Images\n", "authors": ["Fusheng Hao", "Fengxiang He", "Yikai Wang", "Fuxiang Wu", "Jing Zhang", "Jun\n Cheng", "Dacheng Tao\n"], "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG", "stat.ML\n"], "abstract": "Massive human-related data is collected to train neural networks for computer vision tasks. A major conflict is exposed relating to software engineers between better developing AI systems and distancing from the sensitive training data. To reconcile this conflict, this paper proposes an efficient privacy-preserving learning paradigm, where images are first encrypted to become ``human-imperceptible, machine-recognizable'' via one of the two encryption strategies: (1) random shuffling to a set of equally-sized patches and (2) mixing-up sub-patches of the images. Then, minimal adaptations are made to vision transformer to enable it to learn on the encrypted images for vision tasks, including image classification and object detection. Extensive experiments on ImageNet and COCO show that the proposed paradigm achieves comparable accuracy with the competitive methods. Decrypting the encrypted images requires solving an NP-hard jigsaw puzzle or an ill-posed inverse problem, which is empirically shown intractable to be recovered by various attackers, including the powerful vision transformer-based attacker. We thus show that the proposed paradigm can ensure the encrypted images have become human-imperceptible while preserving machine-recognizable information. The code is available at \\url{https://github.com/FushengHao/PrivacyPreservingML.}", "link": "https://arxiv.org/abs/2306.03679"}, {"id": "2306.03727", "date": "Tue, 6 Jun 2023 14:45:44 GMT", "title": "Towards Visual Foundational Models of Physical Scenes\n", "authors": ["Chethan Parameshwara", "Alessandro Achille", "Matthew Trager", "Xiaolong Li,\n Jiawei Mo", "Matthew Trager", "Ashwin Swaminathan", "CJ Taylor", "Dheera Venkatraman,\n Xiaohan Fei", "Stefano Soatto\n"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO\nComments:", "TLDR:", "Physical", "scenes", "are", "equivalence", "classes", "of", "sufficient\n", "statistics,", "and", "can", "be", "inferred", "uniquely", "by", "any", "agent", "measuring", "the", "same\n", "finite", "data;", "We", "formalize", "and", "implement", "an", "approach", "to", "representation\n", "learning", "that", "overturns", "\"naive", "realism\"", "in", "favor", "of", "an", "analytical", "approach", "of\n", "Russell", "and", "Koenderink.", "NeRFs", "cannot", "capture", "the", "physical", "scenes,", "but\n", "combined", "with", "Diffusion", "Models", "they", "can\n"], "abstract": "We describe a first step towards learning general-purpose visual representations of physical scenes using only image prediction as a training criterion. To do so, we first define \"physical scene\" and show that, even though different agents may maintain different representations of the same scene, the underlying physical scene that can be inferred is unique. Then, we show that NeRFs cannot represent the physical scene, as they lack extrapolation mechanisms. Those, however, could be provided by Diffusion Models, at least in theory. To test this hypothesis empirically, NeRFs can be combined with Diffusion Models, a process we refer to as NeRF Diffusion, used as unsupervised representations of the physical scene. Our analysis is limited to visual data, without external grounding mechanisms that can be provided by independent sensory modalities.", "link": "https://arxiv.org/abs/2306.03727"}, {"id": "2306.03179", "date": "Mon, 5 Jun 2023 18:40:35 GMT", "title": "Fair Patient Model: Mitigating Bias in the Patient Representation\n Learned from the Electronic Health Records\n", "authors": ["Sonish Sivarajkumar", "Yufei Huang", "Yanshan Wang\n"], "categories": ["cs.LG", "cs.AI", "cs.CY\n"], "abstract": "Objective: To pre-train fair and unbiased patient representations from Electronic Health Records (EHRs) using a novel weighted loss function that reduces bias and improves fairness in deep representation learning models. Methods: We defined a new loss function, called weighted loss function, in the deep representation learning model to balance the importance of different groups of patients and features. We applied the proposed model, called Fair Patient Model (FPM), to a sample of 34,739 patients from the MIMIC-III dataset and learned patient representations for four clinical outcome prediction tasks. Results: FPM outperformed the baseline models in terms of three fairness metrics: demographic parity, equality of opportunity difference, and equalized odds ratio. FPM also achieved comparable predictive performance with the baselines, with an average accuracy of 0.7912. Feature analysis revealed that FPM captured more information from clinical features than the baselines. Conclusion: FPM is a novel method to pre-train fair and unbiased patient representations from EHR data using a weighted loss function. The learned representations can be used for various downstream tasks in healthcare and can be extended to other domains where bias and fairness are important.", "link": "https://arxiv.org/abs/2306.03179"}, {"id": "2306.03221", "date": "Mon, 5 Jun 2023 20:11:30 GMT", "title": "Structural Re-weighting Improves Graph Domain Adaptation\n", "authors": ["Shikun Liu", "Tianchun Li", "Yongbin Feng", "Nhan Tran", "Han Zhao", "Qiu Qiang,\n Pan Li\n"], "categories": ["cs.LG", "cs.AI", "cs.SI\nComments:", "ICML", "2023,", "Codes:", "https://github.com/Graph-COM/StruRW\n"], "abstract": "In many real-world applications, graph-structured data used for training and testing have differences in distribution, such as in high energy physics (HEP) where simulation data used for training may not match real experiments. Graph domain adaptation (GDA) is a method used to address these differences. However, current GDA primarily works by aligning the distributions of node representations output by a single graph neural network encoder shared across the training and testing domains, which may often yield sub-optimal solutions. This work examines different impacts of distribution shifts caused by either graph structure or node attributes and identifies a new type of shift, named conditional structure shift (CSS), which current GDA approaches are provably sub-optimal to deal with. A novel approach, called structural reweighting (StruRW), is proposed to address this issue and is tested on synthetic graphs, four benchmark datasets, and a new application in HEP. StruRW has shown significant performance improvement over the baselines in the settings with large graph structure shifts, and reasonable performance improvement when node attribute shift dominates.", "link": "https://arxiv.org/abs/2306.03221"}, {"id": "2306.03241", "date": "Mon, 5 Jun 2023 20:51:44 GMT", "title": "Understanding the Effectiveness of Early Weight Averaging for Training\n Large Language Models\n", "authors": ["Sunny Sanyal", "Jean Kaddour", "Abhishek Kumar and Sujay Sanghavi\n"], "categories": ["cs.LG", "cs.AI", "cs.CL\nComments:", "17", "pages,", "12", "figures,", "under", "review\n"], "abstract": "Training LLMs is expensive, and recent evidence indicates training all the way to convergence is inefficient. In this paper, we investigate the ability of a simple idea, checkpoint averaging along the trajectory of a training run to improve the quality of models before they have converged. This approach incurs no extra cost during training or inference. Specifically, we analyze the training trajectories of Pythia LLMs with 1 to 12 billion parameters and demonstrate that, particularly during the early to mid stages of training, this idea accelerates convergence and improves both test and zero-shot generalization. Loss spikes are a well recognized problem in LLM training; in our analysis we encountered two instances of this in the underlying trajectories, and both instances were mitigated by our averaging. For a 6.9B parameter LLM, for example, our early weight averaging recipe can save upto 4200 hours of GPU time, which corresponds to significant savings in cloud compute costs.", "link": "https://arxiv.org/abs/2306.03241"}, {"id": "2306.03341", "date": "Tue, 6 Jun 2023 01:26:53 GMT", "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language\n Model\n", "authors": ["Kenneth Li", "Oam Patel", "Fernanda Vi\\'egas", "Hanspeter Pfister", "Martin\n Wattenberg\n"], "categories": ["cs.LG", "cs.AI", "cs.CL\nComments:", "code:", "https://github.com/likenneth/honest_llama\n"], "abstract": "We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.", "link": "https://arxiv.org/abs/2306.03341"}, {"id": "2306.03360", "date": "Tue, 6 Jun 2023 02:24:41 GMT", "title": "Vid2Act: Activate Offline Videos for Visual RL\n", "authors": ["Pan Minting", "Zheng Yitao", "Wang Yunbo", "Yang Xiaokang\n"], "categories": ["cs.LG", "cs.AI", "cs.RO\n"], "abstract": "Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring the most useful source knowledge to facilitate dynamics learning, and (ii) learning to replay the most relevant source actions to guide the target policy. We demonstrate the advantages of Vid2Act over the action-free visual RL pretraining method in both Meta-World and DeepMind Control Suite.", "link": "https://arxiv.org/abs/2306.03360"}, {"id": "2306.03406", "date": "Tue, 6 Jun 2023 04:57:39 GMT", "title": "Deep neural networks architectures from the perspective of manifold\n learning\n", "authors": ["German Magai\n"], "categories": ["cs.LG", "cs.AI", "cs.CV", "math.AT\nComments:", "11", "pages,", "12", "figures,", "PRAI2023.", "arXiv", "admin", "note:", "substantial", "text\n", "overlap", "with", "arXiv:2204.08624\n"], "abstract": "Despite significant advances in the field of deep learning in ap-plications to various areas, an explanation of the learning pro-cess of neural network models remains an important open ques-tion. The purpose of this paper is a comprehensive comparison and description of neural network architectures in terms of ge-ometry and topology. We focus on the internal representation of neural networks and on the dynamics of changes in the topology and geometry of a data manifold on different layers. In this paper, we use the concepts of topological data analysis (TDA) and persistent homological fractal dimension. We present a wide range of experiments with various datasets and configurations of convolutional neural network (CNNs) architectures and Transformers in CV and NLP tasks. Our work is a contribution to the development of the important field of explainable and interpretable AI within the framework of geometrical deep learning.", "link": "https://arxiv.org/abs/2306.03406"}, {"id": "2306.03438", "date": "Tue, 6 Jun 2023 06:35:27 GMT", "title": "Large Language Models of Code Fail at Completing Code with Potential\n Bugs\n", "authors": ["Tuan Dinh", "Jinman Zhao", "Samson Tan", "Renato Negrinho", "Leonard Lausen,\n Sheng Zha", "George Karypis\n"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SE\nComments:", "25", "pages\n"], "abstract": "Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a large gap in post-mitigation performance.", "link": "https://arxiv.org/abs/2306.03438"}, {"id": "2306.03515", "date": "Tue, 6 Jun 2023 09:01:17 GMT", "title": "Logic Diffusion for Knowledge Graph Reasoning\n", "authors": ["Xiaoying Xie", "Biao Gong", "Yiliang Lv", "Zhen Han", "Guoshuai Zhao", "Xueming\n Qian\n"], "categories": ["cs.LG", "cs.AI", "cs.LO\nComments:", "10", "pages,", "6", "figures\n"], "abstract": "Most recent works focus on answering first order logical queries to explore the knowledge graph reasoning via multi-hop logic predictions. However, existing reasoning models are limited by the circumscribed logical paradigms of training samples, which leads to a weak generalization of unseen logic. To address these issues, we propose a plug-in module called Logic Diffusion (LoD) to discover unseen queries from surroundings and achieves dynamical equilibrium between different kinds of patterns. The basic idea of LoD is relation diffusion and sampling sub-logic by random walking as well as a special training mechanism called gradient adaption. Besides, LoD is accompanied by a novel loss function to further achieve the robust logical diffusion when facing noisy data in training or testing sets. Extensive experiments on four public datasets demonstrate the superiority of mainstream knowledge graph reasoning models with LoD over state-of-the-art. Moreover, our ablation study proves the general effectiveness of LoD on the noise-rich knowledge graph.", "link": "https://arxiv.org/abs/2306.03515"}, {"id": "2306.03530", "date": "Tue, 6 Jun 2023 09:26:43 GMT", "title": "BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for\n Continuous Control\n", "authors": ["Jonas Eschmann", "Dario Albani", "Giuseppe Loianno\n"], "categories": ["cs.LG", "cs.AI", "cs.RO\nComments:", "Project", "page:", "https://backprop.tools\n"], "abstract": "Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simulation environments, BackpropTools can solve popular RL problems like the Pendulum-v1 swing-up about 7 to 15 times faster in terms of wall-clock training time compared to other popular RL frameworks when using TD3. We also provide a low-overhead and parallelized interface to the MuJoCo simulator, showing that our PPO implementation achieves state of the art returns in the Ant-v4 environment while achieving a 25 to 30 percent faster wall-clock training time. Finally, we also benchmark the policy inference on a diverse set of microcontrollers and show that in most cases our optimized inference implementation is much faster than even the manufacturer's DSP libraries. To the best of our knowledge, BackpropTools enables the first-ever demonstration of training a deep RL algorithm directly on a microcontroller, giving rise to the field of Tiny Reinforcement Learning (TinyRL). Project page: https://backprop.tools", "link": "https://arxiv.org/abs/2306.03530"}, {"id": "2306.03725", "date": "Tue, 6 Jun 2023 14:44:52 GMT", "title": "Towards Memory-Efficient Training for Extremely Large Output Spaces --\n Learning with 500k Labels on a Single Commodity GPU\n", "authors": ["Erik Schultheis", "Rohit Babbar\n"], "categories": ["cs.LG", "cs.AI", "cs.DC\n"], "abstract": "In classification problems with large output spaces (up to millions of labels), the last layer can require an enormous amount of memory. Using sparse connectivity would drastically reduce the memory requirements, but as we show below, it can result in much diminished predictive performance of the model. Fortunately, we found that this can be mitigated by introducing a penultimate layer of intermediate size. We further demonstrate that one can constrain the connectivity of the sparse layer to be uniform, in the sense that each output neuron will have the exact same number of incoming connections. This allows for efficient implementations of sparse matrix multiplication and connection redistribution on GPU hardware. Via a custom CUDA implementation, we show that the proposed approach can scale to datasets with 670,000 labels on a single commodity GPU with only 4GB memory.", "link": "https://arxiv.org/abs/2306.03725"}, {"id": "2306.03739", "date": "Tue, 6 Jun 2023 14:56:47 GMT", "title": "Learning to Do or Learning While Doing: Reinforcement Learning and\n Bayesian Optimisation for Online Continuous Tuning\n", "authors": ["Jan Kaiser", "Chenran Xu", "Annika Eichler", "Andrea Santamaria Garcia,\n Oliver Stein", "Erik Br\\\"undermann", "Willi Kuropka", "Hannes Dinter", "Frank Mayet,\n Thomas Vinatier", "Florian Burkart", "Holger Schlarb\n"], "categories": ["cs.LG", "cs.AI", "physics.acc-ph\nComments:", "17", "pages,", "8", "figures,", "2", "tables\n"], "abstract": "Online tuning of real-world plants is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods, such as Reinforcement Learning-trained Optimisation (RLO) and Bayesian optimisation (BO), hold great promise for achieving outstanding plant performance and reducing tuning times. Which algorithm to choose in different scenarios, however, remains an open question. Here we present a comparative study using a routine task in a real particle accelerator as an example, showing that RLO generally outperforms BO, but is not always the best choice. Based on the study's results, we provide a clear set of criteria to guide the choice of algorithm for a given tuning task. These can ease the adoption of learning-based autonomous tuning solutions to the operation of complex real-world plants, ultimately improving the availability and pushing the limits of operability of these facilities, thereby enabling scientific and engineering advancements.", "link": "https://arxiv.org/abs/2306.03739"}, {"id": "2306.03830", "date": "Tue, 6 Jun 2023 16:15:56 GMT", "title": "Inductive Bias for Emergent Communication in a Continuous Setting\n", "authors": ["John Isak Fjellvang Villanger and Troels Arnfred Bojesen\n"], "categories": ["cs.LG", "cs.AI", "cs.MA\nComments:", "NIPS", "2023", "Preprint.", "12", "pages,", "5", "figures,", "3", "tables\nACM-class:", "I.2.11\n"], "abstract": "We study emergent communication in a multi-agent reinforcement learning setting, where the agents solve cooperative tasks and have access to a communication channel. The communication channel may consist of either discrete symbols or continuous variables. We introduce an inductive bias to aid with the emergence of good communication protocols for continuous messages, and we look at the effect this type of inductive bias has for continuous and discrete messages in itself or when used in combination with reinforcement learning. We demonstrate that this type of inductive bias has a beneficial effect on the communication protocols learnt in two toy environments, Negotiation and Sequence Guess.", "link": "https://arxiv.org/abs/2306.03830"}, {"id": "2306.03757", "date": "Tue, 6 Jun 2023 15:17:34 GMT", "title": "Exploring the effects of robotic design on learning and neural control\n", "authors": ["Joshua Paul Powers\n"], "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.NE\nComments:", "arXiv", "admin", "note:", "text", "overlap", "with", "arXiv:2008.06397\n"], "abstract": "The ongoing deep learning revolution has allowed computers to outclass humans in various games and perceive features imperceptible to humans during classification tasks. Current machine learning techniques have clearly distinguished themselves in specialized tasks. However, we have yet to see robots capable of performing multiple tasks at an expert level. Most work in this field is focused on the development of more sophisticated learning algorithms for a robot's controller given a largely static and presupposed robotic design. By focusing on the development of robotic bodies, rather than neural controllers, I have discovered that robots can be designed such that they overcome many of the current pitfalls encountered by neural controllers in multitask settings. Through this discovery, I also present novel metrics to explicitly measure the learning ability of a robotic design and its resistance to common problems such as catastrophic interference. Traditionally, the physical robot design requires human engineers to plan every aspect of the system, which is expensive and often relies on human intuition. In contrast, within the field of evolutionary robotics, evolutionary algorithms are used to automatically create optimized designs, however, such designs are often still limited in their ability to perform in a multitask setting. The metrics created and presented here give a novel path to automated design that allow evolved robots to synergize with their controller to improve the computational efficiency of their learning while overcoming catastrophic interference. Overall, this dissertation intimates the ability to automatically design robots that are more general purpose than current robots and that can perform various tasks while requiring less computation.", "link": "https://arxiv.org/abs/2306.03757"}, {"id": "2306.03112", "date": "Mon, 5 Jun 2023 08:38:30 GMT", "title": "Synthesizing Affective Neurophysiological Signals Using Generative\n Models: A Review Paper\n", "authors": ["Alireza F. Nia", "Vanessa Tang", "Gonzalo Maso Talou", "Mark Billinghurst\n"], "categories": ["cs.HC", "cs.AI", "cs.LG", "q-bio.NC\n"], "abstract": "The integration of emotional intelligence in machines is an important step in advancing human-computer interaction. This demands the development of reliable end-to-end emotion recognition systems. However, the scarcity of public affective datasets presents a challenge. In this literature review, we emphasize the use of generative models to address this issue in neurophysiological signals, particularly Electroencephalogram (EEG) and Functional Near-Infrared Spectroscopy (fNIRS). We provide a comprehensive analysis of different generative models used in the field, examining their input formulation, deployment strategies, and methodologies for evaluating the quality of synthesized data. This review serves as a comprehensive overview, offering insights into the advantages, challenges, and promising future directions in the application of generative models in emotion recognition systems. Through this review, we aim to facilitate the progression of neurophysiological data augmentation, thereby supporting the development of more efficient and reliable emotion recognition systems.", "link": "https://arxiv.org/abs/2306.03112"}, {"id": "2306.03481", "date": "Tue, 6 Jun 2023 08:06:43 GMT", "title": "Transition role of entangled data in quantum machine learning\n", "authors": ["Xinbiao Wang", "Yuxuan Du", "Zhuozhuo Tu", "Yong Luo", "Xiao Yuan", "Dacheng Tao\n"], "categories": ["quant-ph", "cs.AI", "cs.IT", "cs.LG", "math.IT\n"], "abstract": "Entanglement serves as the resource to empower quantum computing. Recent progress has highlighted its positive impact on learning quantum dynamics, wherein the integration of entanglement into quantum operations or measurements of quantum machine learning (QML) models leads to substantial reductions in training data size, surpassing a specified prediction error threshold. However, an analytical understanding of how the entanglement degree in data affects model performance remains elusive. In this study, we address this knowledge gap by establishing a quantum no-free-lunch (NFL) theorem for learning quantum dynamics using entangled data. Contrary to previous findings, we prove that the impact of entangled data on prediction error exhibits a dual effect, depending on the number of permitted measurements. With a sufficient number of measurements, increasing the entanglement of training data consistently reduces the prediction error or decreases the required size of the training data to achieve the same prediction error. Conversely, when few measurements are allowed, employing highly entangled data could lead to an increased prediction error. The achieved results provide critical guidance for designing advanced QML protocols, especially for those tailored for execution on early-stage quantum computers with limited access to quantum resources.", "link": "https://arxiv.org/abs/2306.03481"}, {"id": "2306.03580", "date": "Tue, 6 Jun 2023 10:53:26 GMT", "title": "L-C2ST: Local Diagnostics for Posterior Approximations in\n Simulation-Based Inference\n", "authors": ["Julia Linhart", "Alexandre Gramfort", "Pedro L. C. Rodrigues\n"], "categories": ["stat.ML", "cs.AI", "cs.LG", "q-bio.NC\nComments:", "20", "pages,", "4", "figures,", "7", "appendices,", "in", "proceedings\n"], "abstract": "Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficient. On standard SBI benchmarks, L-C2ST provides comparable results to C2ST and outperforms alternative local approaches such as coverage tests based on highest predictive density (HPD). We further highlight the importance of local evaluation and the benefit of interpretability of L-C2ST on a challenging application from computational neuroscience.", "link": "https://arxiv.org/abs/2306.03580"}, {"id": "2306.03763", "date": "Sun, 28 May 2023 21:11:59 GMT", "title": "ChatGPT Informed Graph Neural Network for Stock Movement Prediction\n", "authors": ["Zihan Chen", "Lei Nico Zheng", "Cheng Lu", "Jialu Yuan", "Di Zhu\n"], "categories": ["q-fin.ST", "cs.AI", "cs.CL", "cs.LG", "q-fin.CP\nComments:", "Under", "Review.", "10", "pages,", "2", "figures\nACM-class:", "I.2.7;", "J.1\n"], "abstract": "ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and underscores its promising implications for the financial sector.", "link": "https://arxiv.org/abs/2306.03763"}, {"id": "2306.03812", "date": "Tue, 6 Jun 2023 15:58:09 GMT", "title": "Computation with Sequences in the Brain\n", "authors": ["Max Dabagia", "Christos H. Papadimitriou", "Santosh S. Vempala\n"], "categories": ["cs.NE", "cs.AI", "cs.LG", "q-bio.NC\nComments:", "24", "pages,", "12", "figures\n"], "abstract": "Even as machine learning exceeds human-level performance on many applications, the generality, robustness, and rapidity of the brain's learning capabilities remain unmatched. How cognition arises from neural activity is a central open question in neuroscience, inextricable from the study of intelligence itself. A simple formal model of neural activity was proposed in Papadimitriou [2020] and has been subsequently shown, through both mathematical proofs and simulations, to be capable of implementing certain simple cognitive operations via the creation and manipulation of assemblies of neurons. However, many intelligent behaviors rely on the ability to recognize, store, and manipulate temporal sequences of stimuli (planning, language, navigation, to list a few). Here we show that, in the same model, time can be captured naturally as precedence through synaptic weights and plasticity, and, as a result, a range of computations on sequences of assemblies can be carried out. In particular, repeated presentation of a sequence of stimuli leads to the memorization of the sequence through corresponding neural assemblies: upon future presentation of any stimulus in the sequence, the corresponding assembly and its subsequent ones will be activated, one after the other, until the end of the sequence. Finally, we show that any finite state machine can be learned in a similar way, through the presentation of appropriate patterns of sequences. Through an extension of this mechanism, the model can be shown to be capable of universal computation. We support our analysis with a number of experiments to probe the limits of learning in this model in key ways. Taken together, these results provide a concrete hypothesis for the basis of the brain's remarkable abilities to compute and learn, with sequences playing a vital role.", "link": "https://arxiv.org/abs/2306.03812"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.link + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
