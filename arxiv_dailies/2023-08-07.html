<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2308.01926", "Date": "Wed, 02 Aug 2023 09:40:19 ", "Title": "Are Easy Data Easy (for K-Means)", "Authors": ["Mieczys{\\l}aw A. K{\\l}opotek"], "Categories": "cs.LG", "Comments": ["12 figures", "19 tables"]}, "abstract": "This paper investigates the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm. The concept of well-separatedness used here is derived directly from the common definition of clusters, which imposes an interplay between the requirements of within-cluster-homogenicity and between-clusters-diversity. Conditions are derived for a special case of well-separated clusters such that the global minimum of $k$-means cost function coincides with the well-separatedness. An experimental investigation is performed to find out whether or no various brands of $k$-means are actually capable of discovering well separated clusters. It turns out that they are not. A new algorithm is proposed that is a variation of $k$-means++ via repeated {sub}sampling when choosing a seed. The new algorithm outperforms four other algorithms from $k$-means family on the task.", "url": "https://arxiv.org/abs/2308.01926"}, {"metadata": {"arXiv": "2308.01946", "Date": "Thu, 03 Aug 2023 08:14:07 ", "Title": "Experimental Results regarding multiple Machine Learning via Quaternions", "Authors": ["Tianlei Zhu", "Renzhe Zhu"], "Categories": "cs.LG"}, "abstract": "This paper presents an experimental study on the application of quaternions in several machine learning algorithms. Quaternion is a mathematical representation of rotation in three-dimensional space, which can be used to represent complex data transformations. In this study, we explore the use of quaternions to represent and classify rotation data, using randomly generated quaternion data and corresponding labels, converting quaternions to rotation matrices, and using them as input features. Based on quaternions and multiple machine learning algorithms, it has shown higher accuracy and significantly improved performance in prediction tasks. Overall, this study provides an empirical basis for exploiting quaternions for machine learning tasks.", "url": "https://arxiv.org/abs/2308.01946"}, {"metadata": {"arXiv": "2308.01954", "Date": "Thu, 03 Aug 2023 13:00:50 ", "Title": "Bringing Chemistry to Scale: Loss Weight Adjustment for Multivariate Regression in Deep Learning of Thermochemical Processes", "Authors": ["Franz M. Rohrhofer", "Stefan Posch", "Clemens G\\\"o{\\ss}nitzer", "Jos\\'e M. Garc\\'ia-Oliver", "Bernhard C. Geiger"], "Categories": "cs.LG cs.CE", "Comments": ["8 pages. Part of Scientific Computing 2023 Conference Proceedings (ISBN e-Book: 978-3-903318-20-5)"]}, "abstract": "Flamelet models are widely used in computational fluid dynamics to simulate thermochemical processes in turbulent combustion. These models typically employ memory-expensive lookup tables that are predetermined and represent the combustion process to be simulated. Artificial neural networks (ANNs) offer a deep learning approach that can store this tabular data using a small number of network weights, potentially reducing the memory demands of complex simulations by orders of magnitude. However, ANNs with standard training losses often struggle with underrepresented targets in multivariate regression tasks, e.g., when learning minor species mass fractions as part of lookup tables. This paper seeks to improve the accuracy of an ANN when learning multiple species mass fractions of a hydrogen (\\ce{H2}) combustion lookup table. We assess a simple, yet effective loss weight adjustment that outperforms the standard mean-squared error optimization and enables accurate learning of all species mass fractions, even of minor species where the standard optimization completely fails. Furthermore, we find that the loss weight adjustment leads to more balanced gradients in the network training, which explains its effectiveness.", "url": "https://arxiv.org/abs/2308.01954"}, {"metadata": {"arXiv": "2308.02001", "Date": "Thu, 03 Aug 2023 19:31:15 ", "Title": "Memory capacity of two layer neural networks with smooth activations", "Authors": ["Liam Madden and Christos Thrampoulidis"], "Categories": "cs.LG math.OC", "Comments": ["24 pages"], "MSC-class": "15A03, 26B10"}, "abstract": "Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for extending to deeper models and other architectures.", "url": "https://arxiv.org/abs/2308.02001"}, {"metadata": {"arXiv": "2308.02029", "Date": "Thu, 03 Aug 2023 20:45:11 ", "Title": "Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection", "Authors": ["Hemn Barzan Abdalla", "Awder Ahmed", "Guoquan Li", "Nasser Mustafa", "Abdur Rashid Sangi"], "Categories": "cs.LG"}, "abstract": "Thalassemia is a heritable blood disorder which is the outcome of a genetic defect causing lack of production of hemoglobin polypeptide chains. However, there is less understanding of the precise frequency as well as sharing in these areas. Knowing about the frequency of thalassemia occurrence and dependable mutations is thus a significant step in preventing, controlling, and treatment planning. Here, Political Tangent Search Optimizer based Transfer Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input data obtained from a particular dataset is normalized in the data normalization stage. Quantile normalization is utilized in the data normalization stage, and the data are then passed to the feature fusion phase, in which Weighted Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data augmentation is performed using the oversampling method to increase data dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a convolutional neural network (CNN) is utilized with hyperparameters from a trained model such as Xception. TL is tuned by PTSO, and the training algorithm PTSO is presented by merging of Political Optimizer (PO) and Tangent Search Algorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, and f-measure values of about 94.3%, 96.1%, and 95.2%, respectively.", "url": "https://arxiv.org/abs/2308.02029"}, {"metadata": {"arXiv": "2308.02040", "Date": "Wed, 02 Aug 2023 07:23:50 ", "Title": "Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients", "Authors": ["Ngo Nghi Truyen Huynh (INRAE)", "Pierre-Andr\\'e Garambois (INRAE)", "Fran\\c{c}ois Colleoni (INRAE)", "Benjamin Renard (INRAE)", "H\\'el\\`ene Roux (IMFT)", "Julie Demargne (HYDRIS)", "Pierre Javelle (INRAE)"], "Categories": "cs.LG eess.SP"}, "abstract": "Estimating spatially distributed hydrological parameters in ungauged catchments poses a challenging regionalization problem and requires imposing spatial constraints given the sparsity of discharge data. A possible approach is to search for a transfer function that quantitatively relates physical descriptors to conceptual model parameters. This paper introduces a Hybrid Data Assimilation and Parameter Regionalization (HDA-PR) approach incorporating learnable regionalization mappings, based on either multivariate regressions or neural networks, into a differentiable hydrological model. It enables the exploitation of heterogeneous datasets across extensive spatio-temporal computational domains within a high-dimensional regionalization context, using accurate adjoint-based gradients. The inverse problem is tackled with a multi-gauge calibration cost function accounting for information from multiple observation sites. HDA-PR was tested on high-resolution, hourly and kilometric regional modeling of two flash-flood-prone areas located in the South of France. In both study areas, the median Nash-Sutcliffe efficiency (NSE) scores ranged from 0.52 to 0.78 at pseudo-ungauged sites over calibration and validation periods. These results highlight a strong regionalization performance of HDA-PR, improving NSE by up to 0.57 compared to the baseline model calibrated with lumped parameters, and achieving a performance comparable to the reference solution obtained with local uniform calibration (median NSE from 0.59 to 0.79). Multiple evaluation metrics based on flood-oriented hydrological signatures are also employed to assess the accuracy and robustness of the approach. The regionalization method is amenable to state-parameter correction from multi-source data over a range of time scales needed for operational data assimilation, and it is adaptable to other differentiable geophysical models.", "url": "https://arxiv.org/abs/2308.02040"}, {"metadata": {"arXiv": "2308.02050", "Date": "Thu, 03 Aug 2023 21:08:16 ", "Title": "FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method", "Authors": ["Morteza Fayazi", "Morteza Tavakoli Taba", "Amirata Tabatabavakili", "Ehsan Afshari", "Ronald Dreslinski"], "Categories": "cs.LG", "Comments": ["8 pages", "13 figures", "8 tables", "accepted on International Conference on Computer-Aided Design (ICCAD)"]}, "abstract": "Automatic synthesis of analog and Radio Frequency (RF) circuits is a trending approach that requires an efficient circuit modeling method. This is due to the expensive cost of running a large number of simulations at each synthesis cycle. Artificial intelligence methods are promising approaches for circuit modeling due to their speed and relative accuracy. However, existing approaches require a large amount of training data, which is still collected using simulation runs. In addition, such approaches collect a whole separate dataset for each circuit topology even if a single element is added or removed. These matters are only exacerbated by the need for post-layout modeling simulations, which take even longer. To alleviate these drawbacks, in this paper, we present FuNToM, a functional modeling method for RF circuits. FuNToM leverages the two-port analysis method for modeling multiple topologies using a single main dataset and multiple small datasets. It also leverages neural networks which have shown promising results in predicting the behavior of circuits. Our results show that for multiple RF circuits, in comparison to the state-of-the-art works, while maintaining the same accuracy, the required training data is reduced by 2.8x - 10.9x. In addition, FuNToM needs 176.8x - 188.6x less time for collecting the training set in post-layout modeling.", "url": "https://arxiv.org/abs/2308.02050"}, {"metadata": {"arXiv": "2308.02051", "Date": "Thu, 03 Aug 2023 21:09:59 ", "Title": "A Graphical Approach to Document Layout Analysis", "Authors": ["Jilin Wang", "Michael Krumdick", "Baojia Tong", "Hamima Halim", "Maxim Sokolov", "Vadym Barda", "Delphine Vendryes", "and Chris Tanner"], "Categories": "cs.LG", "Comments": ["ICDAR 2023"]}, "abstract": "Document layout analysis (DLA) is the task of detecting the distinct, semantic content within a document and correctly classifying these items into an appropriate category (e.g., text, title, figure). DLA pipelines enable users to convert documents into structured machine-readable formats that can then be used for many useful downstream tasks. Most existing state-of-the-art (SOTA) DLA models represent documents as images, discarding the rich metadata available in electronically generated PDFs. Directly leveraging this metadata, we represent each PDF page as a structured graph and frame the DLA problem as a graph segmentation and classification problem. We introduce the Graph-based Layout Analysis Model (GLAM), a lightweight graph neural network competitive with SOTA models on two challenging DLA datasets - while being an order of magnitude smaller than existing models. In particular, the 4-million parameter GLAM model outperforms the leading 140M+ parameter computer vision-based model on 5 of the 11 classes on the DocLayNet dataset. A simple ensemble of these two models achieves a new state-of-the-art on DocLayNet, increasing mAP from 76.8 to 80.8. Overall, GLAM is over 5 times more efficient than SOTA models, making GLAM a favorable engineering choice for DLA tasks.", "url": "https://arxiv.org/abs/2308.02051"}, {"metadata": {"arXiv": "2308.02081", "Date": "Thu, 03 Aug 2023 23:43:42 ", "Title": "Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare", "Authors": ["Eran Tal"], "Categories": "cs.LG cs.CY stat.ME", "Comments": ["Proceedings of the 2023 AAAI/ACM Conference on AI", "Ethics", "and Society (AIES23)"], "DOI": "10.1145/3600211.3604678"}, "abstract": "Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology - the science of measurement - suggests ways of counteracting target specification bias and avoiding its harmful consequences.", "url": "https://arxiv.org/abs/2308.02081"}, {"metadata": {"arXiv": "2308.02123", "Date": "Fri, 04 Aug 2023 03:51:38 ", "Title": "Eva: A General Vectorized Approximation Framework for Second-order Optimization", "Authors": ["Lin Zhang", "Shaohuai Shi", "Bo Li"], "Categories": "cs.LG math.OC", "Comments": ["Extension of ICLR2022 Practical second-order optimization with Kronecker-vectorized approximation"]}, "abstract": "Second-order optimization algorithms exhibit excellent convergence properties for training deep learning models, but often incur significant computation and memory overheads. This can result in lower training efficiency than the first-order counterparts such as stochastic gradient descent (SGD). In this work, we present a memory- and time-efficient second-order algorithm named Eva with two novel techniques: 1) we construct the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data to reduce memory consumption, and 2) we derive an efficient update formula without explicitly computing the inverse of matrices using the Sherman-Morrison formula. We further extend Eva to a general vectorized approximation framework to improve the compute and memory efficiency of two existing second-order algorithms (FOOF and Shampoo) without affecting their convergence performance. Extensive experimental results on different models and datasets show that Eva reduces the end-to-end training time up to 2.05x and 2.42x compared to first-order SGD and second-order algorithms (K-FAC and Shampoo), respectively.", "url": "https://arxiv.org/abs/2308.02123"}, {"metadata": {"arXiv": "2308.02157", "Date": "Fri, 04 Aug 2023 06:30:40 ", "Title": "Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling", "Authors": ["Qinsheng Zhang and Jiaming Song and Yongxin Chen"], "Categories": "cs.LG"}, "abstract": "Efficient differential equation solvers have significantly reduced the sampling time of diffusion models (DMs) while retaining high sampling quality. Among these solvers, exponential integrators (EI) have gained prominence by demonstrating state-of-the-art performance. However, existing high-order EI-based sampling algorithms rely on degenerate EI solvers, resulting in inferior error bounds and reduced accuracy in contrast to the theoretically anticipated results under optimal settings. This situation makes the sampling quality extremely vulnerable to seemingly innocuous design choices such as timestep schedules. For example, an inefficient timestep scheduler might necessitate twice the number of steps to achieve a quality comparable to that obtained through carefully optimized timesteps. To address this issue, we reevaluate the design of high-order differential solvers for DMs. Through a thorough order analysis, we reveal that the degeneration of existing high-order EI solvers can be attributed to the absence of essential order conditions. By reformulating the differential equations in DMs and capitalizing on the theory of exponential integrators, we propose refined EI solvers that fulfill all the order conditions, which we designate as Refined Exponential Solver (RES). Utilizing these improved solvers, RES exhibits more favorable error bounds theoretically and achieves superior sampling efficiency and stability in practical applications. For instance, a simple switch from the single-step DPM-Solver++ to our order-satisfied RES solver when Number of Function Evaluations (NFE) $=9$, results in a reduction of numerical defects by $25.2\\%$ and FID improvement of $25.4\\%$ (16.77 vs 12.51) on a pre-trained ImageNet diffusion model.", "url": "https://arxiv.org/abs/2308.02157"}, {"metadata": {"arXiv": "2308.02165", "Date": "Fri, 04 Aug 2023 06:53:22 ", "Title": "Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling", "Authors": ["Teerachote Pakornchote", "Natthaphon Choomphon-anomakhun", "Sorrjit Arrerut", "Chayanon Atthapak", "Sakarn Khamkaeo", "Thiparat Chotibut", "Thiti Bovornratanaraks"], "Categories": "cs.LG cond-mat.mtrl-sci cond-mat.stat-mech physics.comp-ph", "Comments": ["11 pages", "4 figures"], "MSC-class": "68T07"}, "abstract": "The crystal diffusion variational autoencoder (CDVAE) is a machine learning model that leverages score matching to generate realistic crystal structures that preserve crystal symmetry. In this study, we leverage novel diffusion probabilistic (DP) models to denoise atomic coordinates rather than adopting the standard score matching approach in CDVAE. Our proposed DP-CDVAE model can reconstruct and generate crystal structures whose qualities are statistically comparable to those of the original CDVAE. Furthermore, notably, when comparing the carbon structures generated by the DP-CDVAE model with relaxed structures obtained from density functional theory calculations, we find that the DP-CDVAE generated structures are remarkably closer to their respective ground states. The energy differences between these structures and the true ground states are, on average, 68.1 meV/atom lower than those generated by the original CDVAE. This significant improvement in the energy accuracy highlights the effectiveness of the DP-CDVAE model in generating crystal structures that better represent their ground-state configurations.", "url": "https://arxiv.org/abs/2308.02165"}, {"metadata": {"arXiv": "2308.02347", "Date": "Fri, 04 Aug 2023 14:21:02 ", "Title": "Stability and Generalization of Hypergraph Collaborative Networks", "Authors": ["Michael Ng and Hanrui Wu and Andy Yip"], "Categories": "cs.LG"}, "abstract": "Graph neural networks have been shown to be very effective in utilizing pairwise relationships across samples. Recently, there have been several successful proposals to generalize graph neural networks to hypergraph neural networks to exploit more complex relationships. In particular, the hypergraph collaborative networks yield superior results compared to other hypergraph neural networks for various semi-supervised learning tasks. The collaborative network can provide high quality vertex embeddings and hyperedge embeddings together by formulating them as a joint optimization problem and by using their consistency in reconstructing the given hypergraph. In this paper, we aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees. The analysis sheds light on the design of hypergraph filters in collaborative networks, for instance, how the data and hypergraph filters should be scaled to achieve uniform stability of the learning process. Some experimental results on real-world datasets are presented to illustrate the theory.", "url": "https://arxiv.org/abs/2308.02347"}, {"metadata": {"arXiv": "2308.02350", "Date": "Fri, 04 Aug 2023 14:37:12 ", "Title": "RobustMQ: Benchmarking Robustness of Quantized Models", "Authors": ["Yisong Xiao", "Aishan Liu", "Tianyuan Zhang", "Haotong Qin", "Jinyang Guo", "Xianglong Liu"], "Categories": "cs.LG cs.CR cs.CV", "Comments": ["15 pages", "7 figures"]}, "abstract": "Quantization has emerged as an essential technique for deploying deep neural networks (DNNs) on devices with limited resources. However, quantized models exhibit vulnerabilities when exposed to various noises in real-world applications. Despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example: (1) quantized models exhibit higher adversarial robustness than their floating-point counterparts, but are more vulnerable to natural corruptions and systematic noises; (2) in general, increasing the quantization bit-width results in a decrease in adversarial robustness, an increase in natural robustness, and an increase in systematic robustness; (3) among corruption methods, \\textit{impulse noise} and \\textit{glass blur} are the most harmful to quantized models, while \\textit{brightness} has the least impact; (4) among systematic noises, the \\textit{nearest neighbor interpolation} has the highest impact, while bilinear interpolation, cubic interpolation, and area interpolation are the three least harmful. Our research contributes to advancing the robust quantization of models and their deployment in real-world scenarios.", "url": "https://arxiv.org/abs/2308.02350"}, {"metadata": {"arXiv": "2308.02360", "Date": "Fri, 04 Aug 2023 14:52:22 ", "Title": "Intensity-free Integral-based Learning of Marked Temporal Point Processes", "Authors": ["Sishun Liu", "Ke Deng", "Jenny Zhang", "Yongli Ren"], "Categories": "cs.LG stat.ML"}, "abstract": "In the marked temporal point processes (MTPP), a core problem is to parameterize the conditional joint PDF (probability distribution function) $p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history. The majority of existing studies predefine intensity functions. Their utility is challenged by specifying the intensity function's proper form, which is critical to balance expressiveness and processing efficiency. Recently, there are studies moving away from predefining the intensity function -- one models $p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal point processes (TPPs), which do not consider marks. This study aims to develop high-fidelity $p^*(m,t)$ for discrete events where the event marks are either categorical or numeric in a multi-dimensional continuous space. We propose a solution framework IFIB (\\underline{I}ntensity-\\underline{f}ree \\underline{I}ntegral-\\underline{b}ased process) that models conditional joint PDF $p^*(m,t)$ directly without intensity functions. It remarkably simplifies the process to compel the essential mathematical restrictions. We show the desired properties of IFIB and the superior experimental results of IFIB on real-world and synthetic datasets. The code is available at \\url{https://github.com/StepinSilence/IFIB}.", "url": "https://arxiv.org/abs/2308.02360"}, {"metadata": {"arXiv": "2308.02391", "Date": "Fri, 04 Aug 2023 15:40:23 ", "Title": "Learning Optimal Admission Control in Partially Observable Queueing Networks", "Authors": ["Jonatha Anselmi", "Bruno Gaujal", "Louis-S\\'ebastien Rebuffi"], "Categories": "cs.LG math.OC"}, "abstract": "We present an efficient reinforcement learning algorithm that learns the optimal admission control policy in a partially observable queueing network. Specifically, only the arrival and departure times from the network are observable, and optimality refers to the average holding/rejection cost in infinite horizon. While reinforcement learning in Partially Observable Markov Decision Processes (POMDP) is prohibitively expensive in general, we show that our algorithm has a regret that only depends sub-linearly on the maximal number of jobs in the network, $S$. In particular, in contrast with existing regret analyses, our regret bound does not depend on the diameter of the underlying Markov Decision Process (MDP), which in most queueing systems is at least exponential in $S$. The novelty of our approach is to leverage Norton's equivalent theorem for closed product-form queueing networks and an efficient reinforcement learning algorithm for MDPs with the structure of birth-and-death processes.", "url": "https://arxiv.org/abs/2308.02391"}, {"metadata": {"arXiv": "2308.02442", "Date": "Fri, 04 Aug 2023 16:14:43 ", "Title": "Adaptive Preferential Attached kNN Graph With Distribution-Awareness", "Authors": ["Shaojie Min", "Ji Liu"], "Categories": "cs.LG cs.IR cs.SI"}, "abstract": "Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks, due to their simplicity and effectiveness. However, the conventional kNN graph's reliance on a fixed value of k can hinder its performance, especially in scenarios involving complex data distributions. Moreover, like other classification models, the presence of ambiguous samples along decision boundaries often presents a challenge, as they are more prone to incorrect classification. To address these issues, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with distribution-based graph construction. By incorporating distribution information, paNNG can significantly improve performance for ambiguous samples by \"pulling\" them towards their original classes and hence enable enhanced overall accuracy and generalization capability. Through rigorous evaluations on diverse benchmark datasets, paNNG outperforms state-of-the-art algorithms, showcasing its adaptability and efficacy across various real-world scenarios.", "url": "https://arxiv.org/abs/2308.02442"}, {"metadata": {"arXiv": "2308.02462", "Date": "Fri, 04 Aug 2023 17:00:34 ", "Title": "Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning", "Authors": ["Mahmoud Yaseen", "Dewen Yushu", "Peter German", "Xu Wu"], "Categories": "cs.LG stat.ML", "Comments": ["28 pages", "18 figures", "4 tables"]}, "abstract": "One predominant challenge in additive manufacturing (AM) is to achieve specific material properties by manipulating manufacturing process parameters during the runtime. Such manipulation tends to increase the computational load imposed on existing simulation tools employed in AM. The goal of the present work is to construct a fast and accurate reduced-order model (ROM) for an AM model developed within the Multiphysics Object-Oriented Simulation Environment (MOOSE) framework, ultimately reducing the time/cost of AM control and optimization processes. Our adoption of the operator learning (OL) approach enabled us to learn a family of differential equations produced by altering process variables in the laser's Gaussian point heat source. More specifically, we used the Fourier neural operator (FNO) and deep operator network (DeepONet) to develop ROMs for time-dependent responses. Furthermore, we benchmarked the performance of these OL methods against a conventional deep neural network (DNN)-based ROM. Ultimately, we found that OL methods offer comparable performance and, in terms of accuracy and generalizability, even outperform DNN at predicting scalar model responses. The DNN-based ROM afforded the fastest training time. Furthermore, all the ROMs were faster than the original MOOSE model yet still provided accurate predictions. FNO had a smaller mean prediction error than DeepONet, with a larger variance for time-dependent responses. Unlike DNN, both FNO and DeepONet were able to simulate time series data without the need for dimensionality reduction techniques. The present work can help facilitate the AM optimization process by enabling faster execution of simulation tools while still preserving evaluation accuracy.", "url": "https://arxiv.org/abs/2308.02462"}, {"metadata": {"arXiv": "2308.02465", "Date": "Fri, 04 Aug 2023 17:04:58 ", "Title": "BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks", "Authors": ["Marco Arazzi", "Mauro Conti", "Stefanos Koffas", "Marina Krcek", "Antonino Nocera", "Stjepan Picek and Jing Xu"], "Categories": "cs.LG cs.CR"}, "abstract": "Federated learning enables collaborative training of machine learning models by keeping the raw data of the involved workers private. One of its main objectives is to improve the models' privacy, security, and scalability. Vertical Federated Learning (VFL) offers an efficient cross-silo setting where a few parties collaboratively train a model without sharing the same features. In such a scenario, classification labels are commonly considered sensitive information held exclusively by one (active) party, while other (passive) parties use only their local information. Recent works have uncovered important flaws of VFL, leading to possible label inference attacks under the assumption that the attacker has some, even limited, background knowledge on the relation between labels and data. In this work, we are the first (to the best of our knowledge) to investigate label inference attacks on VFL using a zero-background knowledge strategy. To concretely formulate our proposal, we focus on Graph Neural Networks (GNNs) as a target model for the underlying VFL. In particular, we refer to node classification tasks, which are widely studied, and GNNs have shown promising results. Our proposed attack, BlindSage, provides impressive results in the experiments, achieving nearly 100% accuracy in most cases. Even when the attacker has no information about the used architecture or the number of classes, the accuracy remained above 85% in most instances. Finally, we observe that well-known defenses cannot mitigate our attack without affecting the model's performance on the main classification task.", "url": "https://arxiv.org/abs/2308.02465"}, {"metadata": {"arXiv": "2308.01936", "Date": "Wed, 02 Aug 2023 21:13:38 ", "Title": "Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?", "Authors": ["Thilini Wijesiriwardene and Amit Sheth and Valerie L. Shalin and Amitava Das"], "Categories": "cs.AI cs.CL", "Comments": ["12 pages 3 figures"]}, "abstract": "A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of Large Language Models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical analogies, syntactic analogies, semantic analogies, and pragmatic analogies. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. To address this, we discuss the necessity of employing Neuro-symbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction and guide the mapping process. Our knowledge-informed approach maintains the efficiency of LLMs while preserving the ability to explain analogies for pedagogical applications.", "url": "https://arxiv.org/abs/2308.01936"}, {"metadata": {"arXiv": "2308.02317", "Date": "Fri, 04 Aug 2023 13:34:51 ", "Title": "A Controllable Co-Creative Agent for Game System Design", "Authors": ["Rohan Agarwal", "Zhiyu Lin", "Mark Riedl"], "Categories": "cs.AI", "Comments": ["Thesis"]}, "abstract": "Many advancements have been made in procedural content generation for games, and with mixed-initiative co-creativity, have the potential for great benefits to human designers. However, co-creative systems for game generation are typically limited to specific genres, rules, or games, limiting the creativity of the designer. We seek to model games abstractly enough to apply to any genre, focusing on designing game systems and mechanics, and create a controllable, co-creative agent that can collaborate on these designs. We present a model of games using state-machine-like components and resource flows, a set of controllable metrics, a design evaluator simulating playthroughs with these metrics, and an evolutionary design balancer and generator. We find this system to be both able to express a wide range of games and able to be human-controllable for future co-creative applications.", "url": "https://arxiv.org/abs/2308.02317"}, {"metadata": {"arXiv": "2308.02457", "Date": "Fri, 04 Aug 2023 16:49:54 ", "Title": "A Survey on Temporal Knowledge Graph Completion: Taxonomy, Progress, and Prospects", "Authors": ["Jiapu Wang", "Boyue Wang", "Meikang Qiu", "Shirui Pan", "Bo Xiong", "Heng Liu", "Linhao Luo", "Tengfei Liu", "Yongli Hu", "Baocai Yin", "Wen Gao"], "Categories": "cs.AI"}, "abstract": "Temporal characteristics are prominently evident in a substantial volume of knowledge, which underscores the pivotal role of Temporal Knowledge Graphs (TKGs) in both academia and industry. However, TKGs often suffer from incompleteness for three main reasons: the continuous emergence of new knowledge, the weakness of the algorithm for extracting structured information from unstructured data, and the lack of information in the source dataset. Thus, the task of Temporal Knowledge Graph Completion (TKGC) has attracted increasing attention, aiming to predict missing items based on the available information. In this paper, we provide a comprehensive review of TKGC methods and their details. Specifically, this paper mainly consists of three components, namely, 1)Background, which covers the preliminaries of TKGC methods, loss functions required for training, as well as the dataset and evaluation protocol; 2)Interpolation, that estimates and predicts the missing elements or set of elements through the relevant available information. It further categorizes related TKGC methods based on how to process temporal information; 3)Extrapolation, which typically focuses on continuous TKGs and predicts future events, and then classifies all extrapolation methods based on the algorithms they utilize. We further pinpoint the challenges and discuss future research directions of TKGC.", "url": "https://arxiv.org/abs/2308.02457"}, {"metadata": {"arXiv": "2308.01971", "Date": "Thu, 03 Aug 2023 18:03:42 ", "Title": "SpaDen : Sparse and Dense Keypoint Estimation for Real-World Chart Understanding", "Authors": ["Saleem Ahmed", "Pengyu Yan", "David Doermann", "Srirangaraj Setlur", "Venu Govindaraju"], "Categories": "cs.CV cs.AI", "Comments": ["Accepted ORAL at ICDAR 23"]}, "abstract": "We introduce a novel bottom-up approach for the extraction of chart data. Our model utilizes images of charts as inputs and learns to detect keypoints (KP), which are used to reconstruct the components within the plot area. Our novelty lies in detecting a fusion of continuous and discrete KP as predicted heatmaps. A combination of sparse and dense per-pixel objectives coupled with a uni-modal self-attention-based feature-fusion layer is applied to learn KP embeddings. Further leveraging deep metric learning for unsupervised clustering, allows us to segment the chart plot area into various objects. By further matching the chart components to the legend, we are able to obtain the data series names. A post-processing threshold is applied to the KP embeddings to refine the object reconstructions and improve accuracy. Our extensive experiments include an evaluation of different modules for KP estimation and the combination of deep layer aggregation and corner pooling approaches. The results of our experiments provide extensive evaluation for the task of real-world chart data extraction.", "url": "https://arxiv.org/abs/2308.01971"}, {"metadata": {"arXiv": "2308.02116", "Date": "Fri, 04 Aug 2023 02:47:19 ", "Title": "AdvFAS: A robust face anti-spoofing framework against adversarial examples", "Authors": ["Jiawei Chen", "Xiao Yang", "Heng Yin", "Mingzhi Ma", "Bihui Chen", "Jianteng Peng", "Yandong Guo", "Zhaoxia Yin", "Hang Su"], "Categories": "cs.CV cs.AI"}, "abstract": "Ensuring the reliability of face recognition systems against presentation attacks necessitates the deployment of face anti-spoofing techniques. Despite considerable advancements in this domain, the ability of even the most state-of-the-art methods to defend against adversarial examples remains elusive. While several adversarial defense strategies have been proposed, they typically suffer from constrained practicability due to inevitable trade-offs between universality, effectiveness, and efficiency. To overcome these challenges, we thoroughly delve into the coupled relationship between adversarial detection and face anti-spoofing. Based on this, we propose a robust face anti-spoofing framework, namely AdvFAS, that leverages two coupled scores to accurately distinguish between correctly detected and wrongly detected face images. Extensive experiments demonstrate the effectiveness of our framework in a variety of settings, including different attacks, datasets, and backbones, meanwhile enjoying high accuracy on clean examples. Moreover, we successfully apply the proposed method to detect real-world adversarial examples.", "url": "https://arxiv.org/abs/2308.02116"}, {"metadata": {"arXiv": "2308.02239", "Date": "Fri, 04 Aug 2023 10:35:40 ", "Title": "DTF-Net: Category-Level Pose Estimation and Shape Reconstruction via Deformable Template Field", "Authors": ["Haowen Wang", "Zhipeng Fan", "Zhen Zhao", "Zhengping Che", "Zhiyuan Xu", "Dong Liu", "Feifei Feng", "Yakun Huang", "Xiuquan Qiao", "Jian Tang"], "Categories": "cs.CV cs.AI cs.RO", "Comments": ["The first two authors are with equal contributions. Paper accepted by ACM MM 2023"], "DOI": "10.1145/3581783.3612142"}, "abstract": "Estimating 6D poses and reconstructing 3D shapes of objects in open-world scenes from RGB-depth image pairs is challenging. Many existing methods rely on learning geometric features that correspond to specific templates while disregarding shape variations and pose differences among objects in the same category. As a result, these methods underperform when handling unseen object instances in complex environments. In contrast, other approaches aim to achieve category-level estimation and reconstruction by leveraging normalized geometric structure priors, but the static prior-based reconstruction struggles with substantial intra-class variations. To solve these problems, we propose the DTF-Net, a novel framework for pose estimation and shape reconstruction based on implicit neural fields of object categories. In DTF-Net, we design a deformable template field to represent the general category-wise shape latent features and intra-category geometric deformation features. The field establishes continuous shape correspondences, deforming the category template into arbitrary observed instances to accomplish shape reconstruction. We introduce a pose regression module that shares the deformation features and template codes from the fields to estimate the accurate 6D pose of each object in the scene. We integrate a multi-modal representation extraction module to extract object features and semantic masks, enabling end-to-end inference. Moreover, during training, we implement a shape-invariant training strategy and a viewpoint sampling method to further enhance the model's capability to extract object pose features. Extensive experiments on the REAL275 and CAMERA25 datasets demonstrate the superiority of DTF-Net in both synthetic and real scenes. Furthermore, we show that DTF-Net effectively supports grasping tasks with a real robot arm.", "url": "https://arxiv.org/abs/2308.02239"}, {"metadata": {"arXiv": "2308.02369", "Date": "Fri, 04 Aug 2023 15:07:20 ", "Title": "Universal Defensive Underpainting Patch: Making Your Text Invisible to Optical Character Recognition", "Authors": ["JiaCheng Deng", "Li Dong", "Jiahao Chen", "Diqun Yan", "Rangding Wang", "Dengpan Ye", "Lingchen Zhao", "and Jinyu Tian"], "Categories": "cs.CV cs.AI", "DOI": "10.1145/3581783.3613768"}, "abstract": "Optical Character Recognition (OCR) enables automatic text extraction from scanned or digitized text images, but it also makes it easy to pirate valuable or sensitive text from these images. Previous methods to prevent OCR piracy by distorting characters in text images are impractical in real-world scenarios, as pirates can capture arbitrary portions of the text images, rendering the defenses ineffective. In this work, we propose a novel and effective defense mechanism termed the Universal Defensive Underpainting Patch (UDUP) that modifies the underpainting of text images instead of the characters. UDUP is created through an iterative optimization process to craft a small, fixed-size defensive patch that can generate non-overlapping underpainting for text images of any size. Experimental results show that UDUP effectively defends against unauthorized OCR under the setting of any screenshot range or complex image background. It is agnostic to the content, size, colors, and languages of characters, and is robust to typical image operations such as scaling and compressing. In addition, the transferability of UDUP is demonstrated by evading several off-the-shelf OCRs. The code is available at https://github.com/QRICKDD/UDUP.", "url": "https://arxiv.org/abs/2308.02369"}, {"metadata": {"arXiv": "2308.02126", "Date": "Fri, 04 Aug 2023 03:59:10 ", "Title": "Semantics-guided Transformer-based Sensor Fusion for Improved Waypoint Prediction", "Authors": ["Hwan-Soo Choi", "Jongoh Jeong", "Young Hoo Cho", "Kuk-Jin Yoon", "and Jong-Hwan Kim"], "Categories": "cs.RO cs.AI", "Comments": ["7 pages", "4 figures", "2 tables"]}, "abstract": "Sensor fusion approaches for intelligent self-driving agents remain key to driving scene understanding given visual global contexts acquired from input sensors. Specifically, for the local waypoint prediction task, single-modality networks are still limited by strong dependency on the sensitivity of the input sensor, and thus recent works promote the use of multiple sensors in fusion in feature level. While it is well known that multiple data modalities promote mutual contextual exchange, deployment to practical driving scenarios requires global 3D scene understanding in real-time with minimal computations, thus placing greater significance on training strategies given a limited number of practically usable sensors. In this light, we exploit carefully selected auxiliary tasks that are highly correlated with the target task of interest (e.g., traffic light recognition and semantic segmentation) by fusing auxiliary task features and also using auxiliary heads for waypoint prediction based on imitation learning. Our multi-task feature fusion augments and improves the base network, TransFuser, by significant margins for safer and more complete road navigation in CARLA simulator as validated on the Town05 Benchmark through extensive experiments.", "url": "https://arxiv.org/abs/2308.02126"}, {"metadata": {"arXiv": "2308.02000", "Date": "Thu, 03 Aug 2023 19:29:35 ", "Title": "On the Transition from Neural Representation to Symbolic Knowledge", "Authors": ["Junyan Cheng and Peter Chin"], "Categories": "cs.AI cs.CV cs.LG"}, "abstract": "Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.", "url": "https://arxiv.org/abs/2308.02000"}, {"metadata": {"arXiv": "2308.02490", "Date": "Fri, 04 Aug 2023 17:59:47 ", "Title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities", "Authors": ["Weihao Yu", "Zhengyuan Yang", "Linjie Li", "Jianfeng Wang", "Kevin Lin", "Zicheng Liu", "Xinchao Wang", "Lijuan Wang"], "Categories": "cs.AI cs.CL cs.CV cs.LG", "Comments": ["Code and data: https://github.com/yuweihao/MM-Vet"]}, "abstract": "We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. Code and data are available at https://github.com/yuweihao/MM-Vet.", "url": "https://arxiv.org/abs/2308.02490"}, {"metadata": {"arXiv": "2308.01916", "Date": "Sun, 09 Jul 2023 04:09:58 ", "Title": "Semi Supervised Meta Learning for Spatiotemporal Learning", "Authors": ["Faraz Waseem", "Pratyush Muthukumar"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "We approached the goal of applying meta-learning to self-supervised masked autoencoders for spatiotemporal learning in three steps. Broadly, we seek to understand the impact of applying meta-learning to existing state-of-the-art representation learning architectures. Thus, we test spatiotemporal learning through: a meta-learning architecture only, a representation learning architecture only, and an architecture applying representation learning alongside a meta learning architecture. We utilize the Memory Augmented Neural Network (MANN) architecture to apply meta-learning to our framework. Specifically, we first experiment with applying a pre-trained MAE and fine-tuning on our small-scale spatiotemporal dataset for video reconstruction tasks. Next, we experiment with training an MAE encoder and applying a classification head for action classification tasks. Finally, we experiment with applying a pre-trained MAE and fine-tune with MANN backbone for action classification tasks.", "url": "https://arxiv.org/abs/2308.01916"}, {"metadata": {"arXiv": "2308.02065", "Date": "Thu, 03 Aug 2023 22:21:04 ", "Title": "On the Biometric Capacity of Generative Face Models", "Authors": ["Vishnu Naresh Boddeti and Gautam Sreekumar and Arun Ross"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["IJCB 2023"]}, "abstract": "There has been tremendous progress in generating realistic faces with high fidelity over the past few years. Despite this progress, a crucial question remains unanswered: \"Given a generative face model, how many unique identities can it generate?\" In other words, what is the biometric capacity of the generative face model? A scientific basis for answering this question will benefit evaluating and comparing different generative face models and establish an upper bound on their scalability. This paper proposes a statistical approach to estimate the biometric capacity of generated face images in a hyperspherical feature space. We employ our approach on multiple generative models, including unconditional generators like StyleGAN, Latent Diffusion Model, and \"Generated Photos,\" as well as DCFace, a class-conditional generator. We also estimate capacity w.r.t. demographic attributes such as gender and age. Our capacity estimates indicate that (a) under ArcFace representation at a false acceptance rate (FAR) of 0.1%, StyleGAN3 and DCFace have a capacity upper bound of $1.43\\times10^6$ and $1.190\\times10^4$, respectively; (b) the capacity reduces drastically as we lower the desired FAR with an estimate of $1.796\\times10^4$ and $562$ at FAR of 1% and 10%, respectively, for StyleGAN3; (c) there is no discernible disparity in the capacity w.r.t gender; and (d) for some generative models, there is an appreciable disparity in the capacity w.r.t age. Code is available at https://github.com/human-analysis/capacity-generative-face-models.", "url": "https://arxiv.org/abs/2308.02065"}, {"metadata": {"arXiv": "2308.02066", "Date": "Thu, 03 Aug 2023 22:34:16 ", "Title": "Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives", "Authors": ["Chuntao Ding", "Zhichao Lu", "Shangguang Wang", "Ran Cheng and Vishnu Naresh Boddeti"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["CVPR 2023"]}, "abstract": "Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this \\href{https://github.com/zhichao-lu/etr-nlp-mtl}.", "url": "https://arxiv.org/abs/2308.02066"}, {"metadata": {"arXiv": "2308.01923", "Date": "Tue, 25 Jul 2023 14:01:23 ", "Title": "An Empirical Study on Fairness Improvement with Multiple Protected Attributes", "Authors": ["Zhenpeng Chen and Jie M. Zhang and Federica Sarro and Mark Harman"], "Categories": "cs.LG cs.AI cs.CY cs.SE"}, "abstract": "Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling multiple protected attributes is about 5 times and 8 times that of a single attribute. This has important implications for future fairness research: reporting only accuracy as the ML performance metric, which is currently common in the literature, is inadequate.", "url": "https://arxiv.org/abs/2308.01923"}, {"metadata": {"arXiv": "2308.01929", "Date": "Wed, 02 Aug 2023 13:49:44 ", "Title": "A Transformer-based Prediction Method for Depth of Anesthesia During Target-controlled Infusion of Propofol and Remifentanil", "Authors": ["Yongkang He", "Siyuan Peng", "Mingjin Chen", "Zhijing Yang", "Yuanhui Chen"], "Categories": "cs.LG cs.AI"}, "abstract": "Accurately predicting anesthetic effects is essential for target-controlled infusion systems. The traditional (PK-PD) models for Bispectral index (BIS) prediction require manual selection of model parameters, which can be challenging in clinical settings. Recently proposed deep learning methods can only capture general trends and may not predict abrupt changes in BIS. To address these issues, we propose a transformer-based method for predicting the depth of anesthesia (DOA) using drug infusions of propofol and remifentanil. Our method employs long short-term memory (LSTM) and gate residual network (GRN) networks to improve the efficiency of feature fusion and applies an attention mechanism to discover the interactions between the drugs. We also use label distribution smoothing and reweighting losses to address data imbalance. Experimental results show that our proposed method outperforms traditional PK-PD models and previous deep learning methods, effectively predicting anesthetic depth under sudden and deep anesthesia conditions.", "url": "https://arxiv.org/abs/2308.01929"}, {"metadata": {"arXiv": "2308.01930", "Date": "Wed, 02 Aug 2023 14:10:03 ", "Title": "Machine Learning-Based Diabetes Detection Using Photoplethysmography Signal Features", "Authors": ["Filipe A. C. Oliveira", "Felipe M. Dias", "Marcelo A. F. Toledo", "Diego A. C. Cardenas", "Douglas A. Almeida", "Estela Ribeiro", "Jose E. Krieger", "Marco A. Gutierrez"], "Categories": "cs.LG cs.AI eess.SP", "Comments": ["11 pages", "6 figures"]}, "abstract": "Diabetes is a prevalent chronic condition that compromises the health of millions of people worldwide. Minimally invasive methods are needed to prevent and control diabetes but most devices for measuring glucose levels are invasive and not amenable for continuous monitoring. Here, we present an alternative method to overcome these shortcomings based on non-invasive optical photoplethysmography (PPG) for detecting diabetes. We classify non-Diabetic and Diabetic patients using the PPG signal and metadata for training Logistic Regression (LR) and eXtreme Gradient Boosting (XGBoost) algorithms. We used PPG signals from a publicly available dataset. To prevent overfitting, we divided the data into five folds for cross-validation. By ensuring that patients in the training set are not in the testing set, the model's performance can be evaluated on unseen subjects' data, providing a more accurate assessment of its generalization. Our model achieved an F1-Score and AUC of $58.8\\pm20.0\\%$ and $79.2\\pm15.0\\%$ for LR and $51.7\\pm16.5\\%$ and $73.6\\pm17.0\\%$ for XGBoost, respectively. Feature analysis suggested that PPG morphological features contains diabetes-related information alongside metadata. Our findings are within the same range reported in the literature, indicating that machine learning methods are promising for developing remote, non-invasive, and continuous measurement devices for detecting and preventing diabetes.", "url": "https://arxiv.org/abs/2308.01930"}, {"metadata": {"arXiv": "2308.01937", "Date": "Wed, 02 Aug 2023 23:27:49 ", "Title": "Training Data Protection with Compositional Diffusion Models", "Authors": ["Aditya Golatkar", "Alessandro Achille", "Ashwin Swaminathan", "Stefano Soatto"], "Categories": "cs.LG cs.AI cs.CR cs.CV"}, "abstract": "We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.", "url": "https://arxiv.org/abs/2308.01937"}, {"metadata": {"arXiv": "2308.01947", "Date": "Thu, 03 Aug 2023 08:29:26 ", "Title": "Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model", "Authors": ["Fu Lin", "Xuexiong Luo", "Jia Wu", "Jian Yang", "Shan Xue", "Zitong Wang", "Haonan Gong"], "Categories": "cs.LG cs.AI"}, "abstract": "Different from the current node-level anomaly detection task, the goal of graph-level anomaly detection is to find abnormal graphs that significantly differ from others in a graph set. Due to the scarcity of research on the work of graph-level anomaly detection, the detailed description of graph-level anomaly is insufficient. Furthermore, existing works focus on capturing anomalous graph information to learn better graph representations, but they ignore the importance of an effective anomaly score function for evaluating abnormal graphs. Thus, in this work, we first define anomalous graph information including node and graph property anomalies in a graph set and adopt node-level and graph-level information differences to identify them, respectively. Then, we introduce a discriminative graph-level anomaly detection framework with dual-students-teacher model, where the teacher model with a heuristic loss are trained to make graph representations more divergent. Then, two competing student models trained by normal and abnormal graphs respectively fit graph representations of the teacher model in terms of node-level and graph-level representation perspectives. Finally, we combine representation errors between two student models to discriminatively distinguish anomalous graphs. Extensive experiment analysis demonstrates that our method is effective for the graph-level anomaly detection task on graph datasets in the real world.", "url": "https://arxiv.org/abs/2308.01947"}, {"metadata": {"arXiv": "2308.01976", "Date": "Thu, 03 Aug 2023 18:11:00 ", "Title": "Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces", "Authors": ["Dayananda Ubrangala", "Juhi Sharma", "Ravi Prasad Kondapalli", "Kiran R", "Amit Agarwala", "Laurent Bou\\'e"], "Categories": "cs.LG cs.AI cs.CL cs.IR", "Journal-ref": "Microsoft Journal of Applied Research, Volume 19, 2023"}, "abstract": "Typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.", "url": "https://arxiv.org/abs/2308.01976"}, {"metadata": {"arXiv": "2308.02060", "Date": "Thu, 03 Aug 2023 21:49:14 ", "Title": "Accurate Neural Network Pruning Requires Rethinking Sparse Optimization", "Authors": ["Denis Kuznedelev", "Eldar Kurtic", "Eugenia Iofinova", "Elias Frantar", "Alexandra Peste", "Dan Alistarh"], "Categories": "cs.LG cs.AI"}, "abstract": "Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art results in both settings in the high-sparsity regime, and providing detailed analyses for the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity, and should inspire further research into improving sparse model training, to reach higher accuracies under high sparsity, but also to do so efficiently.", "url": "https://arxiv.org/abs/2308.02060"}, {"metadata": {"arXiv": "2308.02084", "Date": "Thu, 03 Aug 2023 23:55:17 ", "Title": "Efficient Model Adaptation for Continual Learning at the Edge", "Authors": ["Zachary A. Daniels", "Jun Hu", "Michael Lomnitz", "Phil Miller", "Aswin Raghavan", "Joe Zhang", "Michael Piacentino", "David Zhang"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Under Review w/ IEEE Transactions on Artificial Intelligence"]}, "abstract": "Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying low-parameter neural adaptors to adapt the model to the OOD data using zero-shot neural architecture search (ZS-NAS), and 3) minimizing catastrophic forgetting on previous tasks by progressively growing the neural architecture as needed and dynamically routing data through the appropriate adaptors and reconfigurators for handling domain-incremental and class-incremental continual learning. We systematically evaluate our approach on several benchmark datasets for domain adaptation and demonstrate strong performance compared to state-of-the-art algorithms for OOD detection and few-/zero-shot NAS.", "url": "https://arxiv.org/abs/2308.02084"}, {"metadata": {"arXiv": "2308.02117", "Date": "Fri, 04 Aug 2023 02:58:08 ", "Title": "VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs", "Authors": ["Ling Yang", "Ye Tian", "Minkai Xu", "Zhongyi Liu", "Shenda Hong", "Wei Qu", "Wentao Zhang", "Bin Cui", "Muhan Zhang", "Jure Leskovec"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["arXiv admin note: text overlap with arXiv:1906.00446 by other authors"]}, "abstract": "Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propose a new token-based distillation objective based on soft token assignments to sufficiently transfer the structural knowledge from GNN to MLP. Extensive experiments and analyses demonstrate the strong performance of VQGraph, where we achieve new state-of-the-art performance on GNN-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively. Code: https://github.com/YangLing0818/VQGraph.", "url": "https://arxiv.org/abs/2308.02117"}, {"metadata": {"arXiv": "2308.02121", "Date": "Fri, 04 Aug 2023 03:46:41 ", "Title": "Model Provenance via Model DNA", "Authors": ["Xin Mu", "Yu Wang", "Yehong Zhang", "Jiaqi Zhang", "Hui Wang", "Yang Xiang", "Yue Yu"], "Categories": "cs.LG cs.AI cs.SE"}, "abstract": "Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model. We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance.", "url": "https://arxiv.org/abs/2308.02121"}, {"metadata": {"arXiv": "2308.02282", "Date": "Fri, 04 Aug 2023 12:27:11 ", "Title": "DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization", "Authors": ["Wang Lu", "Jindong Wang", "Xinwei Sun", "Yiqiang Chen", "Xiangyang Ji", "Qiang Yang", "Xing Xie"], "Categories": "cs.LG cs.AI eess.SP", "Comments": ["Journal version of arXiv:2209.07027; 17 pages"]}, "abstract": "Time series remains one of the most challenging modalities in machine learning research. The out-of-distribution (OOD) detection and generalization on time series tend to suffer due to its non-stationary property, i.e., the distribution changes over time. The dynamic distributions inside time series pose great challenges to existing algorithms to identify invariant distributions since they mainly focus on the scenario where the domain information is given as prior knowledge. In this paper, we attempt to exploit subdomains within a whole dataset to counteract issues induced by non-stationary for generalized representation learning. We propose DIVERSIFY, a general framework, for OOD detection and generalization on dynamic distributions of time series. DIVERSIFY takes an iterative process: it first obtains the \"worst-case\" latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We implement DIVERSIFY via combining existing OOD detection methods according to either extracted features or outputs of models for detection while we also directly utilize outputs for classification. In addition, theoretical insights illustrate that DIVERSIFY is theoretically supported. Extensive experiments are conducted on seven datasets with different OOD settings across gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition. Qualitative and quantitative results demonstrate that DIVERSIFY learns more generalized features and significantly outperforms other baselines.", "url": "https://arxiv.org/abs/2308.02282"}, {"metadata": {"arXiv": "2308.02287", "Date": "Fri, 04 Aug 2023 12:43:54 ", "Title": "Frustratingly Easy Model Generalization by Dummy Risk Minimization", "Authors": ["Juncheng Wang", "Jindong Wang", "Xixu Hu", "Shujun Wang", "Xing Xie"], "Categories": "cs.LG cs.AI", "Comments": ["Technical report; 22 pages"]}, "abstract": "Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the performance under all tasks with an almost free lunch manner. Furthermore, we show that DuRM is compatible with existing generalization techniques and we discuss possible limitations. We hope that DuRM could trigger new interest in the fundamental research on risk minimization.", "url": "https://arxiv.org/abs/2308.02287"}, {"metadata": {"arXiv": "2308.02335", "Date": "Fri, 04 Aug 2023 14:06:44 ", "Title": "RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification", "Authors": ["Zhengyang Mao", "Wei Ju", "Yifang Qin", "Xiao Luo", "and Ming Zhang"], "Categories": "cs.LG cs.AI cs.CV cs.IR cs.SI", "Comments": ["Accepted by the ACM International Conference on Multimedia (MM) 2023"]}, "abstract": "Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant graphs that directly enrich the intra-class diversity for the tail classes. Moreover, we innovatively optimize a category-centered supervised contrastive loss to obtain discriminative representations, which is more suitable for long-tailed scenarios. In the classifier fine-tuning stage, we balance the classifier weights with two weight regularization techniques, i.e., Max-norm and weight decay. Experiments on various popular benchmarks verify the superiority of the proposed method against state-of-the-art approaches.", "url": "https://arxiv.org/abs/2308.02335"}, {"metadata": {"arXiv": "2308.02353", "Date": "Fri, 04 Aug 2023 14:41:03 ", "Title": "Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes", "Authors": ["Bardh Prenkaj", "Mario Villaizan-Vallelado", "Tobias Leemann", "Gjergji Kasneci"], "Categories": "cs.LG cs.AI"}, "abstract": "We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE) methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leverages initial knowledge about the data distribution to search for valid counterfactuals while avoiding using information from potentially outdated decision functions in subsequent time steps. Employing two graph autoencoders (GAEs), DyGRACE learns the representation of each class in a binary classification scenario. The GAEs minimise the reconstruction error between the original graph and its learned representation during training. The method involves (i) optimising a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximising the factual autoencoder's reconstruction error, (ii) minimising the counterfactual autoencoder's error, and (iii) maximising the similarity between the factual and counterfactual graphs. This semi-supervised approach is independent of an underlying black-box oracle. A logistic regression model is trained on a set of graph pairs to learn weights that aid in finding counterfactuals. At inference, for each unseen graph, the logistic regressor identifies the best counterfactual candidate using these learned weights, while the GAEs can be iteratively updated to represent the continual adaptation of the learned graph representation over iterations. DyGRACE is quite effective and can act as a drift detector, identifying distributional drift based on differences in reconstruction errors between iterations. It avoids reliance on the oracle's predictions in successive iterations, thereby increasing the efficiency of counterfactual discovery. DyGRACE, with its capacity for contrastive learning and drift detection, will offer new avenues for semi-supervised learning and explanation generation.", "url": "https://arxiv.org/abs/2308.02353"}, {"metadata": {"arXiv": "2308.02370", "Date": "Fri, 04 Aug 2023 15:10:07 ", "Title": "A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data", "Authors": ["Juliette Ugirumurera", "Joseph Severino", "Erik A. Bensen", "Qichao Wang", "and Jane Macfarlane"], "Categories": "cs.LG cs.AI"}, "abstract": "Traffic signals play an important role in transportation by enabling traffic flow management, and ensuring safety at intersections. In addition, knowing the traffic signal phase and timing data can allow optimal vehicle routing for time and energy efficiency, eco-driving, and the accurate simulation of signalized road networks. In this paper, we present a machine learning (ML) method for estimating traffic signal timing information from vehicle probe data. To the authors best knowledge, very few works have presented ML techniques for determining traffic signal timing parameters from vehicle probe data. In this work, we develop an Extreme Gradient Boosting (XGBoost) model to estimate signal cycle lengths and a neural network model to determine the corresponding red times per phase from probe data. The green times are then be derived from the cycle length and red times. Our results show an error of less than 0.56 sec for cycle length, and red times predictions within 7.2 sec error on average.", "url": "https://arxiv.org/abs/2308.02370"}, {"metadata": {"arXiv": "2308.02382", "Date": "Fri, 04 Aug 2023 15:25:56 ", "Title": "Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics", "Authors": ["Alberto Archetti", "Francesca Ieva", "Matteo Matteucci"], "Categories": "cs.LG cs.AI", "DOI": "10.1016/j.future.2023.07.036"}, "abstract": "Survival analysis is a fundamental tool in medicine, modeling the time until an event of interest occurs in a population. However, in real-world applications, survival data are often incomplete, censored, distributed, and confidential, especially in healthcare settings where privacy is critical. The scarcity of data can severely limit the scalability of survival models to distributed applications that rely on large data pools. Federated learning is a promising technique that enables machine learning models to be trained on multiple datasets without compromising user privacy, making it particularly well-suited for addressing the challenges of survival data and large-scale survival applications. Despite significant developments in federated learning for classification and regression, many directions remain unexplored in the context of survival analysis. In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.", "url": "https://arxiv.org/abs/2308.02382"}, {"metadata": {"arXiv": "2308.02454", "Date": "Thu, 03 Aug 2023 07:28:30 ", "Title": "SoK: Assessing the State of Applied Federated Machine Learning", "Authors": ["Tobias M\\\"uller", "Maximilian St\\\"abler", "Hugo Gasc\\'on", "Frank K\\\"oster", "and Florian Matthes"], "Categories": "cs.LG cs.AI cs.CR cs.DC", "Comments": ["9 pages", "6 figures", "3 tables"]}, "abstract": "Machine Learning (ML) has shown significant potential in various applications; however, its adoption in privacy-critical domains has been limited due to concerns about data privacy. A promising solution to this issue is Federated Machine Learning (FedML), a model-to-data approach that prioritizes data privacy. By enabling ML algorithms to be applied directly to distributed data sources without sharing raw data, FedML offers enhanced privacy protections, making it suitable for privacy-critical environments. Despite its theoretical benefits, FedML has not seen widespread practical implementation. This study aims to explore the current state of applied FedML and identify the challenges hindering its practical adoption. Through a comprehensive systematic literature review, we assess 74 relevant papers to analyze the real-world applicability of FedML. Our analysis focuses on the characteristics and emerging trends of FedML implementations, as well as the motivational drivers and application domains. We also discuss the encountered challenges in integrating FedML into real-life settings. By shedding light on the existing landscape and potential obstacles, this research contributes to the further development and implementation of FedML in privacy-critical scenarios.", "url": "https://arxiv.org/abs/2308.02454"}, {"metadata": {"arXiv": "2308.02459", "Date": "Fri, 04 Aug 2023 16:55:00 ", "Title": "Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration", "Authors": ["Juan Del Aguila Ferrandis", "Jo\\~ao Moura", "Sethu Vijayakumar"], "Categories": "cs.RO cs.AI cs.LG"}, "abstract": "Developing robot controllers capable of achieving dexterous nonprehensile manipulation, such as pushing an object on a table, is challenging. The underactuated and hybrid-dynamics nature of the problem, further complicated by the uncertainty resulting from the frictional interactions, requires sophisticated control behaviors. Reinforcement Learning (RL) is a powerful framework for developing such robot controllers. However, previous RL literature addressing the nonprehensile pushing task achieves low accuracy, non-smooth trajectories, and only simple motions, i.e. without rotation of the manipulated object. We conjecture that previously used unimodal exploration strategies fail to capture the inherent hybrid-dynamics of the task, arising from the different possible contact interaction modes between the robot and the object, such as sticking, sliding, and separation. In this work, we propose a multimodal exploration approach through categorical distributions, which enables us to train planar pushing RL policies for arbitrary starting and target object poses, i.e. positions and orientations, and with improved accuracy. We show that the learned policies are robust to external disturbances and observation noise, and scale to tasks with multiple pushers. Furthermore, we validate the transferability of the learned policies, trained entirely in simulation, to a physical robot hardware using the KUKA iiwa robot arm. See our supplemental video: https://youtu.be/vTdva1mgrk4.", "url": "https://arxiv.org/abs/2308.02459"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
