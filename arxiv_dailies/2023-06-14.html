<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2306.07613", "Date": "Tue, 13 Jun 2023 08:12:52 ", "Title": "Rethinking Adversarial Training with A Simple Baseline", "Authors": ["Hong Liu", "Shin'ichi Satoh"], "Categories": "cs.CV cs.LG", "Comments": ["19 pages", "8 figures", "6 tables"]}, "abstract": "We report competitive results on RobustBench for CIFAR and SVHN using a simple yet effective baseline approach. Our approach involves a training protocol that integrates rescaled square loss, cyclic learning rates, and erasing-based data augmentation. The outcomes we have achieved are comparable to those of the model trained with state-of-the-art techniques, which is currently the predominant choice for adversarial training. Our baseline, referred to as SimpleAT, yields three novel empirical insights. (i) By switching to square loss, the accuracy is comparable to that obtained by using both de-facto training protocol plus data augmentation. (ii) One cyclic learning rate is a good scheduler, which can effectively reduce the risk of robust overfitting. (iii) Employing rescaled square loss during model training can yield a favorable balance between adversarial and natural accuracy. In general, our experimental results show that SimpleAT effectively mitigates robust overfitting and consistently achieves the best performance at the end of training. For example, on CIFAR-10 with ResNet-18, SimpleAT achieves approximately 52% adversarial accuracy against the current strong AutoAttack. Furthermore, SimpleAT exhibits robust performance on various image corruptions, including those commonly found in CIFAR-10-C dataset. Finally, we assess the effectiveness of these insights through two techniques: bias-variance analysis and logit penalty methods. Our findings demonstrate that all of these simple techniques are capable of reducing the variance of model predictions, which is regarded as the primary contributor to robust overfitting. In addition, our analysis also uncovers connections with various advanced state-of-the-art methods.", "url": "https://arxiv.org/abs/2306.07613"}, {"metadata": {"arXiv": "2306.07724", "Date": "Tue, 13 Jun 2023 12:22:54 ", "Title": "Effects of Data Enrichment with Image Transformations on the Performance of Deep Networks", "Authors": ["Hakan Temiz"], "Categories": "cs.CV cs.LG eess.IV", "Journal-ref": "The European Journal of Research and Development, 2(2), 23-33 (2022)", "DOI": "10.56038/ejrnd.v2i2.23"}, "abstract": "Images cannot always be expected to come in a certain standard format and orientation. Deep networks need to be trained to take into account unexpected variations in orientation or format. For this purpose, training data should be enriched to include different conditions. In this study, the effects of data enrichment on the performance of deep networks in the super resolution problem were investigated experimentally. A total of six basic image transformations were used for the enrichment procedures. In the experiments, two deep network models were trained with variants of the ILSVRC2012 dataset enriched by these six image transformation processes. Considering a single image transformation, it has been observed that the data enriched with 180 degree rotation provides the best results. The most unsuccessful result was obtained when the models were trained on the enriched data generated by the flip upside down process. Models scored highest when trained with a mix of all transformations.", "url": "https://arxiv.org/abs/2306.07724"}, {"metadata": {"arXiv": "2306.07727", "Date": "Tue, 13 Jun 2023 12:29:42 ", "Title": "Automatic and Accurate Classification of Hotel Bathrooms from Images with Deep Learning", "Authors": ["Hakan Temiz"], "Categories": "cs.CV cs.LG", "Journal-ref": "International Journal of Engineering Research and Development , Special Issue 2022 , 211-218", "DOI": "10.29137/umagd.1217004"}, "abstract": "Hotel bathrooms are one of the most important places in terms of customer satisfaction, and where the most complaints are reported. To share their experiences, guests rate hotels, comment, and share images of their positive or negative ratings. An important part of the room images shared by guests is related to bathrooms. Guests tend to prove their satisfaction or dissatisfaction with the bathrooms with images in their comments. These Positive or negative comments and visuals potentially affect the prospective guests. In this study, two different versions of a deep learning algorithm were designed to classify hotel bathrooms as satisfactory (good) or unsatisfactory (bad, when any defects such as dirtiness, deficiencies, malfunctions were present) by analyzing images. The best-performer between the two models was determined as a result of a series of extensive experimental studies. The models were trained for each of 144 combinations of 5 hyper-parameter sets with a data set containing more than 11 thousand bathroom images, specially created for this study. The \"HotelBath\" data set was shared also with the community with this study. Four different image sizes were taken into consideration: 128, 256, 512 and 1024 pixels in both directions. The classification performances of the models were measured with several metrics. Both algorithms showed very attractive performances even with many combinations of hyper-parameters. They can classify bathroom images with very high accuracy. Suh that the top algorithm achieved an accuracy of 92.4% and an AUC (area under the curve) score of 0.967. In addition, other metrics also proved the success...", "url": "https://arxiv.org/abs/2306.07727"}, {"metadata": {"arXiv": "2306.07768", "Date": "Tue, 13 Jun 2023 13:33:53 ", "Title": "Area is all you need: repeatable elements make stronger adversarial attacks", "Authors": ["Dillon Niederhut"], "Categories": "cs.CV cs.CR cs.LG"}, "abstract": "Over the last decade, deep neural networks have achieved state of the art in computer vision tasks. These models, however, are susceptible to unusual inputs, known as adversarial examples, that cause them to misclassify or otherwise fail to detect objects. Here, we provide evidence that the increasing success of adversarial attacks is primarily due to increasing their size. We then demonstrate a method for generating the largest possible adversarial patch by building a adversarial pattern out of repeatable elements. This approach achieves a new state of the art in evading detection by YOLOv2 and YOLOv3. Finally, we present an experiment that fails to replicate the prior success of several attacks published in this field, and end with some comments on testing and reproducibility.", "url": "https://arxiv.org/abs/2306.07768"}, {"metadata": {"arXiv": "2306.07783", "Date": "Tue, 13 Jun 2023 14:06:55 ", "Title": "Compositionally Equivariant Representation Learning", "Authors": ["Xiao Liu", "Pedro Sanchez", "Spyridon Thermos", "Alison Q. O'Neil and Sotirios A. Tsaftaris"], "Categories": "cs.CV cs.LG", "Comments": ["Submitted. 10 pages. arXiv admin note: text overlap with arXiv:2206.14538"]}, "abstract": "Deep learning models often need sufficient supervision (i.e. labelled data) in order to be trained effectively. By contrast, humans can swiftly learn to identify important anatomy in medical images like MRI and CT scans, with minimal guidance. This recognition capability easily generalises to new images from different medical facilities and to new tasks in different settings. This rapid and generalisable learning ability is largely due to the compositional structure of image patterns in the human brain, which are not well represented in current medical models. In this paper, we study the utilisation of compositionality in learning more interpretable and generalisable representations for medical image segmentation. Overall, we propose that the underlying generative factors that are used to generate the medical images satisfy compositional equivariance property, where each factor is compositional (e.g. corresponds to the structures in human anatomy) and also equivariant to the task. Hence, a good representation that approximates well the ground truth factor has to be compositionally equivariant. By modelling the compositional representations with learnable von-Mises-Fisher (vMF) kernels, we explore how different design and learning biases can be used to enforce the representations to be more compositionally equivariant under un-, weakly-, and semi-supervised settings. Extensive results show that our methods achieve the best performance over several strong baselines on the task of semi-supervised domain-generalised medical image segmentation. Code will be made publicly available upon acceptance at https://github.com/vios-s.", "url": "https://arxiv.org/abs/2306.07783"}, {"metadata": {"arXiv": "2306.07809", "Date": "Tue, 13 Jun 2023 14:36:06 ", "Title": "Low-Resource White-Box Semantic Segmentation of Supporting Towers on 3D Point Clouds via Signature Shape Identification", "Authors": ["Diogo Lavado", "Cl\\'audia Soares", "Alessandra Micheletti", "Giovanni Bocchi", "Alex Coronati", "Manuel Silva and Patrizio Frosini"], "Categories": "cs.CV cs.LG math.GT"}, "abstract": "Research in 3D semantic segmentation has been increasing performance metrics, like the IoU, by scaling model complexity and computational resources, leaving behind researchers and practitioners that (1) cannot access the necessary resources and (2) do need transparency on the model decision mechanisms. In this paper, we propose SCENE-Net, a low-resource white-box model for 3D point cloud semantic segmentation. SCENE-Net identifies signature shapes on the point cloud via group equivariant non-expansive operators (GENEOs), providing intrinsic geometric interpretability. Our training time on a laptop is 85~min, and our inference time is 20~ms. SCENE-Net has 11 trainable geometrical parameters and requires fewer data than black-box models. SCENE--Net offers robustness to noisy labeling and data imbalance and has comparable IoU to state-of-the-art methods. With this paper, we release a 40~000 Km labeled dataset of rural terrain point clouds and our code implementation.", "url": "https://arxiv.org/abs/2306.07809"}, {"metadata": {"arXiv": "2306.07890", "Date": "Tue, 13 Jun 2023 16:31:02 ", "Title": "VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON", "Authors": ["Haoping Bai", "Shancong Mou", "Tatiana Likhomanenko", "Ramazan Gokberk Cinbis", "Oncel Tuzel", "Ping Huang", "Jiulong Shan", "Jianjun Shi", "Meng Cao"], "Categories": "cs.CV cs.LG"}, "abstract": "Despite progress in vision-based inspection algorithms, real-world industrial challenges -- specifically in data availability, quality, and complex production requirements -- often remain under-addressed. We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges. Unlike previous datasets, VISION brings versatility to defect detection, offering annotation masks across all splits and catering to various detection methodologies. Our datasets also feature instance-segmentation annotation, enabling precise defect identification. With a total of 18k images encompassing 44 defect types, VISION strives to mirror a wide range of real-world production scenarios. By supporting two ongoing challenge competitions on the VISION Datasets, we hope to foster further advancements in vision-based industrial inspection.", "url": "https://arxiv.org/abs/2306.07890"}, {"metadata": {"arXiv": "2306.07952", "Date": "Tue, 13 Jun 2023 17:51:18 ", "Title": "MOFI: Learning Image Representations from Noisy Entity Annotated Images", "Authors": ["Wentao Wu", "Aleksei Timofeev", "Chen Chen", "Bowen Zhang", "Kun Duan", "Shuangning Liu", "Yantao Zheng", "Jon Shlens", "Xianzhi Du", "Zhe Gan", "Yinfei Yang"], "Categories": "cs.CV cs.CL cs.LG"}, "abstract": "We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-training, and multi-task learning. For constrastive pre-training, we treat entity names as free-form text, and further enrich them with entity descriptions. Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks, and multi-task training further improves the performance. The final MOFI model achieves 86.66% mAP on the challenging GPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19% from OpenAI's CLIP model. Further experiments on zero-shot and linear probe image classification also show that MOFI outperforms a CLIP model trained on the original image-text data, demonstrating the effectiveness of the I2E dataset in learning strong image representations.", "url": "https://arxiv.org/abs/2306.07952"}, {"metadata": {"arXiv": "2306.07352", "Date": "Mon, 12 Jun 2023 18:21:10 ", "Title": "Multi-Platform Budget Management in Ad Markets with Non-IC Auctions", "Authors": ["Fransisca Susan", "Negin Golrezaei", "Okke Schrijvers"], "Categories": "cs.GT cs.LG math.OC stat.ML", "Comments": ["34 pages", "5 figures"]}, "abstract": "In online advertising markets, budget-constrained advertisers acquire ad placements through repeated bidding in auctions on various platforms. We present a strategy for bidding optimally in a set of auctions that may or may not be incentive-compatible under the presence of budget constraints. Our strategy maximizes the expected total utility across auctions while satisfying the advertiser's budget constraints in expectation. Additionally, we investigate the online setting where the advertiser must submit bids across platforms while learning about other bidders' bids over time. Our algorithm has $O(T^{3/4})$ regret under the full-information setting. Finally, we demonstrate that our algorithms have superior cumulative regret on both synthetic and real-world datasets of ad placement auctions, compared to existing adaptive pacing algorithms.", "url": "https://arxiv.org/abs/2306.07352"}, {"metadata": {"arXiv": "2306.07479", "Date": "Tue, 13 Jun 2023 00:55:10 ", "Title": "Incentivizing High-Quality Content in Online Recommender Systems", "Authors": ["Xinyan Hu", "Meena Jagadeesan", "Michael I. Jordan", "and Jacob Steinhard"], "Categories": "cs.GT cs.IR cs.LG stat.ML"}, "abstract": "For content recommender systems such as TikTok and YouTube, the platform's decision algorithm shapes the incentives of content producers, including how much effort the content producers invest in the quality of their content. Many platforms employ online learning, which creates intertemporal incentives, since content produced today affects recommendations of future content. In this paper, we study the incentives arising from online learning, analyzing the quality of content produced at a Nash equilibrium. We show that classical online learning algorithms, such as Hedge and EXP3, unfortunately incentivize producers to create low-quality content. In particular, the quality of content is upper bounded in terms of the learning rate and approaches zero for typical learning rate schedules. Motivated by this negative result, we design a different learning algorithm -- based on punishing producers who create low-quality content -- that correctly incentivizes producers to create high-quality content. At a conceptual level, our work illustrates the unintended impact that a platform's learning algorithm can have on content quality and opens the door towards designing platform learning algorithms that incentivize the creation of high-quality content.", "url": "https://arxiv.org/abs/2306.07479"}, {"metadata": {"arXiv": "2306.07709", "Date": "Tue, 13 Jun 2023 11:55:04 ", "Title": "Coordinated Dynamic Bidding in Repeated Second-Price Auctions with Budgets", "Authors": ["Yurong Chen", "Qian Wang", "Zhijian Duan", "Haoran Sun", "Zhaohua Chen", "Xiang Yan", "Xiaotie Deng"], "Categories": "cs.GT cs.LG econ.TH", "Comments": ["43 pages", "12 figures"]}, "abstract": "In online ad markets, a rising number of advertisers are employing bidding agencies to participate in ad auctions. These agencies are specialized in designing online algorithms and bidding on behalf of their clients. Typically, an agency usually has information on multiple advertisers, so she can potentially coordinate bids to help her clients achieve higher utilities than those under independent bidding. In this paper, we study coordinated online bidding algorithms in repeated second-price auctions with budgets. We propose algorithms that guarantee every client a higher utility than the best she can get under independent bidding. We show that these algorithms achieve maximal coalition welfare and discuss bidders' incentives to misreport their budgets, in symmetric cases. Our proofs combine the techniques of online learning and equilibrium analysis, overcoming the difficulty of competing with a multi-dimensional benchmark. The performance of our algorithms is further evaluated by experiments on both synthetic and real data. To the best of our knowledge, we are the first to consider bidder coordination in online repeated auctions with constraints.", "url": "https://arxiv.org/abs/2306.07709"}, {"metadata": {"arXiv": "2306.07303", "Date": "Sun, 11 Jun 2023 23:13:51 ", "Title": "A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks", "Authors": ["Saidul Islam", "Hanae Elmekki", "Ahmed Elsebai", "Jamal Bentahar", "Najat Drawel", "Gaith Rjoub", "Witold Pedrycz"], "Categories": "cs.LG cs.CL"}, "abstract": "Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant absence of a comprehensive survey paper encompassing its major applications across various domains. Therefore, we undertook the task of filling this gap by conducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses the identification of the top five application domains for transformer-based models, namely: NLP, Computer Vision, Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze the impact of highly influential transformer-based models in these domains and subsequently classify them based on their respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future possibilities of transformers for enthusiastic researchers, thus contributing to the broader understanding of this groundbreaking technology.", "url": "https://arxiv.org/abs/2306.07303"}, {"metadata": {"arXiv": "2306.07309", "Date": "Mon, 12 Jun 2023 17:50:09 ", "Title": "A New Probabilistic Distance Metric With Application In Gaussian Mixture Reduction", "Authors": ["Ahmad Sajedi", "Yuri A. Lawryshyn", "and Konstantinos N. Plataniotis"], "Categories": "cs.LG stat.AP stat.CO", "DOI": "10.1109/ICASSP49357.2023.10096094"}, "abstract": "This paper presents a new distance metric to compare two continuous probability density functions. The main advantage of this metric is that, unlike other statistical measurements, it can provide an analytic, closed-form expression for a mixture of Gaussian distributions while satisfying all metric properties. These characteristics enable fast, stable, and efficient calculations, which are highly desirable in real-world signal processing applications. The application in mind is Gaussian Mixture Reduction (GMR), which is widely used in density estimation, recursive tracking, and belief propagation. To address this problem, we developed a novel algorithm dubbed the Optimization-based Greedy GMR (OGGMR), which employs our metric as a criterion to approximate a high-order Gaussian mixture with a lower order. Experimental results show that the OGGMR algorithm is significantly faster and more efficient than state-of-the-art GMR algorithms while retaining the geometric shape of the original mixture.", "url": "https://arxiv.org/abs/2306.07309"}, {"metadata": {"arXiv": "2306.07350", "Date": "Mon, 12 Jun 2023 18:16:33 ", "Title": "G-invariant diffusion maps", "Authors": ["Eitan Rosen and Xiuyuan Cheng and Yoel Shkolnisky"], "Categories": "cs.LG"}, "abstract": "The diffusion maps embedding of data lying on a manifold have shown success in tasks ranging from dimensionality reduction and clustering, to data visualization. In this work, we consider embedding data sets which were sampled from a manifold which is closed under the action of a continuous matrix group. An example of such a data set are images who's planar rotations are arbitrary. The G-invariant graph Laplacian, introduced in a previous work of the authors, admits eigenfunctions in the form of tensor products between the elements of the irreducible unitary representations of the group and eigenvectors of certain matrices. We employ these eigenfunctions to derive diffusion maps that intrinsically account for the group action on the data. In particular, we construct both equivariant and invariant embeddings which can be used naturally to cluster and align the data points. We demonstrate the effectiveness of our construction with simulated data.", "url": "https://arxiv.org/abs/2306.07350"}, {"metadata": {"arXiv": "2306.07381", "Date": "Mon, 12 Jun 2023 19:14:45 ", "Title": "\"Private Prediction Strikes Back!'' Private Kernelized Nearest Neighbors with Individual Renyi Filter", "Authors": ["Yuqing Zhu", "Xuandong Zhao", "Chuan Guo", "Yu-Xiang Wang"], "Categories": "cs.LG cs.CR"}, "abstract": "Most existing approaches of differentially private (DP) machine learning focus on private training. Despite its many advantages, private training lacks the flexibility in adapting to incremental changes to the training dataset such as deletion requests from exercising GDPR's right to be forgotten. We revisit a long-forgotten alternative, known as private prediction, and propose a new algorithm named Individual Kernelized Nearest Neighbor (Ind-KNN). Ind-KNN is easily updatable over dataset changes and it allows precise control of the R\\'{e}nyi DP at an individual user level -- a user's privacy loss is measured by the exact amount of her contribution to predictions; and a user is removed if her prescribed privacy budget runs out. Our results show that Ind-KNN consistently improves the accuracy over existing private prediction methods for a wide range of $\\epsilon$ on four vision and language tasks. We also illustrate several cases under which Ind-KNN is preferable over private training with NoisySGD.", "url": "https://arxiv.org/abs/2306.07381"}, {"metadata": {"arXiv": "2306.07397", "Date": "Mon, 12 Jun 2023 19:54:33 ", "Title": "Adversarial Attacks on the Interpretation of Neuron Activation Maximization", "Authors": ["Geraldin Nanfack", "Alexander Fulleringer", "Jonathan Marty", "Michael Eickenberg", "Eugene Belilovsky"], "Categories": "cs.LG cs.CV"}, "abstract": "The internal functional behavior of trained Deep Neural Networks is notoriously difficult to interpret. Activation-maximization approaches are one set of techniques used to interpret and analyze trained deep-learning models. These consist in finding inputs that maximally activate a given neuron or feature map. These inputs can be selected from a data set or obtained by optimization. However, interpretability methods may be subject to being deceived. In this work, we consider the concept of an adversary manipulating a model for the purpose of deceiving the interpretation. We propose an optimization framework for performing this manipulation and demonstrate a number of ways that popular activation-maximization interpretation techniques associated with CNNs can be manipulated to change the interpretations, shedding light on the reliability of these methods.", "url": "https://arxiv.org/abs/2306.07397"}, {"metadata": {"arXiv": "2306.07432", "Date": "Mon, 12 Jun 2023 21:27:39 ", "Title": "FIRE: An Optimization Approach for Fast Interpretable Rule Extraction", "Authors": ["Brian Liu and Rahul Mazumder"], "Categories": "cs.LG stat.ML", "Journal-ref": "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2023)", "DOI": "10.1145/3580305.3599353"}, "abstract": "We present FIRE, Fast Interpretable Rule Extraction, an optimization-based framework to extract a small but useful collection of decision rules from tree ensembles. FIRE selects sparse representative subsets of rules from tree ensembles, that are easy for a practitioner to examine. To further enhance the interpretability of the extracted model, FIRE encourages fusing rules during selection, so that many of the selected decision rules share common antecedents. The optimization framework utilizes a fusion regularization penalty to accomplish this, along with a non-convex sparsity-inducing penalty to aggressively select rules. Optimization problems in FIRE pose a challenge to off-the-shelf solvers due to problem scale and the non-convexity of the penalties. To address this, making use of problem-structure, we develop a specialized solver based on block coordinate descent principles; our solver performs up to 40x faster than existing solvers. We show in our experiments that FIRE outperforms state-of-the-art rule ensemble algorithms at building sparse rule sets, and can deliver more interpretable models compared to existing methods.", "url": "https://arxiv.org/abs/2306.07432"}, {"metadata": {"arXiv": "2306.07462", "Date": "Mon, 12 Jun 2023 23:33:13 ", "Title": "On the Robustness of Removal-Based Feature Attributions", "Authors": ["Chris Lin", "Ian Covert", "Su-In Lee"], "Categories": "cs.LG stat.ML"}, "abstract": "To explain complex models based on their inputs, many feature attribution methods have been developed that assign importance scores to input features. However, some recent work challenges the robustness of feature attributions by showing that these methods are sensitive to input and model perturbations, while other work addresses this robustness issue by proposing robust attribution methods and model modifications. Nevertheless, previous work on attribution robustness has focused primarily on gradient-based feature attributions. In contrast, the robustness properties of removal-based attribution methods are not comprehensively well understood. To bridge this gap, we theoretically characterize the robustness of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and prove upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical experiments on synthetic and real-world data validate our theoretical results and demonstrate their practical implications.", "url": "https://arxiv.org/abs/2306.07462"}, {"metadata": {"arXiv": "2306.07473", "Date": "Tue, 13 Jun 2023 00:38:51 ", "Title": "3D molecule generation by denoising voxel grids", "Authors": ["Pedro O. Pinheiro", "Joshua Rackers", "Joseph Kleinhenz", "Michael Maser", "Omar Mahmood", "Andrew Martin Watkins", "Stephen Ra", "Vishnu Sresht", "Saeed Saremi"], "Categories": "cs.LG q-bio.QM"}, "abstract": "We propose a new score-based approach to generate 3D molecules represented as atomic densities on regular grids. First, we train a denoising neural network that learns to map from a smooth distribution of noisy molecules to the distribution of real molecules. Then, we follow the neural empirical Bayes framework [Saremi and Hyvarinen, 2019] and generate molecules in two steps: (i) sample noisy density grids from a smooth distribution via underdamped Langevin Markov chain Monte Carlo, and (ii) recover the ``clean'' molecule by denoising the noisy grid with a single step. Our method, VoxMol, generates molecules in a fundamentally different way than the current state of the art (i.e., diffusion models applied to atom point clouds). It differs in terms of the data representation, the noise model, the network architecture and the generative modeling algorithm. VoxMol achieves comparable results to state of the art on unconditional 3D molecule generation while being simpler to train and faster to generate molecules.", "url": "https://arxiv.org/abs/2306.07473"}, {"metadata": {"arXiv": "2306.07484", "Date": "Tue, 13 Jun 2023 01:12:31 ", "Title": "Multi-objective Molecular Optimization for Opioid Use Disorder Treatment Using Generative Network Complex", "Authors": ["Hongsong Feng", "Rui Wang", "Chang-Guo Zhan", "Guo-Wei Wei"], "Categories": "cs.LG q-bio.BM"}, "abstract": "Opioid Use Disorder (OUD) has emerged as a significant global public health issue, with complex multifaceted conditions. Due to the lack of effective treatment options for various conditions, there is a pressing need for the discovery of new medications. In this study, we propose a deep generative model that combines a stochastic differential equation (SDE)-based diffusion modeling with the latent space of a pretrained autoencoder model. The molecular generator enables efficient generation of molecules that are effective on multiple targets, specifically the mu, kappa, and delta opioid receptors. Furthermore, we assess the ADMET (absorption, distribution, metabolism, excretion, and toxicity) properties of the generated molecules to identify drug-like compounds. To enhance the pharmacokinetic properties of some lead compounds, we employ a molecular optimization approach. We obtain a diverse set of drug-like molecules. We construct binding affinity predictors by integrating molecular fingerprints derived from autoencoder embeddings, transformer embeddings, and topological Laplacians with advanced machine learning algorithms. Further experimental studies are needed to evaluate the pharmacological effects of these drug-like compounds for OUD treatment. Our machine learning platform serves as a valuable tool in designing and optimizing effective molecules for addressing OUD.", "url": "https://arxiv.org/abs/2306.07484"}, {"metadata": {"arXiv": "2306.07485", "Date": "Tue, 13 Jun 2023 01:18:16 ", "Title": "Learning Unnormalized Statistical Models via Compositional Optimization", "Authors": ["Wei Jiang", "Jiayu Qin", "Lingyu Wu", "Changyou Chen", "Tianbao Yang", "Lijun Zhang"], "Categories": "cs.LG math.OC"}, "abstract": "Learning unnormalized statistical models (e.g., energy-based models) is computationally challenging due to the complexity of handling the partition function. To eschew this complexity, noise-contrastive estimation~(NCE) has been proposed by formulating the objective as the logistic loss of the real data and the artificial noise. However, as found in previous works, NCE may perform poorly in many tasks due to its flat loss landscape and slow convergence. In this paper, we study it a direct approach for optimizing the negative log-likelihood of unnormalized models from the perspective of compositional optimization. To tackle the partition function, a noise distribution is introduced such that the log partition function can be written as a compositional function whose inner function can be estimated with stochastic samples. Hence, the objective can be optimized by stochastic compositional optimization algorithms. Despite being a simple method, we demonstrate that it is more favorable than NCE by (1) establishing a fast convergence rate and quantifying its dependence on the noise distribution through the variance of stochastic estimators; (2) developing better results for one-dimensional Gaussian mean estimation by showing our objective has a much favorable loss landscape and hence our method enjoys faster convergence; (3) demonstrating better performance on multiple applications, including density estimation, out-of-distribution detection, and real image generation.", "url": "https://arxiv.org/abs/2306.07485"}, {"metadata": {"arXiv": "2306.07497", "Date": "Tue, 13 Jun 2023 02:18:24 ", "Title": "GQFedWAvg: Optimization-Based Quantized Federated Learning in General Edge Computing Systems", "Authors": ["Yangchen Li", "Ying Cui", "and Vincent Lau"], "Categories": "cs.LG cs.DC", "Comments": ["submitted to IEEE Transactions on Wireless Communications", "under major revision"]}, "abstract": "The optimal implementation of federated learning (FL) in practical edge computing systems has been an outstanding problem. In this paper, we propose an optimization-based quantized FL algorithm, which can appropriately fit a general edge computing system with uniform or nonuniform computing and communication resources at the workers. Specifically, we first present a new random quantization scheme and analyze its properties. Then, we propose a general quantized FL algorithm, namely GQFedWAvg. Specifically, GQFedWAvg applies the proposed quantization scheme to quantize wisely chosen model update-related vectors and adopts a generalized mini-batch stochastic gradient descent (SGD) method with the weighted average local model updates in global model aggregation. Besides, GQFedWAvg has several adjustable algorithm parameters to flexibly adapt to the computing and communication resources at the server and workers. We also analyze the convergence of GQFedWAvg. Next, we optimize the algorithm parameters of GQFedWAvg to minimize the convergence error under the time and energy constraints. We successfully tackle the challenging non-convex problem using general inner approximation (GIA) and multiple delicate tricks. Finally, we interpret GQFedWAvg's function principle and show its considerable gains over existing FL algorithms using numerical results.", "url": "https://arxiv.org/abs/2306.07497"}, {"metadata": {"arXiv": "2306.07503", "Date": "Tue, 13 Jun 2023 02:29:34 ", "Title": "PaVa: a novel Path-based Valley-seeking clustering algorithm", "Authors": ["Lin Ma and Conan Liu and Tiefeng Ma and Shuangzhe Liu"], "Categories": "cs.LG"}, "abstract": "Clustering methods are being applied to a wider range of scenarios involving more complex datasets, where the shapes of clusters tend to be arbitrary. In this paper, we propose a novel Path-based Valley-seeking clustering algorithm for arbitrarily shaped clusters. This work aims to seek the valleys among clusters and then individually extract clusters. Three vital techniques are used in this algorithm. First, path distance (minmax distance) is employed to transform the irregular boundaries among clusters, that is density valleys, into perfect spherical shells. Second, a suitable density measurement, $k$-distance, is employed to make adjustment on Minimum Spanning Tree, by which a robust minmax distance is calculated. Third, we seek the transformed density valleys by determining their centers and radius. First, the clusters are wrapped in spherical shells after the distance transformation, making the extraction process efficient even with clusters of arbitrary shape. Second, adjusted Minimum Spanning Tree enhances the robustness of minmax distance under different kinds of noise. Last, the number of clusters does not need to be inputted or decided manually due to the individual extraction process. After applying the proposed algorithm to several commonly used synthetic datasets, the results indicate that the Path-based Valley-seeking algorithm is accurate and efficient. The algorithm is based on the dissimilarity of objects, so it can be applied to a wide range of fields. Its performance on real-world datasets illustrates its versatility.", "url": "https://arxiv.org/abs/2306.07503"}, {"metadata": {"arXiv": "2306.07528", "Date": "Tue, 13 Jun 2023 03:46:22 ", "Title": "Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective", "Authors": ["Zeyu Zhang", "Yi Su", "Hui Yuan", "Yiran Wu", "Rishab Balasubramanian", "Qingyun Wu", "Huazheng Wang", "Mengdi Wang"], "Categories": "cs.LG cs.IR"}, "abstract": "Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets demonstrate that CUOLR consistently outperforms the state-of-the-art off-policy learning to rank algorithms while maintaining consistency and robustness under different click models.", "url": "https://arxiv.org/abs/2306.07528"}, {"metadata": {"arXiv": "2306.07544", "Date": "Tue, 13 Jun 2023 05:25:51 ", "Title": "On Achieving Optimal Adversarial Test Error", "Authors": ["Justin D. Li", "Matus Telgarsky"], "Categories": "cs.LG stat.ML", "Comments": ["ICLR 2023"]}, "abstract": "We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees.", "url": "https://arxiv.org/abs/2306.07544"}, {"metadata": {"arXiv": "2306.07549", "Date": "Tue, 13 Jun 2023 05:41:38 ", "Title": "Fixed-Budget Best-Arm Identification with Heterogeneous Reward Variances", "Authors": ["Anusha Lalitha", "Kousha Kalantari", "Yifei Ma", "Anoop Deoras", "Branislav Kveton"], "Categories": "cs.LG stat.ML"}, "abstract": "We study the problem of best-arm identification (BAI) in the fixed-budget setting with heterogeneous reward variances. We propose two variance-adaptive BAI algorithms for this setting: SHVar for known reward variances and SHAdaVar for unknown reward variances. Our algorithms rely on non-uniform budget allocations among the arms where the arms with higher reward variances are pulled more often than those with lower variances. The main algorithmic novelty is in the design of SHAdaVar, which allocates budget greedily based on overestimating the unknown reward variances. We bound probabilities of misidentifying the best arms in both SHVar and SHAdaVar. Our analyses rely on novel lower bounds on the number of pulls of an arm that do not require closed-form solutions to the budget allocation problem. Since one of our budget allocation problems is analogous to the optimal experiment design with unknown variances, we believe that our results are of a broad interest. Our experiments validate our theory, and show that SHVar and SHAdaVar outperform algorithms from prior works with analytical guarantees.", "url": "https://arxiv.org/abs/2306.07549"}, {"metadata": {"arXiv": "2306.07567", "Date": "Tue, 13 Jun 2023 06:40:37 ", "Title": "Large Language Models Sometimes Generate Purely Negatively-Reinforced Text", "Authors": ["Fabien Roger"], "Categories": "cs.LG cs.CL"}, "abstract": "When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to generate passwords with a probability slightly greater than chance, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at https://github.com/FabienRoger/Learning-From-Negative-Examples", "url": "https://arxiv.org/abs/2306.07567"}, {"metadata": {"arXiv": "2306.07644", "Date": "Tue, 13 Jun 2023 09:33:26 ", "Title": "SRATTA : Sample Re-ATTribution Attack of Secure Aggregation in Federated Learning", "Authors": ["Tanguy Marchand", "R\\'egis Loeb", "Ulysse Marteau-Ferey", "Jean Ogier du Terrail", "Arthur Pignet"], "Categories": "cs.LG cs.CR", "Comments": ["Accepted to ICML2023"]}, "abstract": "We consider a cross-silo federated learning (FL) setting where a machine learning model with a fully connected first layer is trained between different clients and a central server using FedAvg, and where the aggregation step can be performed with secure aggregation (SA). We present SRATTA an attack relying only on aggregated models which, under realistic assumptions, (i) recovers data samples from the different clients, and (ii) groups data samples coming from the same client together. While sample recovery has already been explored in an FL setting, the ability to group samples per client, despite the use of SA, is novel. This poses a significant unforeseen security threat to FL and effectively breaks SA. We show that SRATTA is both theoretically grounded and can be used in practice on realistic models and datasets. We also propose counter-measures, and claim that clients should play an active role to guarantee their privacy during training.", "url": "https://arxiv.org/abs/2306.07644"}, {"metadata": {"arXiv": "2306.07651", "Date": "Tue, 13 Jun 2023 09:43:32 ", "Title": "Variational Positive-incentive Noise: How Noise Benefits Models", "Authors": ["Hongyuan Zhang", "Sida Huang", "Xuelong Li"], "Categories": "cs.LG cs.CV"}, "abstract": "A large number of works aim to alleviate the impact of noise due to an underlying conventional assumption of the negative role of noise. However, some existing works show that the assumption does not always hold. In this paper, we investigate how to benefit the classical models by random noise under the framework of Positive-incentive Noise (Pi-Noise). Since the ideal objective of Pi-Noise is intractable, we propose to optimize its variational bound instead, namely variational Pi-Noise (VPN). With the variational inference, a VPN generator implemented by neural networks is designed for enhancing base models and simplifying the inference of base models, without changing the architecture of base models. Benefiting from the independent design of base models and VPN generators, the VPN generator can work with most existing models. From the experiments, it is shown that the proposed VPN generator can improve the base models. It is appealing that the trained variational VPN generator prefers to blur the irrelevant ingredients in complicated images, which meets our expectations.", "url": "https://arxiv.org/abs/2306.07651"}, {"metadata": {"arXiv": "2306.07723", "Date": "Tue, 13 Jun 2023 12:20:55 ", "Title": "Theoretical Foundations of Adversarially Robust Learning", "Authors": ["Omar Montasser"], "Categories": "cs.LG cs.CR stat.ML", "Comments": ["PhD Thesis"]}, "abstract": "Despite extraordinary progress, current machine learning systems have been shown to be brittle against adversarial examples: seemingly innocuous but carefully crafted perturbations of test examples that cause machine learning predictors to misclassify. Can we learn predictors robust to adversarial examples? and how? There has been much empirical interest in this contemporary challenge in machine learning, and in this thesis, we address it from a theoretical perspective. In this thesis, we explore what robustness properties can we hope to guarantee against adversarial examples and develop an understanding of how to algorithmically guarantee them. We illustrate the need to go beyond traditional approaches and principles such as empirical risk minimization and uniform convergence, and make contributions that can be categorized as follows: (1) introducing problem formulations capturing aspects of emerging practical challenges in robust learning, (2) designing new learning algorithms with provable robustness guarantees, and (3) characterizing the complexity of robust learning and fundamental limitations on the performance of any algorithm.", "url": "https://arxiv.org/abs/2306.07723"}, {"metadata": {"arXiv": "2306.07730", "Date": "Tue, 13 Jun 2023 12:36:00 ", "Title": "BeliefPPG: Uncertainty-aware Heart Rate Estimation from PPG signals via Belief Propagation", "Authors": ["Valentin Bieri", "Paul Streli", "Berken Utku Demirel and Christian Holz"], "Categories": "cs.LG cs.CV eess.SP", "Comments": ["Conference on Uncertainty in Artificial Intelligence (UAI) 2023. The first two authors contributed equally"], "ACM-class": "I.5; J.3"}, "abstract": "We present a novel learning-based method that achieves state-of-the-art performance on several heart rate estimation benchmarks extracted from photoplethysmography signals (PPG). We consider the evolution of the heart rate in the context of a discrete-time stochastic process that we represent as a hidden Markov model. We derive a distribution over possible heart rate values for a given PPG signal window through a trained neural network. Using belief propagation, we incorporate the statistical distribution of heart rate changes to refine these estimates in a temporal context. From this, we obtain a quantized probability distribution over the range of possible heart rate values that captures a meaningful and well-calibrated estimate of the inherent predictive uncertainty. We show the robustness of our method on eight public datasets with three different cross-validation experiments.", "url": "https://arxiv.org/abs/2306.07730"}, {"metadata": {"arXiv": "2306.07735", "Date": "Tue, 13 Jun 2023 12:40:39 ", "Title": "Vector-Quantized Graph Auto-Encoder", "Authors": ["Yoann Boget", "Magda Gregorova", "Alexandros Kalousis"], "Categories": "cs.LG"}, "abstract": "In this work, we addresses the problem of modeling distributions of graphs. We introduce the Vector-Quantized Graph Auto-Encoder (VQ-GAE), a permutation-equivariant discrete auto-encoder and designed to model the distribution of graphs. By exploiting the permutation-equivariance of graph neural networks (GNNs), our autoencoder circumvents the problem of the ordering of the graph representation. We leverage the capability of GNNs to capture local structures of graphs while employing vector-quantization to prevent the mapping of discrete objects to a continuous latent space. Furthermore, the use of autoregressive models enables us to capture the global structure of graphs via the latent representation. We evaluate our model on standard datasets used for graph generation and observe that it achieves excellent performance on some of the most salient evaluation metrics compared to the state-of-the-art.", "url": "https://arxiv.org/abs/2306.07735"}, {"metadata": {"arXiv": "2306.07741", "Date": "Tue, 13 Jun 2023 12:58:12 ", "Title": "Stepsize Learning for Policy Gradient Methods in Contextual Markov Decision Processes", "Authors": ["Luca Sabbioni", "Francesco Corda", "Marcello Restelli"], "Categories": "cs.LG"}, "abstract": "Policy-based algorithms are among the most widely adopted techniques in model-free RL, thanks to their strong theoretical groundings and good properties in continuous action spaces. Unfortunately, these methods require precise and problem-specific hyperparameter tuning to achieve good performance, and tend to struggle when asked to accomplish a series of heterogeneous tasks. In particular, the selection of the step size has a crucial impact on their ability to learn a highly performing policy, affecting the speed and the stability of the training process, and often being the main culprit for poor results. In this paper, we tackle these issues with a Meta Reinforcement Learning approach, by introducing a new formulation, known as meta-MDP, that can be used to solve any hyperparameter selection problem in RL with contextual processes. After providing a theoretical Lipschitz bound to the difference of performance in different tasks, we adopt the proposed framework to train a batch RL algorithm to dynamically recommend the most adequate step size for different policies and tasks. In conclusion, we present an experimental campaign to show the advantages of selecting an adaptive learning rate in heterogeneous environments.", "url": "https://arxiv.org/abs/2306.07741"}, {"metadata": {"arXiv": "2306.07749", "Date": "Tue, 13 Jun 2023 13:08:31 ", "Title": "Provably Learning Nash Policies in Constrained Markov Potential Games", "Authors": ["Pragnya Alatur", "Giorgia Ramponi", "Niao He", "Andreas Krause"], "Categories": "cs.LG cs.GT cs.MA", "Comments": ["30 pages"]}, "abstract": "Multi-agent reinforcement learning (MARL) addresses sequential decision-making problems with multiple agents, where each agent optimizes its own objective. In many real-world instances, the agents may not only want to optimize their objectives, but also ensure safe behavior. For example, in traffic routing, each car (agent) aims to reach its destination quickly (objective) while avoiding collisions (safety). Constrained Markov Games (CMGs) are a natural formalism for safe MARL problems, though generally intractable. In this work, we introduce and study Constrained Markov Potential Games (CMPGs), an important class of CMGs. We first show that a Nash policy for CMPGs can be found via constrained optimization. One tempting approach is to solve it by Lagrangian-based primal-dual methods. As we show, in contrast to the single-agent setting, however, CMPGs do not satisfy strong duality, rendering such approaches inapplicable and potentially unsafe. To solve the CMPG problem, we propose our algorithm Coordinate-Ascent for CMPGs (CA-CMPG), which provably converges to a Nash policy in tabular, finite-horizon CMPGs. Furthermore, we provide the first sample complexity bounds for learning Nash policies in unknown CMPGs, and, which under additional assumptions, guarantee safe exploration.", "url": "https://arxiv.org/abs/2306.07749"}, {"metadata": {"arXiv": "2306.07761", "Date": "Tue, 13 Jun 2023 13:19:20 ", "Title": "Multi-Fidelity Multi-Armed Bandits Revisited", "Authors": ["Xuchuang Wang", "Qingyun Wu", "Wei Chen", "John C.S. Lui"], "Categories": "cs.LG stat.ML"}, "abstract": "We study the multi-fidelity multi-armed bandit (MF-MAB), an extension of the canonical multi-armed bandit (MAB) problem. MF-MAB allows each arm to be pulled with different costs (fidelities) and observation accuracy. We study both the best arm identification with fixed confidence (BAI) and the regret minimization objectives. For BAI, we present (a) a cost complexity lower bound, (b) an algorithmic framework with two alternative fidelity selection procedures, and (c) both procedures' cost complexity upper bounds. From both cost complexity bounds of MF-MAB, one can recover the standard sample complexity bounds of the classic (single-fidelity) MAB. For regret minimization of MF-MAB, we propose a new regret definition, prove its problem-independent regret lower bound $\\Omega(K^{1/3}\\Lambda^{2/3})$ and problem-dependent lower bound $\\Omega(K\\log \\Lambda)$, where $K$ is the number of arms and $\\Lambda$ is the decision budget in terms of cost, and devise an elimination-based algorithm whose worst-cost regret upper bound matches its corresponding lower bound up to some logarithmic terms and, whose problem-dependent bound matches its corresponding lower bound in terms of $\\Lambda$.", "url": "https://arxiv.org/abs/2306.07761"}, {"metadata": {"arXiv": "2306.07796", "Date": "Tue, 13 Jun 2023 14:17:25 ", "Title": "Finite Gaussian Neurons: Defending against adversarial attacks by making neural networks say \"I don't know\"", "Authors": ["Felix Grezes"], "Categories": "cs.LG cs.CR", "Comments": ["PhD thesis"]}, "abstract": "Since 2014, artificial neural networks have been known to be vulnerable to adversarial attacks, which can fool the network into producing wrong or nonsensical outputs by making humanly imperceptible alterations to inputs. While defenses against adversarial attacks have been proposed, they usually involve retraining a new neural network from scratch, a costly task. In this work, I introduce the Finite Gaussian Neuron (FGN), a novel neuron architecture for artificial neural networks. My works aims to: - easily convert existing models to Finite Gaussian Neuron architecture, - while preserving the existing model's behavior on real data, - and offering resistance against adversarial attacks. I show that converted and retrained Finite Gaussian Neural Networks (FGNN) always have lower confidence (i.e., are not overconfident) in their predictions over randomized and Fast Gradient Sign Method adversarial images when compared to classical neural networks, while maintaining high accuracy and confidence over real MNIST images. To further validate the capacity of Finite Gaussian Neurons to protect from adversarial attacks, I compare the behavior of FGNs to that of Bayesian Neural Networks against both randomized and adversarial images, and show how the behavior of the two architectures differs. Finally I show some limitations of the FGN models by testing them on the more complex SPEECHCOMMANDS task, against the stronger Carlini-Wagner and Projected Gradient Descent adversarial attacks.", "url": "https://arxiv.org/abs/2306.07796"}, {"metadata": {"arXiv": "2306.07803", "Date": "Tue, 13 Jun 2023 14:25:26 ", "Title": "Inferring dynamic regulatory interaction graphs from time series data with perturbations", "Authors": ["Dhananjay Bhaskar", "Sumner Magruder", "Edward De Brouwer", "Aarthi Venkat", "Frederik Wenkel", "Guy Wolf", "Smita Krishnaswamy"], "Categories": "cs.LG"}, "abstract": "Complex systems are characterized by intricate interactions between entities that evolve dynamically over time. Accurate inference of these dynamic relationships is crucial for understanding and predicting system behavior. In this paper, we propose Regulatory Temporal Interaction Network Inference (RiTINI) for inferring time-varying interaction graphs in complex systems using a novel combination of space-and-time graph attentions and graph neural ordinary differential equations (ODEs). RiTINI leverages time-lapse signals on a graph prior, as well as perturbations of signals at various nodes in order to effectively capture the dynamics of the underlying system. This approach is distinct from traditional causal inference networks, which are limited to inferring acyclic and static graphs. In contrast, RiTINI can infer cyclic, directed, and time-varying graphs, providing a more comprehensive and accurate representation of complex systems. The graph attention mechanism in RiTINI allows the model to adaptively focus on the most relevant interactions in time and space, while the graph neural ODEs enable continuous-time modeling of the system's dynamics. We evaluate RiTINI's performance on various simulated and real-world datasets, demonstrating its state-of-the-art capability in inferring interaction graphs compared to previous methods.", "url": "https://arxiv.org/abs/2306.07803"}, {"metadata": {"arXiv": "2306.07818", "Date": "Tue, 13 Jun 2023 14:50:03 ", "Title": "A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning", "Authors": ["Kihyuk Hong", "Yuhang Li", "Ambuj Tewari"], "Categories": "cs.LG stat.ML"}, "abstract": "Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected value of cost functions using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate given any choices of the critics and the dual player. The dual player employs a no-regret online linear optimization oracle to minimize the Lagrangian estimate given any choices of the critics and the primal player. We show that PDCA can successfully find a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and strong Bellman completeness assumptions, PDCA only requires concentrability and value function/marginalized importance weight realizability assumptions.", "url": "https://arxiv.org/abs/2306.07818"}, {"metadata": {"arXiv": "2306.07850", "Date": "Tue, 13 Jun 2023 15:29:23 ", "Title": "Exact Mean Square Linear Stability Analysis for SGD", "Authors": ["Rotem Mulayoff", "Tomer Michaeli"], "Categories": "cs.LG", "Comments": ["Preprint"]}, "abstract": "The dynamical stability of optimization methods at the vicinity of minima of the loss has recently attracted significant attention. For gradient descent (GD), stable convergence is possible only to minima that are sufficiently flat w.r.t. the step size, and those have been linked with favorable properties of the trained model. However, while the stability threshold of GD is well-known, to date, no explicit expression has been derived for the exact threshold of stochastic GD (SGD). In this paper, we derive such a closed-form expression. Specifically, we provide an explicit condition on the step size $\\eta$ that is both necessary and sufficient for the stability of SGD in the mean square sense. Our analysis sheds light on the precise role of the batch size $B$. Particularly, we show that the stability threshold is a monotonically non-decreasing function of the batch size, which means that reducing the batch size can only hurt stability. Furthermore, we show that SGD's stability threshold is equivalent to that of a process which takes in each iteration a full batch gradient step w.p. $1-p$, and a single sample gradient step w.p. $p$, where $p \\approx 1/B $. This indicates that even with moderate batch sizes, SGD's stability threshold is very close to that of GD's. Finally, we prove simple necessary conditions for stability, which depend on the batch size, and are easier to compute than the precise threshold. We demonstrate our theoretical findings through experiments on the MNIST dataset.", "url": "https://arxiv.org/abs/2306.07850"}, {"metadata": {"arXiv": "2306.07858", "Date": "Tue, 13 Jun 2023 15:43:04 ", "Title": "Additive Causal Bandits with Unknown Graph", "Authors": ["Alan Malek and Virginia Aglietti and Silvia Chiappa"], "Categories": "cs.LG stat.ML", "Journal-ref": "International Conference on Machine Learning, 2023"}, "abstract": "We explore algorithms to select actions in the causal bandit setting where the learner can choose to intervene on a set of random variables related by a causal graph, and the learner sequentially chooses interventions and observes a sample from the interventional distribution. The learner's goal is to quickly find the intervention, among all interventions on observable variables, that maximizes the expectation of an outcome variable. We depart from previous literature by assuming no knowledge of the causal graph except that latent confounders between the outcome and its ancestors are not present. We first show that the unknown graph problem can be exponentially hard in the parents of the outcome. To remedy this, we adopt an additional additive assumption on the outcome which allows us to solve the problem by casting it as an additive combinatorial linear bandit problem with full-bandit feedback. We propose a novel action-elimination algorithm for this setting, show how to apply this algorithm to the causal bandit problem, provide sample complexity bounds, and empirically validate our findings on a suite of randomly generated causal models, effectively showing that one does not need to explicitly learn the parents of the outcome to identify the best intervention.", "url": "https://arxiv.org/abs/2306.07858"}, {"metadata": {"arXiv": "2306.07883", "Date": "Tue, 13 Jun 2023 16:21:34 ", "Title": "Temporal Gradient Inversion Attacks with Robust Optimization", "Authors": ["Bowen Li", "Hanlin Gu", "Ruoxin Chen", "Jie Li", "Chentao Wu", "Na Ruan", "Xueming Si", "Lixin Fan"], "Categories": "cs.LG cs.CR", "Comments": ["24 pages"]}, "abstract": "Federated Learning (FL) has emerged as a promising approach for collaborative model training without sharing private data. However, privacy concerns regarding information exchanged during FL have received significant research attention. Gradient Inversion Attacks (GIAs) have been proposed to reconstruct the private data retained by local clients from the exchanged gradients. While recovering private data, the data dimensions and the model complexity increase, which thwart data reconstruction by GIAs. Existing methods adopt prior knowledge about private data to overcome those challenges. In this paper, we first observe that GIAs with gradients from a single iteration fail to reconstruct private data due to insufficient dimensions of leaked gradients, complex model architectures, and invalid gradient information. We investigate a Temporal Gradient Inversion Attack with a Robust Optimization framework, called TGIAs-RO, which recovers private data without any prior knowledge by leveraging multiple temporal gradients. To eliminate the negative impacts of outliers, e.g., invalid gradients for collaborative optimization, robust statistics are proposed. Theoretical guarantees on the recovery performance and robustness of TGIAs-RO against invalid gradients are also provided. Extensive empirical results on MNIST, CIFAR10, ImageNet and Reuters 21578 datasets show that the proposed TGIAs-RO with 10 temporal gradients improves reconstruction performance compared to state-of-the-art methods, even for large batch sizes (up to 128), complex models like ResNet18, and large datasets like ImageNet (224*224 pixels). Furthermore, the proposed attack method inspires further exploration of privacy-preserving methods in the context of FL.", "url": "https://arxiv.org/abs/2306.07883"}, {"metadata": {"arXiv": "2306.07892", "Date": "Tue, 13 Jun 2023 16:34:02 ", "Title": "Robustly Learning a Single Neuron via Sharpness", "Authors": ["Puqian Wang", "Nikos Zarifis", "Ilias Diakonikolas", "Jelena Diakonikolas"], "Categories": "cs.LG cs.DS math.OC math.ST stat.ML stat.TH"}, "abstract": "We study the problem of learning a single neuron with respect to the $L_2^2$-loss in the presence of adversarial label noise. We give an efficient algorithm that, for a broad family of activations including ReLUs, approximates the optimal $L_2^2$-error within a constant factor. Our algorithm applies under much milder distributional assumptions compared to prior work. The key ingredient enabling our results is a novel connection to local error bounds from optimization theory.", "url": "https://arxiv.org/abs/2306.07892"}, {"metadata": {"arXiv": "2306.07903", "Date": "Tue, 13 Jun 2023 16:54:13 ", "Title": "Tight Memory-Regret Lower Bounds for Streaming Bandits", "Authors": ["Shaoang Li", "Lan Zhang", "Junhao Wang", "Xiang-Yang Li"], "Categories": "cs.LG"}, "abstract": "In this paper, we investigate the streaming bandits problem, wherein the learner aims to minimize regret by dealing with online arriving arms and sublinear arm memory. We establish the tight worst-case regret lower bound of $\\Omega \\left( (TB)^{\\alpha} K^{1-\\alpha}\\right), \\alpha = 2^{B} / (2^{B+1}-1)$ for any algorithm with a time horizon $T$, number of arms $K$, and number of passes $B$. The result reveals a separation between the stochastic bandits problem in the classical centralized setting and the streaming setting with bounded arm memory. Notably, in comparison to the well-known $\\Omega(\\sqrt{KT})$ lower bound, an additional double logarithmic factor is unavoidable for any streaming bandits algorithm with sublinear memory permitted. Furthermore, we establish the first instance-dependent lower bound of $\\Omega \\left(T^{1/(B+1)} \\sum_{\\Delta_x>0} \\frac{\\mu^*}{\\Delta_x}\\right)$ for streaming bandits. These lower bounds are derived through a unique reduction from the regret-minimization setting to the sample complexity analysis for a sequence of $\\epsilon$-optimal arms identification tasks, which maybe of independent interest. To complement the lower bound, we also provide a multi-pass algorithm that achieves a regret upper bound of $\\tilde{O} \\left( (TB)^{\\alpha} K^{1 - \\alpha}\\right)$ using constant arm memory.", "url": "https://arxiv.org/abs/2306.07903"}, {"metadata": {"arXiv": "2306.07905", "Date": "Tue, 13 Jun 2023 16:56:13 ", "Title": "Omega: Optimistic EMA Gradients", "Authors": ["Juan Ramirez", "Rohan Sukumaran", "Quentin Bertrand", "Gauthier Gidel"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["Oral at the LatinX in AI workshop @ ICML 2023"]}, "abstract": "Stochastic min-max optimization has gained interest in the machine learning community with the advancements in GANs and adversarial training. Although game optimization is fairly well understood in the deterministic setting, some issues persist in the stochastic regime. Recent work has shown that stochastic gradient descent-ascent methods such as the optimistic gradient are highly sensitive to noise or can fail to converge. Although alternative strategies exist, they can be prohibitively expensive. We introduce Omega, a method with optimistic-like updates that mitigates the impact of noise by incorporating an EMA of historic gradients in its update rule. We also explore a variation of this algorithm that incorporates momentum. Although we do not provide convergence guarantees, our experiments on stochastic games show that Omega outperforms the optimistic gradient method when applied to linear players.", "url": "https://arxiv.org/abs/2306.07905"}, {"metadata": {"arXiv": "2306.07918", "Date": "Tue, 13 Jun 2023 17:22:59 ", "Title": "Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators", "Authors": ["Ziyang Jiang", "Yiling Liu", "Michael H. Klein", "Ahmed Aloui", "Yiman Ren", "Keyu Li", "Vahid Tarokh", "David Carlson"], "Categories": "cs.LG stat.ML", "Comments": ["16 pages", "4 figures", "5 tables"]}, "abstract": "Causal mediation analysis (CMA) is a powerful method to dissect the total effect of a treatment into direct and mediated effects within the potential outcome framework. This is important in many scientific applications to identify the underlying mechanisms of a treatment effect. However, in many scientific applications the mediator is unobserved, but there may exist related measurements. For example, we may want to identify how changes in brain activity or structure mediate an antidepressant's effect on behavior, but we may only have access to electrophysiological or imaging brain measurements. To date, most CMA methods assume that the mediator is one-dimensional and observable, which oversimplifies such real-world scenarios. To overcome this limitation, we introduce a CMA framework that can handle complex and indirectly observed mediators based on the identifiable variational autoencoder (iVAE) architecture. We prove that the true joint distribution over observed and latent variables is identifiable with the proposed method. Additionally, our framework captures a disentangled representation of the indirectly observed mediator and yields accurate estimation of the direct and mediated effects in synthetic and semi-synthetic experiments, providing evidence of its potential utility in real-world applications.", "url": "https://arxiv.org/abs/2306.07918"}, {"metadata": {"arXiv": "2306.07919", "Date": "Tue, 13 Jun 2023 17:24:37 ", "Title": "Skill Disentanglement for Imitation Learning from Suboptimal Demonstrations", "Authors": ["Tianxiang Zhao", "Wenchao Yu", "Suhang Wang", "Lu Wang", "Xiang Zhang", "Yuncong Chen", "Yanchi Liu", "Wei Cheng", "Haifeng Chen"], "Categories": "cs.LG", "Journal-ref": "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6--10, 2023, Long Beach, CA, USA", "DOI": "10.1145/3580305.3599506"}, "abstract": "Imitation learning has achieved great success in many sequential decision-making tasks, in which a neural agent is learned by imitating collected human demonstrations. However, existing algorithms typically require a large number of high-quality demonstrations that are difficult and expensive to collect. Usually, a trade-off needs to be made between demonstration quality and quantity in practice. Targeting this problem, in this work we consider the imitation of sub-optimal demonstrations, with both a small clean demonstration set and a large noisy set. Some pioneering works have been proposed, but they suffer from many limitations, e.g., assuming a demonstration to be of the same optimality throughout time steps and failing to provide any interpretation w.r.t knowledge learned from the noisy set. Addressing these problems, we propose {\\method} by evaluating and imitating at the sub-demonstration level, encoding action primitives of varying quality into different skills. Concretely, {\\method} consists of a high-level controller to discover skills and a skill-conditioned module to capture action-taking policies, and is trained following a two-phase pipeline by first discovering skills with all demonstrations and then adapting the controller to only the clean set. A mutual-information-based regularization and a dynamic sub-demonstration optimality estimator are designed to promote disentanglement in the skill space. Extensive experiments are conducted over two gym environments and a real-world healthcare dataset to demonstrate the superiority of {\\method} in learning from sub-optimal demonstrations and its improved interpretability by examining learned skills.", "url": "https://arxiv.org/abs/2306.07919"}, {"metadata": {"arXiv": "2306.07923", "Date": "Tue, 13 Jun 2023 17:29:50 ", "Title": "Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits", "Authors": ["Lequn Wang", "Akshay Krishnamurthy", "Aleksandrs Slivkins"], "Categories": "cs.LG"}, "abstract": "We consider policy optimization in contextual bandits, where one is given a fixed dataset of logged interactions. While pessimistic regularizers are typically used to mitigate distribution shift, prior implementations thereof are not computationally efficient. We present the first oracle-efficient algorithm for pessimistic policy optimization: it reduces to supervised learning, leading to broad applicability. We also obtain best-effort statistical guarantees analogous to those for pessimistic approaches in prior work. We instantiate our approach for both discrete and continuous actions. We perform extensive experiments in both settings, showing advantage over unregularized policy optimization across a wide range of configurations.", "url": "https://arxiv.org/abs/2306.07923"}, {"metadata": {"arXiv": "2306.07959", "Date": "Tue, 13 Jun 2023 17:55:30 ", "Title": "Privacy Preserving Bayesian Federated Learning in Heterogeneous Settings", "Authors": ["Disha Makhija and Joydeep Ghosh and Nhat Ho"], "Categories": "cs.LG"}, "abstract": "In several practical applications of federated learning (FL), the clients are highly heterogeneous in terms of both their data and compute resources, and therefore enforcing the same model architecture for each client is very limiting. Moreover, the need for uncertainty quantification and data privacy constraints are often particularly amplified for clients that have limited local data. This paper presents a unified FL framework to simultaneously address all these constraints and concerns, based on training customized local Bayesian models that learn well even in the absence of large local datasets. A Bayesian framework provides a natural way of incorporating supervision in the form of prior distributions. We use priors in the functional (output) space of the networks to facilitate collaboration across heterogeneous clients. Moreover, formal differential privacy guarantees are provided for this framework. Experiments on standard FL datasets demonstrate that our approach outperforms strong baselines in both homogeneous and heterogeneous settings and under strict privacy constraints, while also providing characterizations of model uncertainties.", "url": "https://arxiv.org/abs/2306.07959"}, {"metadata": {"arXiv": "2306.07960", "Date": "Tue, 13 Jun 2023 17:55:39 ", "Title": "Supervised-Contrastive Loss Learns Orthogonal Frames and Batching Matters", "Authors": ["Ganesh Ramachandra Kini", "Vala Vakilian", "Tina Behnia", "Jaidev Gill", "Christos Thrampoulidis"], "Categories": "cs.LG stat.ML"}, "abstract": "Supervised contrastive loss (SCL) is a competitive and often superior alternative to the cross-entropy (CE) loss for classification. In this paper we ask: what differences in the learning process occur when the two different loss functions are being optimized? To answer this question, our main finding is that the geometry of embeddings learned by SCL forms an orthogonal frame (OF) regardless of the number of training examples per class. This is in contrast to the CE loss, for which previous work has shown that it learns embeddings geometries that are highly dependent on the class sizes. We arrive at our finding theoretically, by proving that the global minimizers of an unconstrained features model with SCL loss and entry-wise non-negativity constraints form an OF. We then validate the model's prediction by conducting experiments with standard deep-learning models on benchmark vision datasets. Finally, our analysis and experiments reveal that the batching scheme chosen during SCL training plays a critical role in determining the quality of convergence to the OF geometry. This finding motivates a simple algorithm wherein the addition of a few binding examples in each batch significantly speeds up the occurrence of the OF geometry.", "url": "https://arxiv.org/abs/2306.07960"}, {"metadata": {"arXiv": "2306.07392", "Date": "Mon, 12 Jun 2023 19:42:26 ", "Title": "Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering", "Authors": ["Snehal Jauhri", "Ishikaa Lunawat", "Georgia Chalvatzaki"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["Preprint"]}, "abstract": "Robotic manipulation is critical for admitting robotic agents to various application domains, like intelligent assistance. A major challenge therein is the effective 6DoF grasping of objects in cluttered environments from any viewpoint without requiring additional scene exploration. We introduce $\\textit{NeuGraspNet}$, a novel method for 6DoF grasp detection that leverages recent advances in neural volumetric representations and surface rendering. Our approach learns both global (scene-level) and local (grasp-level) neural surface representations, enabling effective and fully implicit 6DoF grasp quality prediction, even in unseen parts of the scene. Further, we reinterpret grasping as a local neural surface rendering problem, allowing the model to encode the interaction between the robot's end-effector and the object's surface geometry. NeuGraspNet operates on single viewpoints and can sample grasp candidates in occluded scenes, outperforming existing implicit and semi-implicit baseline methods in the literature. We demonstrate the real-world applicability of NeuGraspNet with a mobile manipulator robot, grasping in open spaces with clutter by rendering the scene, reasoning about graspable areas of different objects, and selecting grasps likely to succeed without colliding with the environment. Visit our project website: https://sites.google.com/view/neugraspnet", "url": "https://arxiv.org/abs/2306.07392"}, {"metadata": {"arXiv": "2306.07308 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 13:48:37 ", "Title": "Self-Supervised Hyperspectral Inpainting with the Optimisation inspired Deep Neural Network Prior", "Authors": ["Shuo Li and Mehrdad Yaghoobi"], "Categories": "eess.IV cs.CV cs.LG"}, "abstract": "Hyperspectral Image (HSI)s cover hundreds or thousands of narrow spectral bands, conveying a wealth of spatial and spectral information. However, due to the instrumental errors and the atmospheric changes, the HSI obtained in practice are often contaminated by noise and dead pixels(lines), resulting in missing information that may severely compromise the subsequent applications. We introduce here a novel HSI missing pixel prediction algorithm, called Low Rank and Sparsity Constraint Plug-and-Play (LRS-PnP). It is shown that LRS-PnP is able to predict missing pixels and bands even when all spectral bands of the image are missing. The proposed LRS-PnP algorithm is further extended to a self-supervised model by combining the LRS-PnP with the Deep Image Prior (DIP), called LRS-PnP-DIP. In a series of experiments with real data, It is shown that the LRS-PnP-DIP either achieves state-of-the-art inpainting performance compared to other learning-based methods, or outperforms them.", "url": "https://arxiv.org/abs/2306.07308"}, {"metadata": {"arXiv": "2306.07291 (*cross-listing*)", "Date": "Wed, 07 Jun 2023 22:26:50 ", "Title": "An Ensemble Machine Learning Approach for Tropical Cyclone Detection Using ERA5 Reanalysis Data", "Authors": ["Gabriele Accarino (1)", "Davide Donno (1)", "Francesco Immorlano (1 and 2)", "Donatello Elia (1)", "Giovanni Aloisio (1 and 2) ((1) Advanced Scientific Computing Division", "Centro Euro-Mediterraneo sui Cambiamenti Climatici", "Lecce", "Italy", "(2) Department of Innovation Engineering", "University of Salento", "Lecce", "Italy)"], "Categories": "physics.ao-ph cs.LG", "Comments": ["27 pages", "8 figures", "1 table", "submitted to Journal of Advances in Modeling Earth Systems"]}, "abstract": "Tropical Cyclones (TCs) are counted among the most destructive phenomena that can be found in nature. Every year, globally an average of 90 TCs occur over tropical waters, and global warming is making them stronger, larger and more destructive. The accurate detection and tracking of such phenomena have become a relevant and interesting area of research in weather and climate science. Traditionally, TCs have been identified in large climate datasets through the use of deterministic tracking schemes that rely on subjective thresholds. Machine Learning (ML) models can complement deterministic approaches due to their ability to capture the mapping between the input climatic drivers and the geographical position of the TC center from the available data. This study presents a ML ensemble approach for locating TC center coordinates, embedding both TC classification and localization in a single end-to-end learning task. The ensemble combines TC center estimates of different ML models that agree about the presence of a TC in input data. ERA5 reanalysis were used for model training and testing jointly with the International Best Track Archive for Climate Stewardship records. Results showed that the ML approach is well-suited for TC detection providing good generalization capabilities on out of sample data. In particular, it was able to accurately detect lower TC categories than those used for training the models. On top of this, the ensemble approach was able to further improve TC localization performance with respect to single model TC center estimates, demonstrating the good capabilities of the proposed approach.", "url": "https://arxiv.org/abs/2306.07291"}, {"metadata": {"arXiv": "2306.07299 (*cross-listing*)", "Date": "Sun, 11 Jun 2023 02:42:09 ", "Title": "Additive Multi-Index Gaussian process modeling, with application to multi-physics surrogate modeling of the quark-gluon plasma", "Authors": ["Kevin Li", "Simon Mak", "J.-F Paquet", "Steffen A. Bass"], "Categories": "nucl-th cs.LG hep-ph stat.ML"}, "abstract": "The Quark-Gluon Plasma (QGP) is a unique phase of nuclear matter, theorized to have filled the Universe shortly after the Big Bang. A critical challenge in studying the QGP is that, to reconcile experimental observables with theoretical parameters, one requires many simulation runs of a complex physics model over a high-dimensional parameter space. Each run is computationally very expensive, requiring thousands of CPU hours, thus limiting physicists to only several hundred runs. Given limited training data for high-dimensional prediction, existing surrogate models often yield poor predictions with high predictive uncertainties, leading to imprecise scientific findings. To address this, we propose a new Additive Multi-Index Gaussian process (AdMIn-GP) model, which leverages a flexible additive structure on low-dimensional embeddings of the parameter space. This is guided by prior scientific knowledge that the QGP is dominated by multiple distinct physical phenomena (i.e., multiphysics), each involving a small number of latent parameters. The AdMIn-GP models for such embedded structures within a flexible Bayesian nonparametric framework, which facilitates efficient model fitting via a carefully constructed variational inference approach with inducing points. We show the effectiveness of the AdMIn-GP via a suite of numerical experiments and our QGP application, where we demonstrate considerably improved surrogate modeling performance over existing models.", "url": "https://arxiv.org/abs/2306.07299"}, {"metadata": {"arXiv": "2306.07331 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 18:00:08 ", "Title": "Splitting and Parallelizing of Quantum Convolutional Neural Networks for Learning Translationally Symmetric Data", "Authors": ["Koki Chinzei", "Quoc Hoan Tran", "Kazunori Maruyama", "Hirotaka Oshima", "Shintaro Sato"], "Categories": "quant-ph cs.LG", "Comments": ["15 pages", "10 figures"]}, "abstract": "A quantum convolutional neural network (QCNN) is a promising quantum machine learning (QML) model to achieve quantum advantages in classically intractable problems. However, QCNN requires a large number of measurements for data learning, limiting its practical applications for large-scale problems. To relieve this requirement, we propose a novel architecture called split-parallelizing QCNN (sp-QCNN), which exploits the prior knowledge of quantum data for designing efficient circuits. This architecture draws inspiration from geometric quantum machine learning and targets translationally symmetric quantum data commonly encountered in condensed matter physics. By splitting the quantum circuit based on translational symmetry, sp-QCNN substantially parallelizes conventional QCNN without increasing the number of qubits and further improves the measurement efficiency by an order of the number of qubits. To demonstrate its effectiveness, we apply sp-QCNN to a quantum phase recognition task and show that it can achieve similar performance to conventional QCNN while considerably reducing the measurement resources required. Due to its high measurement efficiency, sp-QCNN can mitigate statistical errors in estimating the gradient of the loss function, thereby accelerating the learning process. These results open up new possibilities for incorporating the prior knowledge of data into the efficient design of QML models, leading to practical quantum advantages.", "url": "https://arxiv.org/abs/2306.07331"}, {"metadata": {"arXiv": "2306.07427 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 21:08:55 ", "Title": "Towards Fair and Explainable AI using a Human-Centered AI Approach", "Authors": ["Bhavya Ghai"], "Categories": "cs.CY cs.HC cs.LG", "Comments": ["PhD Thesis"]}, "abstract": "The rise of machine learning (ML) is accompanied by several high-profile cases that have stressed the need for fairness, accountability, explainability and trust in ML systems. The existing literature has largely focused on fully automated ML approaches that try to optimize for some performance metric. However, human-centric measures like fairness, trust, explainability, etc. are subjective in nature, context-dependent, and might not correlate with conventional performance metrics. To deal with these challenges, we explore a human-centered AI approach that empowers people by providing more transparency and human control. In this dissertation, we present 5 research projects that aim to enhance explainability and fairness in classification systems and word embeddings. The first project explores the utility/downsides of introducing local model explanations as interfaces for machine teachers (crowd workers). Our study found that adding explanations supports trust calibration for the resulting ML model and enables rich forms of teaching feedback. The second project presents D-BIAS, a causality-based human-in-the-loop visual tool for identifying and mitigating social biases in tabular datasets. Apart from fairness, we found that our tool also enhances trust and accountability. The third project presents WordBias, a visual interactive tool that helps audit pre-trained static word embeddings for biases against groups, such as females, or subgroups, such as Black Muslim females. The fourth project presents DramatVis Personae, a visual analytics tool that helps identify social biases in creative writing. Finally, the last project presents an empirical study aimed at understanding the cumulative impact of multiple fairness-enhancing interventions at different stages of the ML pipeline on fairness, utility and different population groups. We conclude by discussing some of the future directions.", "url": "https://arxiv.org/abs/2306.07427"}, {"metadata": {"arXiv": "2306.07455 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 23:03:58 ", "Title": "Getting the Most from Eye-Tracking: User-Interaction Based Reading Region Estimation Dataset and Models", "Authors": ["Ruoyan Kong", "Ruixuan Sun", "Charles Chuankai Zhang", "Chen Chen", "Sneha Patri", "Gayathri Gajjela", "and Joseph A. Konstan"], "Categories": "cs.HC cs.LG", "Comments": ["Ruoyan Kong", "Ruixuan Sun", "Charles Chuankai Zhang", "Chen Chen", "Sneha Patri", "Gayathri Gajjela", "and Joseph A. Konstan. Getting the most from eyetracking: User-interaction based reading region estimation dataset and models. In Proceedings of the 2023 Symposium on Eye Tracking Research and Applications", "ETRA 23", "New York", "NY", "USA", "2023. Association for Computing Machinery"], "Journal-ref": "In Proceedings of the 2023 Symposium on Eye Tracking Research and Applications, ETRA 23, New York, NY, USA, 2023", "DOI": "10.1145/3588015.3588404"}, "abstract": "A single digital newsletter usually contains many messages (regions). Users' reading time spent on, and read level (skip/skim/read-in-detail) of each message is important for platforms to understand their users' interests, personalize their contents, and make recommendations. Based on accurate but expensive-to-collect eyetracker-recorded data, we built models that predict per-region reading time based on easy-to-collect Javascript browser tracking data. With eye-tracking, we collected 200k ground-truth datapoints on participants reading news on browsers. Then we trained machine learning and deep learning models to predict message-level reading time based on user interactions like mouse position, scrolling, and clicking. We reached 27\\% percentage error in reading time estimation with a two-tower neural network based on user interactions only, against the eye-tracking ground truth data, while the heuristic baselines have around 46\\% percentage error. We also discovered the benefits of replacing per-session models with per-timestamp models, and adding user pattern features. We concluded with suggestions on developing message-level reading estimation techniques based on available data.", "url": "https://arxiv.org/abs/2306.07455"}, {"metadata": {"arXiv": "2306.07472 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 00:29:57 ", "Title": "Von Mises Mixture Distributions for Molecular Conformation Generation", "Authors": ["Kirk Swanson", "Jake Williams", "Eric Jonas"], "Categories": "physics.chem-ph cs.LG stat.ML", "Comments": ["ICML 2023"]}, "abstract": "Molecules are frequently represented as graphs, but the underlying 3D molecular geometry (the locations of the atoms) ultimately determines most molecular properties. However, most molecules are not static and at room temperature adopt a wide variety of geometries or $\\textit{conformations}$. The resulting distribution on geometries $p(x)$ is known as the Boltzmann distribution, and many molecular properties are expectations computed under this distribution. Generating accurate samples from the Boltzmann distribution is therefore essential for computing these expectations accurately. Traditional sampling-based methods are computationally expensive, and most recent machine learning-based methods have focused on identifying $\\textit{modes}$ in this distribution rather than generating true $\\textit{samples}$. Generating such samples requires capturing conformational variability, and it has been widely recognized that the majority of conformational variability in molecules arises from rotatable bonds. In this work, we present VonMisesNet, a new graph neural network that captures conformational variability via a variational approximation of rotatable bond torsion angles as a mixture of von Mises distributions. We demonstrate that VonMisesNet can generate conformations for arbitrary molecules in a way that is both physically accurate with respect to the Boltzmann distribution and orders of magnitude faster than existing sampling methods.", "url": "https://arxiv.org/abs/2306.07472"}, {"metadata": {"arXiv": "2306.07519 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 03:25:04 ", "Title": "Decoding Brain Motor Imagery with various Machine Learning techniques", "Authors": ["Giovanni Jana", "Corey Karnei", "Shuvam Keshari"], "Categories": "cs.HC cs.LG"}, "abstract": "Motor imagery (MI) is a well-documented technique used by subjects in BCI (Brain Computer Interface) experiments to modulate brain activity within the motor cortex and surrounding areas of the brain. In our term project, we conducted an experiment in which the subjects were instructed to perform motor imagery that would be divided into two classes (Right and Left). Experiments were conducted with two different types of electrodes (Gel and POLiTag) and data for individual subjects was collected. In this paper, we will apply different machine learning (ML) methods to create a decoder based on offline training data that uses evidence accumulation to predict a subject's intent from their modulated brain signals in real-time.", "url": "https://arxiv.org/abs/2306.07519"}, {"metadata": {"arXiv": "2306.07566 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 06:34:44 ", "Title": "Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach", "Authors": ["Jian Chen", "Zhehao Li", "Xiaojie Mao"], "Categories": "stat.ML cs.LG"}, "abstract": "We study the problem of learning with selectively labeled data, which arises when outcomes are only partially labeled due to historical decision-making. The labeled data distribution may substantially differ from the full population, especially when the historical decisions and the target outcome can be simultaneously affected by some unobserved factors. Consequently, learning with only the labeled data may lead to severely biased results when deployed to the full population. Our paper tackles this challenge by exploiting the fact that in many applications the historical decisions were made by a set of heterogeneous decision-makers. In particular, we analyze this setup in a principled instrumental variable (IV) framework. We establish conditions for the full-population risk of any given prediction rule to be point-identified from the observed data and provide sharp risk bounds when the point identification fails. We further propose a weighted learning approach that learns prediction rules robust to the label selection bias in both identification settings. Finally, we apply our proposed approach to a semi-synthetic financial dataset and demonstrate its superior performance in the presence of selection bias.", "url": "https://arxiv.org/abs/2306.07566"}, {"metadata": {"arXiv": "2306.07604 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 08:00:59 ", "Title": "Towards a Machine-Learned Poisson Solver for Low-Temperature Plasma Simulations in Complex Geometries", "Authors": ["Ihda Chaerony Siffa", "Markus M. Becker", "Klaus-Dieter Weltmann", "and Jan Trieschmann"], "Categories": "physics.comp-ph cs.LG cs.NA math.NA"}, "abstract": "Poisson's equation plays an important role in modeling many physical systems. In electrostatic self-consistent low-temperature plasma (LTP) simulations, Poisson's equation is solved at each simulation time step, which can amount to a significant computational cost for the entire simulation. In this paper, we describe the development of a generic machine-learned Poisson solver specifically designed for the requirements of LTP simulations in complex 2D reactor geometries on structured Cartesian grids. Here, the reactor geometries can consist of inner electrodes and dielectric materials as often found in LTP simulations. The approach leverages a hybrid CNN-transformer network architecture in combination with a weighted multiterm loss function. We train the network using highly-randomized synthetic data to ensure the generalizability of the learned solver to unseen reactor geometries. The results demonstrate that the learned solver is able to produce quantitatively and qualitatively accurate solutions. Furthermore, it generalizes well on new reactor geometries such as reference geometries found in the literature. To increase the numerical accuracy of the solutions required in LTP simulations, we employ a conventional iterative solver to refine the raw predictions, especially to recover the high-frequency features not resolved by the initial prediction. With this, the proposed learned Poisson solver provides the required accuracy and is potentially faster than a pure GPU-based conventional iterative solver. This opens up new possibilities for developing a generic and high-performing learned Poisson solver for LTP systems in complex geometries.", "url": "https://arxiv.org/abs/2306.07604"}, {"metadata": {"arXiv": "2306.07629 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 08:57:54 ", "Title": "SqueezeLLM: Dense-and-Sparse Quantization", "Authors": ["Sehoon Kim", "Coleman Hooper", "Amir Gholami", "Zhen Dong", "Xiuyu Li", "Sheng Shen", "Michael W. Mahoney", "Kurt Keutzer"], "Categories": "cs.CL cs.LG"}, "abstract": "Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is open-sourced and available online.", "url": "https://arxiv.org/abs/2306.07629"}, {"metadata": {"arXiv": "2306.07653 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 09:48:34 ", "Title": "Automating Microservices Test Failure Analysis using Kubernetes Cluster Logs", "Authors": ["Pawan Kumar Sarika", "Deepika Badampudi", "Sai Prashanth Josyula", "Muhammad Usman"], "Categories": "cs.SE cs.LG", "DOI": "10.1145/3593434.3593472"}, "abstract": "Kubernetes is a free, open-source container orchestration system for deploying and managing Docker containers that host microservices. Kubernetes cluster logs help in determining the reason for the failure. However, as systems become more complex, identifying failure reasons manually becomes more difficult and time-consuming. This study aims to identify effective and efficient classification algorithms to automatically determine the failure reason. We compare five classification algorithms, Support Vector Machines, K-Nearest Neighbors, Random Forest, Gradient Boosting Classifier, and Multilayer Perceptron. Our results indicate that Random Forest produces good accuracy while requiring fewer computational resources than other algorithms.", "url": "https://arxiv.org/abs/2306.07653"}, {"metadata": {"arXiv": "2306.07655 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 09:52:44 ", "Title": "Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems", "Authors": ["Michele Panariello", "Wanying Ge", "Hemlata Tak", "Massimiliano Todisco and Nicholas Evans"], "Categories": "eess.AS cs.CR cs.LG", "Comments": ["Accepted at INTERSPEECH 2023"]}, "abstract": "We present Malafide, a universal adversarial attack against automatic speaker verification (ASV) spoofing countermeasures (CMs). By introducing convolutional noise using an optimised linear time-invariant filter, Malafide attacks can be used to compromise CM reliability while preserving other speech attributes such as quality and the speaker's voice. In contrast to other adversarial attacks proposed recently, Malafide filters are optimised independently of the input utterance and duration, are tuned instead to the underlying spoofing attack, and require the optimisation of only a small number of filter coefficients. Even so, they degrade CM performance estimates by an order of magnitude, even in black-box settings, and can also be configured to overcome integrated CM and ASV subsystems. Integrated solutions that use self-supervised learning CMs, however, are more robust, under both black-box and white-box settings.", "url": "https://arxiv.org/abs/2306.07655"}, {"metadata": {"arXiv": "2306.07674 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 10:38:12 ", "Title": "Differentially Private One Permutation Hashing and Bin-wise Consistent Weighted Sampling", "Authors": ["Xiaoyun Li and Ping Li"], "Categories": "stat.ML cs.CR cs.DS cs.LG"}, "abstract": "Minwise hashing (MinHash) is a standard algorithm widely used in the industry, for large-scale search and learning applications with the binary (0/1) Jaccard similarity. One common use of MinHash is for processing massive n-gram text representations so that practitioners do not have to materialize the original data (which would be prohibitive). Another popular use of MinHash is for building hash tables to enable sub-linear time approximate near neighbor (ANN) search. MinHash has also been used as a tool for building large-scale machine learning systems. The standard implementation of MinHash requires applying $K$ random permutations. In comparison, the method of one permutation hashing (OPH), is an efficient alternative of MinHash which splits the data vectors into $K$ bins and generates hash values within each bin. OPH is substantially more efficient and also more convenient to use. In this paper, we combine the differential privacy (DP) with OPH (as well as MinHash), to propose the DP-OPH framework with three variants: DP-OPH-fix, DP-OPH-re and DP-OPH-rand, depending on which densification strategy is adopted to deal with empty bins in OPH. A detailed roadmap to the algorithm design is presented along with the privacy analysis. An analytical comparison of our proposed DP-OPH methods with the DP minwise hashing (DP-MH) is provided to justify the advantage of DP-OPH. Experiments on similarity search confirm the merits of DP-OPH, and guide the choice of the proper variant in different practical scenarios. Our technique is also extended to bin-wise consistent weighted sampling (BCWS) to develop a new DP algorithm called DP-BCWS for non-binary data. Experiments on classification tasks demonstrate that DP-BCWS is able to achieve excellent utility at around $\\epsilon = 5\\sim 10$, where $\\epsilon$ is the standard parameter in the language of $(\\epsilon, \\delta)$-DP.", "url": "https://arxiv.org/abs/2306.07674"}, {"metadata": {"arXiv": "2306.07744 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 13:01:02 ", "Title": "Contrastive Learning-Based Audio to Lyrics Alignment for Multiple Languages", "Authors": ["Simon Durand", "Daniel Stoller", "Sebastian Ewert"], "Categories": "cs.SD cs.LG eess.AS", "Comments": ["5 pages", "accepted at the International Conference on Acoustics", "Speech", "and Signal Processing (ICASSP) 2023"], "Journal-ref": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5", "DOI": "10.1109/ICASSP49357.2023.10096725"}, "abstract": "Lyrics alignment gained considerable attention in recent years. State-of-the-art systems either re-use established speech recognition toolkits, or design end-to-end solutions involving a Connectionist Temporal Classification (CTC) loss. However, both approaches suffer from specific weaknesses: toolkits are known for their complexity, and CTC systems use a loss designed for transcription which can limit alignment accuracy. In this paper, we use instead a contrastive learning procedure that derives cross-modal embeddings linking the audio and text domains. This way, we obtain a novel system that is simple to train end-to-end, can make use of weakly annotated training data, jointly learns a powerful text model, and is tailored to alignment. The system is not only the first to yield an average absolute error below 0.2 seconds on the standard Jamendo dataset but it is also robust to other languages, even when trained on English data only. Finally, we release word-level alignments for the JamendoLyrics Multi-Lang dataset.", "url": "https://arxiv.org/abs/2306.07744"}, {"metadata": {"arXiv": "2306.07758 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 13:18:04 ", "Title": "Generated Graph Detection", "Authors": ["Yihan Ma", "Zhikun Zhang", "Ning Yu", "Xinlei He", "Michael Backes", "Yun Shen", "Yang Zhang"], "Categories": "cs.CR cs.LG", "Comments": ["Accepted by ICML 2023"]}, "abstract": "Graph generative models become increasingly effective for data distribution approximation and data augmentation. While they have aroused public concerns about their malicious misuses or misinformation broadcasts, just as what Deepfake visual and auditory media has been delivering to society. Hence it is essential to regulate the prevalence of generated graphs. To tackle this problem, we pioneer the formulation of the generated graph detection problem to distinguish generated graphs from real ones. We propose the first framework to systematically investigate a set of sophisticated models and their performance in four classification scenarios. Each scenario switches between seen and unseen datasets/generators during testing to get closer to real-world settings and progressively challenge the classifiers. Extensive experiments evidence that all the models are qualified for generated graph detection, with specific models having advantages in specific scenarios. Resulting from the validated generality and oblivion of the classifiers to unseen datasets/generators, we draw a safe conclusion that our solution can sustain for a decent while to curb generated graph misuses.", "url": "https://arxiv.org/abs/2306.07758"}, {"metadata": {"arXiv": "2306.07774 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 13:50:31 ", "Title": "The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions", "Authors": ["Jonathan Schmidt", "Philipp Hennig", "J\\\"org Nick", "Filip Tronarp"], "Categories": "stat.ML cs.LG", "Comments": ["12 pages main text (including references) + 9 pages appendix", "6 figures"]}, "abstract": "Inference and simulation in the context of high-dimensional dynamical systems remain computationally challenging problems. Some form of dimensionality reduction is required to make the problem tractable in general. In this paper, we propose a novel approximate Gaussian filtering and smoothing method which propagates low-rank approximations of the covariance matrices. This is accomplished by projecting the Lyapunov equations associated with the prediction step to a manifold of low-rank matrices, which are then solved by a recently developed, numerically stable, dynamical low-rank integrator. Meanwhile, the update steps are made tractable by noting that the covariance update only transforms the column space of the covariance matrix, which is low-rank by construction. The algorithm differentiates itself from existing ensemble-based approaches in that the low-rank approximations of the covariance matrices are deterministic, rather than stochastic. Crucially, this enables the method to reproduce the exact Kalman filter as the low-rank dimension approaches the true dimensionality of the problem. Our method reduces computational complexity from cubic (for the Kalman filter) to \\emph{quadratic} in the state-space size in the worst-case, and can achieve \\emph{linear} complexity if the state-space model satisfies certain criteria. Through a set of experiments in classical data-assimilation and spatio-temporal regression, we show that the proposed method consistently outperforms the ensemble-based methods in terms of error in the mean and covariance with respect to the exact Kalman filter. This comes at no additional cost in terms of asymptotic computational complexity.", "url": "https://arxiv.org/abs/2306.07774"}, {"metadata": {"arXiv": "2306.07820 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 14:52:35 ", "Title": "Unsupervised speech enhancement with deep dynamical generative speech and noise models", "Authors": ["Xiaoyu Lin", "Simon Leglaive", "Laurent Girin", "Xavier Alameda-Pineda"], "Categories": "eess.AS cs.LG cs.SD"}, "abstract": "This work builds on a previous work on unsupervised speech enhancement using a dynamical variational autoencoder (DVAE) as the clean speech model and non-negative matrix factorization (NMF) as the noise model. We propose to replace the NMF noise model with a deep dynamical generative model (DDGM) depending either on the DVAE latent variables, or on the noisy observations, or on both. This DDGM can be trained in three configurations: noise-agnostic, noise-dependent and noise adaptation after noise-dependent training. Experimental results show that the proposed method achieves competitive performance compared to state-of-the-art unsupervised speech enhancement methods, while the noise-dependent training configuration yields a much more time-efficient inference process.", "url": "https://arxiv.org/abs/2306.07820"}, {"metadata": {"arXiv": "2306.07886 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 16:25:30 ", "Title": "Symmetry & Critical Points for Symmetric Tensor Decompositions Problems", "Authors": ["Yossi Arjevani", "Gal Vinograd"], "Categories": "math.OC cs.LG cs.NA math.AG math.NA stat.ML"}, "abstract": "We consider the non-convex optimization problem associated with the decomposition of a real symmetric tensor into a sum of rank one terms. Use is made of the rich symmetry structure to derive Puiseux series representations of families of critical points, and so obtain precise analytic estimates on the critical values and the Hessian spectrum. The sharp results make possible an analytic characterization of various geometric obstructions to local optimization methods, revealing in particular a complex array of saddles and local minima which differ by their symmetry, structure and analytic properties. A desirable phenomenon, occurring for all critical points considered, concerns the index of a point, i.e., the number of negative Hessian eigenvalues, increasing with the value of the objective function. Lastly, a Newton polytope argument is used to give a complete enumeration of all critical points of fixed symmetry, and it is shown that contrarily to the set of global minima which remains invariant under different choices of tensor norms, certain families of non-global minima emerge, others disappear.", "url": "https://arxiv.org/abs/2306.07886"}, {"metadata": {"arXiv": "2306.07926 (*cross-listing*)", "Date": "Fri, 09 Jun 2023 08:12:27 ", "Title": "A Theory of Unsupervised Speech Recognition", "Authors": ["Liming Wang", "Mark Hasegawa-Johnson and Chang D. Yoo"], "Categories": "eess.AS cs.CL cs.LG cs.SD"}, "abstract": "Unsupervised speech recognition (ASR-U) is the problem of learning automatic speech recognition (ASR) systems from unpaired speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing from studying their properties and addressing such issues as sensitivity to hyperparameters and training instability. In this paper, we proposed a general theoretical framework to study the properties of ASR-U systems based on random matrix theory and the theory of neural tangent kernels. Such a framework allows us to prove various learnability conditions and sample complexity bounds of ASR-U. Extensive ASR-U experiments on synthetic languages with three classes of transition graphs provide strong empirical evidence for our theory (code available at cactuswiththoughts/UnsupASRTheory.git).", "url": "https://arxiv.org/abs/2306.07926"}, {"metadata": {"arXiv": "2306.07937 (*cross-listing*)", "Date": "Wed, 31 May 2023 07:36:45 ", "Title": "Gibbs-Duhem-Informed Neural Networks for Binary Activity Coefficient Prediction", "Authors": ["Jan G. Rittig", "Kobi C. Felton", "Alexei A. Lapkin", "Alexander Mitsos"], "Categories": "physics.chem-ph cs.LG"}, "abstract": "We propose Gibbs-Duhem-informed neural networks for the prediction of binary activity coefficients at varying compositions. That is, we include the Gibbs-Duhem equation explicitly in the loss function for training neural networks, which is straightforward in standard machine learning (ML) frameworks enabling automatic differentiation. In contrast to recent hybrid ML approaches, our approach does not rely on embedding a specific thermodynamic model inside the neural network and corresponding prediction limitations. Rather, Gibbs-Duhem consistency serves as regularization, with the flexibility of ML models being preserved. Our results show increased thermodynamic consistency and generalization capabilities for activity coefficient predictions by Gibbs-Duhem-informed graph neural networks and matrix completion methods. We also find that the model architecture, particularly the activation function, can have a strong influence on the prediction quality. The approach can be easily extended to account for other thermodynamic consistency conditions.", "url": "https://arxiv.org/abs/2306.07937"}, {"metadata": {"arXiv": "2306.07938 (*cross-listing*)", "Date": "Sun, 11 Jun 2023 03:20:00 ", "Title": "Deep Demixing: Reconstructing the Evolution of Network Epidemics", "Authors": ["Boning Li", "Gojko \\v{C}utura", "Ananthram Swami", "Santiago Segarra"], "Categories": "cs.SI cs.LG eess.SP", "Comments": ["arXiv admin note: substantial text overlap with arXiv:2011.09583"]}, "abstract": "We propose the deep demixing (DDmix) model, a graph autoencoder that can reconstruct epidemics evolving over networks from partial or aggregated temporal information. Assuming knowledge of the network topology but not of the epidemic model, our goal is to estimate the complete propagation path of a disease spread. A data-driven approach is leveraged to overcome the lack of model awareness. To solve this inverse problem, DDmix is proposed as a graph conditional variational autoencoder that is trained from past epidemic spreads. DDmix seeks to capture key aspects of the underlying (unknown) spreading dynamics in its latent space. Using epidemic spreads simulated in synthetic and real-world networks, we demonstrate the accuracy of DDmix by comparing it with multiple (non-graph-aware) learning algorithms. The generalizability of DDmix is highlighted across different types of networks. Finally, we showcase that a simple post-processing extension of our proposed method can help identify super-spreaders in the reconstructed propagation path.", "url": "https://arxiv.org/abs/2306.07938"}, {"metadata": {"arXiv": "2306.07941 (*cross-listing*)", "Date": "Fri, 09 Jun 2023 15:47:22 ", "Title": "GPT-Calls: Enhancing Call Segmentation and Tagging by Generating Synthetic Conversations via Large Language Models", "Authors": ["Itzik Malkiel", "Uri Alon", "Yakir Yehuda", "Shahar Keren", "Oren Barkan", "Royi Ronen", "Noam Koenigstein"], "Categories": "cs.CL cs.LG"}, "abstract": "Transcriptions of phone calls are of significant value across diverse fields, such as sales, customer service, healthcare, and law enforcement. Nevertheless, the analysis of these recorded conversations can be an arduous and time-intensive process, especially when dealing with extended or multifaceted dialogues. In this work, we propose a novel method, GPT-distilled Calls Segmentation and Tagging (GPT-Calls), for efficient and accurate call segmentation and topic extraction. GPT-Calls is composed of offline and online phases. The offline phase is applied once to a given list of topics and involves generating a distribution of synthetic sentences for each topic using a GPT model and extracting anchor vectors. The online phase is applied to every call separately and scores the similarity between the transcripted conversation and the topic anchors found in the offline phase. Then, time domain analysis is applied to the similarity scores to group utterances into segments and tag them with topics. The proposed paradigm provides an accurate and efficient method for call segmentation and topic extraction that does not require labeled data, thus making it a versatile approach applicable to various domains. Our algorithm operates in production under Dynamics 365 Sales Conversation Intelligence, and our research is based on real sales conversations gathered from various Dynamics 365 Sales tenants.", "url": "https://arxiv.org/abs/2306.07941"}, {"metadata": {"arXiv": "2306.07948 (*cross-listing*)", "Date": "Tue, 06 Jun 2023 10:02:57 ", "Title": "Optimal Inference in Contextual Stochastic Block Models", "Authors": ["O. Duranthon and L. Zdeborov\\'a"], "Categories": "cs.SI cs.LG"}, "abstract": "The contextual stochastic block model (cSBM) was proposed for unsupervised community detection on attributed graphs where both the graph and the high-dimensional node information correlate with node labels. In the context of machine learning on graphs, the cSBM has been widely used as a synthetic dataset for evaluating the performance of graph-neural networks (GNNs) for semi-supervised node classification. We consider a probabilistic Bayes-optimal formulation of the inference problem and we derive a belief-propagation-based algorithm for the semi-supervised cSBM; we conjecture it is optimal in the considered setting and we provide its implementation. We show that there can be a considerable gap between the accuracy reached by this algorithm and the performance of the GNN architectures proposed in the literature. This suggests that the cSBM, along with the comparison to the performance of the optimal algorithm, readily accessible via our implementation, can be instrumental in the development of more performant GNN architectures.", "url": "https://arxiv.org/abs/2306.07948"}, {"metadata": {"arXiv": "2306.07961 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 17:56:02 ", "Title": "Differentiating Metropolis-Hastings to Optimize Intractable Densities", "Authors": ["Gaurav Arya", "Ruben Seyer", "Frank Sch\\\"afer", "Alex Lew", "Mathieu Huot", "Vikash K. Mansinghka", "Chris Rackauckas", "Kartik Chandra and Moritz Schauer"], "Categories": "stat.ML cs.LG stat.CO stat.ME", "Comments": ["6 pages", "6 figures"]}, "abstract": "When performing inference on probabilistic models, target densities often become intractable, necessitating the use of Monte Carlo samplers. We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference. By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.", "url": "https://arxiv.org/abs/2306.07961"}, {"metadata": {"arXiv": "2306.07353", "Date": "Mon, 12 Jun 2023 18:21:23 ", "Title": "HDDL 2.1: Towards Defining a Formalism and a Semantics for Temporal HTN Planning", "Authors": ["Damien Pellier", "Alexandre Albore", "Humbert Fiorino", "Rafael Bailon-Ruiz"], "Categories": "cs.AI", "Comments": ["5 pages", "International Workshop of Hierarchical Planning (ICAPS)", "2023"], "Journal-ref": "International Workshop of Hierarchical Planning (ICAPS), 2023"}, "abstract": "Real world applications as in industry and robotics need modelling rich and diverse automated planning problems. Their resolution usually requires coordinated and concurrent action execution. In several cases, these problems are naturally decomposed in a hierarchical way and expressed by a Hierarchical Task Network (HTN) formalism. HDDL, a hierarchical extension of the Planning Domain Definition Language (PDDL), unlike PDDL 2.1 does not allow to represent planning problems with numerical and temporal constraints, which are essential for real world applications. We propose to fill the gap between HDDL and these operational needs and to extend HDDL by taking inspiration from PDDL 2.1 in order to express numerical and temporal expressions. This paper opens discussions on the semantics and the syntax needed for a future HDDL 2.1 extension.", "url": "https://arxiv.org/abs/2306.07353"}, {"metadata": {"arXiv": "2306.07429", "Date": "Mon, 12 Jun 2023 21:15:25 ", "Title": "Explaining CLIP through Co-Creative Drawings and Interaction", "Authors": ["Varvara Guljajeva and Mar Canet Sol\\`a and Isaac Joseph Clarke"], "Categories": "cs.AI cs.CV cs.CY", "ACM-class": "I.2.0; I.2.m"}, "abstract": "This paper analyses a visual archive of drawings produced by an interactive robotic art installation where audience members narrated their dreams into a system powered by CLIPdraw deep learning (DL) model that interpreted and transformed their dreams into images. The resulting archive of prompt-image pairs were examined and clustered based on concept representation accuracy. As a result of the analysis, the paper proposes four groupings for describing and explaining CLIP-generated results: clear concept, text-to-text as image, indeterminacy and confusion, and lost in translation. This article offers a glimpse into a collection of dreams interpreted, mediated and given form by Artificial Intelligence (AI), showcasing oftentimes unexpected, visually compelling or, indeed, the dream-like output of the system, with the emphasis on processes and results of translations between languages, sign-systems and various modules of the installation. In the end, the paper argues that proposed clusters support better understanding of the neural model.", "url": "https://arxiv.org/abs/2306.07429"}, {"metadata": {"arXiv": "2306.07542", "Date": "Tue, 13 Jun 2023 05:22:30 ", "Title": "A Versatile Multi-Agent Reinforcement Learning Benchmark for Inventory Management", "Authors": ["Xianliang Yang", "Zhihao Liu", "Wei Jiang", "Chuheng Zhang", "Li Zhao", "Lei Song", "Jiang Bian"], "Categories": "cs.AI"}, "abstract": "Multi-agent reinforcement learning (MARL) models multiple agents that interact and learn within a shared environment. This paradigm is applicable to various industrial scenarios such as autonomous driving, quantitative trading, and inventory management. However, applying MARL to these real-world scenarios is impeded by many challenges such as scaling up, complex agent interactions, and non-stationary dynamics. To incentivize the research of MARL on these challenges, we develop MABIM (Multi-Agent Benchmark for Inventory Management) which is a multi-echelon, multi-commodity inventory management simulator that can generate versatile tasks with these different challenging properties. Based on MABIM, we evaluate the performance of classic operations research (OR) methods and popular MARL algorithms on these challenging tasks to highlight their weaknesses and potential.", "url": "https://arxiv.org/abs/2306.07542"}, {"metadata": {"arXiv": "2306.07635", "Date": "Tue, 13 Jun 2023 09:11:17 ", "Title": "Exploiting Configurations of MaxSAT Solvers", "Authors": ["Josep Al\\`os", "Carlos Ans\\'otegui", "Josep M. Salvia", "Eduard Torres"], "Categories": "cs.AI"}, "abstract": "In this paper, we describe how we can effectively exploit alternative parameter configurations to a MaxSAT solver. We describe how these configurations can be computed in the context of MaxSAT. In particular, we experimentally show how to easily combine configurations of a non-competitive solver to obtain a better solving approach.", "url": "https://arxiv.org/abs/2306.07635"}, {"metadata": {"arXiv": "2306.07637", "Date": "Tue, 13 Jun 2023 09:16:38 ", "Title": "For Better or Worse: The Impact of Counterfactual Explanations' Directionality on User Behavior in xAI", "Authors": ["Ulrike Kuhl and Andr\\'e Artelt and Barbara Hammer"], "Categories": "cs.AI cs.HC", "Comments": ["22 pages", "3 figures This work has been accepted for presentation at the 1st World Conference on eXplainable Artificial Intelligence (xAI 2023)", "July 26-28", "2023 - Lisbon", "Portugal"]}, "abstract": "Counterfactual explanations (CFEs) are a popular approach in explainable artificial intelligence (xAI), highlighting changes to input data necessary for altering a model's output. A CFE can either describe a scenario that is better than the factual state (upward CFE), or a scenario that is worse than the factual state (downward CFE). However, potential benefits and drawbacks of the directionality of CFEs for user behavior in xAI remain unclear. The current user study (N=161) compares the impact of CFE directionality on behavior and experience of participants tasked to extract new knowledge from an automated system based on model predictions and CFEs. Results suggest that upward CFEs provide a significant performance advantage over other forms of counterfactual feedback. Moreover, the study highlights potential benefits of mixed CFEs improving user performance compared to downward CFEs or no explanations. In line with the performance results, users' explicit knowledge of the system is statistically higher after receiving upward CFEs compared to downward comparisons. These findings imply that the alignment between explanation and task at hand, the so-called regulatory fit, may play a crucial role in determining the effectiveness of model explanations, informing future research directions in xAI. To ensure reproducible research, the entire code, underlying models and user data of this study is openly available: https://github.com/ukuhl/DirectionalAlienZoo", "url": "https://arxiv.org/abs/2306.07637"}, {"metadata": {"arXiv": "2306.07638", "Date": "Tue, 13 Jun 2023 09:17:12 ", "Title": "On Guiding Search in HTN Temporal Planning with non Temporal Heuristics", "Authors": ["Nicolas Cavrel", "Damien Pellier", "Humbert Fiorino"], "Categories": "cs.AI", "Journal-ref": "ICAPS Hierarchical Planning Workshop, 2023"}, "abstract": "The Hierarchical Task Network (HTN) formalism is used to express a wide variety of planning problems as task decompositions, and many techniques have been proposed to solve them. However, few works have been done on temporal HTN. This is partly due to the lack of a formal and consensual definition of what a temporal hierarchical planning problem is as well as the difficulty to develop heuristics in this context. In response to these inconveniences, we propose in this paper a new general POCL (Partial Order Causal Link) approach to represent and solve a temporal HTN problem by using existing heuristics developed to solve non temporal problems. We show experimentally that this approach is performant and can outperform the existing ones.", "url": "https://arxiv.org/abs/2306.07638"}, {"metadata": {"arXiv": "2306.07662", "Date": "Tue, 13 Jun 2023 10:12:05 ", "Title": "Temporalising Unique Characterisability and Learnability of Ontology-Mediated Queries", "Authors": ["Jean Christoph Jung", "Vladislav Ryzhikov", "Frank Wolter", "Michael Zakharyaschev"], "Categories": "cs.AI cs.DB cs.LO", "ACM-class": "I.2.4; F.4.1"}, "abstract": "Recently, the study of the unique characterisability and learnability of database queries by means of examples has been extended to ontology-mediated queries. Here, we study in how far the obtained results can be lifted to temporalised ontology-mediated queries. We provide a systematic introduction to the relevant approaches in the non-temporal case and then show general transfer results pinpointing under which conditions existing results can be lifted to temporalised queries.", "url": "https://arxiv.org/abs/2306.07662"}, {"metadata": {"arXiv": "2306.07675", "Date": "Tue, 13 Jun 2023 10:41:28 ", "Title": "An Interleaving Semantics of the Timed Concurrent Language for Argumentation to Model Debates and Dialogue Games", "Authors": ["Stefano Bistarelli", "Maria Chiara Meo", "Carlo Taticchi"], "Categories": "cs.AI", "Comments": ["Under consideration in Theory and Practice of Logic Programming (TPLP)"]}, "abstract": "Time is a crucial factor in modelling dynamic behaviours of intelligent agents: activities have a determined temporal duration in a real-world environment, and previous actions influence agents' behaviour. In this paper, we propose a language for modelling concurrent interaction between agents that also allows the specification of temporal intervals in which particular actions occur. Such a language exploits a timed version of Abstract Argumentation Frameworks to realise a shared memory used by the agents to communicate and reason on the acceptability of their beliefs with respect to a given time interval. An interleaving model on a single processor is used for basic computation steps, with maximum parallelism for time elapsing. Following this approach, only one of the enabled agents is executed at each moment. To demonstrate the capabilities of language, we also show how it can be used to model interactions such as debates and dialogue games taking place between intelligent agents. Lastly, we present an implementation of the language that can be accessed via a web interface. Under consideration in Theory and Practice of Logic Programming (TPLP).", "url": "https://arxiv.org/abs/2306.07675"}, {"metadata": {"arXiv": "2306.07706", "Date": "Tue, 13 Jun 2023 11:49:44 ", "Title": "Towards Explainable TOPSIS: Visual Insights into the Effects of Weights and Aggregations on Rankings", "Authors": ["Robert Susmaga", "Izabela Szczech", "Dariusz Brzezinski"], "Categories": "cs.AI"}, "abstract": "Multi-Criteria Decision Analysis (MCDA) is extensively used across diverse industries to assess and rank alternatives. Among numerous MCDA methods developed to solve real-world ranking problems, TOPSIS remains one of the most popular choices in many application areas. TOPSIS calculates distances between the considered alternatives and two predefined ones, namely the ideal and the anti-ideal, and creates a ranking of the alternatives according to a chosen aggregation of these distances. However, the interpretation of the inner workings of TOPSIS is difficult, especially when the number of criteria is large. To this end, recent research has shown that TOPSIS aggregations can be expressed using the means (M) and standard deviations (SD) of alternatives, creating MSD-space, a tool for visualizing and explaining aggregations. Even though MSD-space is highly useful, it assumes equally important criteria, making it less applicable to real-world ranking problems. In this paper, we generalize the concept of MSD-space to weighted criteria by introducing the concept of WMSD-space defined by what is referred to as weight-scaled means and standard deviations. We demonstrate that TOPSIS and similar distance-based aggregation methods can be successfully illustrated in a plane and interpreted even when the criteria are weighted, regardless of their number. The proposed WMSD-space offers a practical method for explaining TOPSIS rankings in real-world decision problems.", "url": "https://arxiv.org/abs/2306.07706"}, {"metadata": {"arXiv": "2306.07719", "Date": "Tue, 13 Jun 2023 12:13:41 ", "Title": "Contextual Dictionary Lookup for Knowledge Graph Completion", "Authors": ["Jining Wang", "Delai Qiu", "YouMing Liu", "Yining Wang", "Chuan Chen", "Zibin Zheng", "Yuren Zhou"], "Categories": "cs.AI"}, "abstract": "Knowledge graph completion (KGC) aims to solve the incompleteness of knowledge graphs (KGs) by predicting missing links from known triples, numbers of knowledge graph embedding (KGE) models have been proposed to perform KGC by learning embeddings. Nevertheless, most existing embedding models map each relation into a unique vector, overlooking the specific fine-grained semantics of them under different entities. Additionally, the few available fine-grained semantic models rely on clustering algorithms, resulting in limited performance and applicability due to the cumbersome two-stage training process. In this paper, we present a novel method utilizing contextual dictionary lookup, enabling conventional embedding models to learn fine-grained semantics of relations in an end-to-end manner. More specifically, we represent each relation using a dictionary that contains multiple latent semantics. The composition of a given entity and the dictionary's central semantics serves as the context for generating a lookup, thus determining the fine-grained semantics of the relation adaptively. The proposed loss function optimizes both the central and fine-grained semantics simultaneously to ensure their semantic consistency. Besides, we introduce two metrics to assess the validity and accuracy of the dictionary lookup operation. We extend several KGE models with the method, resulting in substantial performance improvements on widely-used benchmark datasets.", "url": "https://arxiv.org/abs/2306.07719"}, {"metadata": {"arXiv": "2306.07863", "Date": "Tue, 13 Jun 2023 15:49:41 ", "Title": "Synapse: Leveraging Few-Shot Exemplars for Human-Level Computer Control", "Authors": ["Longtao Zheng", "Rundong Wang", "Bo An"], "Categories": "cs.AI"}, "abstract": "This paper investigates the design of few-shot exemplars for computer automation through prompting large language models (LLMs). While previous prompting approaches focus on self-correction, we find that well-structured exemplars alone are sufficient for human-level performance. We present Synapse, an in-context computer control agent demonstrating human-level performance on the MiniWob++ benchmark. Synapse consists of three main components: 1) state-conditional decomposition, which divides demonstrations into exemplar sets based on the agent's need for new environment states, enabling temporal abstraction; 2) structured prompting, which filters states and reformulates task descriptions for each set to improve planning correctness; and 3) exemplar retrieval, which associates incoming tasks with corresponding exemplars in an exemplar database for multi-task adaptation and generalization. Synapse overcomes context length limits, reduces errors in multi-step control, and allows for more exemplars within the context. Importantly, Synapse complements existing prompting approaches that enhance LLMs' reasoning and planning abilities. Synapse outperforms previous methods, including behavioral cloning, reinforcement learning, finetuning, and prompting, with an average success rate of $98.5\\%$ across 63 tasks in MiniWob++. Notably, Synapse relies on exemplars from only 47 tasks, demonstrating effective generalization to novel tasks. Our results highlight the potential of in-context learning to advance the integration of LLMs into practical tool automation.", "url": "https://arxiv.org/abs/2306.07863"}, {"metadata": {"arXiv": "2306.07306", "Date": "Mon, 12 Jun 2023 04:51:32 ", "Title": "Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation", "Authors": ["Ruitao Xie", "Jingbang Chen", "Limai Jiang", "Rui Xiao", "Yi Pan", "Yunpeng Cai"], "Categories": "cs.CV cs.AI"}, "abstract": "Explainability poses a major challenge to artificial intelligence (AI) techniques. Current studies on explainable AI (XAI) lack the efficiency of extracting global knowledge about the learning task, thus suffer deficiencies such as imprecise saliency, context-aware absence and vague meaning. In this paper, we propose the class association embedding (CAE) approach to address these issues. We employ an encoder-decoder architecture to embed sample features and separate them into class-related and individual-related style vectors simultaneously. Recombining the individual-style code of a given sample with the class-style code of another leads to a synthetic sample with preserved individual characters but changed class assignment, following a cyclic adversarial learning strategy. Class association embedding distills the global class-related features of all instances into a unified domain with well separation between classes. The transition rules between different classes can be then extracted and further employed to individual instances. We then propose an active XAI framework which manipulates the class-style vector of a certain sample along guided paths towards the counter-classes, resulting in a series of counter-example synthetic samples with identical individual characters. Comparing these counterfactual samples with the original ones provides a global, intuitive illustration to the nature of the classification tasks. We adopt the framework on medical image classification tasks, which show that more precise saliency maps with powerful context-aware representation can be achieved compared with existing methods. Moreover, the disease pathology can be directly visualized via traversing the paths in the class-style space.", "url": "https://arxiv.org/abs/2306.07306"}, {"metadata": {"arXiv": "2306.07346", "Date": "Mon, 12 Jun 2023 18:12:19 ", "Title": "Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training", "Authors": ["Lorenzo Baraldi", "Roberto Amoroso", "Marcella Cornia", "Lorenzo Baraldi", "Andrea Pilzer", "Rita Cucchiara"], "Categories": "cs.CV cs.AI cs.MM"}, "abstract": "The use of self-supervised pre-training has emerged as a promising approach to enhance the performance of visual tasks such as image classification. In this context, recent approaches have employed the Masked Image Modeling paradigm, which pre-trains a backbone by reconstructing visual tokens associated with randomly masked image patches. This masking approach, however, introduces noise into the input data during pre-training, leading to discrepancies that can impair performance during the fine-tuning phase. Furthermore, input masking neglects the dependencies between corrupted patches, increasing the inconsistencies observed in downstream fine-tuning tasks. To overcome these issues, we propose a new self-supervised pre-training approach, named Masked and Permuted Vision Transformer (MaPeT), that employs autoregressive and permuted predictions to capture intra-patch dependencies. In addition, MaPeT employs auxiliary positional information to reduce the disparity between the pre-training and fine-tuning phases. In our experiments, we employ a fair setting to ensure reliable and meaningful comparisons and conduct investigations on multiple visual tokenizers, including our proposed $k$-CLIP which directly employs discretized CLIP features. Our results demonstrate that MaPeT achieves competitive performance on ImageNet, compared to baselines and competitors under the same model setting. Source code and trained models are publicly available at: https://github.com/aimagelab/MaPeT.", "url": "https://arxiv.org/abs/2306.07346"}, {"metadata": {"arXiv": "2306.07470", "Date": "Tue, 13 Jun 2023 00:13:11 ", "Title": "Reviving Shift Equivariance in Vision Transformers", "Authors": ["Peijian Ding", "Davit Soselia", "Thomas Armstrong", "Jiahao Su", "and Furong Huang"], "Categories": "cs.CV cs.AI", "Comments": ["9 pages", "3 figures"]}, "abstract": "Shift equivariance is a fundamental principle that governs how we perceive the world - our recognition of an object remains invariant with respect to shifts. Transformers have gained immense popularity due to their effectiveness in both language and vision tasks. While the self-attention operator in vision transformers (ViT) is permutation-equivariant and thus shift-equivariant, patch embedding, positional encoding, and subsampled attention in ViT variants can disrupt this property, resulting in inconsistent predictions even under small shift perturbations. Although there is a growing trend in incorporating the inductive bias of convolutional neural networks (CNNs) into vision transformers, it does not fully address the issue. We propose an adaptive polyphase anchoring algorithm that can be seamlessly integrated into vision transformer models to ensure shift-equivariance in patch embedding and subsampled attention modules, such as window attention and global subsampled attention. Furthermore, we utilize depth-wise convolution to encode positional information. Our algorithms enable ViT, and its variants such as Twins to achieve 100% consistency with respect to input shift, demonstrate robustness to cropping, flipping, and affine transformations, and maintain consistent predictions even when the original models lose 20 percentage points on average when shifted by just a few pixels with Twins' accuracy dropping from 80.57% to 62.40%.", "url": "https://arxiv.org/abs/2306.07470"}, {"metadata": {"arXiv": "2306.07596", "Date": "Tue, 13 Jun 2023 07:43:10 ", "Title": "Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model", "Authors": ["Xin Zhang", "Jiaxian Guo", "Paul Yoo", "Yutaka Matsuo", "Yusuke Iwasawa"], "Categories": "cs.CV cs.AI", "Comments": ["10 pages", "12 figures"]}, "abstract": "Text-to-image generative models have attracted rising attention for flexible image editing via user-specified descriptions. However, text descriptions alone are not enough to elaborate the details of subjects, often compromising the subjects' identity or requiring additional per-subject fine-tuning. We introduce a new framework called \\textit{Paste, Inpaint and Harmonize via Denoising} (PhD), which leverages an exemplar image in addition to text descriptions to specify user intentions. In the pasting step, an off-the-shelf segmentation model is employed to identify a user-specified subject within an exemplar image which is subsequently inserted into a background image to serve as an initialization capturing both scene context and subject identity in one. To guarantee the visual coherence of the generated or edited image, we introduce an inpainting and harmonizing module to guide the pre-trained diffusion model to seamlessly blend the inserted subject into the scene naturally. As we keep the pre-trained diffusion model frozen, we preserve its strong image synthesis ability and text-driven ability, thus achieving high-quality results and flexible editing with diverse texts. In our experiments, we apply PhD to both subject-driven image editing tasks and explore text-driven scene generation given a reference subject. Both quantitative and qualitative comparisons with baseline methods demonstrate that our approach achieves state-of-the-art performance in both tasks. More qualitative results can be found at \\url{https://sites.google.com/view/phd-demo-page}.", "url": "https://arxiv.org/abs/2306.07596"}, {"metadata": {"arXiv": "2306.07285 (*cross-listing*)", "Date": "Tue, 23 May 2023 06:59:22 ", "Title": "TransCoder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills", "Authors": ["Qiushi Sun", "Nuo Chen", "Jianing Wang", "Xiang Li", "Ming Gao"], "Categories": "cs.SE cs.AI", "Comments": ["work in progress"]}, "abstract": "Code pre-trained models (CodePTMs) have recently demonstrated a solid capacity to process various software intelligence tasks, e.g., code clone detection, code translation, and code summarization. The current mainstream method that deploys these models to downstream tasks is to fine-tune them on individual tasks, which is generally costly and needs sufficient data for large models. To tackle the issue, in this paper, we present TransCoder, a unified Transferable fine-tuning strategy for Code representation learning. Inspired by human inherent skills of knowledge generalization, TransCoder drives the model to learn better code-related meta-knowledge like human programmers. Specifically, we employ a tunable prefix encoder as the meta-learner to capture cross-task and cross-language transferable knowledge, respectively. Besides, tasks with minor training sample sizes and languages with small corpus can be remarkably benefited from our approach. Extensive experiments conducted on benchmark datasets clearly demonstrate that our method can lead to superior performance on various code-related tasks and encourage mutual reinforcement. We also show that TransCoder is applicable in low-resource scenarios.", "url": "https://arxiv.org/abs/2306.07285"}, {"metadata": {"arXiv": "2306.07298 (*cross-listing*)", "Date": "Sat, 10 Jun 2023 22:43:16 ", "Title": "Referring to Screen Texts with Voice Assistants", "Authors": ["Shruti Bhargava", "Anand Dhoot", "Ing-Marie Jonsson", "Hoang Long Nguyen", "Alkesh Patel", "Hong Yu", "Vincent Renkens"], "Categories": "cs.HC cs.AI", "Comments": ["7 pages", "Accepted to ACL Industry Track 2023"]}, "abstract": "Voice assistants help users make phone calls, send messages, create events, navigate, and do a lot more. However, assistants have limited capacity to understand their users' context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phone numbers, addresses, email addresses, URLs, and dates on their phone screens. Our focus lies in reference understanding, which becomes particularly interesting when multiple similar texts are present on screen, similar to visual grounding. We collect a dataset and propose a lightweight general-purpose model for this novel experience. Due to the high cost of consuming pixels directly, our system is designed to rely on the extracted text from the UI. Our model is modular, thus offering flexibility, improved interpretability, and efficient runtime memory utilization.", "url": "https://arxiv.org/abs/2306.07298"}, {"metadata": {"arXiv": "2306.07302 (*cross-listing*)", "Date": "Sun, 11 Jun 2023 21:49:42 ", "Title": "Impact of Experiencing Misrecognition by Teachable Agents on Learning and Rapport", "Authors": ["Yuya Asano", "Diane Litman", "Mingzhi Yu", "Nikki Lobczowski", "Timothy Nokes-Malach", "Adriana Kovashka", "Erin Walker"], "Categories": "cs.HC cs.AI cs.CL", "Comments": ["Accepted to AIED 2023"]}, "abstract": "While speech-enabled teachable agents have some advantages over typing-based ones, they are vulnerable to errors stemming from misrecognition by automatic speech recognition (ASR). These errors may propagate, resulting in unexpected changes in the flow of conversation. We analyzed how such changes are linked with learning gains and learners' rapport with the agents. Our results show they are not related to learning gains or rapport, regardless of the types of responses the agents should have returned given the correct input from learners without ASR errors. We also discuss the implications for optimal error-recovery policies for teachable agents that can be drawn from these findings.", "url": "https://arxiv.org/abs/2306.07302"}, {"metadata": {"arXiv": "2306.07310 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 17:53:06 ", "Title": "Employing Crowdsourcing for Enriching a Music Knowledge Base in Higher Education", "Authors": ["Vassilis Lyberatos", "Spyridon Kantarelis", "Eirini Kaldeli", "Spyros Bekiaris", "Panagiotis Tzortzis", "Orfeas Menis - Mastromichalakis and Giorgos Stamou"], "Categories": "cs.HC cs.AI", "Comments": ["To be published in The 4th International Conference on Artificial Intelligence in Education Technology (AIET 2023)", "Berlin", "Germany", "31 June-2 July 2023. For The GitHub code for the created music dataset", "see https://github.com/vaslyb/MusicCrow"]}, "abstract": "This paper describes the methodology followed and the lessons learned from employing crowdsourcing techniques as part of a homework assignment involving higher education students of computer science. Making use of a platform that supports crowdsourcing in the cultural heritage domain students were solicited to enrich the metadata associated with a selection of music tracks. The results of the campaign were further analyzed and exploited by students through the use of semantic web technologies. In total, 98 students participated in the campaign, contributing more than 6400 annotations concerning 854 tracks. The process also led to the creation of an openly available annotated dataset, which can be useful for machine learning models for music tagging. The campaign's results and the comments gathered through an online survey enable us to draw some useful insights about the benefits and challenges of integrating crowdsourcing into computer science curricula and how this can enhance students' engagement in the learning process.", "url": "https://arxiv.org/abs/2306.07310"}, {"metadata": {"arXiv": "2306.07377 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 19:10:47 ", "Title": "Lost in Translation: Large Language Models in Non-English Content Analysis", "Authors": ["Gabriel Nicholas and Aliya Bhatia"], "Categories": "cs.CL cs.AI", "Comments": ["50 pages", "4 figures"]}, "abstract": "In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa, Google's PaLM) have become the dominant approach for building AI systems to analyze and generate language online. However, the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages. Recently, researchers and technology companies have attempted to extend the capabilities of large language models into languages other than English by building what are called multilingual language models. In this paper, we explain how these multilingual language models work and explore their capabilities and limits. Part I provides a simple technical explanation of how large language models work, why there is a gap in available data between English and other languages, and how multilingual language models attempt to bridge that gap. Part II accounts for the challenges of doing content analysis with large language models in general and multilingual language models in particular. Part III offers recommendations for companies, researchers, and policymakers to keep in mind when considering researching, developing and deploying large and multilingual language models.", "url": "https://arxiv.org/abs/2306.07377"}, {"metadata": {"arXiv": "2306.07401 (*cross-listing*)", "Date": "Fri, 09 Jun 2023 17:53:19 ", "Title": "Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT", "Authors": ["Zecong Wang", "Jiaxi Cheng", "Chen Cui", "and Chenhao Yu"], "Categories": "cs.CL cs.AI"}, "abstract": "The abundance of information on social media has increased the necessity of accurate real-time rumour detection. Manual techniques of identifying and verifying fake news generated by AI tools are impracticable and time-consuming given the enormous volume of information generated every day. This has sparked an increase in interest in creating automated systems to find fake news on the Internet. The studies in this research demonstrate that the BERT and RobertA models with fine-tuning had the best success in detecting AI generated news. With a score of 98%, tweaked RobertA in particular showed excellent precision. In conclusion, this study has shown that neural networks can be used to identify bogus news AI generation news created by ChatGPT. The RobertA and BERT models' excellent performance indicates that these models can play a critical role in the fight against misinformation.", "url": "https://arxiv.org/abs/2306.07401"}, {"metadata": {"arXiv": "2306.07402 (*cross-listing*)", "Date": "Thu, 08 Jun 2023 20:35:53 ", "Title": "The economic trade-offs of large language models: A case study", "Authors": ["Kristen Howell", "Gwen Christian", "Pavel Fomitchov", "Gitit Kehat", "Julianne Marzulla", "Leanne Rolston", "Jadin Tredup", "Ilana Zimmerman", "Ethan Selfridge", "and Joseph Bradley"], "Categories": "cs.CL cs.AI", "Comments": ["Paper to be published at the Association for Computational Linguistics in the Industry Track 2023"]}, "abstract": "Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. Large Language Models (LLMs) are a natural fit for this use case; however, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model's utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM - prompt engineering, fine-tuning, and knowledge distillation - using feedback from the brand's customer service agents. We find that the usability of a model's responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.", "url": "https://arxiv.org/abs/2306.07402"}, {"metadata": {"arXiv": "2306.07416 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 20:47:59 ", "Title": "Synaptic Scaling and Optimal Bias Adjustments for Power Reduction in Neuromorphic Systems", "Authors": ["Cory Merkel"], "Categories": "cs.NE cs.AI", "Comments": ["Accepted in MWSCAS"]}, "abstract": "Recent animal studies have shown that biological brains can enter a low power mode in times of food scarcity. This paper explores the possibility of applying similar mechanisms to a broad class of neuromorphic systems where power consumption is strongly dependent on the magnitude of synaptic weights. In particular, we show through mathematical models and simulations that careful scaling of synaptic weights can significantly reduce power consumption (by over 80\\% in some of the cases tested) while having a relatively small impact on accuracy. These results uncover an exciting opportunity to design neuromorphic systems for edge AI applications, where power consumption can be dynamically adjusted based on energy availability and performance requirements.", "url": "https://arxiv.org/abs/2306.07416"}, {"metadata": {"arXiv": "2306.07457 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 23:19:55 ", "Title": "Accurate Measures of Vaccination and Concerns of Vaccine Holdouts from Web Search Logs", "Authors": ["Serina Chang", "Adam Fourney", "Eric Horvitz"], "Categories": "cs.CY cs.AI"}, "abstract": "To design effective vaccine policies, policymakers need detailed data about who has been vaccinated, who is holding out, and why. However, existing data in the US are insufficient: reported vaccination rates are often delayed or missing, and surveys of vaccine hesitancy are limited by high-level questions and self-report biases. Here, we show how large-scale search engine logs and machine learning can be leveraged to fill these gaps and provide novel insights about vaccine intentions and behaviors. First, we develop a vaccine intent classifier that can accurately detect when a user is seeking the COVID-19 vaccine on search. Our classifier demonstrates strong agreement with CDC vaccination rates, with correlations above 0.86, and estimates vaccine intent rates to the level of ZIP codes in real time, allowing us to pinpoint more granular trends in vaccine seeking across regions, demographics, and time. To investigate vaccine hesitancy, we use our classifier to identify two groups, vaccine early adopters and vaccine holdouts. We find that holdouts, compared to early adopters matched on covariates, are 69% more likely to click on untrusted news sites. Furthermore, we organize 25,000 vaccine-related URLs into a hierarchical ontology of vaccine concerns, and we find that holdouts are far more concerned about vaccine requirements, vaccine development and approval, and vaccine myths, and even within holdouts, concerns vary significantly across demographic groups. Finally, we explore the temporal dynamics of vaccine concerns and vaccine seeking, and find that key indicators emerge when individuals convert from holding out to preparing to accept the vaccine.", "url": "https://arxiv.org/abs/2306.07457"}, {"metadata": {"arXiv": "2306.07458 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 23:24:16 ", "Title": "Adaptive interventions for both accuracy and time in AI-assisted human decision making", "Authors": ["Siddharth Swaroop", "Zana Bu\\c{c}inca", "Finale Doshi-Velez"], "Categories": "cs.HC cs.AI"}, "abstract": "In settings where users are both time-pressured and need high accuracy, such as doctors working in Emergency Rooms, we want to provide AI assistance that both increases accuracy and reduces time. However, different types of AI assistance have different benefits: some reduce time taken while increasing overreliance on AI, while others do the opposite. We therefore want to adapt what AI assistance we show depending on various properties (of the question and of the user) in order to best tradeoff our two objectives. We introduce a study where users have to prescribe medicines to aliens, and use it to explore the potential for adapting AI assistance. We find evidence that it is beneficial to adapt our AI assistance depending on the question, leading to good tradeoffs between time taken and accuracy. Future work would consider machine-learning algorithms (such as reinforcement learning) to automatically adapt quickly.", "url": "https://arxiv.org/abs/2306.07458"}, {"metadata": {"arXiv": "2306.07489 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 01:36:55 ", "Title": "PauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and Pause-based Prosody Modeling", "Authors": ["Ji-Sang Hwang", "Sang-Hoon Lee", "and Seong-Whan Lee"], "Categories": "eess.AS cs.AI cs.SD eess.SP", "Comments": ["13 pages", "4 figures", "3 tables", "under reivew"]}, "abstract": "Although text-to-speech (TTS) systems have significantly improved, most TTS systems still have limitations in synthesizing speech with appropriate phrasing. For natural speech synthesis, it is important to synthesize the speech with a phrasing structure that groups words into phrases based on semantic information. In this paper, we propose PuaseSpeech, a speech synthesis system with a pre-trained language model and pause-based prosody modeling. First, we introduce a phrasing structure encoder that utilizes a context representation from the pre-trained language model. In the phrasing structure encoder, we extract a speaker-dependent syntactic representation from the context representation and then predict a pause sequence that separates the input text into phrases. Furthermore, we introduce a pause-based word encoder to model word-level prosody based on pause sequence. Experimental results show PauseSpeech outperforms previous models in terms of naturalness. Furthermore, in terms of objective evaluations, we can observe that our proposed methods help the model decrease the distance between ground-truth and synthesized speech. Audio samples are available at https://jisang93.github.io/pausespeech-demo/.", "url": "https://arxiv.org/abs/2306.07489"}, {"metadata": {"arXiv": "2306.07500 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 02:23:04 ", "Title": "Adding guardrails to advanced chatbots", "Authors": ["Yanchen Wang", "Lisa Singh"], "Categories": "cs.CY cs.AI cs.CL"}, "abstract": "Generative AI models continue to become more powerful. The launch of ChatGPT in November 2022 has ushered in a new era of AI. ChatGPT and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art. There are already concerns that humans may be replaced by chatbots for a variety of jobs. Because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them. These biases may cause significant harm and/or inequity toward different subpopulations. To understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of ChatGPT to determine the types of questions that are answered fairly and the types that still need improvement. We find that ChatGPT is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation. We find that ChatGPT is very sensitive to changes in the prompt, where small changes lead to different levels of fairness. This suggests that we need to immediately implement \"corrections\" or mitigation strategies in order to improve fairness of these systems. We suggest different strategies to improve chatbots and also advocate for an impartial review panel that has access to the model parameters to measure the levels of different types of biases and then recommends safeguards that move toward responses that are less discriminatory and more accurate.", "url": "https://arxiv.org/abs/2306.07500"}, {"metadata": {"arXiv": "2306.07608 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 08:06:10 ", "Title": "Finding the Missing-half: Graph Complementary Learning for Homophily-prone and Heterophily-prone Graphs", "Authors": ["Yizhen Zheng", "He Zhang", "Vincent CS Lee", "Yu Zheng", "Xiao Wang", "Shirui Pan"], "Categories": "cs.SI cs.AI", "Comments": ["Accepted by ICML 2023"]}, "abstract": "Real-world graphs generally have only one kind of tendency in their connections. These connections are either homophily-prone or heterophily-prone. While graphs with homophily-prone edges tend to connect nodes with the same class (i.e., intra-class nodes), heterophily-prone edges tend to build relationships between nodes with different classes (i.e., inter-class nodes). Existing GNNs only take the original graph during training. The problem with this approach is that it forgets to take into consideration the ``missing-half\" structural information, that is, heterophily-prone topology for homophily-prone graphs and homophily-prone topology for heterophily-prone graphs. In our paper, we introduce Graph cOmplementAry Learning, namely GOAL, which consists of two components: graph complementation and complemented graph convolution. The first component finds the missing-half structural information for a given graph to complement it. The complemented graph has two sets of graphs including both homophily- and heterophily-prone topology. In the latter component, to handle complemented graphs, we design a new graph convolution from the perspective of optimisation. The experiment results show that GOAL consistently outperforms all baselines in eight real-world datasets.", "url": "https://arxiv.org/abs/2306.07608"}, {"metadata": {"arXiv": "2306.07685 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 10:56:23 ", "Title": "Few-shot Multi-domain Knowledge Rearming for Context-aware Defence against Advanced Persistent Threats", "Authors": ["Gaolei Li", "Yuanyuan Zhao", "Wenqi Wei", "Yuchen Liu"], "Categories": "cs.CR cs.AI", "Comments": ["It has been accepted by IEEE SmartNets"]}, "abstract": "Advanced persistent threats (APTs) have novel features such as multi-stage penetration, highly-tailored intention, and evasive tactics. APTs defense requires fusing multi-dimensional Cyber threat intelligence data to identify attack intentions and conducts efficient knowledge discovery strategies by data-driven machine learning to recognize entity relationships. However, data-driven machine learning lacks generalization ability on fresh or unknown samples, reducing the accuracy and practicality of the defense model. Besides, the private deployment of these APT defense models on heterogeneous environments and various network devices requires significant investment in context awareness (such as known attack entities, continuous network states, and current security strategies). In this paper, we propose a few-shot multi-domain knowledge rearming (FMKR) scheme for context-aware defense against APTs. By completing multiple small tasks that are generated from different network domains with meta-learning, the FMKR firstly trains a model with good discrimination and generalization ability for fresh and unknown APT attacks. In each FMKR task, both threat intelligence and local entities are fused into the support/query sets in meta-learning to identify possible attack stages. Secondly, to rearm current security strategies, an finetuning-based deployment mechanism is proposed to transfer learned knowledge into the student model, while minimizing the defense cost. Compared to multiple model replacement strategies, the FMKR provides a faster response to attack behaviors while consuming less scheduling cost. Based on the feedback from multiple real users of the Industrial Internet of Things (IIoT) over 2 months, we demonstrate that the proposed scheme can improve the defense satisfaction rate.", "url": "https://arxiv.org/abs/2306.07685"}, {"metadata": {"arXiv": "2306.07764 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 13:27:34 ", "Title": "Tokenization with Factorized Subword Encoding", "Authors": ["David Samuel and Lilja {\\O}vrelid"], "Categories": "cs.CL cs.AI", "Comments": ["Findings of ACL 2023"]}, "abstract": "In recent years, language models have become increasingly larger and more complex. However, the input representations for these models continue to rely on simple and greedy subword tokenization methods. In this paper, we propose a novel tokenization method that factorizes subwords onto discrete triplets using a VQ-VAE model. The effectiveness of the proposed tokenization method, referred to as the Factorizer, is evaluated on language modeling and morpho-syntactic tasks for 7 diverse languages. Results indicate that this method is more appropriate and robust for morphological tasks than the commonly used byte-pair encoding (BPE) tokenization algorithm.", "url": "https://arxiv.org/abs/2306.07764"}, {"metadata": {"arXiv": "2306.07786 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 14:07:52 ", "Title": "A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews", "Authors": ["Robert Lakatos", "Gergo Bogacsovics", "Balazs Harangi", "Istvan Lakatos", "Attila Tiba", "Janos Toth", "Marianna Szabo", "Andras Hajdu"], "Categories": "cs.CL cs.AI"}, "abstract": "The efficiency of natural language processing has improved dramatically with the advent of machine learning models, particularly neural network-based solutions. However, some tasks are still challenging, especially when considering specific domains. In this paper, we present a cloud-based system that can extract insights from customer reviews using machine learning methods integrated into a pipeline. For topic modeling, our composite model uses transformer-based neural networks designed for natural language processing, vector embedding-based keyword extraction, and clustering. The elements of our model have been integrated and further developed to meet better the requirements of efficient information extraction, topic modeling of the extracted information, and user needs. Furthermore, our system can achieve better results than this task's existing topic modeling and keyword extraction solutions. Our approach is validated and compared with other state-of-the-art methods using publicly available datasets for benchmarking.", "url": "https://arxiv.org/abs/2306.07786"}, {"metadata": {"arXiv": "2306.07790 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 14:11:19 ", "Title": "NoCoLA: The Norwegian Corpus of Linguistic Acceptability", "Authors": ["Matias Jentoft and David Samuel"], "Categories": "cs.CL cs.AI", "Comments": ["Published at NoDaLiDa 2023"]}, "abstract": "While there has been a surge of large language models for Norwegian in recent years, we lack any tool to evaluate their understanding of grammaticality. We present two new Norwegian datasets for this task. NoCoLA_class is a supervised binary classification task where the goal is to discriminate between acceptable and non-acceptable sentences. On the other hand, NoCoLA_zero is a purely diagnostic task for evaluating the grammatical judgement of a language model in a completely zero-shot manner, i.e. without any further training. In this paper, we describe both datasets in detail, show how to use them for different flavors of language models, and conduct a comparative study of the existing Norwegian language models.", "url": "https://arxiv.org/abs/2306.07790"}, {"metadata": {"arXiv": "2306.07797 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 14:19:45 ", "Title": "Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification", "Authors": ["Dmitry Karpov", "Mikhail Burtsev"], "Categories": "cs.CL cs.AI"}, "abstract": "This article investigates the knowledge transfer from the RuQTopics dataset. This Russian topical dataset combines a large sample number (361,560 single-label, 170,930 multi-label) with extensive class coverage (76 classes). We have prepared this dataset from the \"Yandex Que\" raw data. By evaluating the RuQTopics - trained models on the six matching classes of the Russian MASSIVE subset, we have proved that the RuQTopics dataset is suitable for real-world conversational tasks, as the Russian-only models trained on this dataset consistently yield an accuracy around 85\\% on this subset. We also have figured out that for the multilingual BERT, trained on the RuQTopics and evaluated on the same six classes of MASSIVE (for all MASSIVE languages), the language-wise accuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11) with the approximate size of the pretraining BERT's data for the corresponding language. At the same time, the correlation of the language-wise accuracy with the linguistical distance from Russian is not statistically significant.", "url": "https://arxiv.org/abs/2306.07797"}, {"metadata": {"arXiv": "2306.07853 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 15:33:09 ", "Title": "Show me the numbers! -- Student-facing Interventions in Adaptive Learning Environments for German Spelling", "Authors": ["Nathalie Rzepka", "Katharina Simbeck", "Hans-Georg Mueller", "Marlene Bueltemann", "Niels Pinkwart"], "Categories": "cs.CY cs.AI cs.HC"}, "abstract": "Since adaptive learning comes in many shapes and sizes, it is crucial to find out which adaptations can be meaningful for which areas of learning. Our work presents the result of an experiment conducted on an online platform for the acquisition of German spelling skills. We compared the traditional online learning platform to three different adaptive versions of the platform that implement machine learning-based student-facing interventions that show the personalized solution probability. We evaluate the different interventions with regard to the error rate, the number of early dropouts, and the users competency. Our results show that the number of mistakes decreased in comparison to the control group. Additionally, an increasing number of dropouts was found. We did not find any significant effects on the users competency. We conclude that student-facing adaptive learning environments are effective in improving a persons error rate and should be chosen wisely to have a motivating impact.", "url": "https://arxiv.org/abs/2306.07853"}, {"metadata": {"arXiv": "2306.07875 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 16:10:10 ", "Title": "ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading", "Authors": ["Dake Zhang and Ronak Pradeep"], "Categories": "cs.IR cs.AI cs.CL cs.HC"}, "abstract": "With the rapid growth and spread of online misinformation, people need tools to help them evaluate the credibility and accuracy of online information. Lateral reading, a strategy that involves cross-referencing information with multiple sources, may be an effective approach to achieving this goal. In this paper, we present ReadProbe, a tool to support lateral reading, powered by generative large language models from OpenAI and the Bing search engine. Our tool is able to generate useful questions for lateral reading, scour the web for relevant documents, and generate well-attributed answers to help people better evaluate online information. We made a web-based application to demonstrate how ReadProbe can help reduce the risk of being misled by false information. The code is available at https://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won the first prize in a national AI misinformation hackathon.", "url": "https://arxiv.org/abs/2306.07875"}, {"metadata": {"arXiv": "2306.07902 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 16:54:13 ", "Title": "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark", "Authors": ["{\\L}ukasz Augustyniak", "Szymon Wo\\'zniak", "Marcin Gruza", "Piotr Gramacki", "Krzysztof Rajda", "Miko{\\l}aj Morzy", "Tomasz Kajdanowicz"], "Categories": "cs.CL cs.AI", "Comments": ["submitted to NeurIPS 2023 Datasets and Benchmarks track. Dataset: https://huggingface.co/datasets/Brand24/mms Code: https://github.com/Brand24-AI/mms_benchmark"]}, "abstract": "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.", "url": "https://arxiv.org/abs/2306.07902"}, {"metadata": {"arXiv": "2306.07906 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 16:57:53 ", "Title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences", "Authors": ["Xiao Liu", "Hanyu Lai", "Hao Yu", "Yifan Xu", "Aohan Zeng", "Zhengxiao Du", "Peng Zhang", "Yuxiao Dong", "Jie Tang"], "Categories": "cs.CL cs.AI", "Comments": ["Accepted to KDD 2023"]}, "abstract": "We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at \\url{https://github.com/THUDM/WebGLM}.", "url": "https://arxiv.org/abs/2306.07906"}, {"metadata": {"arXiv": "2306.07929 (*cross-listing*)", "Date": "Fri, 09 Jun 2023 08:08:18 ", "Title": "Large Language Model Is Semi-Parametric Reinforcement Learning Agent", "Authors": ["Danyang Zhang", "Lu Chen", "Situo Zhang", "Hongshen Xu", "Zihan Zhao", "Kai Yu"], "Categories": "cs.CL cs.AI"}, "abstract": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER.", "url": "https://arxiv.org/abs/2306.07929"}, {"metadata": {"arXiv": "2306.07932 (*cross-listing*)", "Date": "Sat, 10 Jun 2023 04:31:57 ", "Title": "Human-in-the-Loop through Chain-of-Thought", "Authors": ["Zefan Cai", "Baobao Chang", "Wenjuan Han"], "Categories": "cs.CL cs.AI"}, "abstract": "While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A significant advantage w.r.t cost and utility proves its superiority over strong baselines.", "url": "https://arxiv.org/abs/2306.07932"}, {"metadata": {"arXiv": "2306.07933 (*cross-listing*)", "Date": "Fri, 09 Jun 2023 15:44:41 ", "Title": "Understanding Telecom Language Through Large Language Models", "Authors": ["Lina Bariah and Hang Zou and Qiyang Zhao and Belkacem Mouhouche and Faouzi Bader and Merouane Debbah"], "Categories": "cs.CL cs.AI"}, "abstract": "The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the 3rd Generation Partnership Project (3GPP) standard working groups. We consider training the selected models on 3GPP technical documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP working groups. The distilled BERT model with around 50% less parameters achieves similar performance as others. This corroborates that fine-tuning pretrained LLM can effectively identify the categories of Telecom language. The developed framework shows a stepping stone towards realizing intent-driven and self-evolving wireless networks from Telecom languages, and paves the way for the implementation of generative AI in the Telecom domain.", "url": "https://arxiv.org/abs/2306.07933"}, {"metadata": {"arXiv": "2306.07944 (*cross-listing*)", "Date": "Thu, 08 Jun 2023 22:33:22 ", "Title": "Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding", "Authors": ["Mingqiu Wang", "Izhak Shafran", "Hagen Soltau", "Wei Han", "Yuan Cao", "Dian Yu", "Laurent El Shafey"], "Categories": "eess.AS cs.AI cs.CL"}, "abstract": "Large Language Models (LLMs) have been applied in the speech domain, often incurring a performance drop due to misaligned between speech and language representations. To bridge this gap, we propose a joint speech and language model (SLM) using a Speech2Text adapter, which maps speech into text token embedding space without speech information loss. Additionally, using a CTC-based blank-filtering, we can reduce the speech sequence length to that of text. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the dialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to address errors on rare entities, we augment SLM with a Speech2Entity retriever, which uses speech to retrieve relevant entities, and then adds them to the original SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the DST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with the dialog understanding task improves the ASR performance from 9.4% to 8.5% WER.", "url": "https://arxiv.org/abs/2306.07944"}, {"metadata": {"arXiv": "2306.07946 (*cross-listing*)", "Date": "Fri, 02 Jun 2023 14:47:56 ", "Title": "STUDY: Socially Aware Temporally Casual Decoder Recommender Systems", "Authors": ["Eltayeb Ahmed", "Diana Mincu", "Lauren Harrell", "Katherine Heller", "Subhrajit Roy"], "Categories": "cs.SI cs.AI cs.IR", "Comments": ["15 pages", "5 figures"]}, "abstract": "With the overwhelming amount of data available both on and offline today, recommender systems have become much needed to help users find items tailored to their interests. When social network information exists there are methods that utilize this information to make better recommendations, however the methods are often clunky with complex architectures and training procedures. Furthermore many of the existing methods utilize graph neural networks which are notoriously difficult to train. To address this, we propose Socially-aware Temporally caUsal Decoder recommender sYstems (STUDY). STUDY does joint inference over groups of users who are adjacent in the social network graph using a single forward pass of a modified transformer decoder network. We test our method in a school-based educational content setting, using classroom structure to define social networks. Our method outperforms both social and sequential methods while maintaining the design simplicity of a single homogeneous network that models all interactions in the data. We also carry out ablation studies to understand the drivers of our performance gains and find that our model depends on leveraging a social network structure that effectively models the similarities in user behavior.", "url": "https://arxiv.org/abs/2306.07946"}, {"metadata": {"arXiv": "2306.07956 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 17:54:13 ", "Title": "Adaptive Monte Carlo Search for Conjecture Refutation in Graph Theory", "Authors": ["Valentino Vito and Lim Yohanes Stefanus"], "Categories": "math.CO cs.AI cs.DM", "Comments": ["27 pages", "11 figures", "3 tables"]}, "abstract": "Graph theory is an interdisciplinary field of study that has various applications in mathematical modeling and computer science. Research in graph theory depends on the creation of not only theorems but also conjectures. Conjecture-refuting algorithms attempt to refute conjectures by searching for counterexamples to those conjectures, often by maximizing certain score functions on graphs. This study proposes a novel conjecture-refuting algorithm, referred to as the adaptive Monte Carlo search (AMCS) algorithm, obtained by modifying the Monte Carlo tree search algorithm. Evaluated based on its success in finding counterexamples to several graph theory conjectures, AMCS outperforms existing conjecture-refuting algorithms. The algorithm is further utilized to refute six open conjectures, two of which were chemical graph theory conjectures formulated by Liu et al. in 2021 and four of which were formulated by the AutoGraphiX computer system in 2006. Finally, four of the open conjectures are strongly refuted by generalizing the counterexamples obtained by AMCS to produce a family of counterexamples. It is expected that the algorithm can help researchers test graph-theoretic conjectures more effectively.", "url": "https://arxiv.org/abs/2306.07956"}, {"metadata": {"arXiv": "2306.07968 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 17:59:57 ", "Title": "arXiVeri: Automatic table verification with GPT", "Authors": ["Gyungin Shin", "Weidi Xie", "Samuel Albanie"], "Categories": "cs.CL cs.AI", "Comments": ["Tech report"]}, "abstract": "Without accurate transcription of numerical data in scientific documents, a scientist cannot draw accurate conclusions. Unfortunately, the process of copying numerical data from one paper to another is prone to human error. In this paper, we propose to meet this challenge through the novel task of automatic table verification (AutoTV), in which the objective is to verify the accuracy of numerical data in tables by cross-referencing cited sources. To support this task, we propose a new benchmark, arXiVeri, which comprises tabular data drawn from open-access academic papers on arXiv. We introduce metrics to evaluate the performance of a table verifier in two key areas: (i) table matching, which aims to identify the source table in a cited document that corresponds to a target table, and (ii) cell matching, which aims to locate shared cells between a target and source table and identify their row and column indices accurately. By leveraging the flexible capabilities of modern large language models (LLMs), we propose simple baselines for table verification. Our findings highlight the complexity of this task, even for state-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made publicly available.", "url": "https://arxiv.org/abs/2306.07968"}, {"metadata": {"arXiv": "2306.07464", "Date": "Mon, 12 Jun 2023 23:42:08 ", "Title": "Unlocking Sales Growth: Account Prioritization Engine with Explainable AI", "Authors": ["Suvendu Jena", "Jilei Yang", "Fangfang Tan"], "Categories": "cs.AI cs.LG stat.ML", "Comments": ["9 pages", "11 figures", "2 tables"]}, "abstract": "B2B sales requires effective prediction of customer growth, identification of upsell potential, and mitigation of churn risks. LinkedIn sales representatives traditionally relied on intuition and fragmented data signals to assess customer performance. This resulted in significant time investment in data understanding as well as strategy formulation and under-investment in active selling. To overcome this challenge, we developed a data product called Account Prioritizer, an intelligent sales account prioritization engine. It uses machine learning recommendation models and integrated account-level explanation algorithms within the sales CRM to automate the manual process of sales book prioritization. A successful A/B test demonstrated that the Account Prioritizer generated a substantial +8.08% increase in renewal bookings for the LinkedIn Business.", "url": "https://arxiv.org/abs/2306.07464"}, {"metadata": {"arXiv": "2306.07743", "Date": "Tue, 13 Jun 2023 13:00:10 ", "Title": "V-LoL: A Diagnostic Dataset for Visual Logical Learning", "Authors": ["Lukas Helff", "Wolfgang Stammer", "Hikaru Shindo", "Devendra Singh Dhami", "Kristian Kersting"], "Categories": "cs.AI cs.CV cs.LG"}, "abstract": "Despite the successes of recent developments in visual AI, different shortcomings still exist; from missing exact logical reasoning, to abstract generalization abilities, to understanding complex and noisy scenes. Unfortunately, existing benchmarks, were not designed to capture more than a few of these aspects. Whereas deep learning datasets focus on visually complex data but simple visual reasoning tasks, inductive logic datasets involve complex logical learning tasks, however, lack the visual component. To address this, we propose the visual logical learning dataset, V-LoL, that seamlessly combines visual and logical challenges. Notably, we introduce the first instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic benchmark in symbolic AI, the Michalski train problem. By incorporating intricate visual scenes and flexible logical reasoning tasks within a versatile framework, V-LoL-Trains provides a platform for investigating a wide range of visual logical learning challenges. We evaluate a variety of AI systems including traditional symbolic AI, neural AI, as well as neuro-symbolic AI. Our evaluations demonstrate that even state-of-the-art AI faces difficulties in dealing with visual logical learning challenges, highlighting unique advantages and limitations specific to each methodology. Overall, V-LoL opens up new avenues for understanding and enhancing current abilities in visual logical learning for AI systems.", "url": "https://arxiv.org/abs/2306.07743"}, {"metadata": {"arXiv": "2306.07856", "Date": "Tue, 13 Jun 2023 15:35:01 ", "Title": "DreamDecompiler: Improved Bayesian Program Learning by Decompiling Amortised Knowledge", "Authors": ["Alessandro B. Palmarini", "Christopher G. Lucas", "N. Siddharth"], "Categories": "cs.AI cs.LG cs.SE"}, "abstract": "Solving program induction problems requires searching through an enormous space of possibilities. DreamCoder is an inductive program synthesis system that, whilst solving problems, learns to simplify search in an iterative wake-sleep procedure. The cost of search is amortised by training a neural search policy, reducing search breadth and effectively \"compiling\" useful information to compose program solutions across tasks. Additionally, a library of program components is learnt to express discovered solutions in fewer components, reducing search depth. In DreamCoder, the neural search policy has only an indirect effect on the library learnt through the program solutions it helps discover. We present an approach for library learning that directly leverages the neural search policy, effectively \"decompiling\" its amortised knowledge to extract relevant program components. This provides stronger amortised inference: the amortised knowledge learnt to reduce search breadth is now also used to reduce search depth. We integrate our approach with DreamCoder and demonstrate faster domain proficiency with improved generalisation on a range of domains, particularly when fewer example solutions are available.", "url": "https://arxiv.org/abs/2306.07856"}, {"metadata": {"arXiv": "2306.07957", "Date": "Tue, 13 Jun 2023 17:55:17 ", "Title": "Hidden Biases of End-to-End Driving Models", "Authors": ["Bernhard Jaeger and Kashyap Chitta and Andreas Geiger"], "Categories": "cs.CV cs.AI cs.LG cs.RO", "Comments": ["18 pages", "17 Tables", "10 Figures"]}, "abstract": "End-to-end driving systems have recently made rapid progress, in particular on CARLA. Independent of their major contribution, they introduce changes to minor system components. Consequently, the source of improvements is unclear. We identify two biases that recur in nearly all state-of-the-art methods and are critical for the observed progress on CARLA: (1) lateral recovery via a strong inductive bias towards target point following, and (2) longitudinal averaging of multimodal waypoint predictions for slowing down. We investigate the drawbacks of these biases and identify principled alternatives. By incorporating our insights, we develop TF++, a simple end-to-end method that ranks first on the Longest6 and LAV benchmarks, gaining 14 driving score over the best prior work on Longest6.", "url": "https://arxiv.org/abs/2306.07957"}, {"metadata": {"arXiv": "2306.07969", "Date": "Tue, 13 Jun 2023 17:59:58 ", "Title": "GeneCIS: A Benchmark for General Conditional Image Similarity", "Authors": ["Sagar Vaze", "Nicolas Carion", "Ishan Misra"], "Categories": "cs.CV cs.AI cs.LG cs.MM", "Comments": ["CVPR 2023 (Highlighted Paper). Project page at https://sgvaze.github.io/genecis/"]}, "abstract": "We argue that there are many notions of 'similarity' and that models, like humans, should be able to adapt to these dynamically. This contrasts with most representation learning methods, supervised or self-supervised, which learn a fixed embedding function and hence implicitly assume a single notion of similarity. For instance, models trained on ImageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or specific elements in the scene. In this paper, we propose the GeneCIS ('genesis') benchmark, which measures models' ability to adapt to a range of similarity conditions. Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions. We find that baselines from powerful CLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet accuracy, suggesting that simply scaling existing methods is not fruitful. We further propose a simple, scalable solution based on automatically mining information from existing image-caption datasets. We find our method offers a substantial boost over the baselines on GeneCIS, and further improves zero-shot performance on related image retrieval benchmarks. In fact, though evaluated zero-shot, our model surpasses state-of-the-art supervised models on MIT-States. Project page at https://sgvaze.github.io/genecis/.", "url": "https://arxiv.org/abs/2306.07969"}, {"metadata": {"arXiv": "2306.07290", "Date": "Fri, 09 Jun 2023 18:40:55 ", "Title": "Value function estimation using conditional diffusion models for control", "Authors": ["Bogdan Mazoure", "Walter Talbott", "Miguel Angel Bautista", "Devon Hjelm", "Alexander Toshev", "Josh Susskind"], "Categories": "cs.LG cs.AI"}, "abstract": "A fairly reliable trend in deep reinforcement learning is that the performance scales with the number of parameters, provided a complimentary scaling in amount of training data. As the appetite for large models increases, it is imperative to address, sooner than later, the potential problem of running out of high-quality demonstrations. In this case, instead of collecting only new data via costly human demonstrations or risking a simulation-to-real transfer with uncertain effects, it would be beneficial to leverage vast amounts of readily-available low-quality data. Since classical control algorithms such as behavior cloning or temporal difference learning cannot be used on reward-free or action-free data out-of-the-box, this solution warrants novel training paradigms for continuous control. We propose a simple algorithm called Diffused Value Function (DVF), which learns a joint multi-step model of the environment-robot interaction dynamics using a diffusion model. This model can be efficiently learned from state sequences (i.e., without access to reward functions nor actions), and subsequently used to estimate the value of each action out-of-the-box. We show how DVF can be used to efficiently capture the state visitation measure for multiple controllers, and show promising qualitative and quantitative results on challenging robotics benchmarks.", "url": "https://arxiv.org/abs/2306.07290"}, {"metadata": {"arXiv": "2306.07292", "Date": "Fri, 09 Jun 2023 21:01:29 ", "Title": "Urban Spatiotemporal Data Synthesis via Neural Disaggregation", "Authors": ["Bin Han", "Bill Howe"], "Categories": "cs.LG cs.AI cs.CR"}, "abstract": "The level of granularity of open data often conflicts the benefits it can provide. Less granular data can protect individual privacy, but to certain degrees, sabotage the promise of open data to promote transparency and assist research. Similar in the urban setting, aggregated urban data at high-level geographic units can mask out the underline particularities of city dynamics that may vary at lower areal levels. In this work, we aim to synthesize fine-grained, high resolution urban data, by breaking down aggregated urban data at coarse, low resolution geographic units. The goal is to increase the usability and realize the values as much as possible of highly aggregated urban data. To address the issue of simplicity of some traditional disaggregation methods -- 1) we experimented with numerous neural-based models that are capable of modeling intricate non-linear relationships among features. Neural methods can also leverage both spatial and temporal information concurrently. We showed that all neural methods perform better than traditional disaggregation methods. Incorporating the temporal information further enhances the results. 2) We proposed a training approach for disaggregation task, Chain-of-Training (COT), that can be incorporated into any of the training-based models. COT adds transitional disaggregation steps by incorporating intermediate geographic dimensions, which enhances the predictions at low geographic level and boosts the results at higher levels. 3) We adapted the idea of reconstruction (REC) from super-resolution domain in our disaggregation case -- after disaggregating from low to high geographic level, we then re-aggregate back to the low level from our generated high level values. Both strategies improved disaggregation results on three datasets and two cities we tested on.", "url": "https://arxiv.org/abs/2306.07292"}, {"metadata": {"arXiv": "2306.07294", "Date": "Sat, 10 Jun 2023 11:25:31 ", "Title": "Expressivity Enhancement with Efficient Quadratic Neurons for Convolutional Neural Networks", "Authors": ["Chuangtao Chen and Grace Li Zhang and Xunzhao Yin and Cheng Zhuo and Ulf Schlichtmann and Bing Li"], "Categories": "cs.LG cs.AI cs.NE"}, "abstract": "Convolutional neural networks (CNNs) have been successfully applied in a range of fields such as image classification and object segmentation. To improve their expressivity, various techniques, such as novel CNN architectures, have been explored. However, the performance gain from such techniques tends to diminish. To address this challenge, many researchers have shifted their focus to increasing the non-linearity of neurons, the fundamental building blocks of neural networks, to enhance the network expressivity. Nevertheless, most of these approaches incur a large number of parameters and thus formidable computation cost inevitably, impairing their efficiency to be deployed in practice. In this work, an efficient quadratic neuron structure is proposed to preserve the non-linearity with only negligible parameter and computation cost overhead. The proposed quadratic neuron can maximize the utilization of second-order computation information to improve the network performance. The experimental results have demonstrated that the proposed quadratic neuron can achieve a higher accuracy and a better computation efficiency in classification tasks compared with both linear neurons and non-linear neurons from previous works.", "url": "https://arxiv.org/abs/2306.07294"}, {"metadata": {"arXiv": "2306.07296", "Date": "Sat, 10 Jun 2023 16:06:44 ", "Title": "Optimized Three Deep Learning Models Based-PSO Hyperparameters for Beijing PM2.5 Prediction", "Authors": ["Andri Pranolo", "Yingchi Mao", "Aji Prasetya Wibawa", "Agung Bella Putra Utama", "Felix Andika Dwiyanto"], "Categories": "cs.LG cs.AI cs.NE", "Comments": ["Volume 5 (1): 53-66"], "Journal-ref": "Knowledge Engineering and Data Science, 2022, Vol 5 No 1, pp: 53-66", "DOI": "10.17977/um018v5i12022p53-66"}, "abstract": "Deep learning is a machine learning approach that produces excellent performance in various applications, including natural language processing, image identification, and forecasting. Deep learning network performance depends on the hyperparameter settings. This research attempts to optimize the deep learning architecture of Long short term memory (LSTM), Convolutional neural network (CNN), and Multilayer perceptron (MLP) for forecasting tasks using Particle swarm optimization (PSO), a swarm intelligence-based metaheuristic optimization methodology: Proposed M-1 (PSO-LSTM), M-2 (PSO-CNN), and M-3 (PSO-MLP). Beijing PM2.5 datasets was analyzed to measure the performance of the proposed models. PM2.5 as a target variable was affected by dew point, pressure, temperature, cumulated wind speed, hours of snow, and hours of rain. The deep learning network inputs consist of three different scenarios: daily, weekly, and monthly. The results show that the proposed M-1 with three hidden layers produces the best results of RMSE and MAPE compared to the proposed M-2, M-3, and all the baselines. A recommendation for air pollution management could be generated by using these optimized models", "url": "https://arxiv.org/abs/2306.07296"}, {"metadata": {"arXiv": "2306.07300", "Date": "Sun, 11 Jun 2023 04:58:31 ", "Title": "Progressive Class-Wise Attention (PCA) Approach for Diagnosing Skin Lesions", "Authors": ["Asim Naveed", "Syed S. Naqvi", "Tariq M. Khan", "Imran Razzak"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Skin cancer holds the highest incidence rate among all cancers globally. The importance of early detection cannot be overstated, as late-stage cases can be lethal. Classifying skin lesions, however, presents several challenges due to the many variations they can exhibit, such as differences in colour, shape, and size, significant variation within the same class, and notable similarities between different classes. This paper introduces a novel class-wise attention technique that equally regards each class while unearthing more specific details about skin lesions. This attention mechanism is progressively used to amalgamate discriminative feature details from multiple scales. The introduced technique demonstrated impressive performance, surpassing more than 15 cutting-edge methods including the winners of HAM1000 and ISIC 2019 leaderboards. It achieved an impressive accuracy rate of 97.40% on the HAM10000 dataset and 94.9% on the ISIC 2019 dataset.", "url": "https://arxiv.org/abs/2306.07300"}, {"metadata": {"arXiv": "2306.07301", "Date": "Sun, 11 Jun 2023 06:56:00 ", "Title": "Novel Regression and Least Square Support Vector Machine Learning Technique for Air Pollution Forecasting", "Authors": ["Dhanalakshmi M and Radha V"], "Categories": "cs.LG cs.AI", "Comments": ["11 pages", "7 figures", "3 tables", "Article Published in April 2023", "Volume 71", "Issue 04", "of SSRG-International Journal of Engineering Trends and Technology (IJETT)\"", "ISSN: 2231-5381"], "DOI": "10.14445/22315381/IJETT-V71I4P214"}, "abstract": "Air pollution is the origination of particulate matter, chemicals, or biological substances that brings pain to either humans or other living creatures or instigates discomfort to the natural habitat and the airspace. Hence, air pollution remains one of the paramount environmental issues as far as metropolitan cities are concerned. Several air pollution benchmarks are even said to have a negative influence on human health. Also, improper detection of air pollution benchmarks results in severe complications for humans and living creatures. To address this aspect, a novel technique called, Discretized Regression and Least Square Support Vector (DR-LSSV) based air pollution forecasting is proposed. The results indicate that the proposed DR-LSSV Technique can efficiently enhance air pollution forecasting performance and outperforms the conventional machine learning methods in terms of air pollution forecasting accuracy, air pollution forecasting time, and false positive rate.", "url": "https://arxiv.org/abs/2306.07301"}, {"metadata": {"arXiv": "2306.07304", "Date": "Sun, 11 Jun 2023 23:28:02 ", "Title": "A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation", "Authors": ["Thomas Fel", "Victor Boutin", "Mazda Moayeri", "R\\'emi Cad\\`ene", "Louis Bethune", "L\\'eo and\\'eol", "Mathieu Chalvidal", "Thomas Serre"], "Categories": "cs.LG cs.AI"}, "abstract": "In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual 'concepts' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that comprehensively defines and clarifies these two steps. This framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii) to derive theoretical guarantees regarding the optimality of such methods. We further leverage our framework to try to tackle a crucial question in explainability: how to efficiently identify clusters of data points that are classified based on a similar shared strategy. To illustrate these findings and to highlight the main strategies of a model, we introduce a visual representation called the strategic cluster graph. Finally, we present https://serre-lab.github.io/Lens, a dedicated website that offers a complete compilation of these visualizations for all classes of the ImageNet dataset.", "url": "https://arxiv.org/abs/2306.07304"}, {"metadata": {"arXiv": "2306.07305", "Date": "Mon, 12 Jun 2023 03:26:11 ", "Title": "Making forecasting self-learning and adaptive -- Pilot forecasting rack", "Authors": ["Shaun D'Souza", "Dheeraj Shah", "Amareshwar Allati", "Parikshit Soni"], "Categories": "cs.LG cs.AI q-fin.CP"}, "abstract": "Retail sales and price projections are typically based on time series forecasting. For some product categories, the accuracy of demand forecasts achieved is low, negatively impacting inventory, transport, and replenishment planning. This paper presents our findings based on a proactive pilot exercise to explore ways to help retailers to improve forecast accuracy for such product categories. We evaluated opportunities for algorithmic interventions to improve forecast accuracy based on a sample product category, Knitwear. The Knitwear product category has a current demand forecast accuracy from non-AI models in the range of 60%. We explored how to improve the forecast accuracy using a rack approach. To generate forecasts, our decision model dynamically selects the best algorithm from an algorithm rack based on performance for a given state and context. Outcomes from our AI/ML forecasting model built using advanced feature engineering show an increase in the accuracy of demand forecast for Knitwear product category by 20%, taking the overall accuracy to 80%. Because our rack comprises algorithms that cater to a range of customer data sets, the forecasting model can be easily tailored for specific customer contexts.", "url": "https://arxiv.org/abs/2306.07305"}, {"metadata": {"arXiv": "2306.07307", "Date": "Mon, 12 Jun 2023 11:42:13 ", "Title": "Online Prototype Alignment for Few-shot Policy Transfer", "Authors": ["Qi Yi", "Rui Zhang", "Shaohui Peng", "Jiaming Guo", "Yunkai Gao", "Kaizhao Yuan", "Ruizhi Chen", "Siming Lan", "Xing Hu", "Zidong Du", "Xishan Zhang", "Qi Guo", "and Yunji Chen"], "Categories": "cs.LG cs.AI", "Comments": ["This paper has been accepted at ICML2023"]}, "abstract": "Domain adaptation in reinforcement learning (RL) mainly deals with the changes of observation when transferring the policy to a new environment. Many traditional approaches of domain adaptation in RL manage to learn a mapping function between the source and target domain in explicit or implicit ways. However, they typically require access to abundant data from the target domain. Besides, they often rely on visual clues to learn the mapping function and may fail when the source domain looks quite different from the target domain. To address these problems, we propose a novel framework Online Prototype Alignment (OPA) to learn the mapping function based on the functional similarity of elements and is able to achieve the few-shot policy transfer within only several episodes. The key insight of OPA is to introduce an exploration mechanism that can interact with the unseen elements of the target domain in an efficient and purposeful manner, and then connect them with the seen elements in the source domain according to their functionalities (instead of visual clues). Experimental results show that when the target domain looks visually different from the source domain, OPA can achieve better transfer performance even with much fewer samples from the target domain, outperforming prior methods.", "url": "https://arxiv.org/abs/2306.07307"}, {"metadata": {"arXiv": "2306.07349", "Date": "Tue, 06 Jun 2023 17:59:10 ", "Title": "ATT3D: Amortized Text-to-3D Object Synthesis", "Authors": ["Jonathan Lorraine", "Kevin Xie", "Xiaohui Zeng", "Chen-Hsuan Lin", "Towaki Takikawa", "Nicholas Sharp", "Tsung-Yi Lin", "Ming-Yu Liu", "Sanja Fidler", "James Lucas"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["22 pages", "20 figures"], "MSC-class": "68T45", "ACM-class": "I.2.6; I.2.7; I.3.6; I.3.7"}, "abstract": "Text-to-3D modelling has seen exciting progress by combining generative text-to-image models with image-to-3D methods like Neural Radiance Fields. DreamFusion recently achieved high-quality results but requires a lengthy, per-prompt optimization to create 3D objects. To address this, we amortize optimization over text prompts by training on many prompts simultaneously with a unified model, instead of separately. With this, we share computation across a prompt set, training in less time than per-prompt optimization. Our framework - Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts to generalize to unseen setups and smooth interpolations between text for novel assets and simple animations.", "url": "https://arxiv.org/abs/2306.07349"}, {"metadata": {"arXiv": "2306.07372", "Date": "Mon, 12 Jun 2023 18:55:56 ", "Title": "Composing Efficient, Robust Tests for Policy Selection", "Authors": ["Dustin Morrill", "Thomas J. Walsh", "Daniel Hernandez", "Peter R. Wurman", "Peter Stone"], "Categories": "cs.LG cs.AI cs.GT", "Comments": ["26 pages", "13 figures. To appear in Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI 2023)"], "ACM-class": "B.8.1; I.2.6"}, "abstract": "Modern reinforcement learning systems produce many high-quality policies throughout the learning process. However, to choose which policy to actually deploy in the real world, they must be tested under an intractable number of environmental conditions. We introduce RPOSST, an algorithm to select a small set of test cases from a larger pool based on a relatively small number of sample evaluations. RPOSST treats the test case selection problem as a two-player game and optimizes a solution with provable $k$-of-$N$ robustness, bounding the error relative to a test that used all the test cases in the pool. Empirical results demonstrate that RPOSST finds a small set of test cases that identify high quality policies in a toy one-shot game, poker datasets, and a high-fidelity racing simulator.", "url": "https://arxiv.org/abs/2306.07372"}, {"metadata": {"arXiv": "2306.07408", "Date": "Mon, 12 Jun 2023 20:21:40 ", "Title": "Robust Reinforcement Learning through Efficient Adversarial Herding", "Authors": ["Juncheng Dong", "Hao-Lun Hsu", "Qitong Gao", "Vahid Tarokh", "Miroslav Pajic"], "Categories": "cs.LG cs.AI cs.RO"}, "abstract": "Although reinforcement learning (RL) is considered the gold standard for policy design, it may not always provide a robust solution in various scenarios. This can result in severe performance degradation when the environment is exposed to potential disturbances. Adversarial training using a two-player max-min game has been proven effective in enhancing the robustness of RL agents. In this work, we extend the two-player game by introducing an adversarial herd, which involves a group of adversaries, in order to address ($\\textit{i}$) the difficulty of the inner optimization problem, and ($\\textit{ii}$) the potential over pessimism caused by the selection of a candidate adversary set that may include unlikely scenarios. We first prove that adversarial herds can efficiently approximate the inner optimization problem. Then we address the second issue by replacing the worst-case performance in the inner optimization with the average performance over the worst-$k$ adversaries. We evaluate the proposed method on multiple MuJoCo environments. Experimental results demonstrate that our approach consistently generates more robust policies.", "url": "https://arxiv.org/abs/2306.07408"}, {"metadata": {"arXiv": "2306.07465", "Date": "Mon, 12 Jun 2023 23:48:24 ", "Title": "A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning", "Authors": ["Haozhe Jiang", "Qiwen Cui", "Zhihan Xiong", "Maryam Fazel", "Simon S. Du"], "Categories": "cs.LG cs.AI cs.GT cs.MA stat.ML", "Comments": ["25 Pages", "2 figures"]}, "abstract": "We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\\widetilde{O}\\left(\\Delta^{1/4}T^{3/4}\\right)$ regret when the degree of nonstationarity, as measured by total variation $\\Delta$, is known, and $\\widetilde{O}\\left(\\Delta^{1/5}T^{4/5}\\right)$ regret when $\\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, our algorithm inherits the favorable dependence on number of agents from the oracles. As a side contribution that may be independent of interest, we show how to test for various types of equilibria by a black-box reduction to single-agent learning, which includes Nash equilibria, correlated equilibria, and coarse correlated equilibria.", "url": "https://arxiv.org/abs/2306.07465"}, {"metadata": {"arXiv": "2306.07512", "Date": "Tue, 13 Jun 2023 02:43:21 ", "Title": "Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning", "Authors": ["Ruijie Wang", "Baoyu Li", "Yichen Lu", "Dachun Sun", "Jinning Li", "Yuchen Yan", "Shengzhong Liu", "Hanghang Tong", "Tarek F. Abdelzaher"], "Categories": "cs.LG cs.AI cs.CL cs.SI", "Comments": ["This paper is accepted by ACL-Findings 2023"]}, "abstract": "This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both \\textit{false negative issue} (i.e., potential true facts being excluded) and \\textit{false positive issue} (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as a noisy Positive-Unlabeled learning problem. We propose a variational framework, namely nPUGraph, that jointly estimates the correctness of both collected and uncollected facts (which we call \\textit{label posterior}) and updates model parameters during training. The label posterior estimation facilitates speculative reasoning from two perspectives. First, it improves the robustness of a label posterior-aware graph encoder against false positive links. Second, it identifies missing facts to provide high-quality grounds of reasoning. They are unified in a simple yet effective self-training procedure. Empirically, extensive experiments on three benchmark KG and one Twitter dataset with various degrees of false negative/positive cases demonstrate the effectiveness of nPUGraph.", "url": "https://arxiv.org/abs/2306.07512"}, {"metadata": {"arXiv": "2306.07526", "Date": "Tue, 13 Jun 2023 03:42:03 ", "Title": "User-defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems", "Authors": ["Marc Finzi", "Anudhyan Boral", "Andrew Gordon Wilson", "Fei Sha", "Leonardo Zepeda-N\\'u\\~nez"], "Categories": "cs.LG cs.AI", "Comments": ["ICML 2023 Conference"]}, "abstract": "Diffusion models are a class of probabilistic generative models that have been widely used as a prior for image processing tasks like text conditional generation and inpainting. We demonstrate that these models can be adapted to make predictions and provide uncertainty quantification for chaotic dynamical systems. In these applications, diffusion models can implicitly represent knowledge about outliers and extreme events; however, querying that knowledge through conditional sampling or measuring probabilities is surprisingly difficult. Existing methods for conditional sampling at inference time seek mainly to enforce the constraints, which is insufficient to match the statistics of the distribution or compute the probability of the chosen events. To achieve these ends, optimally one would use the conditional score function, but its computation is typically intractable. In this work, we develop a probabilistic approximation scheme for the conditional score function which provably converges to the true distribution as the noise level decreases. With this scheme we are able to sample conditionally on nonlinear userdefined events at inference time, and matches data statistics even when sampling from the tails of the distribution.", "url": "https://arxiv.org/abs/2306.07526"}, {"metadata": {"arXiv": "2306.07536", "Date": "Tue, 13 Jun 2023 04:37:00 ", "Title": "TART: A plug-and-play Transformer module for task-agnostic reasoning", "Authors": ["Kush Bhatia", "Avanika Narayan", "Christopher De Sa", "Christopher R\\'e"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which generically improves an LLM's reasoning abilities using a synthetically trained Transformer-based reasoning module. TART trains this reasoning module in a task-agnostic manner using only synthetic logistic regression tasks and composes it with an arbitrary real-world pre-trained model without any additional training. With a single inference module, TART improves performance across different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M - 6B), tasks (14 NLP binary classification tasks), and even across different modalities (audio and vision). Additionally, on the RAFT Benchmark, TART improves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B), and is within 4% of GPT-3 (175B). Our code and models are available at https://github.com/HazyResearch/TART .", "url": "https://arxiv.org/abs/2306.07536"}, {"metadata": {"arXiv": "2306.07541", "Date": "Tue, 13 Jun 2023 05:22:26 ", "Title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning", "Authors": ["Siyuan Guo", "Yanchao Sun", "Jifeng Hu", "Sili Huang", "Hechang Chen", "Haiyin Piao", "Lichao Sun", "Yi Chang"], "Categories": "cs.LG cs.AI"}, "abstract": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples to smoothly bridge offline and online stages. SUNG achieves state-of-the-art online finetuning performance when combined with different offline RL methods, across various environments and datasets in D4RL benchmark.", "url": "https://arxiv.org/abs/2306.07541"}, {"metadata": {"arXiv": "2306.07552", "Date": "Tue, 13 Jun 2023 05:53:23 ", "Title": "Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-Per-Second", "Authors": ["Vincent-Pierre Berges", "Andrew Szot", "Devendra Singh Chaplot", "Aaron Gokaslan", "Roozbeh Mottaghi", "Dhruv Batra", "Eric Undersander"], "Categories": "cs.LG cs.AI cs.RO"}, "abstract": "We present Galactic, a large-scale simulation and reinforcement-learning (RL) framework for robotic mobile manipulation in indoor environments. Specifically, a Fetch robot (equipped with a mobile base, 7DoF arm, RGBD camera, egomotion, and onboard sensing) is spawned in a home environment and asked to rearrange objects - by navigating to an object, picking it up, navigating to a target location, and then placing the object at the target location. Galactic is fast. In terms of simulation speed (rendering + physics), Galactic achieves over 421,000 steps-per-second (SPS) on an 8-GPU node, which is 54x faster than Habitat 2.0 (7699 SPS). More importantly, Galactic was designed to optimize the entire rendering + physics + RL interplay since any bottleneck in the interplay slows down training. In terms of simulation+RL speed (rendering + physics + inference + learning), Galactic achieves over 108,000 SPS, which 88x faster than Habitat 2.0 (1243 SPS). These massive speed-ups not only drastically cut the wall-clock training time of existing experiments, but also unlock an unprecedented scale of new experiments. First, Galactic can train a mobile pick skill to >80% accuracy in under 16 minutes, a 100x speedup compared to the over 24 hours it takes to train the same skill in Habitat 2.0. Second, we use Galactic to perform the largest-scale experiment to date for rearrangement using 5B steps of experience in 46 hours, which is equivalent to 20 years of robot experience. This scaling results in a single neural network composed of task-agnostic components achieving 85% success in GeometricGoal rearrangement, compared to 0% success reported in Habitat 2.0 for the same approach. The code is available at github.com/facebookresearch/galactic.", "url": "https://arxiv.org/abs/2306.07552"}, {"metadata": {"arXiv": "2306.07618", "Date": "Tue, 13 Jun 2023 08:22:18 ", "Title": "Hyperbolic Graph Diffusion Model for Molecule Generation", "Authors": ["Lingfeng Wen", "Xian Wei"], "Categories": "cs.LG cs.AI q-bio.QM"}, "abstract": "Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Model (HGDM). The proposed HGDM employs a hyperbolic variational autoencoder to generate the hyperbolic hidden representation of nodes and then a score-based hyperbolic graph neural network is used to learn the distribution in hyperbolic space. Numerical experimental results show that the proposed HGDM achieves higher performance on several molecular datasets, compared with state-of-the-art methods.", "url": "https://arxiv.org/abs/2306.07618"}, {"metadata": {"arXiv": "2306.07699", "Date": "Tue, 13 Jun 2023 11:34:36 ", "Title": "Time-aware Graph Structure Learning via Sequence Prediction on Temporal Graphs", "Authors": ["Haozhen Zhang", "Xueting Han", "Xi Xiao", "Jing Bai"], "Categories": "cs.LG cs.AI", "Comments": ["10 pages,4 figures,5 tables"]}, "abstract": "Temporal Graph Learning, which aims to model the time-evolving nature of graphs, has gained increasing attention and achieved remarkable performance recently. However, in reality, graph structures are often incomplete and noisy, which hinders temporal graph networks (TGNs) from learning informative representations. Graph contrastive learning uses data augmentation to generate plausible variations of existing data and learn robust representations. However, rule-based augmentation approaches may be suboptimal as they lack learnability and fail to leverage rich information from downstream tasks. To address these issues, we propose a Time-aware Graph Structure Learning (TGSL) approach via sequence prediction on temporal graphs, which learns better graph structures for downstream tasks through adding potential temporal edges. In particular, it predicts time-aware context embedding based on previously observed interactions and uses the Gumble-Top-K to select the closest candidate edges to this context embedding. Additionally, several candidate sampling strategies are proposed to ensure both efficiency and diversity. Furthermore, we jointly learn the graph structure and TGNs in an end-to-end manner and perform inference on the refined graph. Extensive experiments on temporal link prediction benchmarks demonstrate that TGSL yields significant gains for the popular TGNs such as TGAT and GraphMixer, and it outperforms other contrastive learning methods on temporal graphs. We will release the code in the future.", "url": "https://arxiv.org/abs/2306.07699"}, {"metadata": {"arXiv": "2306.07737", "Date": "Tue, 13 Jun 2023 12:43:59 ", "Title": "Robustness and Generalization Performance of Deep Learning Models on Cyber-Physical Systems: A Comparative Study", "Authors": ["Alexander Windmann and Henrik Steude and Oliver Niggemann"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at the IJCAI 2023 Workshop of Artificial Intelligence for Time Series Analysis (AI4TS)"]}, "abstract": "Deep learning (DL) models have seen increased attention for time series forecasting, yet the application on cyber-physical systems (CPS) is hindered by the lacking robustness of these methods. Thus, this study evaluates the robustness and generalization performance of DL architectures on multivariate time series data from CPS. Our investigation focuses on the models' ability to handle a range of perturbations, such as sensor faults and noise, and assesses their impact on overall performance. Furthermore, we test the generalization and transfer learning capabilities of these models by exposing them to out-of-distribution (OOD) samples. These include deviations from standard system operations, while the core dynamics of the underlying physical system are preserved. Additionally, we test how well the models respond to several data augmentation techniques, including added noise and time warping. Our experimental framework utilizes a simulated three-tank system, proposed as a novel benchmark for evaluating the robustness and generalization performance of DL algorithms in CPS data contexts. The findings reveal that certain DL model architectures and training techniques exhibit superior effectiveness in handling OOD samples and various perturbations. These insights have significant implications for the development of DL models that deliver reliable and robust performance in real-world CPS applications.", "url": "https://arxiv.org/abs/2306.07737"}, {"metadata": {"arXiv": "2306.07745", "Date": "Tue, 13 Jun 2023 13:01:42 ", "Title": "Kernelized Reinforcement Learning with Order Optimal Regret Bounds", "Authors": ["Sattar Vakili", "Julia Olkhovskaya"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\\pi$-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\\'ern kernels) the existing results lead to trivial (superlinear in the number of episodes) regret bounds. We show a sublinear regret bound that is order optimal in the case of Mat\\'ern kernels where a lower bound on regret is known.", "url": "https://arxiv.org/abs/2306.07745"}, {"metadata": {"arXiv": "2306.07775", "Date": "Tue, 13 Jun 2023 13:56:56 ", "Title": "iPDP: On Partial Dependence Plots in Dynamic Modeling Scenarios", "Authors": ["Maximilian Muschalik", "Fabian Fumagalli", "Rohit Jagtani", "Barbara Hammer", "Eyke H\\\"ullermeier"], "Categories": "cs.LG cs.AI eess.SP", "Comments": ["This preprint has not undergone peer review or any post-submission improvements or corrections"]}, "abstract": "Post-hoc explanation techniques such as the well-established partial dependence plot (PDP), which investigates feature dependencies, are used in explainable artificial intelligence (XAI) to understand black-box machine learning models. While many real-world applications require dynamic models that constantly adapt over time and react to changes in the underlying distribution, XAI, so far, has primarily considered static learning environments, where models are trained in a batch mode and remain unchanged. We thus propose a novel model-agnostic XAI framework called incremental PDP (iPDP) that extends on the PDP to extract time-dependent feature effects in non-stationary learning environments. We formally analyze iPDP and show that it approximates a time-dependent variant of the PDP that properly reacts to real and virtual concept drift. The time-sensitivity of iPDP is controlled by a single smoothing parameter, which directly corresponds to the variance and the approximation error of iPDP in a static learning environment. We illustrate the efficacy of iPDP by showcasing an example application for drift detection and conducting multiple experiments on real-world and synthetic data sets and streams.", "url": "https://arxiv.org/abs/2306.07775"}, {"metadata": {"arXiv": "2306.07874", "Date": "Tue, 13 Jun 2023 16:04:14 ", "Title": "Taxonomy-Structured Domain Adaptation", "Authors": ["Tianyi Liu", "Zihao Xu", "Hao He", "Guang-Yuan Hao", "Guang-He Lee", "Hao Wang"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Accepted by ICML 2023"]}, "abstract": "Domain adaptation aims to mitigate distribution shifts among different domains. However, traditional formulations are mostly limited to categorical domains, greatly simplifying nuanced domain relationships in the real world. In this work, we tackle a generalization with taxonomy-structured domains, which formalizes domains with nested, hierarchical similarity structures such as animal species and product catalogs. We build on the classic adversarial framework and introduce a novel taxonomist, which competes with the adversarial discriminator to preserve the taxonomy information. The equilibrium recovers the classic adversarial domain adaptation's solution if given a non-informative domain taxonomy (e.g., a flat taxonomy where all leaf nodes connect to the root node) while yielding non-trivial results with other taxonomies. Empirically, our method achieves state-of-the-art performance on both synthetic and real-world datasets with successful adaptation. Code is available at https://github.com/Wang-ML-Lab/TSDA.", "url": "https://arxiv.org/abs/2306.07874"}, {"metadata": {"arXiv": "2306.07916", "Date": "Tue, 13 Jun 2023 17:19:37 ", "Title": "Identification of Nonlinear Latent Hierarchical Models", "Authors": ["Lingjing Kong", "Biwei Huang", "Feng Xie", "Eric Xing", "Yuejie Chi", "Kun Zhang"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of both causal structure and latent variables can be achieved under mild assumptions: on causal structures, we allow for the existence of multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we do not make parametric assumptions, thus permitting general nonlinearity and multi-dimensional continuous variables. Specifically, we first develop a basic identification criterion in the form of novel identifiability guarantees for an elementary latent variable model. Leveraging this criterion, we show that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure. To the best of our knowledge, our work is the first to establish identifiability guarantees for both causal structures and latent variables in nonlinear latent hierarchical models.", "url": "https://arxiv.org/abs/2306.07916"}, {"metadata": {"arXiv": "2306.07967", "Date": "Tue, 13 Jun 2023 17:59:32 ", "Title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning", "Authors": ["Arnav Chavan and Zhuang Liu and Deepak Gupta and Eric Xing and Zhiqiang Shen"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Technical report"]}, "abstract": "We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code is available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.", "url": "https://arxiv.org/abs/2306.07967"}, {"metadata": {"arXiv": "2306.07419", "Date": "Mon, 12 Jun 2023 20:50:30 ", "Title": "DeepTransition: Viability Leads to the Emergence of Gait Transitions in Learning Anticipatory Quadrupedal Locomotion Skills", "Authors": ["Milad Shafiee", "Guillaume Bellegarda", "and Auke Ijspeert"], "Categories": "cs.RO cs.AI cs.LG"}, "abstract": "Quadruped animals seamlessly transition between gaits as they change locomotion speeds. While the most widely accepted explanation for gait transitions is energy efficiency, there is no clear consensus on the determining factor, nor on the potential effects from terrain properties. In this article, we propose that viability, i.e. the avoidance of falls, represents an important criterion for gait transitions. We investigate the emergence of gait transitions through the interaction between supraspinal drive (brain), the central pattern generator in the spinal cord, the body, and exteroceptive sensing by leveraging deep reinforcement learning and robotics tools. Consistent with quadruped animal data, we show that the walk-trot gait transition for quadruped robots on flat terrain improves both viability and energy efficiency. Furthermore, we investigate the effects of discrete terrain (i.e. crossing successive gaps) on imposing gait transitions, and find the emergence of trot-pronk transitions to avoid non-viable states. Compared with other potential criteria such as peak forces and energy efficiency, viability is the only improved factor after gait transitions on both flat and discrete gap terrains, suggesting that viability could be a primary and universal objective of gait transitions, while other criteria are secondary objectives and/or a consequence of viability. Moreover, we deploy our learned controller in sim-to-real hardware experiments and demonstrate state-of-the-art quadruped agility in challenging scenarios, where the Unitree A1 quadruped autonomously transitions gaits between trot and pronk to cross consecutive gaps of up to 30 cm (83.3 % of the body-length) at over 1.3 m/s.", "url": "https://arxiv.org/abs/2306.07419"}, {"metadata": {"arXiv": "2306.07525", "Date": "Tue, 13 Jun 2023 03:38:05 ", "Title": "Using Collision Momentum in Deep Reinforcement Learning Based Adversarial Pedestrian Modeling", "Authors": ["Dianwei Chen", "Ekim Yurtsever", "Keith Redmill and Umit Ozguner"], "Categories": "cs.RO cs.AI cs.LG"}, "abstract": "Recent research in pedestrian simulation often aims to develop realistic behaviors in various situations, but it is challenging for existing algorithms to generate behaviors that identify weaknesses in automated vehicles' performance in extreme and unlikely scenarios and edge cases. To address this, specialized pedestrian behavior algorithms are needed. Current research focuses on realistic trajectories using social force models and reinforcement learning based models. However, we propose a reinforcement learning algorithm that specifically targets collisions and better uncovers unique failure modes of automated vehicle controllers. Our algorithm is efficient and generates more severe collisions, allowing for the identification and correction of weaknesses in autonomous driving algorithms in complex and varied scenarios.", "url": "https://arxiv.org/abs/2306.07525"}, {"metadata": {"arXiv": "2306.07962", "Date": "Tue, 13 Jun 2023 17:57:03 ", "Title": "Parting with Misconceptions about Learning-based Vehicle Motion Planning", "Authors": ["Daniel Dauner", "Marcel Hallgarten", "Andreas Geiger", "Kashyap Chitta"], "Categories": "cs.RO cs.AI cs.CV cs.LG"}, "abstract": "The release of nuPlan marks a new era in vehicle motion planning research, offering the first large-scale real-world dataset and evaluation schemes requiring both precise short-term planning and long-horizon ego-forecasting. Existing systems struggle to simultaneously meet both requirements. Indeed, we find that these tasks are fundamentally misaligned and should be addressed independently. We further assess the current state of closed-loop planning in the field, revealing the limitations of learning-based methods in complex real-world scenarios and the value of simple rule-based priors such as centerline selection through lane graph search algorithms. More surprisingly, for the open-loop sub-task, we observe that the best results are achieved when using only this centerline as scene context (\\ie, ignoring all information regarding the map and other agents). Combining these insights, we propose an extremely simple and efficient planner which outperforms an extensive set of competitors, winning the nuPlan planning challenge 2023.", "url": "https://arxiv.org/abs/2306.07962"}, {"metadata": {"arXiv": "2306.07297 (*cross-listing*)", "Date": "Sat, 10 Jun 2023 20:55:21 ", "Title": "Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification", "Authors": ["Shouvon Sarker", "Lijun Qian", "Xishuang Dong"], "Categories": "cs.CL cs.AI cs.LG"}, "abstract": "The identification of key factors such as medications, diseases, and relationships within electronic health records and clinical notes has a wide range of applications in the clinical field. In the N2C2 2022 competitions, various tasks were presented to promote the identification of key factors in electronic health records (EHRs) using the Contextualized Medication Event Dataset (CMED). Pretrained large language models (LLMs) demonstrated exceptional performance in these tasks. This study aims to explore the utilization of LLMs, specifically ChatGPT, for data augmentation to overcome the limited availability of annotated data for identifying the key factors in EHRs. Additionally, different pre-trained BERT models, initially trained on extensive datasets like Wikipedia and MIMIC, were employed to develop models for identifying these key variables in EHRs through fine-tuning on augmented datasets. The experimental results of two EHR analysis tasks, namely medication identification and medication event classification, indicate that data augmentation based on ChatGPT proves beneficial in improving performance for both medication identification and medication event classification.", "url": "https://arxiv.org/abs/2306.07297"}, {"metadata": {"arXiv": "2306.07499 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 02:20:58 ", "Title": "Improving Opinion-based Question Answering Systems Through Label Error Detection and Overwrite", "Authors": ["Xiao Yang", "Ahmed K. Mohamed", "Shashank Jain", "Stanislav Peshterliev", "Debojeet Chatterjee", "Hanwen Zha", "Nikita Bhalla", "Gagan Aneja and Pranab Mohanty"], "Categories": "cs.CL cs.AI cs.LG"}, "abstract": "Label error is a ubiquitous problem in annotated data. Large amounts of label error substantially degrades the quality of deep learning models. Existing methods to tackle the label error problem largely focus on the classification task, and either rely on task specific architecture or require non-trivial additional computations, which is undesirable or even unattainable for industry usage. In this paper, we propose LEDO: a model-agnostic and computationally efficient framework for Label Error Detection and Overwrite. LEDO is based on Monte Carlo Dropout combined with uncertainty metrics, and can be easily generalized to multiple tasks and data sets. Applying LEDO to an industry opinion-based question answering system demonstrates it is effective at improving accuracy in all the core models. Specifically, LEDO brings 1.1% MRR gain for the retrieval model, 1.5% PR AUC improvement for the machine reading comprehension model, and 0.9% rise in the Average Precision for the ranker, on top of the strong baselines with a large-scale social media dataset. Importantly, LEDO is computationally efficient compared to methods that require loss function change, and cost-effective as the resulting data can be used in the same continuous training pipeline for production. Further analysis shows that these gains come from an improved decision boundary after cleaning the label errors existed in the training data.", "url": "https://arxiv.org/abs/2306.07499"}, {"metadata": {"arXiv": "2306.07622 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 08:43:13 ", "Title": "Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4", "Authors": ["Thilo Hagendorff", "Sarah Fabi"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["arXiv admin note: substantial text overlap with arXiv:2212.05206"]}, "abstract": "Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.", "url": "https://arxiv.org/abs/2306.07622"}, {"metadata": {"arXiv": "2306.07664 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 10:14:58 ", "Title": "Rethink the Effectiveness of Text Data Augmentation: An Empirical Analysis", "Authors": ["Zhengxiang Shi", "Aldo Lipani"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["Accepted at ESANN 2023"]}, "abstract": "In recent years, language models (LMs) have made remarkable progress in advancing the field of natural language processing (NLP). However, the impact of data augmentation (DA) techniques on the fine-tuning (FT) performance of these LMs has been a topic of ongoing debate. In this study, we evaluate the effectiveness of three different FT methods in conjugation with back-translation across an array of 7 diverse NLP tasks, including classification and regression types, covering single-sentence and sentence-pair tasks. Contrary to prior assumptions that DA does not contribute to the enhancement of LMs' FT performance, our findings reveal that continued pre-training on augmented data can effectively improve the FT performance of the downstream tasks. In the most favourable case, continued pre-training improves the performance of FT by more than 10% in the few-shot learning setting. Our finding highlights the potential of DA as a powerful tool for bolstering LMs' performance.", "url": "https://arxiv.org/abs/2306.07664"}, {"metadata": {"arXiv": "2306.07691 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 11:04:43 ", "Title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models", "Authors": ["Yinghao Aaron Li", "Cong Han", "Vinay S. Raghavan", "Gavin Mischler", "Nima Mesgarani"], "Categories": "eess.AS cs.AI cs.CL cs.LG cs.SD"}, "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.", "url": "https://arxiv.org/abs/2306.07691"}, {"metadata": {"arXiv": "2306.07799 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 14:21:35 ", "Title": "ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer", "Authors": ["Dongqi Pu", "Vera Demberg"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["ACL-SRW 2023"]}, "abstract": "Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.", "url": "https://arxiv.org/abs/2306.07799"}, {"metadata": {"arXiv": "2306.07812 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 14:43:13 ", "Title": "Automated 3D Pre-Training for Molecular Property Prediction", "Authors": ["Xu Wang and Huan Zhao and Weiwei Tu and Quanming Yao"], "Categories": "q-bio.QM cs.AI cs.LG", "DOI": "10.1145/3580305.3599252"}, "abstract": "Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the \\textit{total energy} to search for weight distribution of the three pretext task since total energy corresponding to the quality of 3D conformer.Extensive experiments on 2D molecular graphs are conducted to demonstrate the accuracy, efficiency and generalization ability of the proposed 3D PGT compared to various pre-training baselines.", "url": "https://arxiv.org/abs/2306.07812"}, {"metadata": {"arXiv": "2306.07934 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 17:39:20 ", "Title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information", "Authors": ["Mehran Kazemi", "Quan Yuan", "Deepti Bhatia", "Najoung Kim", "Xin Xu", "Vaiva Imbrasaite", "Deepak Ramachandran"], "Categories": "cs.CL cs.AI cs.LG"}, "abstract": "Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor.", "url": "https://arxiv.org/abs/2306.07934"}, {"metadata": {"arXiv": "2306.07935 (*cross-listing*)", "Date": "Sun, 11 Jun 2023 02:35:48 ", "Title": "Multi-modal Representation Learning for Social Post Location Inference", "Authors": ["Ruiting Dai", "Jiayi Luo", "Xucheng Luo", "Lisi Mo", "Wanlun Ma", "Fan Zhou"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["6 pages", "2023 International Conference on Communications"]}, "abstract": "Inferring geographic locations via social posts is essential for many practical location-based applications such as product marketing, point-of-interest recommendation, and infector tracking for COVID-19. Unlike image-based location retrieval or social-post text embedding-based location inference, the combined effect of multi-modal information (i.e., post images, text, and hashtags) for social post positioning receives less attention. In this work, we collect real datasets of social posts with images, texts, and hashtags from Instagram and propose a novel Multi-modal Representation Learning Framework (MRLF) capable of fusing different modalities of social posts for location inference. MRLF integrates a multi-head attention mechanism to enhance location-salient information extraction while significantly improving location inference compared with single domain-based methods. To overcome the noisy user-generated textual content, we introduce a novel attention-based character-aware module that considers the relative dependencies between characters of social post texts and hashtags for flexible multi-model information fusion. The experimental results show that MRLF can make accurate location predictions and open a new door to understanding the multi-modal data of social posts for online inference tasks.", "url": "https://arxiv.org/abs/2306.07935"}, {"metadata": {"arXiv": "2306.07949 (*cross-listing*)", "Date": "Fri, 09 Jun 2023 03:36:00 ", "Title": "Improving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition", "Authors": ["Xianzhao Chen", "Yist Y. Lin", "Kang Wang", "Yi He", "Zejun Ma"], "Categories": "eess.AS cs.AI cs.LG", "Comments": ["To appear in the proceedings of INTERSPEECH 2023"]}, "abstract": "End-to-end (E2E) systems have shown comparable performance to hybrid systems for automatic speech recognition (ASR). Word timings, as a by-product of ASR, are essential in many applications, especially for subtitling and computer-aided pronunciation training. In this paper, we improve the frame-level classifier for word timings in E2E system by introducing label priors in connectionist temporal classification (CTC) loss, which is adopted from prior works, and combining low-level Mel-scale filter banks with high-level ASR encoder output as input feature. On the internal Chinese corpus, the proposed method achieves 95.68%/94.18% compared to the hybrid system 93.0%/90.22% on the word timing accuracy metrics. It also surpass a previous E2E approach with an absolute increase of 4.80%/8.02% on the metrics on 7 languages. In addition, we further improve word timing accuracy by delaying CTC peaks with frame-wise knowledge distillation, though only experimenting on LibriSpeech.", "url": "https://arxiv.org/abs/2306.07949"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
