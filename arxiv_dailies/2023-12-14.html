<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2312.07550", "Date": "Wed, 06 Dec 2023 19:53:17 ", "Title": "Understanding (Un)Intended Memorization in Text-to-Image Generative Models", "Authors": ["Ali Naseh", "Jaechul Roh", "Amir Houmansadr"], "Categories": "cs.CV cs.CL cs.CR cs.LG"}, "abstract": "Multimodal machine learning, especially text-to-image models like Stable Diffusion and DALL-E 3, has gained significance for transforming text into detailed images. Despite their growing use and remarkable generative capabilities, there is a pressing need for a detailed examination of these models' behavior, particularly with respect to memorization. Historically, memorization in machine learning has been context-dependent, with diverse definitions emerging from classification tasks to complex models like Large Language Models (LLMs) and Diffusion models. Yet, a definitive concept of memorization that aligns with the intricacies of text-to-image synthesis remains elusive. This understanding is vital as memorization poses privacy risks yet is essential for meeting user expectations, especially when generating representations of underrepresented entities. In this paper, we introduce a specialized definition of memorization tailored to text-to-image models, categorizing it into three distinct types according to user expectations. We closely examine the subtle distinctions between intended and unintended memorization, emphasizing the importance of balancing user privacy with the generative quality of the model outputs. Using the Stable Diffusion model, we offer examples to validate our memorization definitions and clarify their application.", "url": "https://arxiv.org/abs/2312.07550"}, {"metadata": {"arXiv": "2312.07560", "Date": "Fri, 08 Dec 2023 21:56:19 ", "Title": "AI-driven Structure Detection and Information Extraction from Historical Cadastral Maps (Early 19th Century Franciscean Cadastre in the Province of Styria) and Current High-resolution Satellite and Aerial Imagery for Remote Sensing", "Authors": ["Wolfgang G\\\"oderle", "Christian Macher", "Katrin Mauthner", "Oliver Pimas", "Fabian Rampetsreiter"], "Categories": "cs.CV cs.LG", "Comments": ["18 pages", "7 figures"]}, "abstract": "Cadastres from the 19th century are a complex as well as rich source for historians and archaeologists, whose use presents them with great challenges. For archaeological and historical remote sensing, we have trained several Deep Learning models, CNNs as well as Vision Transformers, to extract large-scale data from this knowledge representation. We present the principle results of our work here and we present a the demonstrator of our browser-based tool that allows researchers and public stakeholders to quickly identify spots that featured buildings in the 19th century Franciscean Cadastre. The tool not only supports scholars and fellow researchers in building a better understanding of the settlement history of the region of Styria, it also helps public administration and fellow citizens to swiftly identify areas of heightened sensibility with regard to the cultural heritage of the region.", "url": "https://arxiv.org/abs/2312.07560"}, {"metadata": {"arXiv": "2312.07627", "Date": "Tue, 12 Dec 2023 07:24:02 ", "Title": "Multimodal Sentiment Analysis: Perceived vs Induced Sentiments", "Authors": ["Aditi Aggarwal", "Deepika Varshney", "Saurabh Patel"], "Categories": "cs.CV cs.LG cs.SI"}, "abstract": "Social media has created a global network where people can easily access and exchange vast information. This information gives rise to a variety of opinions, reflecting both positive and negative viewpoints. GIFs stand out as a multimedia format offering a visually engaging way for users to communicate. In this research, we propose a multimodal framework that integrates visual and textual features to predict the GIF sentiment. It also incorporates attributes including face emotion detection and OCR generated captions to capture the semantic aspects of the GIF. The developed classifier achieves an accuracy of 82.7% on Twitter GIFs, which is an improvement over state-of-the-art models. Moreover, we have based our research on the ReactionGIF dataset, analysing the variance in sentiment perceived by the author and sentiment induced in the reader", "url": "https://arxiv.org/abs/2312.07627"}, {"metadata": {"arXiv": "2312.07661", "Date": "Tue, 12 Dec 2023 19:00:04 ", "Title": "CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor", "Authors": ["Shuyang Sun", "Runjia Li", "Philip Torr", "Xiuye Gu", "Siyang Li"], "Categories": "cs.CV cs.CL cs.LG cs.MM", "Comments": ["Project page: https://torrvision.com/clip_as_rnn/"]}, "abstract": "Existing open-vocabulary image segmentation methods require a fine-tuning step on mask annotations and/or image-text datasets. Mask labels are labor-intensive, which limits the number of categories in segmentation datasets. As a result, the open-vocabulary capacity of pre-trained VLMs is severely reduced after fine-tuning. However, without fine-tuning, VLMs trained under weak image-text supervision tend to make suboptimal mask predictions when there are text queries referring to non-existing concepts in the image. To alleviate these issues, we introduce a novel recurrent framework that progressively filters out irrelevant texts and enhances mask quality without training efforts. The recurrent unit is a two-stage segmenter built upon a VLM with frozen weights. Thus, our model retains the VLM's broad vocabulary space and strengthens its segmentation capability. Experimental results show that our method outperforms not only the training-free counterparts, but also those fine-tuned with millions of additional data samples, and sets new state-of-the-art records for both zero-shot semantic and referring image segmentation tasks. Specifically, we improve the current record by 28.8, 16.0, and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.", "url": "https://arxiv.org/abs/2312.07661"}, {"metadata": {"arXiv": "2312.07833", "Date": "Wed, 13 Dec 2023 01:40:21 ", "Title": "Stable Rivers: A Case Study in the Application of Text-to-Image Generative Models for Earth Sciences", "Authors": ["C Kupferschmidt", "A.D. Binns", "K.L. Kupferschmidt", "and G.W Taylor"], "Categories": "cs.CV cs.LG"}, "abstract": "Text-to-image (TTI) generative models can be used to generate photorealistic images from a given text-string input. These models offer great potential to mitigate challenges to the uptake of machine learning in the earth sciences. However, the rapid increase in their use has raised questions about fairness and biases, with most research to-date focusing on social and cultural areas rather than domain-specific considerations. We conducted a case study for the earth sciences, focusing on the field of fluvial geomorphology, where we evaluated subject-area specific biases in the training data and downstream model performance of Stable Diffusion (v1.5). In addition to perpetuating Western biases, we found that the training data over-represented scenic locations, such as famous rivers and waterfalls, and showed serious under- and over-representation of many morphological and environmental terms. Despite biased training data, we found that with careful prompting, the Stable Diffusion model was able to generate photorealistic synthetic river images reproducing many important environmental and morphological characteristics. Furthermore, conditional control techniques, such as the use of condition maps with ControlNet were effective for providing additional constraints on output images. Despite great potential for the use of TTI models in the earth sciences field, we advocate for caution in sensitive applications, and advocate for domain-specific reviews of training data and image generation biases to mitigate perpetuation of existing biases.", "url": "https://arxiv.org/abs/2312.07833"}, {"metadata": {"arXiv": "2312.07854", "Date": "Wed, 13 Dec 2023 02:48:11 ", "Title": "Diffusion Models Enable Zero-Shot Pose Estimation for Lower-Limb Prosthetic Users", "Authors": ["Tianxun Zhou", "Muhammad Nur Shahril Iskandar", "Keng-Hwee Chiam"], "Categories": "cs.CV cs.LG", "Comments": ["25 pages", "6 figures. Supplementary documents in source file"]}, "abstract": "The application of 2D markerless gait analysis has garnered increasing interest and application within clinical settings. However, its effectiveness in the realm of lower-limb amputees has remained less than optimal. In response, this study introduces an innovative zero-shot method employing image generation diffusion models to achieve markerless pose estimation for lower-limb prosthetics, presenting a promising solution to gait analysis for this specific population. Our approach demonstrates an enhancement in detecting key points on prosthetic limbs over existing methods, and enables clinicians to gain invaluable insights into the kinematics of lower-limb amputees across the gait cycle. The outcomes obtained not only serve as a proof-of-concept for the feasibility of this zero-shot approach but also underscore its potential in advancing rehabilitation through gait analysis for this unique population.", "url": "https://arxiv.org/abs/2312.07854"}, {"metadata": {"arXiv": "2312.08010", "Date": "Wed, 13 Dec 2023 09:33:08 ", "Title": "EZ-CLIP: Efficient Zeroshot Video Action Recognition", "Authors": ["Shahzad Ahmad", "Sukalpa Chanda", "Yogesh S Rawat"], "Categories": "cs.CV cs.LG"}, "abstract": "Recent advancements in large-scale pre-training of visual-language models on paired image-text data have demonstrated impressive generalization capabilities for zero-shot tasks. Building on this success, efforts have been made to adapt these image-based visual-language models, such as CLIP, for videos extending their zero-shot capabilities to the video domain. While these adaptations have shown promising results, they come at a significant computational cost and struggle with effectively modeling the crucial temporal aspects inherent to the video domain. In this study, we present EZ-CLIP, a simple and efficient adaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal visual prompting for seamless temporal adaptation, requiring no fundamental alterations to the core CLIP architecture while preserving its remarkable generalization abilities. Moreover, we introduce a novel learning objective that guides the temporal visual prompts to focus on capturing motion, thereby enhancing its learning capabilities from video data. We conducted extensive experiments on five different benchmark datasets, thoroughly evaluating EZ-CLIP for zero-shot learning and base-to-novel video action recognition, and also demonstrating its potential for few-shot generalization.Impressively, with a mere 5.2 million learnable parameters (as opposed to the 71.1 million in the prior best model), EZ-CLIP can be efficiently trained on a single GPU, outperforming existing approaches in several evaluations.", "url": "https://arxiv.org/abs/2312.08010"}, {"metadata": {"arXiv": "2312.08230", "Date": "Wed, 13 Dec 2023 15:48:50 ", "Title": "Partial Symmetry Detection for 3D Geometry using Contrastive Learning with Geodesic Point Cloud Patches", "Authors": ["Gregor Kobsik", "Isaak Lim", "Leif Kobbelt"], "Categories": "cs.CV cs.LG"}, "abstract": "Symmetry detection, especially partial and extrinsic symmetry, is essential for various downstream tasks, like 3D geometry completion, segmentation, compression and structure-aware shape encoding or generation. In order to detect partial extrinsic symmetries, we propose to learn rotation, reflection, translation and scale invariant local shape features for geodesic point cloud patches via contrastive learning, which are robust across multiple classes and generalize over different datasets. We show that our approach is able to extract multiple valid solutions for this ambiguous problem. Furthermore, we introduce a novel benchmark test for partial extrinsic symmetry detection to evaluate our method. Lastly, we incorporate the detected symmetries together with a region growing algorithm to demonstrate a downstream task with the goal of computing symmetry-aware partitions of 3D shapes. To our knowledge, we are the first to propose a self-supervised data-driven method for partial extrinsic symmetry detection.", "url": "https://arxiv.org/abs/2312.08230"}, {"metadata": {"arXiv": "2312.08288", "Date": "Wed, 13 Dec 2023 17:04:16 ", "Title": "Hybrid Sample Synthesis-based Debiasing of Classifier in Limited Data Setting", "Authors": ["Piyush Arora", "Pratik Mazumder"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted in WACV 2024"]}, "abstract": "Deep learning models are known to suffer from the problem of bias, and researchers have been exploring methods to address this issue. However, most of these methods require prior knowledge of the bias and are not always practical. In this paper, we focus on a more practical setting with no prior information about the bias. Generally, in this setting, there are a large number of bias-aligned samples that cause the model to produce biased predictions and a few bias-conflicting samples that do not conform to the bias. If the training data is limited, the influence of the bias-aligned samples may become even stronger on the model predictions, and we experimentally demonstrate that existing debiasing techniques suffer severely in such cases. In this paper, we examine the effects of unknown bias in small dataset regimes and present a novel approach to mitigate this issue. The proposed approach directly addresses the issue of the extremely low occurrence of bias-conflicting samples in limited data settings through the synthesis of hybrid samples that can be used to reduce the effect of bias. We perform extensive experiments on several benchmark datasets and experimentally demonstrate the effectiveness of our proposed approach in addressing any unknown bias in the presence of limited data. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiAN debiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% when only 10% of the Corrupted CIFAR-10 Type 1 dataset is available with a bias-conflicting sample ratio of 0.05.", "url": "https://arxiv.org/abs/2312.08288"}, {"metadata": {"arXiv": "2312.08298", "Date": "Wed, 13 Dec 2023 17:13:08 ", "Title": "Venn: Resource Management Across Federated Learning Jobs", "Authors": ["Jiachen Liu", "Fan Lai", "Ding Ding", "Yiwen Zhang", "Mosharaf Chowdhury"], "Categories": "cs.DC cs.LG", "Comments": ["15 pages", "15 figrues"]}, "abstract": "In recent years, federated learning (FL) has emerged as a promising approach for machine learning (ML) and data science across distributed edge devices. With the increasing popularity of FL, resource contention between multiple FL jobs training on the same device population is increasing as well. Scheduling edge resources among multiple FL jobs is different from GPU scheduling for cloud ML because of the ephemeral nature and planetary scale of participating devices as well as the overlapping resource requirements of diverse FL jobs. Existing resource managers for FL jobs opt for random assignment of devices to FL jobs for simplicity and scalability, which leads to poor performance. In this paper, we present Venn, an FL resource manager, that efficiently schedules ephemeral, heterogeneous devices among many FL jobs, with the goal of reducing their average job completion time (JCT). Venn formulates the Intersection Resource Scheduling (IRS) problem to identify complex resource contention among multiple FL jobs. Then, Venn proposes a contention-aware scheduling heuristic to minimize the average scheduling delay. Furthermore, it proposes a resource-aware device-to-job matching heuristic that focuses on optimizing response collection time by mitigating stragglers. Our evaluation shows that, compared to the state-of-the-art FL resource managers, Venn improves the average JCT by up to 1.88X.", "url": "https://arxiv.org/abs/2312.08298"}, {"metadata": {"arXiv": "2312.07929", "Date": "Wed, 13 Dec 2023 06:54:49 ", "Title": "Robust and Performance Incentivizing Algorithms for Multi-Armed Bandits with Strategic Agents", "Authors": ["Seyed A. Esmaeili", "Suho Shin", "Aleksandrs Slivkins"], "Categories": "cs.GT cs.LG"}, "abstract": "We consider a variant of the stochastic multi-armed bandit problem. Specifically, the arms are strategic agents who can improve their rewards or absorb them. The utility of an agent increases if she is pulled more or absorbs more of her rewards but decreases if she spends more effort improving her rewards. Agents have heterogeneous properties, specifically having different means and able to improve their rewards up to different levels. Further, a non-empty subset of agents are ''honest'' and in the worst case always give their rewards without absorbing any part. The principal wishes to obtain a high revenue (cumulative reward) by designing a mechanism that incentives top level performance at equilibrium. At the same time, the principal wishes to be robust and obtain revenue at least at the level of the honest agent with the highest mean in case of non-equilibrium behaviour. We identify a class of MAB algorithms which we call performance incentivizing which satisfy a collection of properties and show that they lead to mechanisms that incentivize top level performance at equilibrium and are robust under any strategy profile. Interestingly, we show that UCB is an example of such a MAB algorithm. Further, in the case where the top performance level is unknown we show that combining second price auction ideas with performance incentivizing algorithms achieves performance at least at the second top level while also being robust.", "url": "https://arxiv.org/abs/2312.07929"}, {"metadata": {"arXiv": "2312.08008", "Date": "Wed, 13 Dec 2023 09:31:30 ", "Title": "Learning Nash Equilibria in Zero-Sum Markov Games: A Single Time-scale Algorithm Under Weak Reachability", "Authors": ["Reda Ouhamma and Maryam Kamgarpour"], "Categories": "cs.GT cs.LG", "Comments": ["arXiv admin note: text overlap with arXiv:2303.03100 by other authors"]}, "abstract": "We consider decentralized learning for zero-sum games, where players only see their payoff information and are agnostic to actions and payoffs of the opponent. Previous works demonstrated convergence to a Nash equilibrium in this setting using double time-scale algorithms under strong reachability assumptions. We address the open problem of achieving an approximate Nash equilibrium efficiently with an uncoupled and single time-scale algorithm under weaker conditions. Our contribution is a rational and convergent algorithm, utilizing Tsallis-entropy regularization in a value-iteration-based approach. The algorithm learns an approximate Nash equilibrium in polynomial time, requiring only the existence of a policy pair that induces an irreducible and aperiodic Markov chain, thus considerably weakening past assumptions. Our analysis leverages negative drift inequalities and introduces novel properties of Tsallis entropy that are of independent interest.", "url": "https://arxiv.org/abs/2312.08008"}, {"metadata": {"arXiv": "2312.07577", "Date": "Sun, 10 Dec 2023 18:19:07 ", "Title": "Benchmarking Distribution Shift in Tabular Data with TableShift", "Authors": ["Josh Gardner", "Zoran Popovic", "Ludwig Schmidt"], "Categories": "cs.LG", "Comments": ["NeurIPS 2023 Dataset and Benchmarks Track accepted version"]}, "abstract": "Robustness to distribution shift has become a growing concern for text and image models as they transition from research subjects to deployment in the real world. However, high-quality benchmarks for distribution shift in tabular machine learning tasks are still lacking despite the widespread real-world use of tabular data and differences in the models used for tabular data in comparison to text and images. As a consequence, the robustness of tabular models to distribution shift is poorly understood. To address this issue, we introduce TableShift, a distribution shift benchmark for tabular data. TableShift contains 15 binary classification tasks in total, each with an associated shift, and includes a diverse set of data sources, prediction targets, and distribution shifts. The benchmark covers domains including finance, education, public policy, healthcare, and civic participation, and is accessible using only a few lines of Python code via the TableShift API. We conduct a large-scale study comparing several state-of-the-art tabular data models alongside robust learning and domain generalization methods on the benchmark tasks. Our study demonstrates (1) a linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy; (2) domain robustness methods can reduce shift gaps but at the cost of reduced ID accuracy; (3) a strong relationship between shift gap (difference between ID and OOD performance) and shifts in the label distribution. The benchmark data, Python package, model implementations, and more information about TableShift are available at https://github.com/mlfoundations/tableshift and https://tableshift.org .", "url": "https://arxiv.org/abs/2312.07577"}, {"metadata": {"arXiv": "2312.07583", "Date": "Mon, 11 Dec 2023 01:41:19 ", "Title": "Classification with Partially Private Features", "Authors": ["Zeyu Shen", "Anilesh Krishnaswamy", "Janardhan Kulkarni", "Kamesh Munagala"], "Categories": "cs.LG cs.CR"}, "abstract": "In this paper, we consider differentially private classification when some features are sensitive, while the rest of the features and the label are not. We adapt the definition of differential privacy naturally to this setting. Our main contribution is a novel adaptation of AdaBoost that is not only provably differentially private, but also significantly outperforms a natural benchmark that assumes the entire data of the individual is sensitive in the experiments. As a surprising observation, we show that boosting randomly generated classifiers suffices to achieve high accuracy. Our approach easily adapts to the classical setting where all the features are sensitive, providing an alternate algorithm for differentially private linear classification with a much simpler privacy proof and comparable or higher accuracy than differentially private logistic regression on real-world datasets.", "url": "https://arxiv.org/abs/2312.07583"}, {"metadata": {"arXiv": "2312.07615", "Date": "Mon, 11 Dec 2023 21:06:07 ", "Title": "Optimizing Likelihood-free Inference using Self-supervised Neural Symmetry Embeddings", "Authors": ["Deep Chatterjee", "Philip C. Harris", "Maanas Goel", "Malina Desai", "Michael W. Coughlin and Erik Katsavounidis"], "Categories": "cs.LG astro-ph.IM", "Comments": ["Accepted for Machine Learning and the Physical Sciences Workshop (submission 69) at NeurIPS 2023; for codes", "see https://github.com/ML4GW/summer-projects-2023/blob/neurips-2023/symmetry-informed-flows/README.md"]}, "abstract": "Likelihood-free inference is quickly emerging as a powerful tool to perform fast/effective parameter estimation. We demonstrate a technique of optimizing likelihood-free inference to make it even faster by marginalizing symmetries in a physical problem. In this approach, physical symmetries, for example, time-translation are learned using joint-embedding via self-supervised learning with symmetry data augmentations. Subsequently, parameter inference is performed using a normalizing flow where the embedding network is used to summarize the data before conditioning the parameters. We present this approach on two simple physical problems and we show faster convergence in a smaller number of parameters compared to a normalizing flow that does not use a pre-trained symmetry-informed representation.", "url": "https://arxiv.org/abs/2312.07615"}, {"metadata": {"arXiv": "2312.07633", "Date": "Tue, 12 Dec 2023 09:33:54 ", "Title": "SE(3)-Invariant Multiparameter Persistent Homology for Chiral-Sensitive Molecular Property Prediction", "Authors": ["Andac Demir", "Francis Prael III", "Bulent Kiziltan"], "Categories": "cs.LG cs.CG math.AT", "Comments": ["NeurIPS 2023 AI for Science Workshop"]}, "abstract": "In this study, we present a novel computational method for generating molecular fingerprints using multiparameter persistent homology (MPPH). This technique holds considerable significance for drug discovery and materials science, where precise molecular property prediction is vital. By integrating SE(3)-invariance with Vietoris-Rips persistent homology, we effectively capture the three-dimensional representations of molecular chirality. This non-superimposable mirror image property directly influences the molecular interactions, serving as an essential factor in molecular property prediction. We explore the underlying topologies and patterns in molecular structures by applying Vietoris-Rips persistent homology across varying scales and parameters such as atomic weight, partial charge, bond type, and chirality. Our method's efficacy can be improved by incorporating additional parameters such as aromaticity, orbital hybridization, bond polarity, conjugated systems, as well as bond and torsion angles. Additionally, we leverage Stochastic Gradient Langevin Boosting in a Bayesian ensemble of GBDTs to obtain aleatoric and epistemic uncertainty estimates for gradient boosting models. With these uncertainty estimates, we prioritize high-uncertainty samples for active learning and model fine-tuning, benefiting scenarios where data labeling is costly or time consuming. Compared to conventional GNNs which usually suffer from oversmoothing and oversquashing, MPPH provides a more comprehensive and interpretable characterization of molecular data topology. We substantiate our approach with theoretical stability guarantees and demonstrate its superior performance over existing state-of-the-art methods in predicting molecular properties through extensive evaluations on the MoleculeNet benchmark datasets.", "url": "https://arxiv.org/abs/2312.07633"}, {"metadata": {"arXiv": "2312.07636", "Date": "Tue, 12 Dec 2023 10:25:31 ", "Title": "Go beyond End-to-End Training: Boosting Greedy Local Learning with Context Supply", "Authors": ["Chengting Yu", "Fengzhao Zhang", "Hanzhi Ma", "Aili Wang and Erping Li"], "Categories": "cs.LG cs.CV stat.ML", "Comments": ["9 figures", "12 tables"]}, "abstract": "Traditional end-to-end (E2E) training of deep networks necessitates storing intermediate activations for back-propagation, resulting in a large memory footprint on GPUs and restricted model parallelization. As an alternative, greedy local learning partitions the network into gradient-isolated modules and trains supervisely based on local preliminary losses, thereby providing asynchronous and parallel training methods that substantially reduce memory cost. However, empirical experiments reveal that as the number of segmentations of the gradient-isolated module increases, the performance of the local learning scheme degrades substantially, severely limiting its expansibility. To avoid this issue, we theoretically analyze the greedy local learning from the standpoint of information theory and propose a ContSup scheme, which incorporates context supply between isolated modules to compensate for information loss. Experiments on benchmark datasets (i.e. CIFAR, SVHN, STL-10) achieve SOTA results and indicate that our proposed method can significantly improve the performance of greedy local learning with minimal memory and computational overhead, allowing for the boost of the number of isolated modules. Our codes are available at https://github.com/Tab-ct/ContSup.", "url": "https://arxiv.org/abs/2312.07636"}, {"metadata": {"arXiv": "2312.07679", "Date": "Tue, 12 Dec 2023 19:18:04 ", "Title": "Bayesian Online Learning for Consensus Prediction", "Authors": ["Sam Showalter", "Alex Boyd", "Padhraic Smyth", "Mark Steyvers"], "Categories": "cs.LG stat.ML"}, "abstract": "Given a pre-trained classifier and multiple human experts, we investigate the task of online classification where model predictions are provided for free but querying humans incurs a cost. In this practical but under-explored setting, oracle ground truth is not available. Instead, the prediction target is defined as the consensus vote of all experts. Given that querying full consensus can be costly, we propose a general framework for online Bayesian consensus estimation, leveraging properties of the multivariate hypergeometric distribution. Based on this framework, we propose a family of methods that dynamically estimate expert consensus from partial feedback by producing a posterior over expert and model beliefs. Analyzing this posterior induces an interpretable trade-off between querying cost and classification performance. We demonstrate the efficacy of our framework against a variety of baselines on CIFAR-10H and ImageNet-16H, two large-scale crowdsourced datasets.", "url": "https://arxiv.org/abs/2312.07679"}, {"metadata": {"arXiv": "2312.07680", "Date": "Tue, 12 Dec 2023 19:22:07 ", "Title": "I Open at the Close: A Deep Reinforcement Learning Evaluation of Open Streets Initiatives", "Authors": ["R. Teal Witter", "Lucas Rosenblatt"], "Categories": "cs.LG", "Comments": ["camera ready for AAAI 2024"]}, "abstract": "The open streets initiative \"opens\" streets to pedestrians and bicyclists by closing them to cars and trucks. The initiative, adopted by many cities across North America, increases community space in urban environments. But could open streets also make cities safer and less congested? We study this question by framing the choice of which streets to open as a reinforcement learning problem. In order to simulate the impact of opening streets, we first compare models for predicting vehicle collisions given network and temporal data. We find that a recurrent graph neural network, leveraging the graph structure and the short-term temporal dependence of the data, gives the best predictive performance. Then, with the ability to simulate collisions and traffic, we frame a reinforcement learning problem to find which streets to open. We compare the streets in the NYC Open Streets program to those proposed by a Q-learning algorithm. We find that the streets proposed by the Q-learning algorithm have reliably better outcomes, while streets in the program have similar outcomes to randomly selected streets. We present our work as a step toward principally choosing which streets to open for safer and less congested cities. All our code and data are available on Github.", "url": "https://arxiv.org/abs/2312.07680"}, {"metadata": {"arXiv": "2312.07682", "Date": "Tue, 12 Dec 2023 19:23:54 ", "Title": "An Online, Adaptive and Unsupervised Regression Framework with Drift Detection for Label Scarcity Contexts", "Authors": ["Rene Richard and Nabil Belacel"], "Categories": "cs.LG", "Comments": ["29 pages", "25 figures", "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD) 2024"]}, "abstract": "In scenarios where obtaining real-time labels proves challenging, conventional approaches may result in sub-optimal performance. This paper presents an optimal strategy for streaming contexts with limited labeled data, introducing an adaptive technique for unsupervised regression. The proposed method leverages a sparse set of initial labels and introduces an innovative drift detection mechanism to enable dynamic model adaptations in response to evolving patterns in the data. To enhance adaptability, we integrate the ADWIN (ADaptive WINdowing) algorithm with error generalization based on Root Mean Square Error (RMSE). ADWIN facilitates real-time drift detection, while RMSE provides a robust measure of model prediction accuracy. This combination enables our multivariate method to effectively navigate the challenges of streaming data, continuously adapting to changing patterns while maintaining a high level of predictive precision. Finally, we evaluate the performance of our multivariate method across various public datasets, comparing it to non-adapting baselines. Through comprehensive assessments, we demonstrate the superior efficacy of our adaptive regression technique for tasks where obtaining labels in real-time is a significant challenge. The results underscore the method's capacity to outperform traditional approaches and highlight its potential in scenarios characterized by label scarcity and evolving data patterns.", "url": "https://arxiv.org/abs/2312.07682"}, {"metadata": {"arXiv": "2312.07694", "Date": "Tue, 12 Dec 2023 19:39:40 ", "Title": "GP+: A Python Library for Kernel-based learning via Gaussian Processes", "Authors": ["Amin Yousefpour", "Zahra Zanjani Foumani", "Mehdi Shishehbor", "Carlos Mora", "Ramin Bostanabad"], "Categories": "cs.LG"}, "abstract": "In this paper we introduce GP+, an open-source library for kernel-based learning via Gaussian processes (GPs) which are powerful statistical models that are completely characterized by their parametric covariance and mean functions. GP+ is built on PyTorch and provides a user-friendly and object-oriented tool for probabilistic learning and inference. As we demonstrate with a host of examples, GP+ has a few unique advantages over other GP modeling libraries. We achieve these advantages primarily by integrating nonlinear manifold learning techniques with GPs' covariance and mean functions. As part of introducing GP+, in this paper we also make methodological contributions that (1) enable probabilistic data fusion and inverse parameter estimation, and (2) equip GPs with parsimonious parametric mean functions which span mixed feature spaces that have both categorical and quantitative variables. We demonstrate the impact of these contributions in the context of Bayesian optimization, multi-fidelity modeling, sensitivity analysis, and calibration of computer models.", "url": "https://arxiv.org/abs/2312.07694"}, {"metadata": {"arXiv": "2312.07698", "Date": "Tue, 12 Dec 2023 19:49:47 ", "Title": "Machine Learning and Citizen Science Approaches for Monitoring the Changing Environment", "Authors": ["Sulong Zhou"], "Categories": "cs.LG", "Comments": ["PhD thesis", "Environment and Resources", "U Wisconson Madison (2021)"]}, "abstract": "This dissertation will combine new tools and methodologies to answer pressing questions regarding inundation area and hurricane events in complex, heterogeneous changing environments. In addition to remote sensing approaches, citizen science and machine learning are both emerging fields that harness advancing technology to answer environmental management and disaster response questions.", "url": "https://arxiv.org/abs/2312.07698"}, {"metadata": {"arXiv": "2312.07718", "Date": "Tue, 12 Dec 2023 20:24:19 ", "Title": "CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize with Binary Linear Programs", "Authors": ["Bo Tang", "Elias B. Khalil"], "Categories": "cs.LG"}, "abstract": "The end-to-end predict-then-optimize framework, also known as decision-focused learning, has gained popularity for its ability to integrate optimization into the training procedure of machine learning models that predict the unknown cost (objective function) coefficients of optimization problems from contextual instance information. Naturally, most of the problems of interest in this space can be cast as integer linear programs. In this work, we focus on binary linear programs (BLPs) and propose a new end-to-end training method for predict-then-optimize. Our method, Cone-aligned Vector Estimation (CaVE), aligns the predicted cost vectors with the cone corresponding to the true optimal solution of a training instance. When the predicted cost vector lies inside the cone, the optimal solution to the linear relaxation of the binary problem is optimal w.r.t. to the true cost vector. Not only does this alignment produce decision-aware learning models, but it also dramatically reduces training time as it circumvents the need to solve BLPs to compute a loss function with its gradients. Experiments across multiple datasets show that our method exhibits a favorable trade-off between training time and solution quality, particularly with large-scale optimization problems such as vehicle routing, a hard BLP that has yet to benefit from predict-then-optimize methods in the literature due to its difficulty.", "url": "https://arxiv.org/abs/2312.07718"}, {"metadata": {"arXiv": "2312.07730", "Date": "Tue, 12 Dec 2023 20:51:41 ", "Title": "Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings and Taxonomy-aware Attention Layer", "Authors": ["Antonio J. G. Busson", "Rafael Rocha", "Rennan Gaio", "Rafael Miceli", "Ivan Pereira", "Daniel de S. Moraes", "S\\'ergio Colcher", "Alvaro Veiga", "Bruno Rizzi", "Francisco Evangelista", "Leandro Santos", "Fellipe Marques", "Marcos Rabaioli", "Diego Feldberg", "Debora Mattos", "Jo\\~ao Pasqua", "Diogo Dias"], "Categories": "cs.LG", "DOI": "10.5753/bwaif.2023.229322"}, "abstract": "This work proposes the Two-headed DragoNet, a Transformer-based model for hierarchical multi-label classification of financial transactions. Our model is based on a stack of Transformers encoder layers that generate contextual embeddings from two short textual descriptors (merchant name and business activity), followed by a Context Fusion layer and two output heads that classify transactions according to a hierarchical two-level taxonomy (macro and micro categories). Finally, our proposed Taxonomy-aware Attention Layer corrects predictions that break categorical hierarchy rules defined in the given taxonomy. Our proposal outperforms classical machine learning methods in experiments of macro-category classification by achieving an F1-score of 93\\% on a card dataset and 95% on a current account dataset.", "url": "https://arxiv.org/abs/2312.07730"}, {"metadata": {"arXiv": "2312.07743", "Date": "Tue, 12 Dec 2023 21:22:07 ", "Title": "FULL-W2V: Fully Exploiting Data Reuse for W2V on GPU-Accelerated Systems", "Authors": ["Thomas Randall", "Tyler Allen and Rong Ge"], "Categories": "cs.LG cs.CL cs.DC", "Comments": ["12 pages", "7 figures", "7 tables", "the definitive version of this work is published in the Proceedings of the ACM International Conference on Supercomputing 2021", "available at https://doi.org/10.1145/3447818.3460373"], "ACM-class": "I.2.7; D.1.3; G.4", "Journal-ref": "Proceedings of the ACM International Conference on Supercomputing (2021) 455-466", "DOI": "10.1145/3447818.3460373"}, "abstract": "Word2Vec remains one of the highly-impactful innovations in the field of Natural Language Processing (NLP) that represents latent grammatical and syntactical information in human text with dense vectors in a low dimension. Word2Vec has high computational cost due to the algorithm's inherent sequentiality, intensive memory accesses, and the large vocabularies it represents. While prior studies have investigated technologies to explore parallelism and improve memory system performance, they struggle to effectively gain throughput on powerful GPUs. We identify memory data access and latency as the primary bottleneck in prior works on GPUs, which prevents highly optimized kernels from attaining the architecture's peak performance. We present a novel algorithm, FULL-W2V, which maximally exploits the opportunities for data reuse in the W2V algorithm and leverages GPU architecture and resources to reduce access to low memory levels and improve temporal locality. FULL-W2V is capable of reducing accesses to GPU global memory significantly, e.g., by more than 89\\%, compared to prior state-of-the-art GPU implementations, resulting in significant performance improvement that scales across successive hardware generations. Our prototype implementation achieves 2.97X speedup when ported from Nvidia Pascal P100 to Volta V100 cards, and outperforms the state-of-the-art by 5.72X on V100 cards with the same embedding quality. In-depth analysis indicates that the reduction of memory accesses through register and shared memory caching and high-throughput shared memory reduction leads to a significantly improved arithmetic intensity. FULL-W2V can potentially benefit many applications in NLP and other domains.", "url": "https://arxiv.org/abs/2312.07743"}, {"metadata": {"arXiv": "2312.07759", "Date": "Tue, 12 Dec 2023 22:02:57 ", "Title": "IDKM: Memory Efficient Neural Network Quantization via Implicit, Differentiable $k$-Means", "Authors": ["Sean Jaffe", "Ambuj K. Singh", "Francesco Bullo"], "Categories": "cs.LG"}, "abstract": "Compressing large neural networks with minimal performance loss is crucial to enabling their deployment on edge devices. (Cho et al., 2022) proposed a weight quantization method that uses an attention-based clustering algorithm called differentiable $k$-means (DKM). Despite achieving state-of-the-art results, DKM's performance is constrained by its heavy memory dependency. We propose an implicit, differentiable $k$-means algorithm (IDKM), which eliminates the major memory restriction of DKM. Let $t$ be the number of $k$-means iterations, $m$ be the number of weight-vectors, and $b$ be the number of bits per cluster address. IDKM reduces the overall memory complexity of a single $k$-means layer from $\\mathcal{O}(t \\cdot m \\cdot 2^b)$ to $\\mathcal{O}( m \\cdot 2^b)$. We also introduce a variant, IDKM with Jacobian-Free-Backpropagation (IDKM-JFB), for which the time complexity of the gradient calculation is independent of $t$ as well. We provide a proof of concept of our methods by showing that, under the same settings, IDKM achieves comparable performance to DKM with less compute time and less memory. We also use IDKM and IDKM-JFB to quantize a large neural network, Resnet18, on hardware where DKM cannot train at all.", "url": "https://arxiv.org/abs/2312.07759"}, {"metadata": {"arXiv": "2312.07762", "Date": "Tue, 12 Dec 2023 22:10:38 ", "Title": "Interpretable factorization of clinical questionnaires to identify latent factors of psychopathology", "Authors": ["Ka Chun Lam", "Bridget W Mahony", "Armin Raznahan", "Francisco Pereira"], "Categories": "cs.LG cs.NA math.NA", "MSC-class": "68", "ACM-class": "G.1.3"}, "abstract": "Psychiatry research seeks to understand the manifestations of psychopathology in behavior, as measured in questionnaire data, by identifying a small number of latent factors that explain them. While factor analysis is the traditional tool for this purpose, the resulting factors may not be interpretable, and may also be subject to confounding variables. Moreover, missing data are common, and explicit imputation is often required. To overcome these limitations, we introduce interpretability constrained questionnaire factorization (ICQF), a non-negative matrix factorization method with regularization tailored for questionnaire data. Our method aims to promote factor interpretability and solution stability. We provide an optimization procedure with theoretical convergence guarantees, and an automated procedure to detect latent dimensionality accurately. We validate these procedures using realistic synthetic data. We demonstrate the effectiveness of our method in a widely used general-purpose questionnaire, in two independent datasets (the Healthy Brain Network and Adolescent Brain Cognitive Development studies). Specifically, we show that ICQF improves interpretability, as defined by domain experts, while preserving diagnostic information across a range of disorders, and outperforms competing methods for smaller dataset sizes. This suggests that the regularization in our method matches domain characteristics. The python implementation for ICQF is available at \\url{https://github.com/jefferykclam/ICQF}.", "url": "https://arxiv.org/abs/2312.07762"}, {"metadata": {"arXiv": "2312.07769", "Date": "Tue, 12 Dec 2023 22:27:29 ", "Title": "Incremental hierarchical text clustering methods: a review", "Authors": ["Fernando Simeone", "Maik Olher Chaves", "Ahmed Esmin"], "Categories": "cs.LG", "Comments": ["26 pages", "2 figures"]}, "abstract": "The growth in Internet usage has contributed to a large volume of continuously available data, and has created the need for automatic and efficient organization of the data. In this context, text clustering techniques are significant because they aim to organize documents according to their characteristics. More specifically, hierarchical and incremental clustering techniques can organize dynamic data in a hierarchical form, thus guaranteeing that this organization is updated and its exploration is facilitated. Based on the relevance and contemporary nature of the field, this study aims to analyze various hierarchical and incremental clustering techniques; the main contribution of this research is the organization and comparison of the techniques used by studies published between 2010 and 2018 that aimed to texts documents clustering. We describe the principal concepts related to the challenge and the different characteristics of these published works in order to provide a better understanding of the research in this field.", "url": "https://arxiv.org/abs/2312.07769"}, {"metadata": {"arXiv": "2312.07781", "Date": "Tue, 12 Dec 2023 22:49:24 ", "Title": "Combining propensity score methods with variational autoencoders for generating synthetic data in presence of latent sub-groups", "Authors": ["Kiana Farhadyar", "Federico Bonofiglio", "Maren Hackenberg", "Daniela Zoeller", "Harald Binder"], "Categories": "cs.LG"}, "abstract": "In settings requiring synthetic data generation based on a clinical cohort, e.g., due to data protection regulations, heterogeneity across individuals might be a nuisance that we need to control or faithfully preserve. The sources of such heterogeneity might be known, e.g., as indicated by sub-groups labels, or might be unknown and thus reflected only in properties of distributions, such as bimodality or skewness. We investigate how such heterogeneity can be preserved and controlled when obtaining synthetic data from variational autoencoders (VAEs), i.e., a generative deep learning technique that utilizes a low-dimensional latent representation. To faithfully reproduce unknown heterogeneity reflected in marginal distributions, we propose to combine VAEs with pre-transformations. For dealing with known heterogeneity due to sub-groups, we complement VAEs with models for group membership, specifically from propensity score regression. The evaluation is performed with a realistic simulation design that features sub-groups and challenging marginal distributions. The proposed approach faithfully recovers the latter, compared to synthetic data approaches that focus purely on marginal distributions. Propensity scores add complementary information, e.g., when visualized in the latent space, and enable sampling of synthetic data with or without sub-group specific characteristics. We also illustrate the proposed approach with real data from an international stroke trial that exhibits considerable distribution differences between study sites, in addition to bimodality. These results indicate that describing heterogeneity by statistical approaches, such as propensity score regression, might be more generally useful for complementing generative deep learning for obtaining synthetic data that faithfully reflects structure from clinical cohorts.", "url": "https://arxiv.org/abs/2312.07781"}, {"metadata": {"arXiv": "2312.07790", "Date": "Tue, 12 Dec 2023 23:15:07 ", "Title": "Characteristic Circuits", "Authors": ["Zhongjie Yu", "Martin Trapp", "Kristian Kersting"], "Categories": "cs.LG", "Comments": ["Published at NeurIPS 2023"]}, "abstract": "In many real-world scenarios, it is crucial to be able to reliably and efficiently reason under uncertainty while capturing complex relationships in data. Probabilistic circuits (PCs), a prominent family of tractable probabilistic models, offer a remedy to this challenge by composing simple, tractable distributions into a high-dimensional probability distribution. However, learning PCs on heterogeneous data is challenging and densities of some parametric distributions are not available in closed form, limiting their potential use. We introduce characteristic circuits (CCs), a family of tractable probabilistic models providing a unified formalization of distributions over heterogeneous data in the spectral domain. The one-to-one relationship between characteristic functions and probability measures enables us to learn high-dimensional distributions on heterogeneous data domains and facilitates efficient probabilistic inference even when no closed-form density function is available. We show that the structure and parameters of CCs can be learned efficiently from the data and find that CCs outperform state-of-the-art density estimators for heterogeneous data domains on common benchmark data sets.", "url": "https://arxiv.org/abs/2312.07790"}, {"metadata": {"arXiv": "2312.07795", "Date": "Tue, 12 Dec 2023 23:21:57 ", "Title": "Traffic Signal Control Using Lightweight Transformers: An Offline-to-Online RL Approach", "Authors": ["Xingshuai Huang", "Di Wu", "and Benoit Boulet"], "Categories": "cs.LG"}, "abstract": "Efficient traffic signal control is critical for reducing traffic congestion and improving overall transportation efficiency. The dynamic nature of traffic flow has prompted researchers to explore Reinforcement Learning (RL) for traffic signal control (TSC). Compared with traditional methods, RL-based solutions have shown preferable performance. However, the application of RL-based traffic signal controllers in the real world is limited by the low sample efficiency and high computational requirements of these solutions. In this work, we propose DTLight, a simple yet powerful lightweight Decision Transformer-based TSC method that can learn policy from easily accessible offline datasets. DTLight novelly leverages knowledge distillation to learn a lightweight controller from a well-trained larger teacher model to reduce implementation computation. Additionally, it integrates adapter modules to mitigate the expenses associated with fine-tuning, which makes DTLight practical for online adaptation with minimal computation and only a few fine-tuning steps during real deployment. Moreover, DTLight is further enhanced to be more applicable to real-world TSC problems. Extensive experiments on synthetic and real-world scenarios show that DTLight pre-trained purely on offline datasets can outperform state-of-the-art online RL-based methods in most scenarios. Experiment results also show that online fine-tuning further improves the performance of DTLight by up to 42.6% over the best online RL baseline methods. In this work, we also introduce Datasets specifically designed for TSC with offline RL (referred to as DTRL). Our datasets and code are publicly available.", "url": "https://arxiv.org/abs/2312.07795"}, {"metadata": {"arXiv": "2312.07802", "Date": "Tue, 12 Dec 2023 23:41:59 ", "Title": "Estimation of embedding vectors in high dimensions", "Authors": ["Golara Ahmadi Azar", "Melika Emami", "Alyson Fletcher", "Sundeep Rangan"], "Categories": "cs.LG cs.IT math.IT stat.ML", "Comments": ["12 pages", "7 figures"]}, "abstract": "Embeddings are a basic initial feature extraction step in many machine learning models, particularly in natural language processing. An embedding attempts to map data tokens to a low-dimensional space where similar tokens are mapped to vectors that are close to one another by some metric in the embedding space. A basic question is how well can such embedding be learned? To study this problem, we consider a simple probability model for discrete data where there is some \"true\" but unknown embedding where the correlation of random variables is related to the similarity of the embeddings. Under this model, it is shown that the embeddings can be learned by a variant of low-rank approximate message passing (AMP) method. The AMP approach enables precise predictions of the accuracy of the estimation in certain high-dimensional limits. In particular, the methodology provides insight on the relations of key parameters such as the number of samples per value, the frequency of the terms, and the strength of the embedding correlation on the probability distribution. Our theoretical findings are validated by simulations on both synthetic data and real text data.", "url": "https://arxiv.org/abs/2312.07802"}, {"metadata": {"arXiv": "2312.07822", "Date": "Wed, 13 Dec 2023 01:15:00 ", "Title": "Prototypical Self-Explainable Models Without Re-training", "Authors": ["Srishti Gautam", "Ahcene Boubekki", "Marina M. C. H\\\"ohne and Michael C. Kampffmeyer"], "Categories": "cs.LG"}, "abstract": "Explainable AI (XAI) has unfolded in two distinct research directions with, on the one hand, post-hoc methods that explain the predictions of a pre-trained black-box model and, on the other hand, self-explainable models (SEMs) which are trained directly to provide explanations alongside their predictions. While the latter is preferred in most safety-critical scenarios, post-hoc approaches have received the majority of attention until now, owing to their simplicity and ability to explain base models without retraining. Current SEMs instead, require complex architectures and heavily regularized loss functions, thus necessitating specific and costly training. To address this shortcoming and facilitate wider use of SEMs, we propose a simple yet efficient universal method called KMEx (K-Means Explainer), which can convert any existing pre-trained model into a prototypical SEM. The motivation behind KMEx is to push towards more transparent deep learning-based decision-making via class-prototype-based explanations that are guaranteed to be diverse and trustworthy without retraining the base model. We compare models obtained from KMEx to state-of-the-art SEMs using an extensive qualitative evaluation to highlight the strengths and weaknesses of each model, further paving the way toward a more reliable and objective evaluation of SEMs.", "url": "https://arxiv.org/abs/2312.07822"}, {"metadata": {"arXiv": "2312.07837", "Date": "Wed, 13 Dec 2023 02:04:41 ", "Title": "Synthetic Data: Can We Trust Statistical Estimators?", "Authors": ["Alexander Decruyenaere", "Heidelinde Dehaene", "Paloma Rabaey", "Christiaan Polet", "Johan Decruyenaere", "Stijn Vansteelandt", "Thomas Demeester"], "Categories": "cs.LG stat.ML"}, "abstract": "The increasing interest in data sharing makes synthetic data appealing. However, the analysis of synthetic data raises a unique set of methodological challenges. In this work, we highlight the importance of inferential utility and provide empirical evidence against naive inference from synthetic data (that handles these as if they were really observed). We argue that the rate of false-positive findings (type 1 error) will be unacceptably high, even when the estimates are unbiased. One of the reasons is the underestimation of the true standard error, which may even progressively increase with larger sample sizes due to slower convergence. This is especially problematic for deep generative models. Before publishing synthetic data, it is essential to develop statistical inference tools for such data.", "url": "https://arxiv.org/abs/2312.07837"}, {"metadata": {"arXiv": "2312.07841", "Date": "Wed, 13 Dec 2023 02:11:07 ", "Title": "On the Dynamics Under the Unhinged Loss and Beyond", "Authors": ["Xiong Zhou", "Xianming Liu", "Hanzhang Wang", "Deming Zhai", "Junjun Jiang", "Xiangyang Ji"], "Categories": "cs.LG", "Comments": ["62 pages", "19 figures"]}, "abstract": "Recent works have studied implicit biases in deep learning, especially the behavior of last-layer features and classifier weights. However, they usually need to simplify the intermediate dynamics under gradient flow or gradient descent due to the intractability of loss functions and model architectures. In this paper, we introduce the unhinged loss, a concise loss function, that offers more mathematical opportunities to analyze the closed-form dynamics while requiring as few simplifications or assumptions as possible. The unhinged loss allows for considering more practical techniques, such as time-vary learning rates and feature normalization. Based on the layer-peeled model that views last-layer features as free optimization variables, we conduct a thorough analysis in the unconstrained, regularized, and spherical constrained cases, as well as the case where the neural tangent kernel remains invariant. To bridge the performance of the unhinged loss to that of Cross-Entropy (CE), we investigate the scenario of fixing classifier weights with a specific structure, (e.g., a simplex equiangular tight frame). Our analysis shows that these dynamics converge exponentially fast to a solution depending on the initialization of features and classifier weights. These theoretical results not only offer valuable insights, including explicit feature regularization and rescaled learning rates for enhancing practical training with the unhinged loss, but also extend their applicability to other loss functions. Finally, we empirically demonstrate these theoretical results and insights through extensive experiments.", "url": "https://arxiv.org/abs/2312.07841"}, {"metadata": {"arXiv": "2312.07851", "Date": "Wed, 13 Dec 2023 02:39:10 ", "Title": "Noise in the reverse process improves the approximation capabilities of diffusion models", "Authors": ["Karthik Elamvazhuthi and Samet Oymak and Fabio Pasqualetti"], "Categories": "cs.LG cs.SY eess.SY math.OC", "Comments": ["Extended preprint for submission to Learning for Dynamics & Control Conference"]}, "abstract": "In Score based Generative Modeling (SGMs), the state-of-the-art in generative modeling, stochastic reverse processes are known to perform better than their deterministic counterparts. This paper delves into the heart of this phenomenon, comparing neural ordinary differential equations (ODEs) and neural stochastic differential equations (SDEs) as reverse processes. We use a control theoretic perspective by posing the approximation of the reverse process as a trajectory tracking problem. We analyze the ability of neural SDEs to approximate trajectories of the Fokker-Planck equation, revealing the advantages of stochasticity. First, neural SDEs exhibit a powerful regularizing effect, enabling $L^2$ norm trajectory approximation surpassing the Wasserstein metric approximation achieved by neural ODEs under similar conditions, even when the reference vector field or score function is not Lipschitz. Applying this result, we establish the class of distributions that can be sampled using score matching in SGMs, relaxing the Lipschitz requirement on the gradient of the data distribution in existing literature. Second, we show that this approximation property is preserved when network width is limited to the input dimension of the network. In this limited width case, the weights act as control inputs, framing our analysis as a controllability problem for neural SDEs in probability density space. This sheds light on how noise helps to steer the system towards the desired solution and illuminates the empirical success of stochasticity in generative modeling.", "url": "https://arxiv.org/abs/2312.07851"}, {"metadata": {"arXiv": "2312.07859", "Date": "Wed, 13 Dec 2023 02:56:26 ", "Title": "Invariant Graph Transformer", "Authors": ["Zhe Xu (1)", "Menghai Pan (2)", "Yuzhong Chen (2)", "Huiyuan Chen (2)", "Yuchen Yan (1)", "Mahashweta Das (2)", "Hanghang Tong (1) ((1) University of Illinois Urbana-Champaign", "(2) Visa Research)"], "Categories": "cs.LG"}, "abstract": "Rationale discovery is defined as finding a subset of the input data that maximally supports the prediction of downstream tasks. In graph machine learning context, graph rationale is defined to locate the critical subgraph in the given graph topology, which fundamentally determines the prediction results. In contrast to the rationale subgraph, the remaining subgraph is named the environment subgraph. Graph rationalization can enhance the model performance as the mapping between the graph rationale and prediction label is viewed as invariant, by assumption. To ensure the discriminative power of the extracted rationale subgraphs, a key technique named \"intervention\" is applied. The core idea of intervention is that given any changing environment subgraphs, the semantics from the rationale subgraph is invariant, which guarantees the correct prediction result. However, most, if not all, of the existing rationalization works on graph data develop their intervention strategies on the graph level, which is coarse-grained. In this paper, we propose well-tailored intervention strategies on graph data. Our idea is driven by the development of Transformer models, whose self-attention module provides rich interactions between input nodes. Based on the self-attention module, our proposed invariant graph Transformer (IGT) can achieve fine-grained, more specifically, node-level and virtual node-level intervention. Our comprehensive experiments involve 7 real-world datasets, and the proposed IGT shows significant performance advantages compared to 13 baseline methods.", "url": "https://arxiv.org/abs/2312.07859"}, {"metadata": {"arXiv": "2312.07861", "Date": "Wed, 13 Dec 2023 02:59:37 ", "Title": "GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks", "Authors": ["Bang Wu", "He Zhang", "Xiangwen Yang", "Shuo Wang", "Minhui Xue", "Shirui Pan", "Xingliang Yuan"], "Categories": "cs.LG cs.CR"}, "abstract": "The emergence of Graph Neural Networks (GNNs) in graph data analysis and their deployment on Machine Learning as a Service platforms have raised critical concerns about data misuse during model training. This situation is further exacerbated due to the lack of transparency in local training processes, potentially leading to the unauthorized accumulation of large volumes of graph data, thereby infringing on the intellectual property rights of data owners. Existing methodologies often address either data misuse detection or mitigation, and are primarily designed for local GNN models rather than cloud-based MLaaS platforms. These limitations call for an effective and comprehensive solution that detects and mitigates data misuse without requiring exact training data while respecting the proprietary nature of such data. This paper introduces a pioneering approach called GraphGuard, to tackle these challenges. We propose a training-data-free method that not only detects graph data misuse but also mitigates its impact via targeted unlearning, all without relying on the original training data. Our innovative misuse detection technique employs membership inference with radioactive data, enhancing the distinguishability between member and non-member data distributions. For mitigation, we utilize synthetic graphs that emulate the characteristics previously learned by the target model, enabling effective unlearning even in the absence of exact graph data. We conduct comprehensive experiments utilizing four real-world graph datasets to demonstrate the efficacy of GraphGuard in both detection and unlearning. We show that GraphGuard attains a near-perfect detection rate of approximately 100% across these datasets with various GNN models. In addition, it performs unlearning by eliminating the impact of the unlearned graph with a marginal decrease in accuracy (less than 5%).", "url": "https://arxiv.org/abs/2312.07861"}, {"metadata": {"arXiv": "2312.07931", "Date": "Wed, 13 Dec 2023 07:20:27 ", "Title": "Levenshtein Distance Embedding with Poisson Regression for DNA Storage", "Authors": ["Xiang Wei", "Alan J.X. Guo", "Sihan Sun", "Mengyi Wei", "Wei Yu"], "Categories": "cs.LG q-bio.QM"}, "abstract": "Efficient computation or approximation of Levenshtein distance, a widely-used metric for evaluating sequence similarity, has attracted significant attention with the emergence of DNA storage and other biological applications. Sequence embedding, which maps Levenshtein distance to a conventional distance between embedding vectors, has emerged as a promising solution. In this paper, a novel neural network-based sequence embedding technique using Poisson regression is proposed. We first provide a theoretical analysis of the impact of embedding dimension on model performance and present a criterion for selecting an appropriate embedding dimension. Under this embedding dimension, the Poisson regression is introduced by assuming the Levenshtein distance between sequences of fixed length following a Poisson distribution, which naturally aligns with the definition of Levenshtein distance. Moreover, from the perspective of the distribution of embedding distances, Poisson regression approximates the negative log likelihood of the chi-squared distribution and offers advancements in removing the skewness. Through comprehensive experiments on real DNA storage data, we demonstrate the superior performance of the proposed method compared to state-of-the-art approaches.", "url": "https://arxiv.org/abs/2312.07931"}, {"metadata": {"arXiv": "2312.07950", "Date": "Wed, 13 Dec 2023 07:56:27 ", "Title": "CBQ: Cross-Block Quantization for Large Language Models", "Authors": ["Xin Ding", "Xiaoyu Liu", "Yun Zhang", "Zhijun Tu", "Wei Li", "Jie Hu", "Hanting Chen", "Yehui Tang", "Zhiwei Xiong", "Baoqun Yin", "Yunhe Wang"], "Categories": "cs.LG cs.CL"}, "abstract": "Post-training quantization (PTQ) has driven attention to producing efficient large language models (LLMs) with ultra-low costs. Since hand-craft quantization parameters lead to low performance in low-bit quantization, recent methods optimize the quantization parameters through block-wise reconstruction between the floating-point and quantized models. However, these methods suffer from two challenges: accumulated errors from independent one-by-one block quantization and reconstruction difficulties from extreme weight and activation outliers. To address these two challenges, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. To reduce error accumulation, we introduce a cross-block dependency with the aid of a homologous reconstruction scheme to build the long-range dependency between adjacent multi-blocks with overlapping. To reduce reconstruction difficulty, we design a coarse-to-fine pre-processing (CFP) to truncate weight outliers and dynamically scale activation outliers before optimization, and an adaptive rounding scheme, called LoRA-Rounding, with two low-rank learnable matrixes to further rectify weight quantization errors. Extensive experiments demonstrate that: (1) CBQ pushes both activation and weight quantization to low-bit settings W4A4, W4A8, and W2A16. (2) CBQ achieves better performance than the existing state-of-the-art methods on various LLMs and benchmark datasets.", "url": "https://arxiv.org/abs/2312.07950"}, {"metadata": {"arXiv": "2312.07981", "Date": "Wed, 13 Dec 2023 08:53:37 ", "Title": "Time Series Diffusion Method: A Denoising Diffusion Probabilistic Model for Vibration Signal Generation", "Authors": ["Haiming Yi", "Lei Hou", "Yuhong Jin", "Nasser A. Saeed"], "Categories": "cs.LG cs.SD eess.SP"}, "abstract": "Diffusion models have demonstrated robust data generation capabilities in various research fields. In this paper, a Time Series Diffusion Method (TSDM) is proposed for vibration signal generation, leveraging the foundational principles of diffusion models. The TSDM uses an improved U-net architecture with attention block to effectively segment and extract features from one-dimensional time series data. It operates based on forward diffusion and reverse denoising processes for time-series generation. Experimental validation is conducted using single-frequency, multi-frequency datasets, and bearing fault datasets. The results show that TSDM can accurately generate the single-frequency and multi-frequency features in the time series and retain the basic frequency features for the diffusion generation results of the bearing fault series. Finally, TSDM is applied to the small sample fault diagnosis of three public bearing fault datasets, and the results show that the accuracy of small sample fault diagnosis of the three datasets is improved by 32.380%, 18.355% and 9.298% at most, respectively", "url": "https://arxiv.org/abs/2312.07981"}, {"metadata": {"arXiv": "2312.07987", "Date": "Wed, 13 Dec 2023 09:00:21 ", "Title": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention", "Authors": ["R\\'obert Csord\\'as", "Piotr Pi\\k{e}kos", "Kazuki Irie"], "Categories": "cs.LG cs.CL cs.NE"}, "abstract": "The costly self-attention layers in modern Transformers require memory and compute quadratic in sequence length. Existing approximation methods usually underperform and fail to obtain significant speedups in practice. Here we present SwitchHead - a novel method that reduces both compute and memory requirements and achieves wall-clock speedup, while matching the language modeling performance of baseline Transformers with the same parameter budget. SwitchHead uses Mixture-of-Experts (MoE) layers for the value and output projections and requires 4 to 8 times fewer attention matrices than standard Transformers. Our novel attention can also be combined with MoE MLP layers, resulting in an efficient fully-MoE \"SwitchHead\" Transformer model. Our code is public.", "url": "https://arxiv.org/abs/2312.07987"}, {"metadata": {"arXiv": "2312.07991", "Date": "Wed, 13 Dec 2023 09:03:01 ", "Title": "Accelerating the Global Aggregation of Local Explanations", "Authors": ["Alon Mor", "Yonatan Belinkov", "Benny Kimelfeld"], "Categories": "cs.LG"}, "abstract": "Local explanation methods highlight the input tokens that have a considerable impact on the outcome of classifying the document at hand. For example, the Anchor algorithm applies a statistical analysis of the sensitivity of the classifier to changes in the token. Aggregating local explanations over a dataset provides a global explanation of the model. Such aggregation aims to detect words with the most impact, giving valuable insights about the model, like what it has learned in training and which adversarial examples expose its weaknesses. However, standard aggregation methods bear a high computational cost: a na\\\"ive implementation applies a costly algorithm to each token of each document, and hence, it is infeasible for a simple user running in the scope of a short analysis session. % We devise techniques for accelerating the global aggregation of the Anchor algorithm. Specifically, our goal is to compute a set of top-$k$ words with the highest global impact according to different aggregation functions. Some of our techniques are lossless and some are lossy. We show that for a very mild loss of quality, we are able to accelerate the computation by up to 30$\\times$, reducing the computation from hours to minutes. We also devise and study a probabilistic model that accounts for noise in the Anchor algorithm and diminishes the bias toward words that are frequent yet low in impact.", "url": "https://arxiv.org/abs/2312.07991"}, {"metadata": {"arXiv": "2312.08016", "Date": "Wed, 13 Dec 2023 09:39:32 ", "Title": "Secure Deep Reinforcement Learning for Dynamic Resource Allocation in Wireless MEC Networks", "Authors": ["Xin Hao", "Phee Lep Yeoh", "Changyang She", "Branka Vucetic", "and Yonghui Li"], "Categories": "cs.LG cs.NI", "DOI": "10.1109/TCOMM.2023.3337376"}, "abstract": "This paper proposes a blockchain-secured deep reinforcement learning (BC-DRL) optimization framework for {data management and} resource allocation in decentralized {wireless mobile edge computing (MEC)} networks. In our framework, {we design a low-latency reputation-based proof-of-stake (RPoS) consensus protocol to select highly reliable blockchain-enabled BSs to securely store MEC user requests and prevent data tampering attacks.} {We formulate the MEC resource allocation optimization as a constrained Markov decision process that balances minimum processing latency and denial-of-service (DoS) probability}. {We use the MEC aggregated features as the DRL input to significantly reduce the high-dimensionality input of the remaining service processing time for individual MEC requests. Our designed constrained DRL effectively attains the optimal resource allocations that are adapted to the dynamic DoS requirements. We provide extensive simulation results and analysis to} validate that our BC-DRL framework achieves higher security, reliability, and resource utilization efficiency than benchmark blockchain consensus protocols and {MEC} resource allocation algorithms.", "url": "https://arxiv.org/abs/2312.08016"}, {"metadata": {"arXiv": "2312.08029", "Date": "Wed, 13 Dec 2023 10:04:06 ", "Title": "ClusterDDPM: An EM clustering framework with Denoising Diffusion Probabilistic Models", "Authors": ["Jie Yan", "Jing Liu and Zhong-yuan Zhang"], "Categories": "cs.LG cs.CV"}, "abstract": "Variational autoencoder (VAE) and generative adversarial networks (GAN) have found widespread applications in clustering and have achieved significant success. However, the potential of these approaches may be limited due to VAE's mediocre generation capability or GAN's well-known instability during adversarial training. In contrast, denoising diffusion probabilistic models (DDPMs) represent a new and promising class of generative models that may unlock fresh dimensions in clustering. In this study, we introduce an innovative expectation-maximization (EM) framework for clustering using DDPMs. In the E-step, we aim to derive a mixture of Gaussian priors for the subsequent M-step. In the M-step, our focus lies in learning clustering-friendly latent representations for the data by employing the conditional DDPM and matching the distribution of latent representations to the mixture of Gaussian priors. We present a rigorous theoretical analysis of the optimization process in the M-step, proving that the optimizations are equivalent to maximizing the lower bound of the Q function within the vanilla EM framework under certain constraints. Comprehensive experiments validate the advantages of the proposed framework, showcasing superior performance in clustering, unsupervised conditional generation and latent representation learning.", "url": "https://arxiv.org/abs/2312.08029"}, {"metadata": {"arXiv": "2312.08052", "Date": "Wed, 13 Dec 2023 10:59:54 ", "Title": "Explainable Trajectory Representation through Dictionary Learning", "Authors": ["Yuanbo Tang", "Zhiyuan Peng and Yang Li"], "Categories": "cs.LG cs.DM"}, "abstract": "Trajectory representation learning on a network enhances our understanding of vehicular traffic patterns and benefits numerous downstream applications. Existing approaches using classic machine learning or deep learning embed trajectories as dense vectors, which lack interpretability and are inefficient to store and analyze in downstream tasks. In this paper, an explainable trajectory representation learning framework through dictionary learning is proposed. Given a collection of trajectories on a network, it extracts a compact dictionary of commonly used subpaths called \"pathlets\", which optimally reconstruct each trajectory by simple concatenations. The resulting representation is naturally sparse and encodes strong spatial semantics. Theoretical analysis of our proposed algorithm is conducted to provide a probabilistic bound on the estimation error of the optimal dictionary. A hierarchical dictionary learning scheme is also proposed to ensure the algorithm's scalability on large networks, leading to a multi-scale trajectory representation. Our framework is evaluated on two large-scale real-world taxi datasets. Compared to previous work, the dictionary learned by our method is more compact and has better reconstruction rate for new trajectories. We also demonstrate the promising performance of this method in downstream tasks including trip time prediction task and data compression.", "url": "https://arxiv.org/abs/2312.08052"}, {"metadata": {"arXiv": "2312.08053", "Date": "Wed, 13 Dec 2023 11:00:48 ", "Title": "Kimad: Adaptive Gradient Compression with Bandwidth Awareness", "Authors": ["Jihao Xin", "Ivan Ilin", "Shunkang Zhang", "Marco Canini", "Peter Richt\\'arik"], "Categories": "cs.LG cs.DC cs.IT math.IT"}, "abstract": "In distributed training, communication often emerges as a bottleneck. In response, we introduce Kimad, a solution that offers adaptive gradient compression. By consistently monitoring bandwidth, Kimad refines compression ratios to match specific neural network layer requirements. Our exhaustive tests and proofs confirm Kimad's outstanding performance, establishing it as a benchmark in adaptive compression for distributed deep learning.", "url": "https://arxiv.org/abs/2312.08053"}, {"metadata": {"arXiv": "2312.08075", "Date": "Wed, 13 Dec 2023 11:39:56 ", "Title": "TERM Model: Tensor Ring Mixture Model for Density Estimation", "Authors": ["Ruituo Wu", "Jiani Liu", "Ce Zhu", "Anh-Huy Phan", "Ivan V. Oseledets", "Yipeng Liu"], "Categories": "cs.LG stat.ML"}, "abstract": "Efficient probability density estimation is a core challenge in statistical machine learning. Tensor-based probabilistic graph methods address interpretability and stability concerns encountered in neural network approaches. However, a substantial number of potential tensor permutations can lead to a tensor network with the same structure but varying expressive capabilities. In this paper, we take tensor ring decomposition for density estimator, which significantly reduces the number of permutation candidates while enhancing expressive capability compared with existing used decompositions. Additionally, a mixture model that incorporates multiple permutation candidates with adaptive weights is further designed, resulting in increased expressive flexibility and comprehensiveness. Different from the prevailing directions of tensor network structure/permutation search, our approach provides a new viewpoint inspired by ensemble learning. This approach acknowledges that suboptimal permutations can offer distinctive information besides that of optimal permutations. Experiments show the superiority of the proposed approach in estimating probability density for moderately dimensional datasets and sampling to capture intricate details.", "url": "https://arxiv.org/abs/2312.08075"}, {"metadata": {"arXiv": "2312.08096", "Date": "Wed, 13 Dec 2023 12:28:37 ", "Title": "An Incentive Mechanism for Federated Learning Based on Multiple Resource Exchange", "Authors": ["Ruonan Dong", "Hui Xu", "Han Zhang", "GuoPeng Zhang"], "Categories": "cs.LG"}, "abstract": "Federated Learning (FL) is a distributed machine learning paradigm that addresses privacy concerns in machine learning and still guarantees high test accuracy. However, achieving the necessary accuracy by having all clients participate in FL is impractical, given the constraints of client local computing resource. In this paper, we introduce a multi-user collaborative computing framework, categorizing users into two roles: model owners (MOs) and data owner (DOs). Without resorting to monetary incentives, an MO can encourage more DOs to join in FL by allowing the DOs to offload extra local computing tasks to the MO for execution. This exchange of \"data\" for \"computing resources\" streamlines the incentives for clients to engage more effectively in FL. We formulate the interaction between MO and DOs as an optimization problem, and the objective is to effectively utilize the communication and computing resource of the MO and DOs to minimize the time to complete an FL task. The proposed problem is a mixed integer nonlinear programming (MINLP) with high computational complexity. We first decompose it into two distinct subproblems, namely the client selection problem and the resource allocation problem to segregate the integer variables from the continuous variables. Then, an effective iterative algorithm is proposed to solve problem. Simulation results demonstrate that the proposed collaborative computing framework can achieve an accuracy of more than 95\\% while minimizing the overall time to complete an FL task.", "url": "https://arxiv.org/abs/2312.08096"}, {"metadata": {"arXiv": "2312.08103", "Date": "Wed, 13 Dec 2023 12:39:25 ", "Title": "Machine Learning for the Multi-Dimensional Bin Packing Problem: Literature Review and Empirical Evaluation", "Authors": ["Wenjie Wu", "Changjun Fan", "Jincai Huang", "Zhong Liu and Junchi Yan"], "Categories": "cs.LG cs.DS"}, "abstract": "The Bin Packing Problem (BPP) is a well-established combinatorial optimization (CO) problem. Since it has many applications in our daily life, e.g. logistics and resource allocation, people are seeking efficient bin packing algorithms. On the other hand, researchers have been making constant advances in machine learning (ML), which is famous for its efficiency. In this article, we first formulate BPP, introducing its variants and practical constraints. Then, a comprehensive survey on ML for multi-dimensional BPP is provided. We further collect some public benchmarks of 3D BPP, and evaluate some online methods on the Cutting Stock Dataset. Finally, we share our perspective on challenges and future directions in BPP. To the best of our knowledge, this is the first systematic review of ML-related methods for BPP.", "url": "https://arxiv.org/abs/2312.08103"}, {"metadata": {"arXiv": "2312.08150", "Date": "Wed, 13 Dec 2023 14:01:58 ", "Title": "Active learning with biased non-response to label requests", "Authors": ["Thomas Robinson", "Niek Tax", "Richard Mudd", "and Ido Guy"], "Categories": "cs.LG stat.ME stat.ML"}, "abstract": "Active learning can improve the efficiency of training prediction models by identifying the most informative new labels to acquire. However, non-response to label requests can impact active learning's effectiveness in real-world contexts. We conceptualise this degradation by considering the type of non-response present in the data, demonstrating that biased non-response is particularly detrimental to model performance. We argue that this sort of non-response is particularly likely in contexts where the labelling process, by nature, relies on user interactions. To mitigate the impact of biased non-response, we propose a cost-based correction to the sampling strategy--the Upper Confidence Bound of the Expected Utility (UCB-EU)--that can, plausibly, be applied to any active learning algorithm. Through experiments, we demonstrate that our method successfully reduces the harm from labelling non-response in many settings. However, we also characterise settings where the non-response bias in the annotations remains detrimental under UCB-EU for particular sampling methods and data generating processes. Finally, we evaluate our method on a real-world dataset from e-commerce platform Taobao. We show that UCB-EU yields substantial performance improvements to conversion models that are trained on clicked impressions. Most generally, this research serves to both better conceptualise the interplay between types of non-response and model improvements via active learning, and to provide a practical, easy to implement correction that helps mitigate model degradation.", "url": "https://arxiv.org/abs/2312.08150"}, {"metadata": {"arXiv": "2312.08194", "Date": "Wed, 13 Dec 2023 14:58:25 ", "Title": "SVInvNet: A Densely Connected Encoder-Decoder Architecture for Seismic Velocity Inversion", "Authors": ["Mojtaba Najafi Khatounabad", "Hacer Yalim Keles", "Selma Kadioglu"], "Categories": "cs.LG cs.CV physics.geo-ph", "Comments": ["14 pages", "11 figures", "submitted to IEEE Transactions on Geoscience and Remote Sensing"]}, "abstract": "This study presents a deep learning-based approach to seismic velocity inversion problem, focusing on both noisy and noiseless training datasets of varying sizes. Our Seismic Velocity Inversion Network (SVInvNet) introduces a novel architecture that contains a multi-connection encoder-decoder structure enhanced with dense blocks. This design is specifically tuned to effectively process complex information, crucial for addressing the challenges of non-linear seismic velocity inversion. For training and testing, we created diverse seismic velocity models, including multi-layered, faulty, and salt dome categories. We also investigated how different kinds of ambient noise, both coherent and stochastic, and the size of the training dataset affect learning outcomes. SVInvNet is trained on datasets ranging from 750 to 6,000 samples and is tested using a large benchmark dataset of 12,000 samples. Despite its fewer parameters compared to the baseline, SVInvNet achieves superior performance with this dataset. The outcomes of the SVInvNet are additionally compared to those of the Full Waveform Inversion (FWI) method. The comparative analysis clearly reveals the effectiveness of the proposed model.", "url": "https://arxiv.org/abs/2312.08194"}, {"metadata": {"arXiv": "2312.08200", "Date": "Wed, 13 Dec 2023 15:08:54 ", "Title": "SPD-DDPM: Denoising Diffusion Probabilistic Models in the Symmetric Positive Definite Space", "Authors": ["Yunchen Li", "Zhou Yu", "Gaoqi He", "Yunhang Shen", "Ke Li", "Xing Sun", "Shaohui Lin"], "Categories": "cs.LG stat.ML", "Comments": ["AAAI2024"]}, "abstract": "Symmetric positive definite~(SPD) matrices have shown important value and applications in statistics and machine learning, such as FMRI analysis and traffic prediction. Previous works on SPD matrices mostly focus on discriminative models, where predictions are made directly on $E(X|y)$, where $y$ is a vector and $X$ is an SPD matrix. However, these methods are challenging to handle for large-scale data, as they need to access and process the whole data. In this paper, inspired by denoising diffusion probabilistic model~(DDPM), we propose a novel generative model, termed SPD-DDPM, by introducing Gaussian distribution in the SPD space to estimate $E(X|y)$. Moreover, our model is able to estimate $p(X)$ unconditionally and flexibly without giving $y$. On the one hand, the model conditionally learns $p(X|y)$ and utilizes the mean of samples to obtain $E(X|y)$ as a prediction. On the other hand, the model unconditionally learns the probability distribution of the data $p(X)$ and generates samples that conform to this distribution. Furthermore, we propose a new SPD net which is much deeper than the previous networks and allows for the inclusion of conditional factors. Experiment results on toy data and real taxi data demonstrate that our models effectively fit the data distribution both unconditionally and unconditionally and provide accurate predictions.", "url": "https://arxiv.org/abs/2312.08200"}, {"metadata": {"arXiv": "2312.08361", "Date": "Wed, 13 Dec 2023 18:52:49 ", "Title": "Distributed Inference and Fine-tuning of Large Language Models Over The Internet", "Authors": ["Alexander Borzunov", "Max Ryabinin", "Artem Chumachenko", "Dmitry Baranchuk", "Tim Dettmers", "Younes Belkada", "Pavel Samygin", "Colin Raffel"], "Categories": "cs.LG cs.DC", "Comments": ["Accepted to Conference on Neural Information Processing Systems (NeurIPS) 2023. 20 pages", "3 figures"]}, "abstract": "Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals - a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.", "url": "https://arxiv.org/abs/2312.08361"}, {"metadata": {"arXiv": "2312.07953", "Date": "Wed, 13 Dec 2023 08:00:26 ", "Title": "Enhancing Robotic Navigation: An Evaluation of Single and Multi-Objective Reinforcement Learning Strategies", "Authors": ["Vicki Young", "Jumman Hossain", "Nirmalya Roy"], "Categories": "cs.RO cs.LG", "Comments": ["REU program project (work in progress)"]}, "abstract": "This study presents a comparative analysis between single-objective and multi-objective reinforcement learning methods for training a robot to navigate effectively to an end goal while efficiently avoiding obstacles. Traditional reinforcement learning techniques, namely Deep Q-Network (DQN), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3), have been evaluated using the Gazebo simulation framework in a variety of environments with parameters such as random goal and robot starting locations. These methods provide a numerical reward to the robot, offering an indication of action quality in relation to the goal. However, their limitations become apparent in complex settings where multiple, potentially conflicting, objectives are present. To address these limitations, we propose an approach employing Multi-Objective Reinforcement Learning (MORL). By modifying the reward function to return a vector of rewards, each pertaining to a distinct objective, the robot learns a policy that effectively balances the different goals, aiming to achieve a Pareto optimal solution. This comparative study highlights the potential for MORL in complex, dynamic robotic navigation tasks, setting the stage for future investigations into more adaptable and robust robotic behaviors.", "url": "https://arxiv.org/abs/2312.07953"}, {"metadata": {"arXiv": "2312.07553", "Date": "Thu, 07 Dec 2023 11:23:29 ", "Title": "Hijacking Context in Large Multi-modal Models", "Authors": ["Joonhyun Jeong"], "Categories": "cs.AI cs.CL", "Comments": ["Technical Report. Preprint"]}, "abstract": "Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to understand the visual contents of images given the instructions regarding the images. Built upon the Large Language Models (LLMs), LMMs also inherit their abilities and characteristics such as in-context learning where a coherent sequence of images and texts are given as the input prompt. However, we identify a new limitation of off-the-shelf LMMs where a small fraction of incoherent images or text descriptions mislead LMMs to only generate biased output about the hijacked context, not the originally intended context. To address this, we propose a pre-filtering method that removes irrelevant contexts via GPT-4V, based on its robustness towards distribution shift within the contexts. We further investigate whether replacing the hijacked visual and textual contexts with the correlated ones via GPT-4V and text-to-image models can help yield coherent responses.", "url": "https://arxiv.org/abs/2312.07553"}, {"metadata": {"arXiv": "2312.07635", "Date": "Tue, 12 Dec 2023 09:52:30 ", "Title": "Clash of the Explainers: Argumentation for Context-Appropriate Explanations", "Authors": ["Leila Methnani", "Virginia Dignum", "Andreas Theodorou"], "Categories": "cs.AI", "Comments": ["17 pages", "3 figures", "Accepted at XAI^3 Workshop at ECAI 2023"]}, "abstract": "Understanding when and why to apply any given eXplainable Artificial Intelligence (XAI) technique is not a straightforward task. There is no single approach that is best suited for a given context. This paper aims to address the challenge of selecting the most appropriate explainer given the context in which an explanation is required. For AI explainability to be effective, explanations and how they are presented needs to be oriented towards the stakeholder receiving the explanation. If -- in general -- no single explanation technique surpasses the rest, then reasoning over the available methods is required in order to select one that is context-appropriate. Due to the transparency they afford, we propose employing argumentation techniques to reach an agreement over the most suitable explainers from a given set of possible explainers. In this paper, we propose a modular reasoning system consisting of a given mental model of the relevant stakeholder, a reasoner component that solves the argumentation problem generated by a multi-explainer component, and an AI model that is to be explained suitably to the stakeholder of interest. By formalising supporting premises -- and inferences -- we can map stakeholder characteristics to those of explanation techniques. This allows us to reason over the techniques and prioritise the best one for the given context, while also offering transparency into the selection decision.", "url": "https://arxiv.org/abs/2312.07635"}, {"metadata": {"arXiv": "2312.07637", "Date": "Tue, 12 Dec 2023 10:41:17 ", "Title": "Responsibility in Extensive Form Games", "Authors": ["Qi Shi"], "Categories": "cs.AI", "Comments": ["The 38th Annual AAAI Conference on Artificial Intelligence (AAAI-24)"]}, "abstract": "Two different forms of responsibility, counterfactual and seeing-to-it, have been extensively discussed in the philosophy and AI in the context of a single agent or multiple agents acting simultaneously. Although the generalisation of counterfactual responsibility to a setting where multiple agents act in some order is relatively straightforward, the same cannot be said about seeing-to-it responsibility. Two versions of seeing-to-it modality applicable to such settings have been proposed in the literature. Neither of them perfectly captures the intuition of responsibility. This paper proposes a definition of seeing-to-it responsibility for such settings that amalgamate the two modalities. This paper shows that the newly proposed notion of responsibility and counterfactual responsibility are not definable through each other and studies the responsibility gap for these two forms of responsibility. It shows that although these two forms of responsibility are not enough to ascribe responsibility in each possible situation, this gap does not exist if higher-order responsibility is taken into account.", "url": "https://arxiv.org/abs/2312.07637"}, {"metadata": {"arXiv": "2312.07711", "Date": "Tue, 12 Dec 2023 20:17:13 ", "Title": "Leveraging Large Language Models to Build and Execute Computational Workflows", "Authors": ["Alejandro Duque", "Abdullah Syed", "Kastan V. Day", "Matthew J. Berry", "Daniel S. Katz", "Volodymyr V. Kindratenko"], "Categories": "cs.AI"}, "abstract": "The recent development of large language models (LLMs) with multi-billion parameters, coupled with the creation of user-friendly application programming interfaces (APIs), has paved the way for automatically generating and executing code in response to straightforward human queries. This paper explores how these emerging capabilities can be harnessed to facilitate complex scientific workflows, eliminating the need for traditional coding methods. We present initial findings from our attempt to integrate Phyloflow with OpenAI's function-calling API, and outline a strategy for developing a comprehensive workflow management system based on these concepts.", "url": "https://arxiv.org/abs/2312.07711"}, {"metadata": {"arXiv": "2312.07721", "Date": "Tue, 12 Dec 2023 20:28:11 ", "Title": "Saturn Platform: Foundation Model Operations and Generative AI for Financial Services", "Authors": ["Antonio J. G. Busson", "Rennan Gaio", "Rafael H. Rocha", "Francisco Evangelista", "Bruno Rizzi", "Luan Carvalho", "Rafael Miceli", "Marcos Rabaioli", "David Favaro"], "Categories": "cs.AI", "DOI": "10.5753/webmedia_estendido.2023.234354"}, "abstract": "Saturn is an innovative platform that assists Foundation Model (FM) building and its integration with IT operations (Ops). It is custom-made to meet the requirements of data scientists, enabling them to effectively create and implement FMs while enhancing collaboration within their technical domain. By offering a wide range of tools and features, Saturn streamlines and automates different stages of FM development, making it an invaluable asset for data science teams. This white paper introduces prospective applications of generative AI models derived from FMs in the financial sector.", "url": "https://arxiv.org/abs/2312.07721"}, {"metadata": {"arXiv": "2312.07753", "Date": "Tue, 12 Dec 2023 21:49:26 ", "Title": "Polynomial-based Self-Attention for Table Representation learning", "Authors": ["Jayoung Kim", "Yehjin Shin", "Noseong Park"], "Categories": "cs.AI"}, "abstract": "Structured data, which constitutes a significant portion of existing data types, has been a long-standing research topic in the field of machine learning. Various representation learning methods for tabular data have been proposed, ranging from encoder-decoder structures to Transformers. Among these, Transformer-based methods have achieved state-of-the-art performance not only in tabular data but also in various other fields, including computer vision and natural language processing. However, recent studies have revealed that self-attention, a key component of Transformers, can lead to an oversmoothing issue. We show that Transformers for tabular data also face this problem, and to address the problem, we propose a novel matrix polynomial-based self-attention layer as a substitute for the original self-attention layer, which enhances model scalability. In our experiments with three representative table learning models equipped with our proposed layer, we illustrate that the layer effectively mitigates the oversmoothing problem and enhances the representation performance of the existing methods, outperforming the state-of-the-art table representation methods.", "url": "https://arxiv.org/abs/2312.07753"}, {"metadata": {"arXiv": "2312.07767", "Date": "Tue, 12 Dec 2023 22:23:04 ", "Title": "Spatial Knowledge-Infused Hierarchical Learning: An Application in Flood Mapping on Earth Imagery", "Authors": ["Zelin Xu", "Tingsong Xiao", "Wenchong He", "Yu Wang", "Zhe Jiang"], "Categories": "cs.AI", "Comments": ["SIGSPATIAL 2023 (Best Paper Award)"]}, "abstract": "Deep learning for Earth imagery plays an increasingly important role in geoscience applications such as agriculture, ecology, and natural disaster management. Still, progress is often hindered by the limited training labels. Given Earth imagery with limited training labels, a base deep neural network model, and a spatial knowledge base with label constraints, our problem is to infer the full labels while training the neural network. The problem is challenging due to the sparse and noisy input labels, spatial uncertainty within the label inference process, and high computational costs associated with a large number of sample locations. Existing works on neuro-symbolic models focus on integrating symbolic logic into neural networks (e.g., loss function, model architecture, and training label augmentation), but these methods do not fully address the challenges of spatial data (e.g., spatial uncertainty, the trade-off between spatial granularity and computational costs). To bridge this gap, we propose a novel Spatial Knowledge-Infused Hierarchical Learning (SKI-HL) framework that iteratively infers sample labels within a multi-resolution hierarchy. Our framework consists of a module to selectively infer labels in different resolutions based on spatial uncertainty and a module to train neural network parameters with uncertainty-aware multi-instance learning. Extensive experiments on real-world flood mapping datasets show that the proposed model outperforms several baseline methods. The code is available at \\url{https://github.com/ZelinXu2000/SKI-HL}.", "url": "https://arxiv.org/abs/2312.07767"}, {"metadata": {"arXiv": "2312.07779", "Date": "Tue, 12 Dec 2023 22:47:42 ", "Title": "Tell, don't show: Declarative facts influence how LLMs generalize", "Authors": ["Alexander Meinke and Owain Evans"], "Categories": "cs.AI"}, "abstract": "We examine how large language models (LLMs) generalize from abstract declarative statements in their training data. As an illustration, consider an LLM that is prompted to generate weather reports for London in 2050. One possibility is that the temperatures in the reports match the mean and variance of reports from 2023 (i.e. matching the statistics of pretraining). Another possibility is that the reports predict higher temperatures, by incorporating declarative statements about climate change from scientific papers written in 2023. An example of such a declarative statement is \"global temperatures will increase by $1^{\\circ} \\mathrm{C}$ by 2050\". To test the influence of abstract declarative statements, we construct tasks in which LLMs are finetuned on both declarative and procedural information. We find that declarative statements influence model predictions, even when they conflict with procedural information. In particular, finetuning on a declarative statement $S$ increases the model likelihood for logical consequences of $S$. The effect of declarative statements is consistent across three domains: aligning an AI assistant, predicting weather, and predicting demographic features. Through a series of ablations, we show that the effect of declarative statements cannot be explained by associative learning based on matching keywords. Nevertheless, the effect of declarative statements on model likelihoods is small in absolute terms and increases surprisingly little with model size (i.e. from 330 million to 175 billion parameters). We argue that these results have implications for AI risk (in relation to the \"treacherous turn\") and for fairness.", "url": "https://arxiv.org/abs/2312.07779"}, {"metadata": {"arXiv": "2312.07838", "Date": "Wed, 13 Dec 2023 02:06:20 ", "Title": "Conflict Transformation and Management. From Cognitive Maps to Value Trees", "Authors": ["Berkay H. Tosunlu and Joseph H.A. Guillaume and Alexis Tsouki\\`as"], "Categories": "cs.AI"}, "abstract": "Conflict transformation and management are complex decision processes with extremely high stakes at hand and could greatly benefit from formal approaches to decision support. For this purpose we develop a general framework about how to use problem structuring methods for such purposes. More precisely we show how to transform cognitive maps to value trees in order to promote a more design-oriented approach to decision support aiming at constructing innovative solutions for conflict management purposes. We show that our findings have a much wider validity since they allow to move from a descriptive representation of a problem situation to a more prescriptive one using formal procedures and models.", "url": "https://arxiv.org/abs/2312.07838"}, {"metadata": {"arXiv": "2312.07850", "Date": "Wed, 13 Dec 2023 02:35:57 ", "Title": "Large Language Model Enhanced Multi-Agent Systems for 6G Communications", "Authors": ["Feibo Jiang", "Li Dong", "Yubo Peng", "Kezhi Wang", "Kun Yang", "Cunhua Pan", "Dusit Niyato", "Octavia A. Dobre"], "Categories": "cs.AI", "Comments": ["Submitted for possible journal publication"]}, "abstract": "The rapid development of the Large Language Model (LLM) presents huge opportunities for 6G communications, e.g., network optimization and management by allowing users to input task requirements to LLMs by nature language. However, directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities. Integrating LLMs with the capabilities of retrieval, planning, memory, evaluation and reflection in agents can greatly enhance the potential of LLMs for 6G communications. To this end, we propose a multi-agent system with customized communication knowledge and tools for solving communication related tasks using natural language, comprising three components: (1) Multi-agent Data Retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication related task from different perspectives based on the retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflexion agent and refinement agent to provide improvement suggestions for current solutions. Finally, we validate the effectiveness of the proposed multi-agent system by designing a semantic communication system, as a case study of 6G communications.", "url": "https://arxiv.org/abs/2312.07850"}, {"metadata": {"arXiv": "2312.07867", "Date": "Wed, 13 Dec 2023 03:08:48 ", "Title": "BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering", "Authors": ["Xiaojie Hong", "Zixin Song", "Liangzhi Li", "Xiaoli Wang", "Feiyan Liu"], "Categories": "cs.AI cs.CL"}, "abstract": "Medical Visual Question Answering (Med-VQA) is a very important task in healthcare industry, which answers a natural language question with a medical image. Existing VQA techniques in information systems can be directly applied to solving the task. However, they often suffer from (i) the data insufficient problem, which makes it difficult to train the state of the arts (SOTAs) for the domain-specific task, and (ii) the reproducibility problem, that many existing models have not been thoroughly evaluated in a unified experimental setup. To address these issues, this paper develops a Benchmark Evaluation SysTem for Medical Visual Question Answering, denoted by BESTMVQA. Given self-collected clinical data, our system provides a useful tool for users to automatically build Med-VQA datasets, which helps overcoming the data insufficient problem. Users also can conveniently select a wide spectrum of SOTA models from our model library to perform a comprehensive empirical study. With simple configurations, our system automatically trains and evaluates the selected models over a benchmark dataset, and reports the comprehensive results for users to develop new techniques or perform medical practice. Limitations of existing work are overcome (i) by the data generation tool, which automatically constructs new datasets from unstructured clinical data, and (ii) by evaluating SOTAs on benchmark datasets in a unified experimental setup. The demonstration video of our system can be found at https://youtu.be/QkEeFlu1x4A. Our code and data will be available soon.", "url": "https://arxiv.org/abs/2312.07867"}, {"metadata": {"arXiv": "2312.07876", "Date": "Wed, 13 Dec 2023 03:35:43 ", "Title": "Causality Analysis for Evaluating the Security of Large Language Models", "Authors": ["Wei Zhao", "Zhe Li", "Jun Sun"], "Categories": "cs.AI"}, "abstract": "Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted in many safety-critical applications. Their security is thus essential. Even with considerable efforts spent on reinforcement learning from human feedback (RLHF), recent studies have shown that LLMs are still subject to attacks such as adversarial perturbation and Trojan attacks. Further research is thus needed to evaluate their security and/or understand the lack of it. In this work, we propose a framework for conducting light-weight causality-analysis of LLMs at the token, layer, and neuron level. We applied our framework to open-source LLMs such as Llama2 and Vicuna and had multiple interesting discoveries. Based on a layer-level causality analysis, we show that RLHF has the effect of overfitting a model to harmful prompts. It implies that such security can be easily overcome by `unusual' harmful prompts. As evidence, we propose an adversarial perturbation method that achieves 100\\% attack success rate on the red-teaming tasks of the Trojan Detection Competition 2023. Furthermore, we show the existence of one mysterious neuron in both Llama2 and Vicuna that has an unreasonably high causal effect on the output. While we are uncertain on why such a neuron exists, we show that it is possible to conduct a ``Trojan'' attack targeting that particular neuron to completely cripple the LLM, i.e., we can generate transferable suffixes to prompts that frequently make the LLM produce meaningless responses.", "url": "https://arxiv.org/abs/2312.07876"}, {"metadata": {"arXiv": "2312.07886", "Date": "Wed, 13 Dec 2023 04:08:59 ", "Title": "Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI", "Authors": ["Kai Huang", "Boyuan Yang and Wei Gao"], "Categories": "cs.AI cs.CL"}, "abstract": "Large Language Models (LLMs) are capable of reasoning over diverse input data modalities through pre-trained encoders. However, the growing diversity of input data modalities prevents incorporating all modalities into LLMs, especially when LLMs are deployed on resource-constrained edge devices for embodied AI applications. Instead, a better option is to adaptively involve only the useful modalities at runtime, depending on the current environmental contexts and task requirements. For such modality adaptation, existing work adopts fixed connections between encoders and the LLM's input layer, leading to high training cost at runtime and ineffective cross-modal interaction. In this paper, we address these limitations by presenting mPnP-LLM, a new technique that allows fully elastic, automated and prompt runtime modality adaptation, by connecting unimodal encoders to a flexible set of last LLM blocks and making such latent connections fully trainable at runtime. Experiments over the nuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction and 30% GPU memory usage reduction, while retaining on-par accuracy with the existing schemes. Under the same compute budget, mPnP-LLM improves the task accuracy by up to 4% compared to the best existing scheme.", "url": "https://arxiv.org/abs/2312.07886"}, {"metadata": {"arXiv": "2312.07966", "Date": "Wed, 13 Dec 2023 08:28:55 ", "Title": "A multi-sourced data and agent-based approach for complementing Time Use Surveys in the context of residential human activity and load curve simulation", "Authors": ["Mathieu Schumann", "Quentin Reynaud", "Fran\\c{c}ois Semp\\'e (OASIS)", "Julien Guibourdenche (RIFT", "UNIGE)", "Jean-Baptiste Ly (CPU)", "Nicolas Sabouret (CPU", "CPU", "CPU)"], "Categories": "cs.AI cs.HC cs.MA", "Journal-ref": "Building Simulation Conference, Sep 2023, Shangai, China"}, "abstract": "To address the major issues associated with using Time-Use Survey (TUS) for simulating residential load curves, we present the SMACH approach, which combines qualitative and quantitative data with agent-based simulation. Our model consists of autonomous agents assigned with daily tasks. The agents try to accomplish their assigned tasks to the best of their abilities. Quantitative data are used to generate tasks assignments. Qualitative studies allow us to define how agents select, based on plausible cognitive principles, the tasks to accomplish depending on the context. Our results show a better representation of weekdays and weekends, a more flexible association of tasks with appliances, and an improved simulation of load curves compared to real data. Highlights $\\bullet$ Discussion about Time-Use Surveys (TUS) limits and the use of TUS in activity and energy simulation $\\bullet$ Presentation of complementary data both qualitative and quantitative used to complement TUS data $\\bullet$ Proposition of an agent-based approach that balances these limitations", "url": "https://arxiv.org/abs/2312.07966"}, {"metadata": {"arXiv": "2312.07993", "Date": "Wed, 13 Dec 2023 09:05:48 ", "Title": "A Unified View on Forgetting and Strong Equivalence Notions in Answer Set Programming", "Authors": ["Zeynep G. Saribatur and Stefan Woltran"], "Categories": "cs.AI", "Comments": ["This is an extended version of a paper to be published at AAAI 2024"]}, "abstract": "Answer Set Programming (ASP) is a prominent rule-based language for knowledge representation and reasoning with roots in logic programming and non-monotonic reasoning. The aim to capture the essence of removing (ir)relevant details in ASP programs led to the investigation of different notions, from strong persistence (SP) forgetting, to faithful abstractions, and, recently, strong simplifications, where the latter two can be seen as relaxed and strengthened notions of forgetting, respectively. Although it was observed that these notions are related, especially given that they have characterizations through the semantics for strong equivalence, it remained unclear whether they can be brought together. In this work, we bridge this gap by introducing a novel relativized equivalence notion, which is a relaxation of the recent simplification notion, that is able to capture all related notions from the literature. We provide necessary and sufficient conditions for relativized simplifiability, which shows that the challenging part is for when the context programs do not contain all the atoms to remove. We then introduce an operator that combines projection and a relaxation of (SP)-forgetting to obtain the relativized simplifications. We furthermore present complexity results that complete the overall picture.", "url": "https://arxiv.org/abs/2312.07993"}, {"metadata": {"arXiv": "2312.08064", "Date": "Wed, 13 Dec 2023 11:17:29 ", "Title": "Exploring the Impact of Lay User Feedback for Improving AI Fairness", "Authors": ["Evdoxia Taka", "Yuri Nakao", "Ryosuke Sonoda", "Takuya Yokota", "Lin Luo", "Simone Stumpf"], "Categories": "cs.AI"}, "abstract": "Fairness in AI is a growing concern for high-stakes decision making. Engaging stakeholders, especially lay users, in fair AI development is promising yet overlooked. Recent efforts explore enabling lay users to provide AI fairness-related feedback, but there is still a lack of understanding of how to integrate users' feedback into an AI model and the impacts of doing so. To bridge this gap, we collected feedback from 58 lay users on the fairness of a XGBoost model trained on the Home Credit dataset, and conducted offline experiments to investigate the effects of retraining models on accuracy, and individual and group fairness. Our work contributes baseline results of integrating user fairness feedback in XGBoost, and a dataset and code framework to bootstrap research in engaging stakeholders in AI fairness. Our discussion highlights the challenges of employing user feedback in AI fairness and points the way to a future application area of interactive machine learning.", "url": "https://arxiv.org/abs/2312.08064"}, {"metadata": {"arXiv": "2312.08084", "Date": "Wed, 13 Dec 2023 12:00:46 ", "Title": "A Novel Energy based Model Mechanism for Multi-modal Aspect-Based Sentiment Analysis", "Authors": ["Tianshuo Peng", "Zuchao Li", "Ping Wang", "Lefei Zhang", "Hai Zhao"], "Categories": "cs.AI", "Comments": ["AAAI2024"]}, "abstract": "Multi-modal aspect-based sentiment analysis (MABSA) has recently attracted increasing attention. The span-based extraction methods, such as FSUIE, demonstrate strong performance in sentiment analysis due to their joint modeling of input sequences and target labels. However, previous methods still have certain limitations: (i) They ignore the difference in the focus of visual information between different analysis targets (aspect or sentiment). (ii) Combining features from uni-modal encoders directly may not be sufficient to eliminate the modal gap and can cause difficulties in capturing the image-text pairwise relevance. (iii) Existing span-based methods for MABSA ignore the pairwise relevance of target span boundaries. To tackle these limitations, we propose a novel framework called DQPSA for multi-modal sentiment analysis. Specifically, our model contains a Prompt as Dual Query (PDQ) module that uses the prompt as both a visual query and a language query to extract prompt-aware visual information and strengthen the pairwise relevance between visual information and the analysis target. Additionally, we introduce an Energy-based Pairwise Expert (EPE) module that models the boundaries pairing of the analysis target from the perspective of an Energy-based Model. This expert predicts aspect or sentiment span based on pairwise stability. Experiments on three widely used benchmarks demonstrate that DQPSA outperforms previous approaches and achieves a new state-of-the-art performance.", "url": "https://arxiv.org/abs/2312.08084"}, {"metadata": {"arXiv": "2312.08157", "Date": "Wed, 13 Dec 2023 14:10:30 ", "Title": "CIDR: A Cooperative Integrated Dynamic Refining Method for Minimal Feature Removal Problem", "Authors": ["Qian Chen", "Taolin Zhang", "Dongyang Li", "Xiaofeng He"], "Categories": "cs.AI"}, "abstract": "The minimal feature removal problem in the post-hoc explanation area aims to identify the minimal feature set (MFS). Prior studies using the greedy algorithm to calculate the minimal feature set lack the exploration of feature interactions under a monotonic assumption which cannot be satisfied in general scenarios. In order to address the above limitations, we propose a Cooperative Integrated Dynamic Refining method (CIDR) to efficiently discover minimal feature sets. Specifically, we design Cooperative Integrated Gradients (CIG) to detect interactions between features. By incorporating CIG and characteristics of the minimal feature set, we transform the minimal feature removal problem into a knapsack problem. Additionally, we devise an auxiliary Minimal Feature Refinement algorithm to determine the minimal feature set from numerous candidate sets. To the best of our knowledge, our work is the first to address the minimal feature removal problem in the field of natural language processing. Extensive experiments demonstrate that CIDR is capable of tracing representative minimal feature sets with improved interpretability across various models and datasets.", "url": "https://arxiv.org/abs/2312.08157"}, {"metadata": {"arXiv": "2312.08248", "Date": "Wed, 13 Dec 2023 16:13:23 ", "Title": "A Survey of Generative AI for Intelligent Transportation Systems", "Authors": ["Huan Yan and Yong Li"], "Categories": "cs.AI"}, "abstract": "Intelligent transportation systems play a crucial role in modern traffic management and optimization, greatly improving traffic efficiency and safety. With the rapid development of generative artificial intelligence (Generative AI) technologies in the fields of image generation and natural language processing, generative AI has also played a crucial role in addressing key issues in intelligent transportation systems, such as data sparsity, difficulty in observing abnormal scenarios, and in modeling data uncertainty. In this review, we systematically investigate the relevant literature on generative AI techniques in addressing key issues in different types of tasks in intelligent transportation systems. First, we introduce the principles of different generative AI techniques, and their potential applications. Then, we classify tasks in intelligent transportation systems into four types: traffic perception, traffic prediction, traffic simulation, and traffic decision-making. We systematically illustrate how generative AI techniques addresses key issues in these four different types of tasks. Finally, we summarize the challenges faced in applying generative AI to intelligent transportation systems, and discuss future research directions based on different application scenarios.", "url": "https://arxiv.org/abs/2312.08248"}, {"metadata": {"arXiv": "2312.07723", "Date": "Tue, 12 Dec 2023 20:36:36 ", "Title": "Automated Behavioral Analysis Using Instance Segmentation", "Authors": ["Chen Yang", "Jeremy Forest", "Matthew Einhorn", "Thomas A. Cleland"], "Categories": "cs.CV cs.AI"}, "abstract": "Animal behavior analysis plays a crucial role in various fields, such as life science and biomedical research. However, the scarcity of available data and the high cost associated with obtaining a large number of labeled datasets pose significant challenges. In this research, we propose a novel approach that leverages instance segmentation-based transfer learning to address these issues. By capitalizing on fine-tuning the classification head of the instance segmentation network, we enable the tracking of multiple animals and facilitate behavior analysis in laboratory-recorded videos. To demonstrate the effectiveness of our method, we conducted a series of experiments, revealing that our approach achieves exceptional performance levels, comparable to human capabilities, across a diverse range of animal behavior analysis tasks. Moreover, we emphasize the practicality of our solution, as it requires only a small number of labeled images for training. To facilitate the adoption and further development of our method, we have developed an open-source implementation named Annolid (An annotation and instance segmentation-based multiple animal tracking and behavior analysis package). The codebase is publicly available on GitHub at https://github.com/cplab/annolid. This resource serves as a valuable asset for researchers and practitioners interested in advancing animal behavior analysis through state-of-the-art techniques.", "url": "https://arxiv.org/abs/2312.07723"}, {"metadata": {"arXiv": "2312.07814", "Date": "Wed, 13 Dec 2023 00:24:37 ", "Title": "A Foundational Multimodal Vision Language AI Assistant for Human Pathology", "Authors": ["Ming Y. Lu", "Bowen Chen", "Drew F. K. Williamson", "Richard J. Chen", "Kenji Ikamura", "Georg Gerber", "Ivy Liang", "Long Phi Le", "Tong Ding", "Anil V Parwani", "Faisal Mahmood"], "Categories": "cs.CV cs.AI"}, "abstract": "The field of computational pathology has witnessed remarkable progress in the development of both task-specific predictive models and task-agnostic self-supervised vision encoders. However, despite the explosive growth of generative artificial intelligence (AI), there has been limited study on building general purpose, multimodal AI assistants tailored to pathology. Here we present PathChat, a vision-language generalist AI assistant for human pathology using an in-house developed foundational vision encoder pretrained on 100 million histology images from over 100,000 patient cases and 1.18 million pathology image-caption pairs. The vision encoder is then combined with a pretrained large language model and the whole system is finetuned on over 250,000 diverse disease agnostic visual language instructions. We compare PathChat against several multimodal vision language AI assistants as well as GPT4V, which powers the commercially available multimodal general purpose AI assistant ChatGPT-4. When relevant clinical context is provided with the histology image, PathChat achieved a diagnostic accuracy of 87% on multiple-choice questions based on publicly available cases of diverse tissue origins and disease models. Additionally, using open-ended questions and human expert evaluation, we found that overall PathChat produced more accurate and pathologist-preferable responses to diverse queries related to pathology. As an interactive and general vision language AI assistant that can flexibly handle both visual and natural language inputs, PathChat can potentially find impactful applications in pathology education, research, and human-in-the-loop clinical decision making.", "url": "https://arxiv.org/abs/2312.07814"}, {"metadata": {"arXiv": "2312.07856", "Date": "Wed, 13 Dec 2023 02:51:26 ", "Title": "DTL: Disentangled Transfer Learning for Visual Recognition", "Authors": ["Minghao Fu", "Ke Zhu", "Jianxin Wu"], "Categories": "cs.CV cs.AI", "Comments": ["To appear in AAAI 2024"]}, "abstract": "When pre-trained models become rapidly larger, the cost of fine-tuning on downstream tasks steadily increases, too. To economically fine-tune these models, parameter-efficient transfer learning (PETL) is proposed, which only tunes a tiny subset of trainable parameters to efficiently learn quality representations. However, current PETL methods are facing the dilemma that during training the GPU memory footprint is not effectively reduced as trainable parameters. PETL will likely fail, too, if the full fine-tuning encounters the out-of-GPU-memory issue. This phenomenon happens because trainable parameters from these methods are generally entangled with the backbone, such that a lot of intermediate states have to be stored in GPU memory for gradient propagation. To alleviate this problem, we introduce Disentangled Transfer Learning (DTL), which disentangles the trainable parameters from the backbone using a lightweight Compact Side Network (CSN). By progressively extracting task-specific information with a few low-rank linear mappings and appropriately adding the information back to the backbone, CSN effectively realizes knowledge transfer in various downstream tasks. We conducted extensive experiments to validate the effectiveness of our method. The proposed method not only reduces a large amount of GPU memory usage and trainable parameters, but also outperforms existing PETL methods by a significant margin in accuracy, achieving new state-of-the-art on several standard benchmarks.", "url": "https://arxiv.org/abs/2312.07856"}, {"metadata": {"arXiv": "2312.07879", "Date": "Wed, 13 Dec 2023 03:48:45 ", "Title": "CoIE: Chain-of-Instruct Editing for Multi-Attribute Face Manipulation", "Authors": ["Zhenduo Zhang", "Bowen Zhang", "Guang Liu"], "Categories": "cs.CV cs.AI"}, "abstract": "Current text-to-image editing models often encounter challenges with smoothly manipulating multiple attributes using a single instruction. Taking inspiration from the Chain-of-Thought prompting technique utilized in language models, we present an innovative concept known as Chain-of-Instruct Editing (CoIE), which enhances the capabilities of these models through step-by-step editing using a series of instructions. In particular, in the context of face manipulation, we leverage the contextual learning abilities of a pretrained Large Language Model (LLM), such as GPT-4, to generate a sequence of instructions from the original input, utilizing a purpose-designed 1-shot template. To further improve the precision of each editing step, we conduct fine-tuning on the editing models using our self-constructed instruction-guided face editing dataset, Instruct-CelebA. And additionally, we incorporate a super-resolution module to mitigate the adverse effects of editability and quality degradation. Experimental results across various challenging cases confirm the significant boost in multi-attribute facial image manipulation using chain-of-instruct editing. This is evident in enhanced editing success rates, measured by CLIPSim and Coverage metrics, improved by 17.86% and 85.45% respectively, and heightened controllability indicated by Preserve L1 and Quality metrics, improved by 11.58% and 4.93% respectively.", "url": "https://arxiv.org/abs/2312.07879"}, {"metadata": {"arXiv": "2312.07971", "Date": "Wed, 13 Dec 2023 08:36:51 ", "Title": "LMD: Faster Image Reconstruction with Latent Masking Diffusion", "Authors": ["Zhiyuan Ma", "zhihuan yu", "Jianjun Li", "Bowen Zhou"], "Categories": "cs.CV cs.AI"}, "abstract": "As a class of fruitful approaches, diffusion probabilistic models (DPMs) have shown excellent advantages in high-resolution image reconstruction. On the other hand, masked autoencoders (MAEs), as popular self-supervised vision learners, have demonstrated simpler and more effective image reconstruction and transfer capabilities on downstream tasks. However, they all require extremely high training costs, either due to inherent high temporal-dependence (i.e., excessively long diffusion steps) or due to artificially low spatial-dependence (i.e., human-formulated high mask ratio, such as 0.75). To the end, this paper presents LMD, a faster image reconstruction framework with latent masking diffusion. First, we propose to project and reconstruct images in latent space through a pre-trained variational autoencoder, which is theoretically more efficient than in the pixel-based space. Then, we combine the advantages of MAEs and DPMs to design a progressive masking diffusion model, which gradually increases the masking proportion by three different schedulers and reconstructs the latent features from simple to difficult, without sequentially performing denoising diffusion as in DPMs or using fixed high masking ratio as in MAEs, so as to alleviate the high training time-consumption predicament. Our approach allows for learning high-capacity models and accelerate their training (by 3x or more) and barely reduces the original accuracy. Inference speed in downstream tasks also significantly outperforms the previous approaches.", "url": "https://arxiv.org/abs/2312.07971"}, {"metadata": {"arXiv": "2312.08012", "Date": "Wed, 13 Dec 2023 09:34:01 ", "Title": "uSF: Learning Neural Semantic Field with Uncertainty", "Authors": ["Vsevolod Skorokhodov", "Darya Drozdova", "Dmitry Yudin"], "Categories": "cs.CV cs.AI", "Comments": ["12 pages", "4 figures"]}, "abstract": "Recently, there has been an increased interest in NeRF methods which reconstruct differentiable representation of three-dimensional scenes. One of the main limitations of such methods is their inability to assess the confidence of the model in its predictions. In this paper, we propose a new neural network model for the formation of extended vector representations, called uSF, which allows the model to predict not only color and semantic label of each point, but also estimate the corresponding values of uncertainty. We show that with a small number of images available for training, a model quantifying uncertainty performs better than a model without such functionality. Code of the uSF approach is publicly available at https://github.com/sevashasla/usf/.", "url": "https://arxiv.org/abs/2312.08012"}, {"metadata": {"arXiv": "2312.08056", "Date": "Wed, 13 Dec 2023 11:03:07 ", "Title": "Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision", "Authors": ["Shengguang Wu", "Zhenglun Chen", "Qi Su"], "Categories": "cs.CV cs.AI"}, "abstract": "Ancient artifacts are an important medium for cultural preservation and restoration. However, many physical copies of artifacts are either damaged or lost, leaving a blank space in archaeological and historical studies that calls for artifact image generation techniques. Despite the significant advancements in open-domain text-to-image synthesis, existing approaches fail to capture the important domain knowledge presented in the textual description, resulting in errors in recreated images such as incorrect shapes and patterns. In this paper, we propose a novel knowledge-aware artifact image synthesis approach that brings lost historical objects accurately into their visual forms. We use a pretrained diffusion model as backbone and introduce three key techniques to enhance the text-to-image generation framework: 1) we construct prompts with explicit archaeological knowledge elicited from large language models (LLMs); 2) we incorporate additional textual guidance to correlated historical expertise in a contrastive manner; 3) we introduce further visual-semantic constraints on edge and perceptual features that enable our model to learn more intricate visual details of the artifacts. Compared to existing approaches, our proposed model produces higher-quality artifact images that align better with the implicit details and historical knowledge contained within written documents, thus achieving significant improvements across automatic metrics and in human evaluation. Our code and data are available at https://github.com/danielwusg/artifact_diffusion.", "url": "https://arxiv.org/abs/2312.08056"}, {"metadata": {"arXiv": "2312.08078", "Date": "Wed, 13 Dec 2023 11:47:28 ", "Title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation", "Authors": ["Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "Categories": "cs.CV cs.AI cs.CL", "Comments": ["11 pages", "8 figures"]}, "abstract": "To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainability for the generation process. AdaMatch exploits the fine-grained relation between adaptive patches and words to provide explanations of specific image regions with corresponding words. To capture the abnormal regions of varying sizes and positions, we introduce the Adaptive Patch extraction (AdaPatch) module to acquire the adaptive patches for these regions adaptively. In order to provide explicit explainability for CXR-report generation task, we propose an AdaMatch-based bidirectional large language model for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords for CXR images and `keypatches' for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets prove the effectiveness of our method and its superior performance to existing methods.", "url": "https://arxiv.org/abs/2312.08078"}, {"metadata": {"arXiv": "2312.08195", "Date": "Wed, 13 Dec 2023 14:59:49 ", "Title": "Concept-centric Personalization with Large-scale Diffusion Priors", "Authors": ["Pu Cao", "Lu Yang", "Feng Zhou", "Tianrui Huang", "Qing Song"], "Categories": "cs.CV cs.AI cs.MM"}, "abstract": "Despite large-scale diffusion models being highly capable of generating diverse open-world content, they still struggle to match the photorealism and fidelity of concept-specific generators. In this work, we present the task of customizing large-scale diffusion priors for specific concepts as concept-centric personalization. Our goal is to generate high-quality concept-centric images while maintaining the versatile controllability inherent to open-world models, enabling applications in diverse tasks such as concept-centric stylization and image translation. To tackle these challenges, we identify catastrophic forgetting of guidance prediction from diffusion priors as the fundamental issue. Consequently, we develop a guidance-decoupled personalization framework specifically designed to address this task. We propose Generalized Classifier-free Guidance (GCFG) as the foundational theory for our framework. This approach extends Classifier-free Guidance (CFG) to accommodate an arbitrary number of guidances, sourced from a variety of conditions and models. Employing GCFG enables us to separate conditional guidance into two distinct components: concept guidance for fidelity and control guidance for controllability. This division makes it feasible to train a specialized model for concept guidance, while ensuring both control and unconditional guidance remain intact. We then present a null-text Concept-centric Diffusion Model as a concept-specific generator to learn concept guidance without the need for text annotations. Code will be available at https://github.com/PRIV-Creation/Concept-centric-Personalization.", "url": "https://arxiv.org/abs/2312.08195"}, {"metadata": {"arXiv": "2312.08344", "Date": "Wed, 13 Dec 2023 18:28:09 ", "Title": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects", "Authors": ["Bowen Wen", "Wei Yang", "Jan Kautz", "Stan Birchfield"], "Categories": "cs.CV cs.AI cs.RO"}, "abstract": "We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without fine-tuning, as long as its CAD model is given, or a small number of reference images are captured. We bridge the gap between these two setups with a neural implicit representation that allows for effective novel view synthesis, keeping the downstream pose estimation modules invariant under the same unified framework. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In addition, it even achieves comparable results to instance-level methods despite the reduced assumptions. Project page: https://nvlabs.github.io/FoundationPose/", "url": "https://arxiv.org/abs/2312.08344"}, {"metadata": {"arXiv": "2312.07910", "Date": "Wed, 13 Dec 2023 05:58:34 ", "Title": "PromptBench: A Unified Library for Evaluation of Large Language Models", "Authors": ["Kaijie Zhu", "Qinlin Zhao", "Hao Chen", "Jindong Wang", "Xing Xie"], "Categories": "cs.AI cs.CL cs.LG", "Comments": ["An extension to PromptBench (arXiv:2306.04528) for unified evaluation of LLMs using the same name; code: https://github.com/microsoft/promptbench"]}, "abstract": "The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.", "url": "https://arxiv.org/abs/2312.07910"}, {"metadata": {"arXiv": "2312.08224", "Date": "Wed, 13 Dec 2023 15:46:58 ", "Title": "GLOP: Learning Global Partition and Local Construction for Solving Large-scale Routing Problems in Real-time", "Authors": ["Haoran Ye", "Jiarui Wang", "Helan Liang", "Zhiguang Cao", "Yong Li", "Fanzhang Li"], "Categories": "cs.AI cs.LG", "Comments": ["Accepted at AAAI 2024"]}, "abstract": "The recent end-to-end neural solvers have shown promise for small-scale routing problems but suffered from limited real-time scaling-up performance. This paper proposes GLOP (Global and Local Optimization Policies), a unified hierarchical framework that efficiently scales toward large-scale routing problems. GLOP partitions large routing problems into Travelling Salesman Problems (TSPs) and TSPs into Shortest Hamiltonian Path Problems. For the first time, we hybridize non-autoregressive neural heuristics for coarse-grained problem partitions and autoregressive neural heuristics for fine-grained route constructions, leveraging the scalability of the former and the meticulousness of the latter. Experimental results show that GLOP achieves competitive and state-of-the-art real-time performance on large-scale routing problems, including TSP, ATSP, CVRP, and PCTSP.", "url": "https://arxiv.org/abs/2312.08224"}, {"metadata": {"arXiv": "2312.07571", "Date": "Sun, 10 Dec 2023 13:16:22 ", "Title": "Investigating YOLO Models Towards Outdoor Obstacle Detection For Visually Impaired People", "Authors": ["Chenhao He and Pramit Saha"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "The utilization of deep learning-based object detection is an effective approach to assist visually impaired individuals in avoiding obstacles. In this paper, we implemented seven different YOLO object detection models \\textit{viz}., YOLO-NAS (small, medium, large), YOLOv8, YOLOv7, YOLOv6, and YOLOv5 and performed comprehensive evaluation with carefully tuned hyperparameters, to analyze how these models performed on images containing common daily-life objects presented on roads and sidewalks. After a systematic investigation, YOLOv8 was found to be the best model, which reached a precision of $80\\%$ and a recall of $68.2\\%$ on a well-known Obstacle Dataset which includes images from VOC dataset, COCO dataset, and TT100K dataset along with images collected by the researchers in the field. Despite being the latest model and demonstrating better performance in many other applications, YOLO-NAS was found to be suboptimal for the obstacle detection task.", "url": "https://arxiv.org/abs/2312.07571"}, {"metadata": {"arXiv": "2312.07586", "Date": "Mon, 11 Dec 2023 02:40:40 ", "Title": "Characteristic Guidance: Non-linear Correction for DDPM at Large Guidance Scale", "Authors": ["Candi Zheng", "Yuan Lan"], "Categories": "cs.CV cs.AI cs.LG physics.data-an", "Comments": ["8 pages", "7 figures"]}, "abstract": "Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a novel method that provides non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is first-principle, training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance is robust to various applications, offers enhanced control over sample generation, suppresses color and exposure issues even for latent space sampling, and can handle physics problems such as the phase transitions.", "url": "https://arxiv.org/abs/2312.07586"}, {"metadata": {"arXiv": "2312.07835", "Date": "Wed, 13 Dec 2023 01:57:11 ", "Title": "Video Dynamics Prior: An Internal Learning Approach for Robust Video Enhancements", "Authors": ["Gaurav Shrivastava", "Ser-Nam Lim", "Abhinav Shrivastava"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["NeurIPS 2023; Webpage - http://www.cs.umd.edu/~gauravsh/vdp.html"]}, "abstract": "In this paper, we present a novel robust framework for low-level vision tasks, including denoising, object removal, frame interpolation, and super-resolution, that does not require any external training data corpus. Our proposed approach directly learns the weights of neural modules by optimizing over the corrupted test sequence, leveraging the spatio-temporal coherence and internal statistics of videos. Furthermore, we introduce a novel spatial pyramid loss that leverages the property of spatio-temporal patch recurrence in a video across the different scales of the video. This loss enhances robustness to unstructured noise in both the spatial and temporal domains. This further results in our framework being highly robust to degradation in input frames and yields state-of-the-art results on downstream tasks such as denoising, object removal, and frame interpolation. To validate the effectiveness of our approach, we conduct qualitative and quantitative evaluations on standard video datasets such as DAVIS, UCF-101, and VIMEO90K-T.", "url": "https://arxiv.org/abs/2312.07835"}, {"metadata": {"arXiv": "2312.07955", "Date": "Wed, 13 Dec 2023 08:01:15 ", "Title": "Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking", "Authors": ["Shengsheng Qian", "Yifei Wang", "Dizhan Xue", "Shengjie Zhang", "Huaiwen Zhang", "Changsheng Xu"], "Categories": "cs.CV cs.AI cs.CR cs.LG"}, "abstract": "Researchers have recently found that Self-Supervised Learning (SSL) is vulnerable to backdoor attacks. The attacker can embed hidden SSL backdoors via a few poisoned examples in the training dataset and maliciously manipulate the behavior of downstream models. To defend against SSL backdoor attacks, a feasible route is to detect and remove the poisonous samples in the training set. However, the existing SSL backdoor defense method fails to detect the poisonous samples precisely. In this paper, we propose to erase the SSL backdoor by cluster activation masking and propose a novel PoisonCAM method. After obtaining the threat model trained on the poisoned dataset, our method can precisely detect poisonous samples based on the assumption that masking the backdoor trigger can effectively change the activation of a downstream clustering model. In experiments, our PoisonCAM achieves 96% accuracy for backdoor trigger detection compared to 3% of the state-of-the-art method on poisoned ImageNet-100. Moreover, our proposed PoisonCAM significantly improves the performance of the trained SSL model under backdoor attacks compared to the state-of-the-art method. Our code will be available at https://github.com/LivXue/PoisonCAM.", "url": "https://arxiv.org/abs/2312.07955"}, {"metadata": {"arXiv": "2312.07624", "Date": "Tue, 12 Dec 2023 06:35:56 ", "Title": "Adaptive Proximal Policy Optimization with Upper Confidence Bound", "Authors": ["Ziqi Zhang", "Jingzehua Xu", "Zifeng Zhuang", "Jinxin Liu", "Donglin wang"], "Categories": "cs.LG cs.AI"}, "abstract": "Trust Region Policy Optimization (TRPO) attractively optimizes the policy while constraining the update of the new policy within a trust region, ensuring the stability and monotonic optimization. Building on the theoretical guarantees of trust region optimization, Proximal Policy Optimization (PPO) successfully enhances the algorithm's sample efficiency and reduces deployment complexity by confining the update of the new and old policies within a surrogate trust region. However, this approach is limited by the fixed setting of surrogate trust region and is not sufficiently adaptive, because there is no theoretical proof that the optimal clipping bound remains consistent throughout the entire training process, truncating the ratio of the new and old policies within surrogate trust region can ensure that the algorithm achieves its best performance, therefore, exploring and researching a dynamic clip bound for improving PPO's performance can be quite beneficial. To design an adaptive clipped trust region and explore the dynamic clip bound's impact on the performance of PPO, we introduce an adaptive PPO-CLIP (Adaptive-PPO) method that dynamically explores and exploits the clip bound using a bandit during the online training process. Furthermore, ample experiments will initially demonstrate that our Adaptive-PPO exhibits sample efficiency and performance compared to PPO-CLIP.", "url": "https://arxiv.org/abs/2312.07624"}, {"metadata": {"arXiv": "2312.07685", "Date": "Tue, 12 Dec 2023 19:24:35 ", "Title": "A Perspective of Q-value Estimation on Offline-to-Online Reinforcement Learning", "Authors": ["Yinmin Zhang", "Jie Liu", "Chuming Li", "Yazhe Niu", "Yaodong Yang", "Yu Liu", "Wanli Ouyang"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at AAAI 2024"]}, "abstract": "Offline-to-online Reinforcement Learning (O2O RL) aims to improve the performance of offline pretrained policy using only a few online samples. Built on offline RL algorithms, most O2O methods focus on the balance between RL objective and pessimism, or the utilization of offline and online samples. In this paper, from a novel perspective, we systematically study the challenges that remain in O2O RL and identify that the reason behind the slow improvement of the performance and the instability of online finetuning lies in the inaccurate Q-value estimation inherited from offline pretraining. Specifically, we demonstrate that the estimation bias and the inaccurate rank of Q-value cause a misleading signal for the policy update, making the standard offline RL algorithms, such as CQL and TD3-BC, ineffective in the online finetuning. Based on this observation, we address the problem of Q-value estimation by two techniques: (1) perturbed value update and (2) increased frequency of Q-value updates. The first technique smooths out biased Q-value estimation with sharp peaks, preventing early-stage policy exploitation of sub-optimal actions. The second one alleviates the estimation bias inherited from offline pretraining by accelerating learning. Extensive experiments on the MuJoco and Adroit environments demonstrate that the proposed method, named SO2, significantly alleviates Q-value estimation issues, and consistently improves the performance against the state-of-the-art methods by up to 83.1%.", "url": "https://arxiv.org/abs/2312.07685"}, {"metadata": {"arXiv": "2312.07930", "Date": "Wed, 13 Dec 2023 06:57:00 ", "Title": "Towards Optimal Statistical Watermarking", "Authors": ["Baihe Huang and Banghua Zhu and Hanlin Zhu and Jason D. Lee and Jiantao Jiao and Michael I. Jordan"], "Categories": "cs.LG cs.AI cs.CL cs.IT math.IT stat.ML"}, "abstract": "We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in this context. In the most common scenario where the output is a sequence of $n$ tokens, we establish matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate scales as $\\Theta(h^{-1} \\log (1/h))$ with respect to the average entropy per token $h$ and thus greatly improves the $O(h^{-2})$ rate in the previous works. For scenarios where the detector lacks knowledge of the model's distribution, we introduce the concept of model-agnostic watermarking and establish the minimax bounds for the resultant increase in Type II error. Moreover, we formulate the robust watermarking problem where user is allowed to perform a class of perturbation on the generated texts, and characterize the optimal type II error of robust UMP tests via a linear programming problem. To the best of our knowledge, this is the first systematic statistical treatment on the watermarking problem with near-optimal rates in the i.i.d. setting, and might be of interest for future works.", "url": "https://arxiv.org/abs/2312.07930"}, {"metadata": {"arXiv": "2312.07983", "Date": "Wed, 13 Dec 2023 08:57:15 ", "Title": "Multi-perspective Feedback-attention Coupling Model for Continuous-time Dynamic Graphs", "Authors": ["Xiaobo Zhu", "Yan Wu", "Zhipeng Li", "Hailong Su", "Jin Che", "Zhanheng Chen", "Liying Wang"], "Categories": "cs.LG cs.AI cs.SI"}, "abstract": "Recently, representation learning over graph networks has gained popularity, with various models showing promising results. Despite this, several challenges persist: 1) most methods are designed for static or discrete-time dynamic graphs; 2) existing continuous-time dynamic graph algorithms focus on a single evolving perspective; and 3) many continuous-time dynamic graph approaches necessitate numerous temporal neighbors to capture long-term dependencies. In response, this paper introduces the Multi-Perspective Feedback-Attention Coupling (MPFA) model. MPFA incorporates information from both evolving and raw perspectives, efficiently learning the interleaved dynamics of observed processes. The evolving perspective employs temporal self-attention to distinguish continuously evolving temporal neighbors for information aggregation. Through dynamic updates, this perspective can capture long-term dependencies using a small number of temporal neighbors. Meanwhile, the raw perspective utilizes a feedback attention module with growth characteristic coefficients to aggregate raw neighborhood information. Experimental results on a self-organizing dataset and seven public datasets validate the efficacy and competitiveness of our proposed model.", "url": "https://arxiv.org/abs/2312.07983"}, {"metadata": {"arXiv": "2312.08033", "Date": "Wed, 13 Dec 2023 10:19:58 ", "Title": "Beyond Top-Class Agreement: Using Divergences to Forecast Performance under Distribution Shift", "Authors": ["Mona Schirmer", "Dan Zhang", "Eric Nalisnick"], "Categories": "cs.LG cs.AI", "Comments": ["Workshop on Distribution Shifts", "37th Conference on Neural Information Processing Systems (NeurIPS 2023)"]}, "abstract": "Knowing if a model will generalize to data 'in the wild' is crucial for safe deployment. To this end, we study model disagreement notions that consider the full predictive distribution - specifically disagreement based on Hellinger distance, Jensen-Shannon and Kullback-Leibler divergence. We find that divergence-based scores provide better test error estimates and detection rates on out-of-distribution data compared to their top-1 counterparts. Experiments involve standard vision and foundation models.", "url": "https://arxiv.org/abs/2312.08033"}, {"metadata": {"arXiv": "2312.08057", "Date": "Wed, 13 Dec 2023 11:08:25 ", "Title": "Combinatorial Stochastic-Greedy Bandit", "Authors": ["Fares Fourati", "Christopher John Quinn", "Mohamed-Slim Alouini", "Vaneet Aggarwal"], "Categories": "cs.LG cs.AI math.CO math.OC stat.ML"}, "abstract": "We propose a novel combinatorial stochastic-greedy bandit (SGB) algorithm for combinatorial multi-armed bandit problems when no extra information other than the joint reward of the selected set of $n$ arms at each time step $t\\in [T]$ is observed. SGB adopts an optimized stochastic-explore-then-commit approach and is specifically designed for scenarios with a large set of base arms. Unlike existing methods that explore the entire set of unselected base arms during each selection step, our SGB algorithm samples only an optimized proportion of unselected arms and selects actions from this subset. We prove that our algorithm achieves a $(1-1/e)$-regret bound of $\\mathcal{O}(n^{\\frac{1}{3}} k^{\\frac{2}{3}} T^{\\frac{2}{3}} \\log(T)^{\\frac{2}{3}})$ for monotone stochastic submodular rewards, which outperforms the state-of-the-art in terms of the cardinality constraint $k$. Furthermore, we empirically evaluate the performance of our algorithm in the context of online constrained social influence maximization. Our results demonstrate that our proposed approach consistently outperforms the other algorithms, increasing the performance gap as $k$ grows.", "url": "https://arxiv.org/abs/2312.08057"}, {"metadata": {"arXiv": "2312.08063", "Date": "Wed, 13 Dec 2023 11:17:27 ", "Title": "Estimation of Concept Explanations Should be Uncertainty Aware", "Authors": ["Vihari Piratla", "Juyeon Heo", "Sukriti Singh", "Adrian Weller"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "Model explanations are very valuable for interpreting and debugging prediction models. We study a specific kind of global explanations called Concept Explanations, where the goal is to interpret a model using human-understandable concepts. Recent advances in multi-modal learning rekindled interest in concept explanations and led to several label-efficient proposals for estimation. However, existing estimation methods are unstable to the choice of concepts or dataset that is used for computing explanations. We observe that instability in explanations is due to high variance in point estimation of importance scores. We propose an uncertainty aware Bayesian estimation method, which readily improved reliability of the concept explanations. We demonstrate with theoretical analysis and empirical evaluation that explanations computed by our method are more reliable while also being label-efficient and faithful.", "url": "https://arxiv.org/abs/2312.08063"}, {"metadata": {"arXiv": "2312.08066", "Date": "Wed, 13 Dec 2023 11:20:09 ", "Title": "A Novel Metric for Measuring Data Quality in Classification Applications (extended version)", "Authors": ["Jouseau Roxane", "Salva S\\'ebastien", "Samir Chafik"], "Categories": "cs.LG cs.AI"}, "abstract": "Data quality is a key element for building and optimizing good learning models. Despite many attempts to characterize data quality, there is still a need for rigorous formalization and an efficient measure of the quality from available observations. Indeed, without a clear understanding of the training and testing processes, it is hard to evaluate the intrinsic performance of a model. Besides, tools allowing to measure data quality specific to machine learning are still lacking. In this paper, we introduce and explain a novel metric to measure data quality. This metric is based on the correlated evolution between the classification performance and the deterioration of data. The proposed method has the major advantage of being model-independent. Furthermore, we provide an interpretation of each criterion and examples of assessment levels. We confirm the utility of the proposed metric with intensive numerical experiments and detail some illustrative cases with controlled and interpretable qualities.", "url": "https://arxiv.org/abs/2312.08066"}, {"metadata": {"arXiv": "2312.08107", "Date": "Wed, 13 Dec 2023 12:54:34 ", "Title": "Causal Optimal Transport of Abstractions", "Authors": ["Yorgos Felekis", "Fabio Massimo Zennaro", "Nicola Branchini and Theodoros Damoulas"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Causal abstraction (CA) theory establishes formal criteria for relating multiple structural causal models (SCMs) at different levels of granularity by defining maps between them. These maps have significant relevance for real-world challenges such as synthesizing causal evidence from multiple experimental environments, learning causally consistent representations at different resolutions, and linking interventions across multiple SCMs. In this work, we propose COTA, the first method to learn abstraction maps from observational and interventional data without assuming complete knowledge of the underlying SCMs. In particular, we introduce a multi-marginal Optimal Transport (OT) formulation that enforces do-calculus causal constraints, together with a cost function that relies on interventional information. We extensively evaluate COTA on synthetic and real world problems, and showcase its advantages over non-causal, independent and aggregated COTA formulations. Finally, we demonstrate the efficiency of our method as a data augmentation tool by comparing it against the state-of-the-art CA learning framework, which assumes fully specified SCMs, on a real-world downstream task.", "url": "https://arxiv.org/abs/2312.08107"}, {"metadata": {"arXiv": "2312.08143", "Date": "Wed, 13 Dec 2023 13:46:14 ", "Title": "Efficient Representation of the Activation Space in Deep Neural Networks", "Authors": ["Tanya Akumu", "Celia Cintas", "Girmaw Abebe Tadesse", "Adebayo Oshingbesan", "Skyler Speakman", "Edward McFowland III"], "Categories": "cs.LG cs.AI"}, "abstract": "The representations of the activation space of deep neural networks (DNNs) are widely utilized for tasks like natural language processing, anomaly detection and speech recognition. Due to the diverse nature of these tasks and the large size of DNNs, an efficient and task-independent representation of activations becomes crucial. Empirical p-values have been used to quantify the relative strength of an observed node activation compared to activations created by already-known inputs. Nonetheless, keeping raw data for these calculations increases memory resource consumption and raises privacy concerns. To this end, we propose a model-agnostic framework for creating representations of activations in DNNs using node-specific histograms to compute p-values of observed activations without retaining already-known inputs. Our proposed approach demonstrates promising potential when validated with multiple network architectures across various downstream tasks and compared with the kernel density estimates and brute-force empirical baselines. In addition, the framework reduces memory usage by 30% with up to 4 times faster p-value computing time while maintaining state of-the-art detection power in downstream tasks such as the detection of adversarial attacks and synthesized content. Moreover, as we do not persist raw data at inference time, we could potentially reduce susceptibility to attacks and privacy issues.", "url": "https://arxiv.org/abs/2312.08143"}, {"metadata": {"arXiv": "2312.08221", "Date": "Wed, 13 Dec 2023 15:42:14 ", "Title": "Curriculum-Enhanced Residual Soft An-Isotropic Normalization for Over-smoothness in Deep GNNs", "Authors": ["Jin Li", "Qirong Zhang", "Shuling Xu", "Xinlong Chen", "Longkun Guo", "Yang-Geng Fu"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted by AAAI 2024"]}, "abstract": "Despite Graph neural networks' significant performance gain over many classic techniques in various graph-related downstream tasks, their successes are restricted in shallow models due to over-smoothness and the difficulties of optimizations among many other issues. In this paper, to alleviate the over-smoothing issue, we propose a soft graph normalization method to preserve the diversities of node embeddings and prevent indiscrimination due to possible over-closeness. Combined with residual connections, we analyze the reason why the method can effectively capture the knowledge in both input graph structures and node features even with deep networks. Additionally, inspired by Curriculum Learning that learns easy examples before the hard ones, we propose a novel label-smoothing-based learning framework to enhance the optimization of deep GNNs, which iteratively smooths labels in an auxiliary graph and constructs many gradual non-smooth tasks for extracting increasingly complex knowledge and gradually discriminating nodes from coarse to fine. The method arguably reduces the risk of overfitting and generalizes better results. Finally, extensive experiments are carried out to demonstrate the effectiveness and potential of the proposed model and learning framework through comparison with twelve existing baselines including the state-of-the-art methods on twelve real-world node classification benchmarks.", "url": "https://arxiv.org/abs/2312.08221"}, {"metadata": {"arXiv": "2312.08287", "Date": "Wed, 13 Dec 2023 17:04:09 ", "Title": "On the verification of Embeddings using Hybrid Markov Logic", "Authors": ["Anup Shakya", "Abisha Thapa Magar", "Somdeb Sarkhel and Deepak Venugopal"], "Categories": "cs.LG cs.AI", "Comments": ["6 pages", "Proceedings of 23rd IEEE International Conference on Data Mining 2023 (ICDM'23)"], "DOI": "10.1109/ICDM58522.2023.00165"}, "abstract": "The standard approach to verify representations learned by Deep Neural Networks is to use them in specific tasks such as classification or regression, and measure their performance based on accuracy in such tasks. However, in many cases, we would want to verify more complex properties of a learned representation. To do this, we propose a framework based on a probabilistic first-order language, namely, Hybrid Markov Logic Networks (HMLNs) where we specify properties over embeddings mixed with symbolic domain knowledge. We present an approach to learn parameters for the properties within this framework. Further, we develop a verification method to test embeddings in this framework by encoding this task as a Mixed Integer Linear Program for which we can leverage existing state-of-the-art solvers. We illustrate verification in Graph Neural Networks, Deep Knowledge Tracing and Intelligent Tutoring Systems to demonstrate the generality of our approach.", "url": "https://arxiv.org/abs/2312.08287"}, {"metadata": {"arXiv": "2312.08358", "Date": "Wed, 13 Dec 2023 18:51:34 ", "Title": "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF", "Authors": ["Anand Siththaranjan and Cassidy Laidlaw and Dylan Hadfield-Menell"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called Borda count. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called distributional preference learning (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability. Our code and data are available at https://github.com/cassidylaidlaw/hidden-context", "url": "https://arxiv.org/abs/2312.08358"}, {"metadata": {"arXiv": "2312.08365", "Date": "Wed, 13 Dec 2023 18:57:23 ", "Title": "An Invitation to Deep Reinforcement Learning", "Authors": ["Bernhard Jaeger and Andreas Geiger"], "Categories": "cs.LG cs.AI"}, "abstract": "Training a deep neural network to maximize a target objective has become the standard recipe for successful machine learning over the last decade. These networks can be optimized with supervised learning, if the target objective is differentiable. For many interesting problems, this is however not the case. Common objectives like intersection over union (IoU), bilingual evaluation understudy (BLEU) score or rewards cannot be optimized with supervised learning. A common workaround is to define differentiable surrogate losses, leading to suboptimal solutions with respect to the actual objective. Reinforcement learning (RL) has emerged as a promising alternative for optimizing deep neural networks to maximize non-differentiable objectives in recent years. Examples include aligning large language models via human feedback, code generation, object detection or control problems. This makes RL techniques relevant to the larger machine learning audience. The subject is, however, time intensive to approach due to the large range of methods, as well as the often very theoretical presentation. In this introduction, we take an alternative approach, different from classic reinforcement learning textbooks. Rather than focusing on tabular problems, we introduce reinforcement learning as a generalization of supervised learning, which we first apply to non-differentiable objectives and later to temporal problems. Assuming only basic knowledge of supervised learning, the reader will be able to understand state-of-the-art deep RL algorithms like proximal policy optimization (PPO) after reading this tutorial.", "url": "https://arxiv.org/abs/2312.08365"}, {"metadata": {"arXiv": "2312.07671", "Date": "Tue, 12 Dec 2023 19:06:44 ", "Title": "Reacting like Humans: Incorporating Intrinsic Human Behaviors into NAO through Sound-Based Reactions for Enhanced Sociability", "Authors": ["Ali Ghadami", "Mohammadreza Taghimohammadi", "Mohammad Mohammadzadeh", "Mohammad Hosseinipour", "Alireza Taheri"], "Categories": "cs.RO cs.AI cs.LG cs.SD eess.AS eess.IV", "Comments": ["14 pages", "10 figures"], "MSC-class": "68T40"}, "abstract": "Robots' acceptability among humans and their sociability can be significantly enhanced by incorporating human-like reactions. Humans can react to environmental events very quickly and without thinking. An instance where humans display natural reactions is when they encounter a sudden and loud sound that startles or frightens them. During such moments, individuals may instinctively move their hands, turn toward the origin of the sound, and try to determine the event's cause. This inherent behavior motivated us to explore this less-studied part of social robotics. In this work, a multi-modal system composed of an action generator, sound classifier, and YOLO object detector was designed to sense the environment and, in the presence of sudden loud sounds, show natural human fear reactions, and finally, locate the fear-causing sound source in the environment. These unique and valid generated motions and inferences could imitate intrinsic human reactions and enhance the sociability of robots. For motion generation, a model based on LSTM and MDN networks was proposed to synthesize various motions. Also, in the case of sound detection, a transfer learning model was preferred that used the spectrogram of sound signals as its input. After developing individual models for sound detection, motion generation, and image recognition, they were integrated into a comprehensive fear module that was implemented on the NAO robot. Finally, the fear module was tested in practical application and two groups of experts and non-experts filled out a questionnaire to evaluate the performance of the robot. Given our promising results, this preliminary exploratory research provides a fresh perspective on social robotics and could be a starting point for modeling intrinsic human behaviors and emotions in robots.", "url": "https://arxiv.org/abs/2312.07671"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
