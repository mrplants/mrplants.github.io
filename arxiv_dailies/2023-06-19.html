<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2306.09483", "Date": "Thu, 15 Jun 2023 20:27:06 ", "Title": "R2-Diff: Denoising by diffusion as a refinement of retrieved motion for image-based motion prediction", "Authors": ["Takeru Oba and Norimichi Ukita"], "Categories": "cs.CV cs.LG cs.RO", "Comments": ["22 pages", "preprint submitted to Neurocomputing"], "MSC-class": "68T40"}, "abstract": "Image-based motion prediction is one of the essential techniques for robot manipulation. Among the various prediction models, we focus on diffusion models because they have achieved state-of-the-art performance in various applications. In image-based motion prediction, diffusion models stochastically predict contextually appropriate motion by gradually denoising random Gaussian noise based on the image context. While diffusion models are able to predict various motions by changing the random noise, they sometimes fail to predict a contextually appropriate motion based on the image because the random noise is sampled independently of the image context. To solve this problem, we propose R2-Diff. In R2-Diff, a motion retrieved from a dataset based on image similarity is fed into a diffusion model instead of random noise. Then, the retrieved motion is refined through the denoising process of the diffusion model. Since the retrieved motion is almost appropriate to the context, it becomes easier to predict contextually appropriate motion. However, traditional diffusion models are not optimized to refine the retrieved motion. Therefore, we propose the method of tuning the hyperparameters based on the distance of the nearest neighbor motion among the dataset to optimize the diffusion model for refinement. Furthermore, we propose an image-based retrieval method to retrieve the nearest neighbor motion in inference. Our proposed retrieval efficiently computes the similarity based on the image features along the motion trajectory. We demonstrate that R2-Diff accurately predicts appropriate motions and achieves high task success rates compared to recent state-of-the-art models in robot manipulation.", "url": "https://arxiv.org/abs/2306.09483"}, {"metadata": {"arXiv": "2306.09592", "Date": "Fri, 16 Jun 2023 02:35:00 ", "Title": "FewSAR: A Few-shot SAR Image Classification Benchmark", "Authors": ["Rui Zhang", "Ziqi Wang", "Yang Li", "Jiabao Wang", "Zhiteng Wang"], "Categories": "cs.CV cs.LG", "Comments": ["7 pages", "4 figures"]}, "abstract": "Few-shot learning (FSL) is one of the significant and hard problems in the field of image classification. However, in contrast to the rapid development of the visible light dataset, the progress in SAR target image classification is much slower. The lack of unified benchmark is a key reason for this phenomenon, which may be severely overlooked by the current literature. The researchers of SAR target image classification always report their new results on their own datasets and experimental setup. It leads to inefficiency in result comparison and impedes the further progress of this area. Motivated by this observation, we propose a novel few-shot SAR image classification benchmark (FewSAR) to address this issue. FewSAR consists of an open-source Python code library of 15 classic methods in three categories for few-shot SAR image classification. It provides an accessible and customizable testbed for different few-shot SAR image classification task. To further understanding the performance of different few-shot methods, we establish evaluation protocols and conduct extensive experiments within the benchmark. By analyzing the quantitative results and runtime under the same setting, we observe that the accuracy of metric learning methods can achieve the best results. Meta-learning methods and fine-tuning methods perform poorly on few-shot SAR images, which is primarily due to the bias of existing datasets. We believe that FewSAR will open up a new avenue for future research and development, on real-world challenges at the intersection of SAR image classification and few-shot deep learning. We will provide our code for the proposed FewSAR at https://github.com/solarlee/FewSAR.", "url": "https://arxiv.org/abs/2306.09592"}, {"metadata": {"arXiv": "2306.09887", "Date": "Fri, 16 Jun 2023 14:55:44 ", "Title": "CANDID: Correspondence AligNment for Deep-burst Image Denoising", "Authors": ["Arijit Mallick", "Raphael Braun", "Hendrik PA Lensch"], "Categories": "cs.CV cs.LG", "Comments": ["Paper accepted and presented as a poster on 20th Conference on Robots and Vision 2023"]}, "abstract": "With the advent of mobile phone photography and point-and-shoot cameras, deep-burst imaging is widely used for a number of photographic effects such as depth of field, super-resolution, motion deblurring, and image denoising. In this work, we propose to solve the problem of deep-burst image denoising by including an optical flow-based correspondence estimation module which aligns all the input burst images with respect to a reference frame. In order to deal with varying noise levels the individual burst images are pre-filtered with different settings. Exploiting the established correspondences one network block predicts a pixel-wise spatially-varying filter kernel to smooth each image in the original and prefiltered bursts before fusing all images to generate the final denoised output. The resulting pipeline achieves state-of-the-art results by combining all available information provided by the burst.", "url": "https://arxiv.org/abs/2306.09887"}, {"metadata": {"arXiv": "2306.09939", "Date": "Fri, 16 Jun 2023 16:19:59 ", "Title": "Towards Better Orthogonality Regularization with Disentangled Norm in Training Deep CNNs", "Authors": ["Changhao Wu", "Shenan Zhang", "Fangsong Long", "Ziliang Yin", "Tuo Leng"], "Categories": "cs.CV cs.LG", "Comments": ["17 pages", "5 figures"]}, "abstract": "Orthogonality regularization has been developed to prevent deep CNNs from training instability and feature redundancy. Among existing proposals, kernel orthogonality regularization enforces orthogonality by minimizing the residual between the Gram matrix formed by convolutional filters and the orthogonality matrix. We propose a novel measure for achieving better orthogonality among filters, which disentangles diagonal and correlation information from the residual. The model equipped with the measure under the principle of imposing strict orthogonality between filters surpasses previous regularization methods in near-orthogonality. Moreover, we observe the benefits of improved strict filter orthogonality in relatively shallow models, but as model depth increases, the performance gains in models employing strict kernel orthogonality decrease sharply. Furthermore, based on the observation of the potential conflict between strict kernel orthogonality and growing model capacity, we propose a relaxation theory on kernel orthogonality regularization. The relaxed kernel orthogonality achieves enhanced performance on models with increased capacity, shedding light on the burden of strict kernel orthogonality on deep model performance. We conduct extensive experiments with our kernel orthogonality regularization toolkit on ResNet and WideResNet in CIFAR-10 and CIFAR-100. We observe state-of-the-art gains in model performance from the toolkit, which includes both strict orthogonality and relaxed orthogonality regularization, and obtain more robust models with expressive features. These experiments demonstrate the efficacy of our toolkit and subtly provide insights into the often overlooked challenges posed by strict orthogonality, addressing the burden of strict orthogonality on capacity-rich models.", "url": "https://arxiv.org/abs/2306.09939"}, {"metadata": {"arXiv": "2306.09998", "Date": "Fri, 16 Jun 2023 17:51:07 ", "Title": "SLACK: Stable Learning of Augmentations with Cold-start and KL regularization", "Authors": ["Juliette Marrie", "Michael Arbel", "Diane Larlus", "Julien Mairal"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted to CVPR 2023"]}, "abstract": "Data augmentation is known to improve the generalization capabilities of neural networks, provided that the set of transformations is chosen with care, a selection often performed manually. Automatic data augmentation aims at automating this process. However, most recent approaches still rely on some prior information; they start from a small pool of manually-selected default transformations that are either used to pretrain the network or forced to be part of the policy learned by the automatic data augmentation algorithm. In this paper, we propose to directly learn the augmentation policy without leveraging such prior knowledge. The resulting bilevel optimization problem becomes more challenging due to the larger search space and the inherent instability of bilevel optimization algorithms. To mitigate these issues (i) we follow a successive cold-start strategy with a Kullback-Leibler regularization, and (ii) we parameterize magnitudes as continuous distributions. Our approach leads to competitive results on standard benchmarks despite a more challenging setting, and generalizes beyond natural images.", "url": "https://arxiv.org/abs/2306.09998"}, {"metadata": {"arXiv": "2306.10006", "Date": "Fri, 16 Jun 2023 17:58:04 ", "Title": "Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances", "Authors": ["Wolfgang Paier and Anna Hilsmann and Peter Eisert"], "Categories": "cs.CV cs.GR cs.LG", "Comments": ["16 pages", "submitted to Graphical Models (Feb 2023)"]}, "abstract": "This paper presents a novel approach for text/speech-driven animation of a photo-realistic head model based on blend-shape geometry, dynamic textures, and neural rendering. Training a VAE for geometry and texture yields a parametric model for accurate capturing and realistic synthesis of facial expressions from a latent feature vector. Our animation method is based on a conditional CNN that transforms text or speech into a sequence of animation parameters. In contrast to previous approaches, our animation model learns disentangling/synthesizing different acting-styles in an unsupervised manner, requiring only phonetic labels that describe the content of training sequences. For realistic real-time rendering, we train a U-Net that refines rasterization-based renderings by computing improved pixel colors and a foreground matte. We compare our framework qualitatively/quantitatively against recent methods for head modeling as well as facial animation and evaluate the perceived rendering/animation quality in a user-study, which indicates large improvements compared to state-of-the-art approaches", "url": "https://arxiv.org/abs/2306.10006"}, {"metadata": {"arXiv": "2306.10008", "Date": "Fri, 16 Jun 2023 17:58:15 ", "Title": "CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search", "Authors": ["Fahad Shamshad", "Muzammal Naseer", "Karthik Nandakumar"], "Categories": "cs.CV cs.CR cs.LG", "Comments": ["Accepted in CVPR 2023. Project page: https://fahadshamshad.github.io/clip2protect"], "Journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 20595-20605"}, "abstract": "The success of deep learning based face recognition systems has given rise to serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Existing methods for enhancing privacy fail to generate naturalistic images that can protect facial privacy without compromising user experience. We propose a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the low-dimensional manifold of a pretrained generative model. The first step inverts the given face image into the latent space and finetunes the generative model to achieve an accurate reconstruction of the given image from its latent code. This step produces a good initialization, aiding the generation of high-quality faces that resemble the given identity. Subsequently, user-defined makeup text prompts and identity-preserving regularization are used to guide the search for adversarial codes in the latent space. Extensive experiments demonstrate that faces generated by our approach have stronger black-box transferability with an absolute gain of 12.06% over the state-of-the-art facial privacy protection approach under the face verification task. Finally, we demonstrate the effectiveness of the proposed approach for commercial face recognition systems. Our code is available at https://github.com/fahadshamshad/Clip2Protect.", "url": "https://arxiv.org/abs/2306.10008"}, {"metadata": {"arXiv": "2306.09375", "Date": "Thu, 15 Jun 2023 05:37:25 ", "Title": "Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials", "Authors": ["Shengchao Liu", "Weitao Du", "Yanjing Li", "Zhuoxinran Li", "Zhiling Zheng", "Chenru Duan", "Zhiming Ma", "Omar Yaghi", "Anima Anandkumar", "Christian Borgs", "Jennifer Chayes", "Hongyu Guo", "Jian Tang"], "Categories": "cs.LG physics.chem-ph q-bio.QM"}, "abstract": "Artificial intelligence for scientific discovery has recently generated significant interest within the machine learning and scientific communities, particularly in the domains of chemistry, biology, and material discovery. For these scientific problems, molecules serve as the fundamental building blocks, and machine learning has emerged as a highly effective and powerful tool for modeling their geometric structures. Nevertheless, due to the rapidly evolving process of the field and the knowledge gap between science (e.g., physics, chemistry, & biology) and machine learning communities, a benchmarking study on geometrical representation for such data has not been conducted. To address such an issue, in this paper, we first provide a unified view of the current symmetry-informed geometric methods, classifying them into three main categories: invariance, equivariance with spherical frame basis, and equivariance with vector frame basis. Then we propose a platform, coined Geom3D, which enables benchmarking the effectiveness of geometric strategies. Geom3D contains 16 advanced symmetry-informed geometric representation models and 14 geometric pretraining methods over 46 diverse datasets, including small molecules, proteins, and crystalline materials. We hope that Geom3D can, on the one hand, eliminate barriers for machine learning researchers interested in exploring scientific problems; and, on the other hand, provide valuable guidance for researchers in computational chemistry, structural biology, and materials science, aiding in the informed selection of representation techniques for specific applications.", "url": "https://arxiv.org/abs/2306.09375"}, {"metadata": {"arXiv": "2306.09397", "Date": "Thu, 15 Jun 2023 17:42:14 ", "Title": "Non-Asymptotic Performance of Social Machine Learning Under Limited Data", "Authors": ["Ping Hu", "Virginia Bordignon", "Mert Kayaalp", "Ali H. Sayed"], "Categories": "cs.LG cs.MA eess.SP"}, "abstract": "This paper studies the probability of error associated with the social machine learning framework, which involves an independent training phase followed by a cooperative decision-making phase over a graph. This framework addresses the problem of classifying a stream of unlabeled data in a distributed manner. We consider two kinds of classification tasks with limited observations in the prediction phase, namely, the statistical classification task and the single-sample classification task. For each task, we describe the distributed learning rule and analyze the probability of error accordingly. To do so, we first introduce a stronger consistent training condition that involves the margin distributions generated by the trained classifiers. Based on this condition, we derive an upper bound on the probability of error for both tasks, which depends on the statistical properties of the data and the combination policy used to combine the distributed classifiers. For the statistical classification problem, we employ the geometric social learning rule and conduct a non-asymptotic performance analysis. An exponential decay of the probability of error with respect to the number of unlabeled samples is observed in the upper bound. For the single-sample classification task, a distributed learning rule that functions as an ensemble classifier is constructed. An upper bound on the probability of error of this ensemble classifier is established.", "url": "https://arxiv.org/abs/2306.09397"}, {"metadata": {"arXiv": "2306.09424", "Date": "Thu, 15 Jun 2023 18:11:20 ", "Title": "SSL4EO-L: Datasets and Foundation Models for Landsat Imagery", "Authors": ["Adam J. Stewart", "Nils Lehmann", "Isaac A. Corley", "Yi Wang", "Yi-Chia Chang", "Nassim Ait Ali Braham", "Shradha Sehgal", "Caleb Robinson", "Arindam Banerjee"], "Categories": "cs.LG cs.CV eess.IV"}, "abstract": "The Landsat program is the longest-running Earth observation program in history, with 50+ years of data acquisition by 8 satellites. The multispectral imagery captured by sensors onboard these satellites is critical for a wide range of scientific fields. Despite the increasing popularity of deep learning and remote sensing, the majority of researchers still use decision trees and random forests for Landsat image analysis due to the prevalence of small labeled datasets and lack of foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for Earth Observation for the Landsat family of satellites (including 3 sensors and 2 product levels) and the largest Landsat dataset in history (5M image patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML benchmark datasets for Landsats 4-5 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first foundation models for Landsat imagery using SSL4EO-L and evaluate their performance on multiple semantic segmentation tasks. All datasets and model weights are available via the TorchGeo (https://github.com/microsoft/torchgeo) library, making reproducibility and experimentation easy, and enabling scientific advancements in the burgeoning field of remote sensing for a myriad of downstream applications.", "url": "https://arxiv.org/abs/2306.09424"}, {"metadata": {"arXiv": "2306.09425", "Date": "Thu, 15 Jun 2023 18:15:46 ", "Title": "Arbitrariness Lies Beyond the Fairness-Accuracy Frontier", "Authors": ["Carol Xuan Long", "Hsiang Hsu", "Wael Alghamdi", "Flavio P. Calmon"], "Categories": "cs.LG cs.CY cs.IT math.IT"}, "abstract": "Machine learning tasks may admit multiple competing models that achieve similar performance yet produce conflicting outputs for individual samples -- a phenomenon known as predictive multiplicity. We demonstrate that fairness interventions in machine learning optimized solely for group fairness and accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art fairness interventions can mask high predictive multiplicity behind favorable group fairness and accuracy metrics. We argue that a third axis of ``arbitrariness'' should be considered when deploying models to aid decision-making in applications of individual-level impact. To address this challenge, we propose an ensemble algorithm applicable to any fairness intervention that provably ensures more consistent predictions.", "url": "https://arxiv.org/abs/2306.09425"}, {"metadata": {"arXiv": "2306.09461", "Date": "Thu, 15 Jun 2023 19:31:59 ", "Title": "Hierarchical confusion matrix for classification performance evaluation", "Authors": ["Kevin Riehl", "Michael Neunteufel", "Martin Hemberg"], "Categories": "cs.LG"}, "abstract": "In this work we propose a novel concept of a hierarchical confusion matrix, opening the door for popular confusion matrix based (flat) evaluation measures from binary classification problems, while considering the peculiarities of hierarchical classification problems. We develop the concept to a generalized form and prove its applicability to all types of hierarchical classification problems including directed acyclic graphs, multi path labelling, and non mandatory leaf node prediction. Finally, we use measures based on the novel confusion matrix to evaluate models within a benchmark for three real world hierarchical classification applications and compare the results to established evaluation measures. The results outline the reasonability of this approach and its usefulness to evaluate hierarchical classification problems. The implementation of hierarchical confusion matrix is available on GitHub.", "url": "https://arxiv.org/abs/2306.09461"}, {"metadata": {"arXiv": "2306.09466", "Date": "Thu, 15 Jun 2023 19:37:43 ", "Title": "Simplified Temporal Consistency Reinforcement Learning", "Authors": ["Yi Zhao", "Wenshuai Zhao", "Rinu Boney", "Juho Kannala", "Joni Pajarinen"], "Categories": "cs.LG cs.RO"}, "abstract": "Reinforcement learning is able to solve complex sequential decision-making tasks but is currently limited by sample efficiency and required computation. To improve sample efficiency, recent work focuses on model-based RL which interleaves model learning with planning. Recent methods further utilize policy learning, value estimation, and, self-supervised learning as auxiliary objectives. In this paper we show that, surprisingly, a simple representation learning approach relying only on a latent dynamics model trained by latent temporal consistency is sufficient for high-performance RL. This applies when using pure planning with a dynamics model conditioned on the representation, but, also when utilizing the representation as policy and value function features in model-free RL. In experiments, our approach learns an accurate dynamics model to solve challenging high-dimensional locomotion tasks with online planners while being 4.1 times faster to train compared to ensemble-based methods. With model-free RL without planning, especially on high-dimensional tasks, such as the DeepMind Control Suite Humanoid and Dog tasks, our approach outperforms model-free methods by a large margin and matches model-based methods' sample efficiency while training 2.4 times faster.", "url": "https://arxiv.org/abs/2306.09466"}, {"metadata": {"arXiv": "2306.09467", "Date": "Thu, 15 Jun 2023 19:42:11 ", "Title": "AQuA: A Benchmarking Tool for Label Quality Assessment", "Authors": ["Mononito Goswami", "Vedant Sanil", "Arjun Choudhry", "Arvind Srinivasan", "Chalisa Udompanyawit", "Artur Dubrawski"], "Categories": "cs.LG", "Comments": ["Submitted to the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks. Source code can be found at www.github.com/autonlab/aqua/"]}, "abstract": "Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to delineate concrete design choices of label error detection models. We hope that our proposed design space and benchmark enable practitioners to choose the right tools to improve their label quality and that our benchmark enables objective and rigorous evaluation of machine learning tools facing mislabeled data.", "url": "https://arxiv.org/abs/2306.09467"}, {"metadata": {"arXiv": "2306.09478", "Date": "Thu, 15 Jun 2023 20:08:42 ", "Title": "Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks", "Authors": ["Lukas Fesser", "Richard Qiu", "Luca D'Amico-Wong"], "Categories": "cs.LG"}, "abstract": "Physics-informed Neural Networks (PINNs) have recently gained popularity in the scientific community due to their effective approximation of partial differential equations (PDEs) using deep neural networks. However, their application has been generally limited to interpolation scenarios, where predictions rely on inputs within the support of the training set. In real-world applications, extrapolation is often required, but the out of domain behavior of PINNs is understudied. In this paper, we provide a detailed investigation of PINNs' extrapolation behavior and provide evidence against several previously held assumptions: we study the effects of different model choices on extrapolation and find that once the model can achieve zero interpolation error, further increases in architecture size or in the number of points sampled have no effect on extrapolation behavior. We also show that for some PDEs, PINNs perform nearly as well in extrapolation as in interpolation. By analyzing the Fourier spectra of the solution functions, we characterize the PDEs that yield favorable extrapolation behavior, and show that the presence of high frequencies in the solution function is not to blame for poor extrapolation behavior. Finally, we propose a transfer learning-based strategy based on our Fourier results, which decreases extrapolation errors in PINNs by up to $82 \\%$.", "url": "https://arxiv.org/abs/2306.09478"}, {"metadata": {"arXiv": "2306.09547", "Date": "Thu, 15 Jun 2023 23:28:45 ", "Title": "Training generative models from privatized data", "Authors": ["Daria Reshetova", "Wei-Ning Chen", "Ayfer \\\"Ozg\\\"ur"], "Categories": "cs.LG cs.CR cs.IT math.IT"}, "abstract": "Local differential privacy (LDP) is a powerful method for privacy-preserving data collection. In this paper, we develop a framework for training Generative Adversarial Networks (GAN) on differentially privatized data. We show that entropic regularization of the Wasserstein distance -- a popular regularization method in the literature that has been often leveraged for its computational benefits -- can be used to denoise the data distribution when data is privatized by common additive noise mechanisms, such as Laplace and Gaussian. This combination uniquely enables the mitigation of both the regularization bias and the effects of privatization noise, thereby enhancing the overall efficacy of the model. We analyse the proposed method, provide sample complexity results and experimental evidence to support its efficacy.", "url": "https://arxiv.org/abs/2306.09547"}, {"metadata": {"arXiv": "2306.09554", "Date": "Thu, 15 Jun 2023 23:51:46 ", "Title": "Low-Switching Policy Gradient with Exploration via Online Sensitivity Sampling", "Authors": ["Yunfan Li", "Yiran Wang", "Yu Cheng", "Lin Yang"], "Categories": "cs.LG"}, "abstract": "Policy optimization methods are powerful algorithms in Reinforcement Learning (RL) for their flexibility to deal with policy parameterization and ability to handle model misspecification. However, these methods usually suffer from slow convergence rates and poor sample complexity. Hence it is important to design provably sample efficient algorithms for policy optimization. Yet, recent advances for this problems have only been successful in tabular and linear setting, whose benign structures cannot be generalized to non-linearly parameterized policies. In this paper, we address this problem by leveraging recent advances in value-based algorithms, including bounded eluder-dimension and online sensitivity sampling, to design a low-switching sample-efficient policy optimization algorithm, LPO, with general non-linear function approximation. We show that, our algorithm obtains an $\\varepsilon$-optimal policy with only $\\widetilde{O}(\\frac{\\text{poly}(d)}{\\varepsilon^3})$ samples, where $\\varepsilon$ is the suboptimality gap and $d$ is a complexity measure of the function class approximating the policy. This drastically improves previously best-known sample bound for policy optimization algorithms, $\\widetilde{O}(\\frac{\\text{poly}(d)}{\\varepsilon^8})$. Moreover, we empirically test our theory with deep neural nets to show the benefits of the theoretical inspiration.", "url": "https://arxiv.org/abs/2306.09554"}, {"metadata": {"arXiv": "2306.09588", "Date": "Fri, 16 Jun 2023 02:27:41 ", "Title": "Understanding the Role of Feedback in Online Learning with Switching Costs", "Authors": ["Duo Cheng", "Xingyu Zhou", "Bo Ji"], "Categories": "cs.LG", "Comments": ["Accepted to ICML 2023"]}, "abstract": "In this paper, we study the role of feedback in online learning with switching costs. It has been shown that the minimax regret is $\\widetilde{\\Theta}(T^{2/3})$ under bandit feedback and improves to $\\widetilde{\\Theta}(\\sqrt{T})$ under full-information feedback, where $T$ is the length of the time horizon. However, it remains largely unknown how the amount and type of feedback generally impact regret. To this end, we first consider the setting of bandit learning with extra observations; that is, in addition to the typical bandit feedback, the learner can freely make a total of $B_{\\mathrm{ex}}$ extra observations. We fully characterize the minimax regret in this setting, which exhibits an interesting phase-transition phenomenon: when $B_{\\mathrm{ex}} = O(T^{2/3})$, the regret remains $\\widetilde{\\Theta}(T^{2/3})$, but when $B_{\\mathrm{ex}} = \\Omega(T^{2/3})$, it becomes $\\widetilde{\\Theta}(T/\\sqrt{B_{\\mathrm{ex}}})$, which improves as the budget $B_{\\mathrm{ex}}$ increases. To design algorithms that can achieve the minimax regret, it is instructive to consider a more general setting where the learner has a budget of $B$ total observations. We fully characterize the minimax regret in this setting as well and show that it is $\\widetilde{\\Theta}(T/\\sqrt{B})$, which scales smoothly with the total budget $B$. Furthermore, we propose a generic algorithmic framework, which enables us to design different learning algorithms that can achieve matching upper bounds for both settings based on the amount and type of feedback. One interesting finding is that while bandit feedback can still guarantee optimal regret when the budget is relatively limited, it no longer suffices to achieve optimal regret when the budget is relatively large.", "url": "https://arxiv.org/abs/2306.09588"}, {"metadata": {"arXiv": "2306.09612", "Date": "Fri, 16 Jun 2023 04:05:58 ", "Title": "GraphSHA: Synthesizing Harder Samples for Class-Imbalanced Node Classification", "Authors": ["Wen-Zhi Li", "Chang-Dong Wang", "Hui Xiong", "Jian-Huang Lai"], "Categories": "cs.LG", "Comments": ["Accepted to KDD 2023 Research Track"]}, "abstract": "Class imbalance is the phenomenon that some classes have much fewer instances than others, which is ubiquitous in real-world graph-structured scenarios. Recent studies find that off-the-shelf Graph Neural Networks (GNNs) would under-represent minor class samples. We investigate this phenomenon and discover that the subspaces of minor classes being squeezed by those of the major ones in the latent space is the main cause of this failure. We are naturally inspired to enlarge the decision boundaries of minor classes and propose a general framework GraphSHA by Synthesizing HArder minor samples. Furthermore, to avoid the enlarged minor boundary violating the subspaces of neighbor classes, we also propose a module called SemiMixup to transmit enlarged boundary information to the interior of the minor classes while blocking information propagation from minor classes to neighbor classes. Empirically, GraphSHA shows its effectiveness in enlarging the decision boundaries of minor classes, as it outperforms various baseline methods in class-imbalanced node classification with different GNN backbone encoders over seven public benchmark datasets. Code is avilable at https://github.com/wenzhilics/GraphSHA.", "url": "https://arxiv.org/abs/2306.09612"}, {"metadata": {"arXiv": "2306.09614", "Date": "Fri, 16 Jun 2023 04:06:52 ", "Title": "HomoGCL: Rethinking Homophily in Graph Contrastive Learning", "Authors": ["Wen-Zhi Li", "Chang-Dong Wang", "Hui Xiong", "Jian-Huang Lai"], "Categories": "cs.LG cs.SI", "Comments": ["Accepted to KDD 2023 Research Track"]}, "abstract": "Contrastive learning (CL) has become the de-facto learning paradigm in self-supervised learning on graphs, which generally follows the \"augmenting-contrasting\" learning scheme. However, we observe that unlike CL in computer vision domain, CL in graph domain performs decently even without augmentation. We conduct a systematic analysis of this phenomenon and argue that homophily, i.e., the principle that \"like attracts like\", plays a key role in the success of graph CL. Inspired to leverage this property explicitly, we propose HomoGCL, a model-agnostic framework to expand the positive set using neighbor nodes with neighbor-specific significances. Theoretically, HomoGCL introduces a stricter lower bound of the mutual information between raw node features and node embeddings in augmented views. Furthermore, HomoGCL can be combined with existing graph CL models in a plug-and-play way with light extra computational overhead. Extensive experiments demonstrate that HomoGCL yields multiple state-of-the-art results across six public datasets and consistently brings notable performance improvements when applied to various graph CL methods. Code is avilable at https://github.com/wenzhilics/HomoGCL.", "url": "https://arxiv.org/abs/2306.09614"}, {"metadata": {"arXiv": "2306.09618", "Date": "Fri, 16 Jun 2023 04:18:04 ", "Title": "Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions", "Authors": ["Mahyar Khayatkhoei", "Wael AbdAlmageed"], "Categories": "cs.LG cs.CV", "Comments": ["To appear in ICML 2023"]}, "abstract": "Precision and Recall are two prominent metrics of generative performance, which were proposed to separately measure the fidelity and diversity of generative models. Given their central role in comparing and improving generative models, understanding their limitations are crucially important. To that end, in this work, we identify a critical flaw in the common approximation of these metrics using k-nearest-neighbors, namely, that the very interpretations of fidelity and diversity that are assigned to Precision and Recall can fail in high dimensions, resulting in very misleading conclusions. Specifically, we empirically and theoretically show that as the number of dimensions grows, two model distributions with supports at equal point-wise distance from the support of the real distribution, can have vastly different Precision and Recall regardless of their respective distributions, hence an emergent asymmetry in high dimensions. Based on our theoretical insights, we then provide simple yet effective modifications to these metrics to construct symmetric metrics regardless of the number of dimensions. Finally, we provide experiments on real-world datasets to illustrate that the identified flaw is not merely a pathological case, and that our proposed metrics are effective in alleviating its impact.", "url": "https://arxiv.org/abs/2306.09618"}, {"metadata": {"arXiv": "2306.09623", "Date": "Fri, 16 Jun 2023 04:40:59 ", "Title": "From Hypergraph Energy Functions to Hypergraph Neural Networks", "Authors": ["Yuxin Wang", "Quan Gan", "Xipeng Qiu", "Xuanjing Huang", "David Wipf"], "Categories": "cs.LG", "Comments": ["Accepted to ICML 2023"]}, "abstract": "Hypergraphs are a powerful abstraction for representing higher-order interactions between entities of interest. To exploit these relationships in making downstream predictions, a variety of hypergraph neural network architectures have recently been proposed, in large part building upon precursors from the more traditional graph neural network (GNN) literature. Somewhat differently, in this paper we begin by presenting an expressive family of parameterized, hypergraph-regularized energy functions. We then demonstrate how minimizers of these energies effectively serve as node embeddings that, when paired with a parameterized classifier, can be trained end-to-end via a supervised bilevel optimization process. Later, we draw parallels between the implicit architecture of the predictive models emerging from the proposed bilevel hypergraph optimization, and existing GNN architectures in common use. Empirically, we demonstrate state-of-the-art results on various hypergraph node classification benchmarks. Code is available at https://github.com/yxzwang/PhenomNN.", "url": "https://arxiv.org/abs/2306.09623"}, {"metadata": {"arXiv": "2306.09633", "Date": "Fri, 16 Jun 2023 05:32:24 ", "Title": "The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement", "Authors": ["Igor L. Markov"], "Categories": "cs.LG cs.AR", "Comments": ["14 pages", "1 figure", "3 tables"]}, "abstract": "Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.", "url": "https://arxiv.org/abs/2306.09633"}, {"metadata": {"arXiv": "2306.09648", "Date": "Fri, 16 Jun 2023 06:47:47 ", "Title": "Learning CO$_2$ plume migration in faulted reservoirs with Graph Neural Networks", "Authors": ["Xin Ju", "Fran\\c{c}ois P. Hamon", "Gege Wen", "Rayan Kanfar", "Mauricio Araya-Polo", "Hamdi A. Tchelepi"], "Categories": "cs.LG physics.ao-ph"}, "abstract": "Deep-learning-based surrogate models provide an efficient complement to numerical simulations for subsurface flow problems such as CO$_2$ geological storage. Accurately capturing the impact of faults on CO$_2$ plume migration remains a challenge for many existing deep learning surrogate models based on Convolutional Neural Networks (CNNs) or Neural Operators. We address this challenge with a graph-based neural model leveraging recent developments in the field of Graph Neural Networks (GNNs). Our model combines graph-based convolution Long-Short-Term-Memory (GConvLSTM) with a one-step GNN model, MeshGraphNet (MGN), to operate on complex unstructured meshes and limit temporal error accumulation. We demonstrate that our approach can accurately predict the temporal evolution of gas saturation and pore pressure in a synthetic reservoir with impermeable faults. Our results exhibit a better accuracy and a reduced temporal error accumulation compared to the standard MGN model. We also show the excellent generalizability of our algorithm to mesh configurations, boundary conditions, and heterogeneous permeability fields not included in the training set. This work highlights the potential of GNN-based methods to accurately and rapidly model subsurface flow with complex faults and fractures.", "url": "https://arxiv.org/abs/2306.09648"}, {"metadata": {"arXiv": "2306.09656", "Date": "Fri, 16 Jun 2023 07:23:28 ", "Title": "Temporal Causal Mediation through a Point Process: Direct and Indirect Effects of Healthcare Interventions", "Authors": ["\\c{C}a\\u{g}lar H{\\i}zl{\\i}", "ST John", "Anne Juuti", "Tuure Saarinen", "Kirsi Pietil\\\"ainen", "Pekka Marttinen"], "Categories": "cs.LG stat.ME"}, "abstract": "Deciding on an appropriate intervention requires a causal model of a treatment, the outcome, and potential mediators. Causal mediation analysis lets us distinguish between direct and indirect effects of the intervention, but has mostly been studied in a static setting. In healthcare, data come in the form of complex, irregularly sampled time-series, with dynamic interdependencies between a treatment, outcomes, and mediators across time. Existing approaches to dynamic causal mediation analysis are limited to regular measurement intervals, simple parametric models, and disregard long-range mediator--outcome interactions. To address these limitations, we propose a non-parametric mediator--outcome model where the mediator is assumed to be a temporal point process that interacts with the outcome process. With this model, we estimate the direct and indirect effects of an external intervention on the outcome, showing how each of these affects the whole future trajectory. We demonstrate on semi-synthetic data that our method can accurately estimate direct and indirect effects. On real-world healthcare data, our model infers clinically meaningful direct and indirect effect trajectories for blood glucose after a surgery.", "url": "https://arxiv.org/abs/2306.09656"}, {"metadata": {"arXiv": "2306.09666", "Date": "Fri, 16 Jun 2023 07:45:32 ", "Title": "A Smooth Binary Mechanism for Efficient Private Continual Observation", "Authors": ["Joel Daniel Andersson", "Rasmus Pagh"], "Categories": "cs.LG cs.CR cs.DS"}, "abstract": "In privacy under continual observation we study how to release differentially private estimates based on a dataset that evolves over time. The problem of releasing private prefix sums of $x_1,x_2,x_3,\\dots \\in\\{0,1\\}$ (where the value of each $x_i$ is to be private) is particularly well-studied, and a generalized form is used in state-of-the-art methods for private stochastic gradient descent (SGD). The seminal binary mechanism privately releases the first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently, Henzinger et al. and Denisov et al. showed that it is possible to improve on the binary mechanism in two ways: The variance of the noise can be reduced by a (large) constant factor, and also made more even across time steps. However, their algorithms for generating the noise distribution are not as efficient as one would like in terms of computation time and (in particular) space. We address the efficiency problem by presenting a simple alternative to the binary mechanism in which 1) generating the noise takes constant average time per value, 2) the variance is reduced by a factor about 4 compared to the binary mechanism, and 3) the noise distribution at each step is identical. Empirically, a simple Python implementation of our approach outperforms the running time of the approach of Henzinger et al., as well as an attempt to improve their algorithm using high-performance algorithms for multiplication with Toeplitz matrices.", "url": "https://arxiv.org/abs/2306.09666"}, {"metadata": {"arXiv": "2306.09675", "Date": "Fri, 16 Jun 2023 08:13:41 ", "Title": "Multi-View Class Incremental Learning", "Authors": ["Depeng Li", "Tianqi Wang", "Junwei Chen", "Kenji Kawaguchi", "Cheng Lian", "Zhigang Zeng"], "Categories": "cs.LG cs.CV", "Comments": ["34 pages,4 figures. Under review"]}, "abstract": "Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation for learning-without-forgetting decision-making while encountering new classes. Extensive experiments on synthetic and real-world datasets validate the effectiveness of our approach.", "url": "https://arxiv.org/abs/2306.09675"}, {"metadata": {"arXiv": "2306.09702", "Date": "Fri, 16 Jun 2023 09:13:25 ", "Title": "A Hierarchical Bayesian Model for Deep Few-Shot Meta Learning", "Authors": ["Minyoung Kim and Timothy Hospedales"], "Categories": "cs.LG"}, "abstract": "We propose a novel hierarchical Bayesian model for learning with a large (possibly infinite) number of tasks/episodes, which suits well the few-shot meta learning problem. We consider episode-wise random variables to model episode-specific target generative processes, where these local random variables are governed by a higher-level global random variate. The global variable helps memorize the important information from historic episodes while controlling how much the model needs to be adapted to new episodes in a principled Bayesian manner. Within our model framework, the prediction on a novel episode/task can be seen as a Bayesian inference problem. However, a main obstacle in learning with a large/infinite number of local random variables in online nature, is that one is not allowed to store the posterior distribution of the current local random variable for frequent future updates, typical in conventional variational inference. We need to be able to treat each local variable as a one-time iterate in the optimization. We propose a Normal-Inverse-Wishart model, for which we show that this one-time iterate optimization becomes feasible due to the approximate closed-form solutions for the local posterior distributions. The resulting algorithm is more attractive than the MAML in that it is not required to maintain computational graphs for the whole gradient optimization steps per episode. Our approach is also different from existing Bayesian meta learning methods in that unlike dealing with a single random variable for the whole episodes, our approach has a hierarchical structure that allows one-time episodic optimization, desirable for principled Bayesian learning with many/infinite tasks. The code is available at \\url{https://github.com/minyoungkim21/niwmeta}.", "url": "https://arxiv.org/abs/2306.09702"}, {"metadata": {"arXiv": "2306.09705", "Date": "Fri, 16 Jun 2023 09:18:08 ", "Title": "Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent Networks vs. Recurrent Networks", "Authors": ["Gabriel Lopez", "Anna Nguyen", "Joe Kaul"], "Categories": "cs.LG cs.CL"}, "abstract": "Anticipating audience reaction towards a certain text is integral to several facets of society ranging from politics, research, and commercial industries. Sentiment analysis (SA) is a useful natural language processing (NLP) technique that utilizes lexical/statistical and deep learning methods to determine whether different-sized texts exhibit positive, negative, or neutral emotions. Recurrent networks are widely used in machine-learning communities for problems with sequential data. However, a drawback of models based on Long-Short Term Memory networks and Gated Recurrent Units is the significantly high number of parameters, and thus, such models are computationally expensive. This drawback is even more significant when the available data are limited. Also, such models require significant over-parameterization and regularization to achieve optimal performance. Tensorized models represent a potential solution. In this paper, we classify the sentiment of some social media posts. We compare traditional recurrent models with their tensorized version, and we show that with the tensorized models, we reach comparable performances with respect to the traditional models while using fewer resources for the training.", "url": "https://arxiv.org/abs/2306.09705"}, {"metadata": {"arXiv": "2306.09707", "Date": "Fri, 16 Jun 2023 09:18:36 ", "Title": "Representation and decomposition of functions in DAG-DNNs and structural network pruning", "Authors": ["Wen-Liang Hwang"], "Categories": "cs.LG"}, "abstract": "The conclusions provided by deep neural networks (DNNs) must be carefully scrutinized to determine whether they are universal or architecture dependent. The term DAG-DNN refers to a graphical representation of a DNN in which the architecture is expressed as a direct-acyclic graph (DAG), on which arcs are associated with functions. The level of a node denotes the maximum number of hops between the input node and the node of interest. In the current study, we demonstrate that DAG-DNNs can be used to derive all functions defined on various sub-architectures of the DNN. We also demonstrate that the functions defined in a DAG-DNN can be derived via a sequence of lower-triangular matrices, each of which provides the transition of functions defined in sub-graphs up to nodes at a specified level. The lifting structure associated with lower-triangular matrices makes it possible to perform the structural pruning of a network in a systematic manner. The fact that decomposition is universally applicable to all DNNs means that network pruning could theoretically be applied to any DNN, regardless of the underlying architecture. We demonstrate that it is possible to obtain the winning ticket (sub-network and initialization) for a weak version of the lottery ticket hypothesis, based on the fact that the sub-network with initialization can achieve training performance on par with that of the original network using the same number of iterations or fewer.", "url": "https://arxiv.org/abs/2306.09707"}, {"metadata": {"arXiv": "2306.09739", "Date": "Fri, 16 Jun 2023 10:16:59 ", "Title": "Stabilized Neural Differential Equations for Learning Constrained Dynamics", "Authors": ["Alistair White", "Niki Kilbertus", "Maximilian Gelbrecht", "Niklas Boers"], "Categories": "cs.LG physics.comp-ph stat.ML", "Comments": ["17 pages", "6 figures"]}, "abstract": "Many successful methods to learn dynamical systems from data have recently been introduced. However, assuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural ordinary differential equation (NODE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while extending the scope of which types of constraints can be incorporated into NODE training.", "url": "https://arxiv.org/abs/2306.09739"}, {"metadata": {"arXiv": "2306.09744", "Date": "Fri, 16 Jun 2023 10:20:10 ", "Title": "Automatic Trade-off Adaptation in Offline RL", "Authors": ["Phillip Swazinna", "Steffen Udluft", "Thomas Runkler"], "Categories": "cs.LG", "Comments": ["Oral Presentation @ ESANN 2023"]}, "abstract": "Recently, offline RL algorithms have been proposed that remain adaptive at runtime. For example, the LION algorithm \\cite{lion} provides the user with an interface to set the trade-off between behavior cloning and optimality w.r.t. the estimated return at runtime. Experts can then use this interface to adapt the policy behavior according to their preferences and find a good trade-off between conservatism and performance optimization. Since expert time is precious, we extend the methodology with an autopilot that automatically finds the correct parameterization of the trade-off, yielding a new algorithm which we term AutoLION.", "url": "https://arxiv.org/abs/2306.09744"}, {"metadata": {"arXiv": "2306.09775", "Date": "Fri, 16 Jun 2023 11:23:19 ", "Title": "Using Machine Learning Methods for Automation of Size Grid Building and Management", "Authors": ["Salim Yunus", "Dries Benoit and Filipa Peleja"], "Categories": "cs.LG", "ACM-class": "I.5.1; I.5.2"}, "abstract": "Fashion apparel companies require planning for the next season, a year in advance for supply chain management. This study focuses on size selection decision making for Levi Strauss. Currently, the region and planning group level size grids are built and managed manually. The company suffers from the workload it creates for sizing, merchant and planning teams. This research is aiming to answer two research questions: \"Which sizes should be available to the planners under each size grid name for the next season(s)?\" and \"Which sizes should be adopted for each planning group for the next season(s)?\". We approach to the problem with a classification model, which is one of the popular models used in machine learning. With this research, a more automated process was created by using machine learning techniques. A decrease in workload of the teams in the company is expected after it is put into practice. Unlike many studies in the state of art for fashion and apparel industry, this study focuses on sizes where the stock keeping unit represents a product with a certain size.", "url": "https://arxiv.org/abs/2306.09775"}, {"metadata": {"arXiv": "2306.09778", "Date": "Fri, 16 Jun 2023 11:30:55 ", "Title": "Gradient is All You Need?", "Authors": ["Konstantin Riedl", "Timo Klock", "Carina Geldhauser", "Massimo Fornasier"], "Categories": "cs.LG cs.NA math.NA math.OC stat.ML", "Comments": ["38 pages", "4 figures"]}, "abstract": "In this paper we provide a novel analytical perspective on the theoretical understanding of gradient-based learning algorithms by interpreting consensus-based optimization (CBO), a recently proposed multi-particle derivative-free optimization method, as a stochastic relaxation of gradient descent. Remarkably, we observe that through communication of the particles, CBO exhibits a stochastic gradient descent (SGD)-like behavior despite solely relying on evaluations of the objective function. The fundamental value of such link between CBO and SGD lies in the fact that CBO is provably globally convergent to global minimizers for ample classes of nonsmooth and nonconvex objective functions, hence, on the one side, offering a novel explanation for the success of stochastic relaxations of gradient descent. On the other side, contrary to the conventional wisdom for which zero-order methods ought to be inefficient or not to possess generalization abilities, our results unveil an intrinsic gradient descent nature of such heuristics. This viewpoint furthermore complements previous insights into the working principles of CBO, which describe the dynamics in the mean-field limit through a nonlinear nonlocal partial differential equation that allows to alleviate complexities of the nonconvex function landscape. Our proofs leverage a completely nonsmooth analysis, which combines a novel quantitative version of the Laplace principle (log-sum-exp trick) and the minimizing movement scheme (proximal iteration). In doing so, we furnish useful and precise insights that explain how stochastic perturbations of gradient descent overcome energy barriers and reach deep levels of nonconvex functions. Instructive numerical illustrations support the provided theoretical insights.", "url": "https://arxiv.org/abs/2306.09778"}, {"metadata": {"arXiv": "2306.09780", "Date": "Fri, 16 Jun 2023 11:33:47 ", "Title": "Understanding Deep Generative Models with Generalized Empirical Likelihoods", "Authors": ["Suman Ravuri", "M\\'elanie Rey", "Shakir Mohamed", "Marc Deisenroth"], "Categories": "cs.LG cs.CV", "Comments": ["Computer Vision and Pattern Recognition 2023 (Highlight", "top 2.6% of submissions)"]}, "abstract": "Understanding how well a deep generative model captures a distribution of high-dimensional data remains an important open challenge. It is especially difficult for certain model classes, such as Generative Adversarial Networks and Diffusion Models, whose models do not admit exact likelihoods. In this work, we demonstrate that generalized empirical likelihood (GEL) methods offer a family of diagnostic tools that can identify many deficiencies of deep generative models (DGMs). We show, with appropriate specification of moment conditions, that the proposed method can identify which modes have been dropped, the degree to which DGMs are mode imbalanced, and whether DGMs sufficiently capture intra-class diversity. We show how to combine techniques from Maximum Mean Discrepancy and Generalized Empirical Likelihood to create not only distribution tests that retain per-sample interpretability, but also metrics that include label information. We find that such tests predict the degree of mode dropping and mode imbalance up to 60% better than metrics such as improved precision/recall.", "url": "https://arxiv.org/abs/2306.09780"}, {"metadata": {"arXiv": "2306.09789", "Date": "Fri, 16 Jun 2023 11:59:18 ", "Title": "Dynamic Decision Tree Ensembles for Energy-Efficient Inference on IoT Edge Nodes", "Authors": ["Francesco Daghero", "Alessio Burrello", "Enrico Macii", "Paolo Montuschi", "Massimo Poncino and Daniele Jahier Pagliari"], "Categories": "cs.LG", "Comments": ["This article has been accepted for publication in IEEE Internet of Things Journal"], "DOI": "10.1109/JIOT.2023.3286276"}, "abstract": "With the increasing popularity of Internet of Things (IoT) devices, there is a growing need for energy-efficient Machine Learning (ML) models that can run on constrained edge nodes. Decision tree ensembles, such as Random Forests (RFs) and Gradient Boosting (GBTs), are particularly suited for this task, given their relatively low complexity compared to other alternatives. However, their inference time and energy costs are still significant for edge hardware. Given that said costs grow linearly with the ensemble size, this paper proposes the use of dynamic ensembles, that adjust the number of executed trees based both on a latency/energy target and on the complexity of the processed input, to trade-off computational cost and accuracy. We focus on deploying these algorithms on multi-core low-power IoT devices, designing a tool that automatically converts a Python ensemble into optimized C code, and exploring several optimizations that account for the available parallelism and memory hierarchy. We extensively benchmark both static and dynamic RFs and GBTs on three state-of-the-art IoT-relevant datasets, using an 8-core ultra-lowpower System-on-Chip (SoC), GAP8, as the target platform. Thanks to the proposed early-stopping mechanisms, we achieve an energy reduction of up to 37.9% with respect to static GBTs (8.82 uJ vs 14.20 uJ per inference) and 41.7% with respect to static RFs (2.86 uJ vs 4.90 uJ per inference), without losing accuracy compared to the static model.", "url": "https://arxiv.org/abs/2306.09789"}, {"metadata": {"arXiv": "2306.09792", "Date": "Fri, 16 Jun 2023 12:03:39 ", "Title": "GPINN: Physics-informed Neural Network with Graph Embedding", "Authors": ["Yuyang Miao", "Haolin Li"], "Categories": "cs.LG cs.CE physics.comp-ph"}, "abstract": "This work proposes a Physics-informed Neural Network framework with Graph Embedding (GPINN) to perform PINN in graph, i.e. topological space instead of traditional Euclidean space, for improved problem-solving efficiency. The method integrates topological data into the neural network's computations, which significantly boosts the performance of the Physics-Informed Neural Network (PINN). The graph embedding technique infuses extra dimensions into the input space to encapsulate the spatial characteristics of a graph while preserving the properties of the original space. The selection of these extra dimensions is guided by the Fiedler vector, offering an optimised pathologic notation of the graph. Two case studies are conducted, which demonstrate significant improvement in the performance of GPINN in comparison to traditional PINN, particularly in its superior ability to capture physical features of the solution.", "url": "https://arxiv.org/abs/2306.09792"}, {"metadata": {"arXiv": "2306.09800", "Date": "Fri, 16 Jun 2023 12:19:32 ", "Title": "$\\pi2\\text{vec}$: Policy Representations with Successor Features", "Authors": ["Gianluca Scarpellini", "Ksenia Konyushkova", "Claudio Fantacci", "Tom Le Paine", "Yutian Chen", "Misha Denil"], "Categories": "cs.LG cs.RO"}, "abstract": "This paper describes $\\pi2\\text{vec}$, a method for representing behaviors of black box policies as feature vectors. The policy representations capture how the statistics of foundation model features change in response to the policy behavior in a task agnostic way, and can be trained from offline data, allowing them to be used in offline policy selection. This work provides a key piece of a recipe for fusing together three modern lines of research: Offline policy evaluation as a counterpart to offline RL, foundation models as generic and powerful state representations, and efficient policy selection in resource constrained environments.", "url": "https://arxiv.org/abs/2306.09800"}, {"metadata": {"arXiv": "2306.09803", "Date": "Fri, 16 Jun 2023 12:38:33 ", "Title": "Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization", "Authors": ["Kamil Dreczkowski", "Antoine Grosnit", "Haitham Bou Ammar"], "Categories": "cs.LG"}, "abstract": "This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively. To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 47 novel MCBO algorithms and benchmark them against seven existing MCBO solvers and five standard black-box optimization algorithms on ten tasks, conducting over 4000 experiments. Our findings reveal a superior combination of MCBO primitives outperforming existing approaches and illustrate the significance of model fit and the use of a trust region. We make our MCBO library available under the MIT license at \\url{https://github.com/huawei-noah/HEBO/tree/master/MCBO}.", "url": "https://arxiv.org/abs/2306.09803"}, {"metadata": {"arXiv": "2306.09805", "Date": "Fri, 16 Jun 2023 12:43:47 ", "Title": "Sample-Efficient On-Policy Imitation Learning from Observations", "Authors": ["Jo\\~ao A. C\\^andido Ramos", "Lionel Blond\\'e", "Naoya Takeishi and Alexandros Kalousis"], "Categories": "cs.LG"}, "abstract": "Imitation learning from demonstrations (ILD) aims to alleviate numerous shortcomings of reinforcement learning through the use of demonstrations. However, in most real-world applications, expert action guidance is absent, making the use of ILD impossible. Instead, we consider imitation learning from observations (ILO), where no expert actions are provided, making it a significantly more challenging problem to address. Existing methods often employ on-policy learning, which is known to be sample-costly. This paper presents SEILO, a novel sample-efficient on-policy algorithm for ILO, that combines standard adversarial imitation learning with inverse dynamics modeling. This approach enables the agent to receive feedback from both the adversarial procedure and a behavior cloning loss. We empirically demonstrate that our proposed algorithm requires fewer interactions with the environment to achieve expert performance compared to other state-of-the-art on-policy ILO and ILD methods.", "url": "https://arxiv.org/abs/2306.09805"}, {"metadata": {"arXiv": "2306.09827", "Date": "Fri, 16 Jun 2023 13:11:15 ", "Title": "Building Blocks for a Complex-Valued Transformer Architecture", "Authors": ["Florian Eilers and Xiaoyi Jiang"], "Categories": "cs.LG cs.CV cs.NE", "Comments": ["ICASSP 2023-2023 IEEE International Conference on Acoustics", "Speech and Signal Processing (ICASSP). IEEE", "2023"], "DOI": "10.1109/ICASSP49357.2023.10095349"}, "abstract": "Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into $\\mathbb{R}^2$. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.", "url": "https://arxiv.org/abs/2306.09827"}, {"metadata": {"arXiv": "2306.09844", "Date": "Fri, 16 Jun 2023 13:41:24 ", "Title": "Wasserstein distributional robustness of neural networks", "Authors": ["Xingjian Bai", "Guangyi He", "Yifan Jiang", "Jan Obloj"], "Categories": "cs.LG cs.CV math.OC math.PR", "Comments": ["23 pages", "6 figures", "8 tables"]}, "abstract": "Deep neural networks are known to be vulnerable to adversarial attacks (AA). For an image recognition task, this means that a small perturbation of the original can result in the image being misclassified. Design of such attacks as well as methods of adversarial training against them are subject of intense research. We re-cast the problem using techniques of Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. We consider a set of distributional threat models. Unlike the traditional pointwise attacks, which assume a uniform bound on perturbation of each input data point, distributional threat models allow attackers to perturb inputs in a non-uniform way. We link these more general attacks with questions of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, we propose a first-order AA algorithm and its multi-step version. Our attack algorithms include Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) as special cases. Furthermore, we provide a new asymptotic estimate of the adversarial accuracy against distributional threat models. The bound is fast to compute and first-order accurate, offering new insights even for the pointwise AA. It also naturally yields out-of-sample performance guarantees. We conduct numerical experiments on the CIFAR-10 dataset using DNNs on RobustBench to illustrate our theoretical results. Our code is available at https://github.com/JanObloj/W-DRO-Adversarial-Methods.", "url": "https://arxiv.org/abs/2306.09844"}, {"metadata": {"arXiv": "2306.09850", "Date": "Fri, 16 Jun 2023 13:47:04 ", "Title": "Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima", "Authors": ["Dongkuk Si", "Chulhee Yun"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["39 pages"]}, "abstract": "Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \\rho \\frac{\\nabla f(x_t)}{\\lVert \\nabla f(x_t) \\rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\\tilde \\Theta(\\frac{1}{T^2})$, the convergence bound of stochastic SAM suffers an inevitable additive term $O(\\rho^2)$, indicating convergence only up to neighborhoods of optima. In fact, such $O(\\rho^2)$ factors arise for stochastic SAM in all the settings we consider, and also for deterministic SAM in nonconvex cases; importantly, we prove by examples that such terms are unavoidable. Our results highlight vastly different characteristics of SAM with vs. without decaying perturbation size or gradient normalization, and suggest that the intuitions gained from one version may not apply to the other.", "url": "https://arxiv.org/abs/2306.09850"}, {"metadata": {"arXiv": "2306.09863", "Date": "Fri, 16 Jun 2023 14:18:47 ", "Title": "Transferability of Winning Lottery Tickets in Neural Network Differential Equation Solvers", "Authors": ["Edward Prideaux-Ghee"], "Categories": "cs.LG"}, "abstract": "Recent work has shown that renormalisation group theory is a useful framework with which to describe the process of pruning neural networks via iterative magnitude pruning. This report formally describes the link between RG theory and IMP and extends previous results around the Lottery Ticket Hypothesis and Elastic Lottery Hypothesis to Hamiltonian Neural Networks for solving differential equations. We find lottery tickets for two Hamiltonian Neural Networks and demonstrate transferability between the two systems, with accuracy being dependent on integration times. The universality of the two systems is then analysed using tools from an RG perspective.", "url": "https://arxiv.org/abs/2306.09863"}, {"metadata": {"arXiv": "2306.09882", "Date": "Fri, 16 Jun 2023 14:50:43 ", "Title": "Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction", "Authors": ["Xinke Jiang", "Dingyi Zhuang", "Xianghui Zhang", "Hao Chen", "Jiayuan Luo", "Xiaowei Gao"], "Categories": "cs.LG stat.ML stat.OT", "Comments": ["In submission to CIKM 2023"]}, "abstract": "crucial for transportation management. However, traditional spatial-temporal deep learning models grapple with addressing the sparse and long-tail characteristics in high-resolution O-D matrices and quantifying prediction uncertainty. This dilemma arises from the numerous zeros and over-dispersed demand patterns within these matrices, which challenge the Gaussian assumption inherent to deterministic deep learning models. To address these challenges, we propose a novel approach: the Spatial-Temporal Tweedie Graph Neural Network (STTD). The STTD introduces the Tweedie distribution as a compelling alternative to the traditional 'zero-inflated' model and leverages spatial and temporal embeddings to parameterize travel demand distributions. Our evaluations using real-world datasets highlight STTD's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.", "url": "https://arxiv.org/abs/2306.09882"}, {"metadata": {"arXiv": "2306.09890", "Date": "Fri, 16 Jun 2023 14:59:17 ", "Title": "Studying Generalization on Memory-Based Methods in Continual Learning", "Authors": ["Felipe del Rio", "Julio Hurtado", "Cristian Buc", "Alvaro Soto and Vincenzo Lomonaco"], "Categories": "cs.LG"}, "abstract": "One of the objectives of Continual Learning is to learn new concepts continually over a stream of experiences and at the same time avoid catastrophic forgetting. To mitigate complete knowledge overwriting, memory-based methods store a percentage of previous data distributions to be used during training. Although these methods produce good results, few studies have tested their out-of-distribution generalization properties, as well as whether these methods overfit the replay memory. In this work, we show that although these methods can help in traditional in-distribution generalization, they can strongly impair out-of-distribution generalization by learning spurious features and correlations. Using a controlled environment, the Synbol benchmark generator (Lacoste et al., 2020), we demonstrate that this lack of out-of-distribution generalization mainly occurs in the linear classifier.", "url": "https://arxiv.org/abs/2306.09890"}, {"metadata": {"arXiv": "2306.09910", "Date": "Fri, 16 Jun 2023 15:36:49 ", "Title": "LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning", "Authors": ["Jifan Zhang", "Yifang Chen", "Gregory Canal", "Stephen Mussmann", "Yinglun Zhu", "Simon Shaolei Du", "Kevin Jamieson", "Robert D Nowak"], "Categories": "cs.LG"}, "abstract": "Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in active learning. LabelBench's modular codebase is open-sourced for the broader community to contribute label-efficient learning methods and benchmarks. The repository can be found at: https://github.com/EfficientTraining/LabelBench.", "url": "https://arxiv.org/abs/2306.09910"}, {"metadata": {"arXiv": "2306.09912", "Date": "Fri, 16 Jun 2023 15:40:21 ", "Title": "Towards Quantum Federated Learning", "Authors": ["Chao Ren", "Han Yu", "Rudai Yan", "Minrui Xu", "Yuan Shen", "Huihui Zhu", "Dusit Niyato", "Zhao Yang Dong", "Leong Chuan Kwek"], "Categories": "cs.LG quant-ph", "Comments": ["Survey of quantum federated learning (QFL)"]}, "abstract": "Quantum Federated Learning (QFL) is an emerging interdisciplinary field that merges the principles of Quantum Computing (QC) and Federated Learning (FL), with the goal of leveraging quantum technologies to enhance privacy, security, and efficiency in the learning process. Currently, there is no comprehensive survey for this interdisciplinary field. This review offers a thorough, holistic examination of QFL. We aim to provide a comprehensive understanding of the principles, techniques, and emerging applications of QFL. We discuss the current state of research in this rapidly evolving field, identify challenges and opportunities associated with integrating these technologies, and outline future directions and open research questions. We propose a unique taxonomy of QFL techniques, categorized according to their characteristics and the quantum techniques employed. As the field of QFL continues to progress, we can anticipate further breakthroughs and applications across various industries, driving innovation and addressing challenges related to data privacy, security, and resource optimization. This review serves as a first-of-its-kind comprehensive guide for researchers and practitioners interested in understanding and advancing the field of QFL.", "url": "https://arxiv.org/abs/2306.09912"}, {"metadata": {"arXiv": "2306.09935", "Date": "Fri, 16 Jun 2023 16:05:34 ", "Title": "Drag-guided diffusion models for vehicle image generation", "Authors": ["Nikos Arechiga", "Frank Permenter", "Binyang Song", "Chenyang Yuan"], "Categories": "cs.LG cs.CV cs.GR"}, "abstract": "Denoising diffusion models trained at web-scale have revolutionized image generation. The application of these tools to engineering design is an intriguing possibility, but is currently limited by their inability to parse and enforce concrete engineering constraints. In this paper, we take a step towards this goal by proposing physics-based guidance, which enables optimization of a performance metric (as predicted by a surrogate model) during the generation process. As a proof-of-concept, we add drag guidance to Stable Diffusion, which allows this tool to generate images of novel vehicles while simultaneously minimizing their predicted drag coefficients.", "url": "https://arxiv.org/abs/2306.09935"}, {"metadata": {"arXiv": "2306.09951", "Date": "Fri, 16 Jun 2023 16:32:27 ", "Title": "You Don't Need Robust Machine Learning to Manage Adversarial Attack Risks", "Authors": ["Edward Raff", "Michel Benaroch", "Andrew L. Farris"], "Categories": "cs.LG stat.ML"}, "abstract": "The robustness of modern machine learning (ML) models has become an increasing concern within the community. The ability to subvert a model into making errant predictions using seemingly inconsequential changes to input is startling, as is our lack of success in building models robust to this concern. Existing research shows progress, but current mitigations come with a high cost and simultaneously reduce the model's accuracy. However, such trade-offs may not be necessary when other design choices could subvert the risk. In this survey we review the current literature on attacks and their real-world occurrences, or limited evidence thereof, to critically evaluate the real-world risks of adversarial machine learning (AML) for the average entity. This is done with an eye toward how one would then mitigate these attacks in practice, the risks for production deployment, and how those risks could be managed. In doing so we elucidate that many AML threats do not warrant the cost and trade-offs of robustness due to a low likelihood of attack or availability of superior non-ML mitigations. Our analysis also recommends cases where an actor should be concerned about AML to the degree where robust ML models are necessary for a complete deployment.", "url": "https://arxiv.org/abs/2306.09951"}, {"metadata": {"arXiv": "2306.09955", "Date": "Fri, 16 Jun 2023 16:40:04 ", "Title": "Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?", "Authors": ["Erin George", "Michael Murray", "William Swartworth", "Deanna Needell"], "Categories": "cs.LG", "Comments": ["48 pages", "2 figures", "1 table"]}, "abstract": "We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. In particular, we consider linearly separable data for which a relatively small proportion of labels are corrupted or flipped. We identify conditions on the margin of the clean data that give rise to three distinct training outcomes: benign overfitting, in which zero loss is achieved and with high probability test data is classified correctly; overfitting, in which zero loss is achieved but test data is misclassified with probability lower bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve zero loss and again with high probability test data is classified correctly. Our analysis provides a fine-grained description of the dynamics of neurons throughout training and reveals two distinct phases: in the first phase clean points achieve close to zero loss, in the second phase clean points oscillate on the boundary of zero loss while corrupt points either converge towards zero loss or are eventually zeroed by the network. We prove these results using a combinatorial approach that involves bounding the number of clean versus corrupt updates across these phases of training.", "url": "https://arxiv.org/abs/2306.09955"}, {"metadata": {"arXiv": "2306.09973", "Date": "Fri, 16 Jun 2023 17:11:55 ", "Title": "Enhancing Fault Resilience of QNNs by Selective Neuron Splitting", "Authors": ["Mohammad Hasan Ahmadilivani", "Mahdi Taheri", "Jaan Raik", "Masoud Daneshtalab", "and Maksim Jenihhin"], "Categories": "cs.LG cs.AR cs.NE", "Comments": ["5 pages", "4 figures", "1 table. The paper is accepted at the AICAS'23 conference"]}, "abstract": "The superior performance of Deep Neural Networks (DNNs) has led to their application in various aspects of human life. Safety-critical applications are no exception and impose rigorous reliability requirements on DNNs. Quantized Neural Networks (QNNs) have emerged to tackle the complexity of DNN accelerators, however, they are more prone to reliability issues. In this paper, a recent analytical resilience assessment method is adapted for QNNs to identify critical neurons based on a Neuron Vulnerability Factor (NVF). Thereafter, a novel method for splitting the critical neurons is proposed that enables the design of a Lightweight Correction Unit (LCU) in the accelerator without redesigning its computational part. The method is validated by experiments on different QNNs and datasets. The results demonstrate that the proposed method for correcting the faults has a twice smaller overhead than a selective Triple Modular Redundancy (TMR) while achieving a similar level of fault resiliency.", "url": "https://arxiv.org/abs/2306.09973"}, {"metadata": {"arXiv": "2306.10015", "Date": "Fri, 16 Jun 2023 17:59:51 ", "Title": "Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness", "Authors": ["Eric Zelikman", "Qian Huang", "Percy Liang", "Nick Haber", "Noah D. Goodman"], "Categories": "cs.LG cs.CL cs.DC"}, "abstract": "Language model training in distributed settings is limited by the communication cost of gradient exchanges. In this short note, we extend recent work from Malladi et al. (2023), using shared randomness to perform distributed fine-tuning with low bandwidth. The method is a natural decentralized extension of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA). Each iteration, each machine seeds a Random Number Generator (RNG) to perform local reproducible perturbations on model weights and calculate and exchange scalar projected gradients, which are then used to update each model. By using a (machine, sample) identifier as the random seed, each model can regenerate one another's perturbations. As machines only exchange single-byte projected gradients, this is highly communication efficient. There are also potential privacy benefits, as projected gradients may be calculated on different training data, and models never access the other's data. Our approach not only drastically reduces communication bandwidth requirements but also accommodates dynamic addition or removal of machines during the training process and retains the memory-efficient and inference-only advantages of recent work. We perform proof-of-concept experiments to demonstrate the potential usefulness of this method, building off of rich literature on distributed optimization and memory-efficient training.", "url": "https://arxiv.org/abs/2306.10015"}, {"metadata": {"arXiv": "2306.10007", "Date": "Fri, 16 Jun 2023 17:58:10 ", "Title": "Robot Learning with Sensorimotor Pre-training", "Authors": ["Ilija Radosavovic", "Baifeng Shi", "Letian Fu", "Ken Goldberg", "Trevor Darrell", "Jitendra Malik"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["Project page: https://robotic-pretrained-transformer.github.io"]}, "abstract": "We present a self-supervised sensorimotor pre-training approach for robotics. Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given a sequence of camera images, proprioceptive robot states, and past actions, we encode the interleaved sequence into tokens, mask out a random subset, and train a model to predict the masked-out content. We hypothesize that if the robot can predict the missing content it has acquired a good model of the physical world that can enable it to act. RPT is designed to operate on latent visual representations which makes prediction tractable, enables scaling to 10x larger models, and 10 Hz inference on a real robot. To evaluate our approach, we collect a dataset of 20,000 real-world trajectories over 9 months using a combination of motion planning and model-based grasping algorithms. We find that pre-training on this data consistently outperforms training from scratch, leads to 2x improvements in the block stacking task, and has favorable scaling properties.", "url": "https://arxiv.org/abs/2306.10007"}, {"metadata": {"arXiv": "2306.09920", "Date": "Wed, 14 Jun 2023 07:10:12 ", "Title": "Feeding control and water quality monitoring in aquaculture systems: Opportunities and challenges", "Authors": ["Fahad Aljehani", "Ibrahima N'Doye", "Taous-Meriem Laleg-Kirati"], "Categories": "eess.SY cs.LG cs.SY"}, "abstract": "Aquaculture systems can benefit from the recent development of advanced control strategies to reduce operating costs and fish loss and increase growth production efficiency, resulting in fish welfare and health. Monitoring the water quality and controlling feeding are fundamental elements of balancing fish productivity and shaping the fish growth process. Currently, most fish-feeding processes are conducted manually in different phases and rely on time-consuming and challenging artificial discrimination. The feeding control approach influences fish growth and breeding through the feed conversion rate; hence, controlling these feeding parameters is crucial for enhancing fish welfare and minimizing general fishery costs. The high concentration of environmental factors, such as a high ammonia concentration and pH, affect the water quality and fish survival. Therefore, there is a critical need to develop control strategies to determine optimal, efficient, and reliable feeding processes and monitor water quality. This paper reviews the main control design techniques for fish growth in aquaculture systems, namely algorithms that optimize the feeding and water quality of a dynamic fish growth process. Specifically, we review model-based control approaches and model-free reinforcement learning strategies to optimize the growth and survival of the fish or track a desired reference live-weight growth trajectory. The model-free framework uses an approximate fish growth dynamic model and does not satisfy constraints. We discuss how model-based approaches can support a reinforcement learning framework to efficiently handle constraint satisfaction and find better trajectories and policies from value-based reinforcement learning.", "url": "https://arxiv.org/abs/2306.09920"}, {"metadata": {"arXiv": "2306.09509", "Date": "Thu, 15 Jun 2023 21:06:54 ", "Title": "Granger-Causal Hierarchical Skill Discovery", "Authors": ["Caleb Chuck", "Kevin Black", "Aditya Arjun", "Yuke Zhu", "Scott Niekum"], "Categories": "cs.AI cs.RO", "Comments": ["Under Submission"]}, "abstract": "Reinforcement Learning (RL) has shown promising results learning policies for complex tasks, but can often suffer from low sample efficiency and limited transfer. We introduce the Hierarchy of Interaction Skills (HIntS) algorithm, which uses learned interaction detectors to discover and train a hierarchy of skills that manipulate factors in factored environments. Inspired by Granger causality, these unsupervised detectors capture key events between factors to sample efficiently learn useful skills and transfer those skills to other related tasks -- tasks where many reinforcement learning techniques struggle. We evaluate HIntS on a robotic pushing task with obstacles -- a challenging domain where other RL and HRL methods fall short. The learned skills not only demonstrate transfer using variants of Breakout, a common RL benchmark, but also show 2-3x improvement in both sample efficiency and final performance compared to comparable RL baselines. Together, HIntS demonstrates a proof of concept for using Granger-causal relationships for skill discovery.", "url": "https://arxiv.org/abs/2306.09509"}, {"metadata": {"arXiv": "2306.09538", "Date": "Thu, 15 Jun 2023 22:47:01 ", "Title": "Graph Extraction for Assisting Crash Simulation Data Analysis", "Authors": ["Anahita Pakiman", "Jochen Garcke", "Axel Schumacher"], "Categories": "cs.AI", "Comments": ["Graph-Based Representation and Reasoning: 28th International Conference on Conceptual Structures", "ICCS 2023", "Berlin", "Germany", "September 11--13", "2023", "Proceedings", "book title to be confirmed"]}, "abstract": "In this work, we establish a method for abstracting information from Computer Aided Engineering (CAE) into graphs. Such graph representations of CAE data can improve design guidelines and support recommendation systems by enabling the comparison of simulations, highlighting unexplored experimental designs, and correlating different designs. We focus on the load-path in crashworthiness analysis, a complex sub-discipline in vehicle design. The load-path is the sequence of parts that absorb most of the energy caused by the impact. To detect the load-path, we generate a directed weighted graph from the CAE data. The vertices represent the vehicle's parts, and the edges are an abstraction of the connectivity of the parts. The edge direction follows the temporal occurrence of the collision, where the edge weights reflect aspects of the energy absorption. We introduce and assess three methods for graph extraction and an additional method for further updating each graph with the sequences of absorption. Based on longest-path calculations, we introduce an automated detection of the load-path, which we analyse for the different graph extraction methods and weights. Finally, we show how our method for the detection of load-paths helps in the classification and labelling of CAE simulations.", "url": "https://arxiv.org/abs/2306.09538"}, {"metadata": {"arXiv": "2306.09966", "Date": "Fri, 16 Jun 2023 16:50:54 ", "Title": "Data-Driven Model Discrimination of Switched Nonlinear Systems with Temporal Logic Inference", "Authors": ["Zeyuan Jin", "Nasim Baharisangari", "Zhe Xu", "and Sze Zheng Yong"], "Categories": "cs.AI"}, "abstract": "This paper addresses the problem of data-driven model discrimination for unknown switched systems with unknown linear temporal logic (LTL) specifications, representing tasks, that govern their mode sequences, where only sampled data of the unknown dynamics and tasks are available. To tackle this problem, we propose data-driven methods to over-approximate the unknown dynamics and to infer the unknown specifications such that both set-membership models of the unknown dynamics and LTL formulas are guaranteed to include the ground truth model and specification/task. Moreover, we present an optimization-based algorithm for analyzing the distinguishability of a set of learned/inferred model-task pairs as well as a model discrimination algorithm for ruling out model-task pairs from this set that are inconsistent with new observations at run time. Further, we present an approach for reducing the size of inferred specifications to increase the computational efficiency of the model discrimination algorithms.", "url": "https://arxiv.org/abs/2306.09966"}, {"metadata": {"arXiv": "2306.09482", "Date": "Thu, 15 Jun 2023 20:24:30 ", "Title": "Sample-Efficient Learning of Novel Visual Concepts", "Authors": ["Sarthak Bhagat", "Simon Stepputtis", "Joseph Campbell", "Katia Sycara"], "Categories": "cs.CV cs.AI"}, "abstract": "Despite the advances made in visual object recognition, state-of-the-art deep learning models struggle to effectively recognize novel objects in a few-shot setting where only a limited number of examples are provided. Unlike humans who excel at such tasks, these models often fail to leverage known relationships between entities in order to draw conclusions about such objects. In this work, we show that incorporating a symbolic knowledge graph into a state-of-the-art recognition model enables a new approach for effective few-shot classification. In our proposed neuro-symbolic architecture and training methodology, the knowledge graph is augmented with additional relationships extracted from a small set of examples, improving its ability to recognize novel objects by considering the presence of interconnected entities. Unlike existing few-shot classifiers, we show that this enables our model to incorporate not only objects but also abstract concepts and affordances. The existence of the knowledge graph also makes this approach amenable to interpretability through analysis of the relationships contained within it. We empirically show that our approach outperforms current state-of-the-art few-shot multi-label classification methods on the COCO dataset and evaluate the addition of abstract concepts and affordances on the Visual Genome dataset.", "url": "https://arxiv.org/abs/2306.09482"}, {"metadata": {"arXiv": "2306.09489", "Date": "Thu, 15 Jun 2023 20:34:43 ", "Title": "The 2023 Video Similarity Dataset and Challenge", "Authors": ["Ed Pizzi and Giorgos Kordopatis-Zilos and Hiral Patel and Gheorghe Postelnicu and Sugosh Nagavara Ravindra and Akshay Gupta and Symeon Papadopoulos and Giorgos Tolias and Matthijs Douze"], "Categories": "cs.CV cs.AI cs.MM"}, "abstract": "This work introduces a dataset, benchmark, and challenge for the problem of video copy detection and localization. The problem comprises two distinct but related tasks: determining whether a query video shares content with a reference video (\"detection\"), and additionally temporally localizing the shared content within each video (\"localization\"). The benchmark is designed to evaluate methods on these two tasks, and simulates a realistic needle-in-haystack setting, where the majority of both query and reference videos are \"distractors\" containing no copied content. We propose a metric that reflects both detection and localization accuracy. The associated challenge consists of two corresponding tracks, each with restrictions that reflect real-world settings. We provide implementation code for evaluation and baselines. We also analyze the results and methods of the top submissions to the challenge. The dataset, baseline methods and evaluation code is publicly available and will be discussed at a dedicated CVPR'23 workshop.", "url": "https://arxiv.org/abs/2306.09489"}, {"metadata": {"arXiv": "2306.09718", "Date": "Fri, 16 Jun 2023 09:37:16 ", "Title": "Label-noise-tolerant medical image classification via self-attention and self-supervised learning", "Authors": ["Hongyang Jiang", "Mengdi Gao", "Yan Hu", "Qiushi Ren", "Zhaoheng Xie", "Jiang Liu"], "Categories": "cs.CV cs.AI", "Comments": ["11pages", "8 figures"]}, "abstract": "Deep neural networks (DNNs) have been widely applied in medical image classification and achieve remarkable classification performance. These achievements heavily depend on large-scale accurately annotated training data. However, label noise is inevitably introduced in the medical image annotation, as the labeling process heavily relies on the expertise and experience of annotators. Meanwhile, DNNs suffer from overfitting noisy labels, degrading the performance of models. Therefore, in this work, we innovatively devise noise-robust training approach to mitigate the adverse effects of noisy labels in medical image classification. Specifically, we incorporate contrastive learning and intra-group attention mixup strategies into the vanilla supervised learning. The contrastive learning for feature extractor helps to enhance visual representation of DNNs. The intra-group attention mixup module constructs groups and assigns self-attention weights for group-wise samples, and subsequently interpolates massive noisy-suppressed samples through weighted mixup operation. We conduct comparative experiments on both synthetic and real-world noisy medical datasets under various noise levels. Rigorous experiments validate that our noise-robust method with contrastive learning and attention mixup can effectively handle with label noise, and is superior to state-of-the-art methods. An ablation study also shows that both components contribute to boost model performance. The proposed method demonstrates its capability of curb label noise and has certain potential toward real-world clinic applications.", "url": "https://arxiv.org/abs/2306.09718"}, {"metadata": {"arXiv": "2306.10001", "Date": "Fri, 16 Jun 2023 17:53:16 ", "Title": "Group Orthogonalization Regularization For Vision Models Adaptation and Robustness", "Authors": ["Yoav Kurtz", "Noga Bar", "Raja Giryes"], "Categories": "cs.CV cs.AI"}, "abstract": "As neural networks become deeper, the redundancy within their parameters increases. This phenomenon has led to several methods that attempt to reduce the correlation between convolutional filters. We propose a computationally efficient regularization technique that encourages orthonormality between groups of filters within the same layer. Our experiments show that when incorporated into recent adaptation methods for diffusion models and vision transformers (ViTs), this regularization improves performance on downstream tasks. We further show improved robustness when group orthogonality is enforced during adversarial training. Our code is available at https://github.com/YoavKurtz/GOR.", "url": "https://arxiv.org/abs/2306.10001"}, {"metadata": {"arXiv": "2306.10012", "Date": "Fri, 16 Jun 2023 17:58:58 ", "Title": "MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing", "Authors": ["Kai Zhang", "Lingbo Mo", "Wenhu Chen", "Huan Sun", "Yu Su"], "Categories": "cs.CV cs.AI cs.CL", "Comments": ["Website: https://osu-nlp-group.github.io/MagicBrush/"]}, "abstract": "Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triples (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate current image editing baselines from multiple dimensions including quantitative, qualitative, and human evaluations. The results reveal the challenging nature of our dataset and the gap between current baselines and real-world editing needs.", "url": "https://arxiv.org/abs/2306.10012"}, {"metadata": {"arXiv": "2306.10014", "Date": "Fri, 16 Jun 2023 17:59:38 ", "Title": "Coaching a Teachable Student", "Authors": ["Jimuyang Zhang", "Zanming Huang", "Eshed Ohn-Bar"], "Categories": "cs.CV cs.AI cs.RO", "Comments": ["Accepted by CVPR2023 (Highlight)"]}, "abstract": "We propose a novel knowledge distillation framework for effectively teaching a sensorimotor student agent to drive from the supervision of a privileged teacher agent. Current distillation for sensorimotor agents methods tend to result in suboptimal learned driving behavior by the student, which we hypothesize is due to inherent differences between the input, modeling capacity, and optimization processes of the two agents. We develop a novel distillation scheme that can address these limitations and close the gap between the sensorimotor agent and its privileged teacher. Our key insight is to design a student which learns to align their input features with the teacher's privileged Bird's Eye View (BEV) space. The student then can benefit from direct supervision by the teacher over the internal representation learning. To scaffold the difficult sensorimotor learning task, the student model is optimized via a student-paced coaching mechanism with various auxiliary supervision. We further propose a high-capacity imitation learned privileged agent that surpasses prior privileged agents in CARLA and ensures the student learns safe driving behavior. Our proposed sensorimotor agent results in a robust image-based behavior cloning agent in CARLA, improving over current models by over 20.6% in driving score without requiring LiDAR, historical observations, ensemble of models, on-policy data aggregation or reinforcement learning.", "url": "https://arxiv.org/abs/2306.10014"}, {"metadata": {"arXiv": "2306.09445", "Date": "Thu, 15 Jun 2023 18:55:48 ", "Title": "Understanding the Application of Utility Theory in Robotics and Artificial Intelligence: A Survey", "Authors": ["Qin Yang and Rui Liu"], "Categories": "cs.RO cs.AI cs.MA cs.NE cs.SY eess.SY", "Comments": ["Preprint"]}, "abstract": "As a unifying concept in economics, game theory, and operations research, even in the Robotics and AI field, the utility is used to evaluate the level of individual needs, preferences, and interests. Especially for decision-making and learning in multi-agent/robot systems (MAS/MRS), a suitable utility model can guide agents in choosing reasonable strategies to achieve their current needs and learning to cooperate and organize their behaviors, optimizing the system's utility, building stable and reliable relationships, and guaranteeing each group member's sustainable development, similar to the human society. Although these systems' complex, large-scale, and long-term behaviors are strongly determined by the fundamental characteristics of the underlying relationships, there has been less discussion on the theoretical aspects of mechanisms and the fields of applications in Robotics and AI. This paper introduces a utility-orient needs paradigm to describe and evaluate inter and outer relationships among agents' interactions. Then, we survey existing literature in relevant fields to support it and propose several promising research directions along with some open problems deemed necessary for further investigations.", "url": "https://arxiv.org/abs/2306.09445"}, {"metadata": {"arXiv": "2306.09470", "Date": "Thu, 15 Jun 2023 19:51:35 ", "Title": "Improving Path Planning Performance through Multimodal Generative Models with Local Critics", "Authors": ["Jorge Ocampo Jimenez and Wael Suleiman"], "Categories": "cs.RO cs.AI", "Comments": ["Under review"]}, "abstract": "This paper presents a novel method for accelerating path planning tasks in unknown scenes with obstacles by utilizing Wasserstein Generative Adversarial Networks (WGANs) with Gradient Penalty (GP) to approximate the distribution of the free conditioned configuration space. Our proposed approach involves conditioning the WGAN-GP with a Variational Auto-Encoder in a continuous latent space to handle multimodal datasets. However, training a Variational Auto-Encoder with WGAN-GP can be challenging for image-to-configuration-space problems, as the Kullback-Leibler loss function often converges to a random distribution. To overcome this issue, we simplify the configuration space as a set of Gaussian distributions and divide the dataset into several local models. This enables us to not only learn the model but also speed up its convergence. We evaluate the reconstructed configuration space using the homology rank of manifolds for datasets with the geometry score. Furthermore, we propose a novel transformation of the robot's configuration space that enables us to measure how well collision-free regions are reconstructed, which could be used with other rank of homology metrics. Our experiments show promising results for accelerating path planning tasks in unknown scenes while generating quasi-optimal paths with our WGAN-GP. The source code is openly available.", "url": "https://arxiv.org/abs/2306.09470"}, {"metadata": {"arXiv": "2306.09869", "Date": "Fri, 16 Jun 2023 14:30:41 ", "Title": "Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models", "Authors": ["Geon Yeong Park", "Jeongsol Kim", "Beomsu Kim", "Sang Wan Lee", "Jong Chul Ye"], "Categories": "cs.CV cs.AI cs.CL cs.LG"}, "abstract": "Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.", "url": "https://arxiv.org/abs/2306.09869"}, {"metadata": {"arXiv": "2306.09940", "Date": "Fri, 16 Jun 2023 16:22:45 ", "Title": "Vehicle Occurrence-based Parking Space Detection", "Authors": ["Paulo R. Lisboa de Almeida", "Jeovane Hon\\'orio Alves", "Luiz S. Oliveira", "Andre Gustavo Hochuli", "Jo\\~ao V. Fr\\\"ohlich", "Rodrigo A. Krauel"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Accepted for presentation at the 2023 IEEE International Conference on Systems", "Man", "and Cybernetics (SMC 2023)"]}, "abstract": "Smart-parking solutions use sensors, cameras, and data analysis to improve parking efficiency and reduce traffic congestion. Computer vision-based methods have been used extensively in recent years to tackle the problem of parking lot management, but most of the works assume that the parking spots are manually labeled, impacting the cost and feasibility of deployment. To fill this gap, this work presents an automatic parking space detection method, which receives a sequence of images of a parking lot and returns a list of coordinates identifying the detected parking spaces. The proposed method employs instance segmentation to identify cars and, using vehicle occurrence, generate a heat map of parking spaces. The results using twelve different subsets from the PKLot and CNRPark-EXT parking lot datasets show that the method achieved an AP25 score up to 95.60\\% and AP50 score up to 79.90\\%.", "url": "https://arxiv.org/abs/2306.09940"}, {"metadata": {"arXiv": "2306.09970", "Date": "Fri, 16 Jun 2023 17:02:12 ", "Title": "HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning", "Authors": ["Shaunak Halbe", "James Seale Smith", "Junjiao Tian", "Zsolt Kira"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients. Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy. In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lightweight generation and distillation scheme to consolidate client models at the server. We formulate this problem for image classification and establish strong baselines for comparison, conduct experiments on CIFAR-100 as well as challenging, large-scale datasets like ImageNet-R and DomainNet. Our approach outperforms both existing methods and our own baselines by as much as 7% while significantly reducing communication and client-level computation costs.", "url": "https://arxiv.org/abs/2306.09970"}, {"metadata": {"arXiv": "2306.09363", "Date": "Wed, 14 Jun 2023 05:46:52 ", "Title": "A Simple Data Augmentation for Feature Distribution Skewed Federated Learning", "Authors": ["Yunlu Yan", "Lei Zhu"], "Categories": "cs.LG cs.AI", "Comments": ["11 pages", "3 figures"]}, "abstract": "Federated learning (FL) facilitates collaborative learning among multiple clients in a distributed manner, while ensuring privacy protection. However, its performance is inevitably degraded as suffering data heterogeneity, i.e., non-IID data. In this paper, we focus on the feature distribution skewed FL scenario, which is widespread in real-world applications. The main challenge lies in the feature shift caused by the different underlying distributions of local datasets. While the previous attempts achieved progress, few studies pay attention to the data itself, the root of this issue. Therefore, the primary goal of this paper is to develop a general data augmentation technique at the input level, to mitigate the feature shift. To achieve this goal, we propose FedRDN, a simple yet remarkably effective data augmentation method for feature distribution skewed FL, which randomly injects the statistics of the dataset from the entire federation into the client's data. By this, our method can effectively improve the generalization of features, thereby mitigating the feature shift. Moreover, FedRDN is a plug-and-play component, which can be seamlessly integrated into the data augmentation flow with only a few lines of code. Extensive experiments on several datasets show that the performance of various representative FL works can be further improved by combining them with FedRDN, which demonstrates the strong scalability and generalizability of FedRDN. The source code will be released.", "url": "https://arxiv.org/abs/2306.09363"}, {"metadata": {"arXiv": "2306.09364", "Date": "Wed, 14 Jun 2023 06:26:23 ", "Title": "TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting", "Authors": ["Vijay Ekambaram", "Arindam Jati", "Nam Nguyen", "Phanwadee Sinthong", "Jayant Kalagnanam"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted in the Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23)", "Research Track", "August 6--10", "2023", "Long Beach", "CA", "USA"], "ACM-class": "I.2", "DOI": "10.1145/3580305.3599533"}, "abstract": "Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).", "url": "https://arxiv.org/abs/2306.09364"}, {"metadata": {"arXiv": "2306.09368", "Date": "Wed, 14 Jun 2023 13:23:14 ", "Title": "Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series", "Authors": ["Jiawen Zhang", "Shun Zheng", "Wei Cao", "Jiang Bian", "Jia Li"], "Categories": "cs.LG cs.AI", "Comments": ["KDD23 Research Track"], "DOI": "10.1145/3580305.3599543"}, "abstract": "Irregularly sampled multivariate time series are ubiquitous in various fields, particularly in healthcare, and exhibit two key characteristics: intra-series irregularity and inter-series discrepancy. Intra-series irregularity refers to the fact that time-series signals are often recorded at irregular intervals, while inter-series discrepancy refers to the significant variability in sampling rates among diverse series. However, recent advances in irregular time series have primarily focused on addressing intra-series irregularity, overlooking the issue of inter-series discrepancy. To bridge this gap, we present Warpformer, a novel approach that fully considers these two characteristics. In a nutshell, Warpformer has several crucial designs, including a specific input representation that explicitly characterizes both intra-series irregularity and inter-series discrepancy, a warping module that adaptively unifies irregular time series in a given scale, and a customized attention module for representation learning. Additionally, we stack multiple warping and attention modules to learn at different scales, producing multi-scale representations that balance coarse-grained and fine-grained signals for downstream tasks. We conduct extensive experiments on widely used datasets and a new large-scale benchmark built from clinical databases. The results demonstrate the superiority of Warpformer over existing state-of-the-art approaches.", "url": "https://arxiv.org/abs/2306.09368"}, {"metadata": {"arXiv": "2306.09373", "Date": "Thu, 15 Jun 2023 03:37:23 ", "Title": "Equitable Multi-task Learning", "Authors": ["Jun Yuan and Rui Zhang"], "Categories": "cs.LG cs.AI", "Comments": ["11 pages", "1 figures", "4 tables"]}, "abstract": "Multi-task learning (MTL) has achieved great success in various research domains, such as CV, NLP and IR etc. Due to the complex and competing task correlation, na\\\"ive training all tasks may lead to inequitable learning, \\textit{i.e.} some tasks are learned well while others are overlooked. Multi-task optimization (MTO) aims to improve all tasks at same time, but conventional methods often perform poor when tasks with large loss scale or gradient norm magnitude difference. To solve the issue, we in-depth investigate the equity problem for MTL and find that regularizing relative contribution of different tasks (\\textit{i.e.} value of task-specific loss divides its raw gradient norm) in updating shared parameter can improve generalization performance of MTL. Based on our theoretical analysis, we propose a novel multi-task optimization method, named \\textit{EMTL}, to achieve equitable MTL. Specifically, we efficiently add variance regularization to make different tasks' relative contribution closer. Extensive experiments have been conduct to evaluate EMTL, our method stably outperforms state-of-the-art methods on the public benchmark datasets of two different research domains. Furthermore, offline and online A/B test on multi-task recommendation are conducted too. EMTL improves multi-task recommendation significantly, demonstrating the superiority and practicability of our method in industrial landscape.", "url": "https://arxiv.org/abs/2306.09373"}, {"metadata": {"arXiv": "2306.09376", "Date": "Thu, 15 Jun 2023 07:45:43 ", "Title": "Modularizing while Training: a New Paradigm for Modularizing DNN Models", "Authors": ["Binhang Qi", "Hailong Sun", "Hongyu Zhang", "Ruobing Zhao", "Xiang Gao"], "Categories": "cs.LG cs.AI cs.SE", "Comments": ["Accepted at ICSE'24"]}, "abstract": "Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and inter-module coupling. We have implemented the proposed approach for modularizing Convolutional Neural Network (CNN) models in this work. The evaluation results on representative models demonstrate that MwT outperforms the state-of-the-art approach. Specifically, the accuracy loss caused by MwT is only 1.13 percentage points, which is 1.76 percentage points less than that of the baseline. The kernel retention rate of the modules generated by MwT is only 14.58%, with a reduction of 74.31% over the state-of-the-art approach. Furthermore, the total time cost required for training and modularizing is only 108 minutes, half of the baseline.", "url": "https://arxiv.org/abs/2306.09376"}, {"metadata": {"arXiv": "2306.09377", "Date": "Thu, 15 Jun 2023 08:18:29 ", "Title": "Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks", "Authors": ["Can Demircan", "Tankred Saanum", "Leonardo Pettini", "Marcel Binz", "Blazej M Baczkowski", "Paula Kaanders", "Christian F Doeller", "Mona M Garvert", "Eric Schulz"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models trained on both text and image data consistently outperformed models trained solely on images, even surpassing models using the features that generated the task itself. These findings suggest that language-aligned visual representations possess sufficient richness to describe human generalization in naturalistic settings and emphasize the role of language in shaping human cognition.", "url": "https://arxiv.org/abs/2306.09377"}, {"metadata": {"arXiv": "2306.09380", "Date": "Thu, 15 Jun 2023 10:48:59 ", "Title": "Understanding Parameter Sharing in Transformers", "Authors": ["Ye Lin", "Mingxuan Wang", "Zhexi Zhang", "Xiaohui Wang", "Tong Xiao", "Jingbo Zhu"], "Categories": "cs.LG cs.AI"}, "abstract": "Parameter sharing has proven to be a parameter-efficient approach. Previous work on Transformers has focused on sharing parameters in different layers, which can improve the performance of models with limited parameters by increasing model depth. In this paper, we study why this approach works from two perspectives. First, increasing model depth makes the model more complex, and we hypothesize that the reason is related to model complexity (referring to FLOPs). Secondly, since each shared parameter will participate in the network computation several times in forward propagation, its corresponding gradient will have a different range of values from the original model, which will affect the model convergence. Based on this, we hypothesize that training convergence may also be one of the reasons. Through further analysis, we show that the success of this approach can be largely attributed to better convergence, with only a small part due to the increased model complexity. Inspired by this, we tune the training hyperparameters related to model convergence in a targeted manner. Experiments on 8 machine translation tasks show that our model achieves competitive performance with only half the model complexity of parameter sharing models.", "url": "https://arxiv.org/abs/2306.09380"}, {"metadata": {"arXiv": "2306.09381", "Date": "Thu, 15 Jun 2023 11:47:45 ", "Title": "Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation", "Authors": ["Yu Wang", "Tongya Zheng", "Shunyu Liu", "Kaixuan Chen", "Zunlei Feng", "Yunzhi Hao", "Mingli Song"], "Categories": "cs.LG cs.AI"}, "abstract": "Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal correspondences and builds a novel dwell branch to simulate the varying durations in locations, which is finally optimized in an adversarial manner. The comprehensive experiments over four real datasets for the human mobility simulation have verified the superiority of STAR to state-of-the-art methods. Our code will be made publicly available.", "url": "https://arxiv.org/abs/2306.09381"}, {"metadata": {"arXiv": "2306.09385", "Date": "Thu, 15 Jun 2023 14:34:16 ", "Title": "Employing Multimodal Machine Learning for Stress Detection", "Authors": ["Rahee Walambe", "Pranav Nayak", "Ashmit Bhardwaj", "Ketan Kotecha"], "Categories": "cs.LG cs.AI eess.SP", "DOI": "10.1155/2021/9356452"}, "abstract": "In the current age, human lifestyle has become more knowledge oriented leading to generation of sedentary employment. This has given rise to a number of health and mental disorders. Mental wellness is one of the most neglected but crucial aspects of today's world. Mental health issues can, both directly and indirectly, affect other sections of human physiology and impede an individual's day-to-day activities and performance. However, identifying the stress and finding the stress trend for an individual leading to serious mental ailments is challenging and involves multiple factors. Such identification can be achieved accurately by fusing these multiple modalities (due to various factors) arising from behavioral patterns. Certain techniques are identified in the literature for this purpose; however, very few machine learning-based methods are proposed for such multimodal fusion tasks. In this work, a multimodal AI-based framework is proposed to monitor a person's working behavior and stress levels. We propose a methodology for efficiently detecting stress due to workload by concatenating heterogeneous raw sensor data streams (e.g., face expressions, posture, heart rate, computer interaction). This data can be securely stored and analyzed to understand and discover personalized unique behavioral patterns leading to mental strain and fatigue. The contribution of this work is twofold; proposing a multimodal AI-based strategy for fusion to detect stress and its level and secondly identify a stress pattern over a period of time. We were able to achieve 96.09% accuracy on the test set in stress detection and classification. Further, we reduce the stress scale prediction model loss to 0.036 using these modalities. This work can prove important for the community at large, specifically those working sedentary jobs to monitor and identify stress levels, especially in current times of COVID-19.", "url": "https://arxiv.org/abs/2306.09385"}, {"metadata": {"arXiv": "2306.09386", "Date": "Thu, 15 Jun 2023 14:50:27 ", "Title": "Adaptive Hierarchical SpatioTemporal Network for Traffic Forecasting", "Authors": ["Yirong Chen", "Ziyue Li", "Wanli Ouyang", "Michael Lepech"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted to IEEE CASE 2023"]}, "abstract": "Accurate traffic forecasting is vital to intelligent transportation systems, which are widely adopted to solve urban traffic issues. Existing traffic forecasting studies focus on modeling spatial-temporal dynamics in traffic data, among which the graph convolution network (GCN) is at the center for exploiting the spatial dependency embedded in the road network graphs. However, these GCN-based methods operate intrinsically on the node level (e.g., road and intersection) only whereas overlooking the spatial hierarchy of the whole city. Nodes such as intersections and road segments can form clusters (e.g., regions), which could also have interactions with each other and share similarities at a higher level. In this work, we propose an Adaptive Hierarchical SpatioTemporal Network (AHSTN) to promote traffic forecasting by exploiting the spatial hierarchy and modeling multi-scale spatial correlations. Apart from the node-level spatiotemporal blocks, AHSTN introduces the adaptive spatiotemporal downsampling module to infer the spatial hierarchy for spatiotemporal modeling at the cluster level. Then, an adaptive spatiotemporal upsampling module is proposed to upsample the cluster-level representations to the node-level and obtain the multi-scale representations for generating predictions. Experiments on two real-world datasets show that AHSTN achieves better performance over several strong baselines.", "url": "https://arxiv.org/abs/2306.09386"}, {"metadata": {"arXiv": "2306.09389", "Date": "Thu, 15 Jun 2023 15:49:13 ", "Title": "ST-PINN: A Self-Training Physics-Informed Neural Network for Partial Differential Equations", "Authors": ["Junjun Yan", "Xinhai Chen", "Zhichao Wang", "Enqiang Zhoui and Jie Liu"], "Categories": "cs.LG cs.AI physics.comp-ph"}, "abstract": "Partial differential equations (PDEs) are an essential computational kernel in physics and engineering. With the advance of deep learning, physics-informed neural networks (PINNs), as a mesh-free method, have shown great potential for fast PDE solving in various applications. To address the issue of low accuracy and convergence problems of existing PINNs, we propose a self-training physics-informed neural network, ST-PINN. Specifically, ST-PINN introduces a pseudo label based self-learning algorithm during training. It employs governing equation as the pseudo-labeled evaluation index and selects the highest confidence examples from the sample points to attach the pseudo labels. To our best knowledge, we are the first to incorporate a self-training mechanism into physics-informed learning. We conduct experiments on five PDE problems in different fields and scenarios. The results demonstrate that the proposed method allows the network to learn more physical information and benefit convergence. The ST-PINN outperforms existing physics-informed neural network methods and improves the accuracy by a factor of 1.33x-2.54x. The code of ST-PINN is available at GitHub: https://github.com/junjun-yan/ST-PINN.", "url": "https://arxiv.org/abs/2306.09389"}, {"metadata": {"arXiv": "2306.09433", "Date": "Thu, 15 Jun 2023 18:23:58 ", "Title": "Towards Practical Federated Causal Structure Learning", "Authors": ["Zhaoyu Wang", "Pingchuan Ma", "Shuai Wang"], "Categories": "cs.LG cs.AI stat.ME"}, "abstract": "Understanding causal relations is vital in scientific discovery. The process of causal structure learning involves identifying causal graphs from observational data to understand such relations. Usually, a central server performs this task, but sharing data with the server poses privacy risks. Federated learning can solve this problem, but existing solutions for federated causal structure learning make unrealistic assumptions about data and lack convergence guarantees. FedC2SL is a federated constraint-based causal structure learning scheme that learns causal graphs using a federated conditional independence test, which examines conditional independence between two variables under a condition set without collecting raw data from clients. FedC2SL requires weaker and more realistic assumptions about data and offers stronger resistance to data variability among clients. FedPC and FedFCI are the two variants of FedC2SL for causal structure learning in causal sufficiency and causal insufficiency, respectively. The study evaluates FedC2SL using both synthetic datasets and real-world data against existing solutions and finds it demonstrates encouraging performance and strong resilience to data heterogeneity among clients.", "url": "https://arxiv.org/abs/2306.09433"}, {"metadata": {"arXiv": "2306.09459", "Date": "Thu, 15 Jun 2023 19:29:08 ", "Title": "Recurrent Memory Decision Transformer", "Authors": ["Arkadii Bessonov and Alexey Staroverov and Huzhenyu Zhang and Alexey K. Kovalev and Dmitry Yudin and Aleksandr I. Panov"], "Categories": "cs.LG cs.AI"}, "abstract": "Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on the potential of incorporating recurrent memory mechanisms to improve the performance of large-scale transformer models in offline reinforcement learning tasks. The Recurrent Memory Decision Transformer code is publicly available in repository \\url{https://anonymous.4open.science/r/RMDT-4FE4}.", "url": "https://arxiv.org/abs/2306.09459"}, {"metadata": {"arXiv": "2306.09463", "Date": "Thu, 15 Jun 2023 19:34:14 ", "Title": "Kriging Convolutional Networks", "Authors": ["Gabriel Appleby", "Linfeng Liu", "Li-Ping Liu"], "Categories": "cs.LG cs.AI", "DOI": "10.1609/aaai.v34i04.5716"}, "abstract": "Spatial interpolation is a class of estimation problems where locations with known values are used to estimate values at other locations, with an emphasis on harnessing spatial locality and trends. Traditional Kriging methods have strong Gaussian assumptions, and as a result, often fail to capture complexities within the data. Inspired by the recent progress of graph neural networks, we introduce Kriging Convolutional Networks (KCN), a method of combining the advantages of Graph Convolutional Networks (GCN) and Kriging. Compared to standard GCNs, KCNs make direct use of neighboring observations when generating predictions. KCNs also contain the Kriging method as a specific configuration. We further improve the model's performance by adding attention. Empirically, we show that this model outperforms GCNs and Kriging in several applications. The implementation of KCN using PyTorch is publicized at the GitHub repository: https://github.com/tufts-ml/kcn-torch.", "url": "https://arxiv.org/abs/2306.09463"}, {"metadata": {"arXiv": "2306.09468", "Date": "Thu, 15 Jun 2023 19:51:28 ", "Title": "FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods", "Authors": ["Xiaotian Han", "Jianfeng Chi", "Yu Chen", "Qifan Wang", "Han Zhao", "Na Zou", "Xia Hu"], "Categories": "cs.LG cs.AI cs.CY"}, "abstract": "This paper introduces the Fair Fairness Benchmark (\\textsf{FFB}), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is critical for ethical and legal compliance. However, there exist challenges in comparing and developing of fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source, standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from $\\mathbf{45,079}$ experiments. We believe our work will significantly facilitate the growth and development of the fairness research community. The benchmark, including code and running logs, is available at https://github.com/ahxt/fair_fairness_benchmark", "url": "https://arxiv.org/abs/2306.09468"}, {"metadata": {"arXiv": "2306.09491", "Date": "Thu, 15 Jun 2023 20:37:30 ", "Title": "A Hybrid Feature Selection and Construction Method for Detection of Wind Turbine Generator Heating Faults", "Authors": ["Ayse Gokcen Kavaz", "Burak Barutcu"], "Categories": "cs.LG cs.AI", "Comments": ["8 pages"], "ACM-class": "I.2.1"}, "abstract": "Preprocessing of information is an essential step for the effective design of machine learning applications. Feature construction and selection are powerful techniques used for this aim. In this paper, a feature selection and construction approach is presented for the detection of wind turbine generator heating faults. Data were collected from Supervisory Control and Data Acquisition (SCADA) system of a wind turbine. The original features directly collected from the data collection system consist of wind characteristics, operational data, temperature measurements and status information. In addition to these original features, new features were created in the feature construction step to obtain information that can be more powerful indications of the faults. After the construction of new features, a hybrid feature selection technique was implemented to find out the most relevant features in the overall set to increase the classification accuracy and decrease the computational burden. Feature selection step consists of filter and wrapper-based parts. Filter based feature selection was applied to exclude the features which are non-discriminative and wrapper-based method was used to determine the final features considering the redundancies and mutual relations amongst them. Artificial Neural Networks were used both in the detection phase and as the induction algorithm of the wrapper-based feature selection part. The results show that, the proposed approach contributes to the fault detection system to be more reliable especially in terms of reducing the number of false fault alarms.", "url": "https://arxiv.org/abs/2306.09491"}, {"metadata": {"arXiv": "2306.09520", "Date": "Thu, 15 Jun 2023 21:42:40 ", "Title": "Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding", "Authors": ["Myrl G. Marmarelis", "Greg Ver Steeg", "Aram Galstyan", "Fred Morstatter"], "Categories": "cs.LG cs.AI stat.ME stat.ML", "Comments": ["Submitted to NeurIPS 2023"]}, "abstract": "Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.", "url": "https://arxiv.org/abs/2306.09520"}, {"metadata": {"arXiv": "2306.09526", "Date": "Thu, 15 Jun 2023 22:01:19 ", "Title": "Residual Q-Learning: Offline and Online Policy Customization without Value", "Authors": ["Chenran Li", "Chen Tang", "Haruki Nishimura", "Jean Mercat", "Masayoshi Tomizuka", "Wei Zhan"], "Categories": "cs.LG cs.AI", "Comments": ["The first two authors contributed equally"]}, "abstract": "Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the customization problem as a Markov Decision Process (MDP) with a reward function that combines 1) the inherent reward of the demonstration; and 2) the add-on reward specified by the downstream task. We propose a novel framework, Residual Q-learning, which can solve the formulated MDP by leveraging the prior policy without knowing the inherent reward or value function of the prior policy. We derive a family of residual Q-learning algorithms that can realize offline and online policy customization, and show that the proposed algorithms can effectively accomplish policy customization tasks in various environments.", "url": "https://arxiv.org/abs/2306.09526"}, {"metadata": {"arXiv": "2306.09586", "Date": "Fri, 16 Jun 2023 02:17:45 ", "Title": "Is the Volume of a Credal Set a Good Measure for Epistemic Uncertainty?", "Authors": ["Yusuf Sale", "Michele Caprio", "Eyke H\\\"ullermeier"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Adequate uncertainty representation and quantification have become imperative in various scientific disciplines, especially in machine learning and artificial intelligence. As an alternative to representing uncertainty via one single probability measure, we consider credal sets (convex sets of probability measures). The geometric representation of credal sets as $d$-dimensional polytopes implies a geometric intuition about (epistemic) uncertainty. In this paper, we show that the volume of the geometric representation of a credal set is a meaningful measure of epistemic uncertainty in the case of binary classification, but less so for multi-class classification. Our theoretical findings highlight the crucial role of specifying and employing uncertainty measures in machine learning in an appropriate way, and for being aware of possible pitfalls.", "url": "https://arxiv.org/abs/2306.09586"}, {"metadata": {"arXiv": "2306.09595", "Date": "Fri, 16 Jun 2023 02:41:31 ", "Title": "Structured Cooperative Learning with Graphical Model Priors", "Authors": ["Shuangtong Li", "Tianyi Zhou", "Xinmei Tian", "Dacheng Tao"], "Categories": "cs.LG cs.AI"}, "abstract": "We study how to train personalized models for different tasks on decentralized devices with limited local data. We propose \"Structured Cooperative Learning (SCooL)\", in which a cooperation graph across devices is generated by a graphical model prior to automatically coordinate mutual learning between devices. By choosing graphical models enforcing different structures, we can derive a rich class of existing and novel decentralized learning algorithms via variational inference. In particular, we show three instantiations of SCooL that adopt Dirac distribution, stochastic block model (SBM), and attention as the prior generating cooperation graphs. These EM-type algorithms alternate between updating the cooperation graph and cooperative learning of local models. They can automatically capture the cross-task correlations among devices by only monitoring their model updating in order to optimize the cooperation graph. We evaluate SCooL and compare it with existing decentralized learning methods on an extensive set of benchmarks, on which SCooL always achieves the highest accuracy of personalized models and significantly outperforms other baselines on communication efficiency. Our code is available at https://github.com/ShuangtongLi/SCooL.", "url": "https://arxiv.org/abs/2306.09595"}, {"metadata": {"arXiv": "2306.09643", "Date": "Fri, 16 Jun 2023 06:10:55 ", "Title": "BISCUIT: Causal Representation Learning from Binary Interactions", "Authors": ["Phillip Lippe", "Sara Magliacane", "Sindy L\\\"owe", "Yuki M. Asano", "Taco Cohen", "Efstratios Gavves"], "Categories": "cs.LG cs.AI stat.ME", "Comments": ["Published in: Uncertainty in Artificial Intelligence (UAI 2023). Project page: https://phlippe.github.io/BISCUIT/"]}, "abstract": "Identifying the causal variables of an environment and how to intervene on them is of core value in applications such as robotics and embodied AI. While an agent can commonly interact with the environment and may implicitly perturb the behavior of some of these causal variables, often the targets it affects remain unknown. In this paper, we show that causal variables can still be identified for many common setups, e.g., additive Gaussian noise models, if the agent's interactions with a causal variable can be described by an unknown binary variable. This happens when each causal variable has two different mechanisms, e.g., an observational and an interventional one. Using this identifiability result, we propose BISCUIT, a method for simultaneously learning causal variables and their corresponding binary interaction variables. On three robotic-inspired datasets, BISCUIT accurately identifies causal variables and can even be scaled to complex, realistic environments for embodied AI.", "url": "https://arxiv.org/abs/2306.09643"}, {"metadata": {"arXiv": "2306.09662", "Date": "Fri, 16 Jun 2023 07:37:05 ", "Title": "Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction", "Authors": ["Cheng Ruei Tang", "Jun Wei Hsieh", "and Shin You Teng"], "Categories": "cs.LG cs.AI cs.MA", "Comments": ["arXiv admin note: substantial text overlap with arXiv:2205.11291"]}, "abstract": "Existing traffic signal control systems rely on oversimplified rule-based methods, and even RL-based methods are often suboptimal and unstable. To address this, we propose a cooperative multi-objective architecture called Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG), which estimates multiple reward terms for traffic signal control optimization using age-decaying weights. Our approach involves two types of agents: one focuses on optimizing local traffic at each intersection, while the other aims to optimize global traffic throughput. We evaluate our method using real-world traffic data collected from an Asian country's traffic cameras. Despite the inclusion of a global agent, our solution remains decentralized as this agent is no longer necessary during the inference stage. Our results demonstrate the effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all performance metrics. Additionally, our proposed system minimizes both waiting time and carbon emissions. Notably, this paper is the first to link carbon emissions and global agents in traffic signal control.", "url": "https://arxiv.org/abs/2306.09662"}, {"metadata": {"arXiv": "2306.09668", "Date": "Fri, 16 Jun 2023 07:54:15 ", "Title": "Multi-Classification using One-versus-One Deep Learning Strategy with Joint Probability Estimates", "Authors": ["Anthony Hei-Long Chan", "Raymond HonFu Chan", "Lingjia Dai"], "Categories": "cs.LG cs.AI"}, "abstract": "The One-versus-One (OvO) strategy is an approach of multi-classification models which focuses on training binary classifiers between each pair of classes. While the OvO strategy takes advantage of balanced training data, the classification accuracy is usually hindered by the voting mechanism to combine all binary classifiers. In this paper, a novel OvO multi-classification model incorporating a joint probability measure is proposed under the deep learning framework. In the proposed model, a two-stage algorithm is developed to estimate the class probability from the pairwise binary classifiers. Given the binary classifiers, the pairwise probability estimate is calibrated by a distance measure on the separating feature hyperplane. From that, the class probability of the subject is estimated by solving a joint probability-based distance minimization problem. Numerical experiments in different applications show that the proposed model achieves generally higher classification accuracy than other state-of-the-art models.", "url": "https://arxiv.org/abs/2306.09668"}, {"metadata": {"arXiv": "2306.09686", "Date": "Fri, 16 Jun 2023 08:34:42 ", "Title": "Collapsed Inference for Bayesian Deep Learning", "Authors": ["Zhe Zeng", "Guy Van den Broeck"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Bayesian neural networks (BNNs) provide a formalism to quantify and calibrate uncertainty in deep learning. Current inference approaches for BNNs often resort to few-sample estimation for scalability, which can harm predictive performance, while its alternatives tend to be computationally prohibitively expensive. We tackle this challenge by revealing a previously unseen connection between inference on BNNs and volume computation problems. With this observation, we introduce a novel collapsed inference scheme that performs Bayesian model averaging using collapsed samples. It improves over a Monte-Carlo sample by limiting sampling to a subset of the network weights while pairing it with some closed-form conditional distribution over the rest. A collapsed sample represents uncountably many models drawn from the approximate posterior and thus yields higher sample efficiency. Further, we show that the marginalization of a collapsed sample can be solved analytically and efficiently despite the non-linearity of neural networks by leveraging existing volume computation solvers. Our proposed use of collapsed samples achieves a balance between scalability and accuracy. On various regression and classification tasks, our collapsed Bayesian deep learning approach demonstrates significant improvements over existing methods and sets a new state of the art in terms of uncertainty estimation as well as predictive performance.", "url": "https://arxiv.org/abs/2306.09686"}, {"metadata": {"arXiv": "2306.09712", "Date": "Fri, 16 Jun 2023 09:24:29 ", "Title": "Semi-Offline Reinforcement Learning for Optimized Text Generation", "Authors": ["Changyu Chen", "Xiting Wang", "Yiqiao Jin", "Victor Ye Dong", "Li Dong", "Jie Cao", "Yi Liu", "Rui Yan"], "Categories": "cs.LG cs.AI cs.CL", "Comments": ["In Proceedings of the 40th International Conference on Machine Learning (ICML 2023)"]}, "abstract": "In reinforcement learning (RL), there are two major settings for interacting with the environment: online and offline. Online methods explore the environment at significant time cost, and offline methods efficiently obtain reward signals by sacrificing exploration capability. We propose semi-offline RL, a novel paradigm that smoothly transits from offline to online settings, balances exploration capability and training cost, and provides a theoretical foundation for comparing different RL settings. Based on the semi-offline formulation, we present the RL setting that is optimal in terms of optimization cost, asymptotic error, and overfitting error bound. Extensive experiments show that our semi-offline approach is efficient and yields comparable or often better performance compared with state-of-the-art methods.", "url": "https://arxiv.org/abs/2306.09712"}, {"metadata": {"arXiv": "2306.09742", "Date": "Fri, 16 Jun 2023 10:18:38 ", "Title": "Meta Generative Flow Networks with Personalization for Task-Specific Adaptation", "Authors": ["Xinyuan Ji", "Xu Zhang", "Wei Xi", "Haozhi Wang", "Olga Gadyatskaya", "Yinchuan Li"], "Categories": "cs.LG cs.AI", "Comments": ["journal"]}, "abstract": "Multi-task reinforcement learning and meta-reinforcement learning have been developed to quickly adapt to new tasks, but they tend to focus on tasks with higher rewards and more frequent occurrences, leading to poor performance on tasks with sparse rewards. To address this issue, GFlowNets can be integrated into meta-learning algorithms (GFlowMeta) by leveraging the advantages of GFlowNets on tasks with sparse rewards. However, GFlowMeta suffers from performance degradation when encountering heterogeneous transitions from distinct tasks. To overcome this challenge, this paper proposes a personalized approach named pGFlowMeta, which combines task-specific personalized policies with a meta policy. Each personalized policy balances the loss on its personalized task and the difference from the meta policy, while the meta policy aims to minimize the average loss of all tasks. The theoretical analysis shows that the algorithm converges at a sublinear rate. Extensive experiments demonstrate that the proposed algorithm outperforms state-of-the-art reinforcement learning algorithms in discrete environments.", "url": "https://arxiv.org/abs/2306.09742"}, {"metadata": {"arXiv": "2306.09746", "Date": "Fri, 16 Jun 2023 10:25:43 ", "Title": "Temporal Difference Learning with Experience Replay", "Authors": ["Han-Dong Lim", "Donghwan Lee"], "Categories": "cs.LG cs.AI"}, "abstract": "Temporal-difference (TD) learning is widely regarded as one of the most popular algorithms in reinforcement learning (RL). Despite its widespread use, it has only been recently that researchers have begun to actively study its finite time behavior, including the finite time bound on mean squared error and sample complexity. On the empirical side, experience replay has been a key ingredient in the success of deep RL algorithms, but its theoretical effects on RL have yet to be fully understood. In this paper, we present a simple decomposition of the Markovian noise terms and provide finite-time error bounds for TD-learning with experience replay. Specifically, under the Markovian observation model, we demonstrate that for both the averaged iterate and final iterate cases, the error term induced by a constant step-size can be effectively controlled by the size of the replay buffer and the mini-batch sampled from the experience replay buffer.", "url": "https://arxiv.org/abs/2306.09746"}, {"metadata": {"arXiv": "2306.09750", "Date": "Fri, 16 Jun 2023 10:34:49 ", "Title": "Fedstellar: A Platform for Decentralized Federated Learning", "Authors": ["Enrique Tom\\'as Mart\\'inez Beltr\\'an and \\'Angel Luis Perales G\\'omez and Chao Feng and Pedro Miguel S\\'anchez S\\'anchez and Sergio L\\'opez Bernal and G\\'er\\^ome Bovet and Manuel Gil P\\'erez and Gregorio Mart\\'inez P\\'erez and Alberto Huertas Celdr\\'an"], "Categories": "cs.LG cs.AI cs.DC cs.NI"}, "abstract": "In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Fedstellar implementation encompasses a web application with an interactive graphical interface, a controller for deploying federations of nodes using physical or virtual devices, and a core deployed on each device which provides the logic needed to train, aggregate, and communicate in the network. The effectiveness of the platform has been demonstrated in two scenarios: a physical deployment involving single-board devices such as Raspberry Pis for detecting cyberattacks, and a virtualized deployment comparing various FL approaches in a controlled environment using MNIST and CIFAR-10 datasets. In both scenarios, Fedstellar demonstrated consistent performance and adaptability, achieving F1 scores of 91%, 98%, and 91.2% using DFL for detecting cyberattacks and classifying MNIST and CIFAR-10, respectively, reducing training time by 32% compared to centralized approaches.", "url": "https://arxiv.org/abs/2306.09750"}, {"metadata": {"arXiv": "2306.09819", "Date": "Fri, 16 Jun 2023 13:02:57 ", "Title": "Amortized Inference for Gaussian Process Hyperparameters of Structured Kernels", "Authors": ["Matthias Bitzer", "Mona Meister", "Christoph Zimmer"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at UAI 2023"]}, "abstract": "Learning the kernel parameters for Gaussian processes is often the computational bottleneck in applications such as online learning, Bayesian optimization, or active learning. Amortizing parameter inference over different datasets is a promising approach to dramatically speed up training time. However, existing methods restrict the amortized inference procedure to a fixed kernel structure. The amortization network must be redesigned manually and trained again in case a different kernel is employed, which leads to a large overhead in design time and training time. We propose amortizing kernel parameter inference over a complete kernel-structure-family rather than a fixed kernel structure. We do that via defining an amortization network over pairs of datasets and kernel structures. This enables fast kernel inference for each element in the kernel family without retraining the amortization network. As a by-product, our amortization network is able to do fast ensembling over kernel structures. In our experiments, we show drastically reduced inference time combined with competitive test performance for a large set of kernels and datasets.", "url": "https://arxiv.org/abs/2306.09819"}, {"metadata": {"arXiv": "2306.09872", "Date": "Wed, 14 Jun 2023 03:37:55 ", "Title": "Generalizable One-shot Rope Manipulation with Parameter-Aware Policy", "Authors": ["So Kuroki", "Jiaxian Guo", "Tatsuya Matsushima", "Takuya Okubo", "Masato Kobayashi", "Yuya Ikeda", "Ryosuke Takanami", "Paul Yoo", "Yutaka Matsuo", "Yusuke Iwasawa"], "Categories": "cs.LG cs.AI cs.RO"}, "abstract": "Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single real-world demonstration. Empirical validations on both simulated and real-world rope manipulation setups clearly show that our method can manipulate different ropes with a single demonstration and significantly outperforms the baseline in both environments (62% improvement in in-domain ropes, and 15% improvement in out-of-distribution ropes in simulation, 26% improvement in real-world), demonstrating the effectiveness of our approach in one-shot rope manipulation.", "url": "https://arxiv.org/abs/2306.09872"}, {"metadata": {"arXiv": "2306.09884", "Date": "Fri, 16 Jun 2023 14:52:24 ", "Title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX", "Authors": ["Cl\\'ement Bonnet", "Daniel Luo", "Donal Byrne", "Shikha Surana", "Vincent Coyette", "Paul Duckworth", "Laurence I. Midgley", "Tristan Kalloniatis", "Sasha Abramowitz", "Cemlyn N. Waters", "Andries P. Smit", "Nathan Grinsztajn", "Ulrich A. Mbou Sob", "Omayma Mahjoub", "Elshadai Tegegn", "Mohamed A. Mimouni", "Raphael Boige", "Ruan de Kock", "Daniel Furelos-Blanco", "Victor Le", "Arnu Pretorius", "Alexandre Laterre"], "Categories": "cs.LG cs.AI", "Comments": ["9 pages + 16 pages of appendices and references. Submitted to NeurIPS 2023 Datasets and Benchmarks Track"]}, "abstract": "Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms. In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications. Therefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable. Jumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks. By leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents. Unlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs. Furthermore, we provide actor-critic baselines for each environment, accompanied by preliminary findings on scaling and generalization scenarios. Jumanji aims to set a new standard for speed, adaptability, and scalability of RL environments.", "url": "https://arxiv.org/abs/2306.09884"}, {"metadata": {"arXiv": "2306.09980", "Date": "Fri, 16 Jun 2023 17:23:49 ", "Title": "Creating Multi-Level Skill Hierarchies in Reinforcement Learning", "Authors": ["Joshua B. Evans and \\\"Ozg\\\"ur \\c{S}im\\c{s}ek"], "Categories": "cs.LG cs.AI", "Comments": ["19 pages", "12 figures"]}, "abstract": "What is a useful skill hierarchy for an autonomous agent? We propose an answer based on the graphical structure of an agent's interaction with its environment. Our approach uses hierarchical graph partitioning to expose the structure of the graph at varying timescales, producing a skill hierarchy with multiple levels of abstraction. At each level of the hierarchy, skills move the agent between regions of the state space that are well connected within themselves but weakly connected to each other. We illustrate the utility of the proposed skill hierarchy in a wide variety of domains in the context of reinforcement learning.", "url": "https://arxiv.org/abs/2306.09980"}, {"metadata": {"arXiv": "2306.09983", "Date": "Fri, 16 Jun 2023 17:26:38 ", "Title": "Evaluating Superhuman Models with Consistency Checks", "Authors": ["Lukas Fluri", "Daniel Paleka", "Florian Tram\\`er"], "Categories": "cs.LG cs.AI cs.CR stat.ML", "Comments": ["30 pages", "15 figures. Under review. Supplementary materials are available at https://github.com/ethz-privsec/superhuman-ai-consistency"]}, "abstract": "If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to semantically identical boards; GPT-4 forecasting that sports records will evolve non-monotonically over time; or an AI judge assigning bail to a defendant only after we add a felony to their criminal record.", "url": "https://arxiv.org/abs/2306.09983"}, {"metadata": {"arXiv": "2306.09989", "Date": "Fri, 16 Jun 2023 17:37:43 ", "Title": "Ensemble Framework for Cardiovascular Disease Prediction", "Authors": ["Achyut Tiwari", "Aryan Chugh", "Aman Sharma"], "Categories": "cs.LG cs.AI", "Journal-ref": "Volume 146, July 2022, 105624", "DOI": "10.1016/j.compbiomed.2022.105624"}, "abstract": "Heart disease is the major cause of non-communicable and silent death worldwide. Heart diseases or cardiovascular diseases are classified into four types: coronary heart disease, heart failure, congenital heart disease, and cardiomyopathy. It is vital to diagnose heart disease early and accurately in order to avoid further injury and save patients' lives. As a result, we need a system that can predict cardiovascular disease before it becomes a critical situation. Machine learning has piqued the interest of researchers in the field of medical sciences. For heart disease prediction, researchers implement a variety of machine learning methods and approaches. In this work, to the best of our knowledge, we have used the dataset from IEEE Data Port which is one of the online available largest datasets for cardiovascular diseases individuals. The dataset isa combination of Hungarian, Cleveland, Long Beach VA, Switzerland & Statlog datasets with important features such as Maximum Heart Rate Achieved, Serum Cholesterol, Chest Pain Type, Fasting blood sugar, and so on. To assess the efficacy and strength of the developed model, several performance measures are used, such as ROC, AUC curve, specificity, F1-score, sensitivity, MCC, and accuracy. In this study, we have proposed a framework with a stacked ensemble classifier using several machine learning algorithms including ExtraTrees Classifier, Random Forest, XGBoost, and so on. Our proposed framework attained an accuracy of 92.34% which is higher than the existing literature.", "url": "https://arxiv.org/abs/2306.09989"}, {"metadata": {"arXiv": "2306.09995", "Date": "Fri, 16 Jun 2023 17:47:36 ", "Title": "Fairness in Preference-based Reinforcement Learning", "Authors": ["Umer Siddique", "Abhinav Sinha", "Yongcan Cao"], "Categories": "cs.LG cs.AI cs.CY cs.SY eess.SY"}, "abstract": "In this paper, we address the issue of fairness in preference-based reinforcement learning (PbRL) in the presence of multiple objectives. The main objective is to design control policies that can optimize multiple objectives while treating each objective fairly. Toward this objective, we design a new fairness-induced preference-based reinforcement learning or FPbRL. The main idea of FPbRL is to learn vector reward functions associated with multiple objectives via new welfare-based preferences rather than reward-based preference in PbRL, coupled with policy learning via maximizing a generalized Gini welfare function. Finally, we provide experiment studies on three different environments to show that the proposed FPbRL approach can achieve both efficiency and equity for learning effective and fair policies.", "url": "https://arxiv.org/abs/2306.09995"}, {"metadata": {"arXiv": "2306.09462", "Date": "Thu, 15 Jun 2023 19:32:04 ", "Title": "Motion Comfort Optimization for Autonomous Vehicles: Concepts, Methods, and Techniques", "Authors": ["Mohammed Aledhari", "Mohamed Rahouti", "Junaid Qadir", "Basheer Qolomany", "Mohsen Guizani", "Ala Al-Fuqaha"], "Categories": "cs.RO cs.AI cs.CV cs.LG", "Comments": ["None"]}, "abstract": "This article outlines the architecture of autonomous driving and related complementary frameworks from the perspective of human comfort. The technical elements for measuring Autonomous Vehicle (AV) user comfort and psychoanalysis are listed here. At the same time, this article introduces the technology related to the structure of automatic driving and the reaction time of automatic driving. We also discuss the technical details related to the automatic driving comfort system, the response time of the AV driver, the comfort level of the AV, motion sickness, and related optimization technologies. The function of the sensor is affected by various factors. Since the sensor of automatic driving mainly senses the environment around a vehicle, including \"the weather\" which introduces the challenges and limitations of second-hand sensors in autonomous vehicles under different weather conditions. The comfort and safety of autonomous driving are also factors that affect the development of autonomous driving technologies. This article further analyzes the impact of autonomous driving on the user's physical and psychological states and how the comfort factors of autonomous vehicles affect the automotive market. Also, part of our focus is on the benefits and shortcomings of autonomous driving. The goal is to present an exhaustive overview of the most relevant technical matters to help researchers and application developers comprehend the different comfort factors and systems of autonomous driving. Finally, we provide detailed automated driving comfort use cases to illustrate the comfort-related issues of autonomous driving. Then, we provide implications and insights for the future of autonomous driving.", "url": "https://arxiv.org/abs/2306.09462"}, {"metadata": {"arXiv": "2306.09537", "Date": "Thu, 15 Jun 2023 22:46:20 ", "Title": "QuadSwarm: A Modular Multi-Quadrotor Simulator for Deep Reinforcement Learning with Direct Thrust Control", "Authors": ["Zhehui Huang", "Sumeet Batra", "Tao Chen", "Rahul Krupani", "Tushar Kumar", "Artem Molchanov", "Aleksei Petrenko", "James A. Preiss", "Zhaojing Yang", "Gaurav S. Sukhatme"], "Categories": "cs.RO cs.AI cs.LG cs.MA cs.SY eess.SY", "Comments": ["Paper published in ICRA 2023 Workshop: The Role of Robotics Simulators for Unmanned Aerial Vehicles. The workshop can be found in https://imrclab.github.io/workshop-uav-sims-icra2023/"]}, "abstract": "Reinforcement learning (RL) has shown promise in creating robust policies for robotics tasks. However, contemporary RL algorithms are data-hungry, often requiring billions of environment transitions to train successful policies. This necessitates the use of fast and highly-parallelizable simulators. In addition to speed, such simulators need to model the physics of the robots and their interaction with the environment to a level acceptable for transferring policies learned in simulation to reality. We present QuadSwarm, a fast, reliable simulator for research in single and multi-robot RL for quadrotors that addresses both issues. QuadSwarm, with fast forward-dynamics propagation decoupled from rendering, is designed to be highly parallelizable such that throughput scales linearly with additional compute. It provides multiple components tailored toward multi-robot RL, including diverse training scenarios, and provides domain randomization to facilitate the development and sim2real transfer of multi-quadrotor control policies. Initial experiments suggest that QuadSwarm achieves over 48,500 simulation samples per second (SPS) on a single quadrotor and over 62,000 SPS on eight quadrotors on a 16-core CPU. The code can be found in https://github.com/Zhehui-Huang/quad-swarm-rl.", "url": "https://arxiv.org/abs/2306.09537"}, {"metadata": {"arXiv": "2306.09922", "Date": "Fri, 16 Jun 2023 15:47:24 ", "Title": "Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions", "Authors": ["Chad DeChant", "Iretiayo Akinola", "Daniel Bauer"], "Categories": "cs.RO cs.AI cs.CL cs.LG"}, "abstract": "When robots perform long action sequences, users will want to easily and reliably find out what they have done. We therefore demonstrate the task of learning to summarize and answer questions about a robot agent's past actions using natural language alone. A single system with a large language model at its core is trained to both summarize and answer questions about action sequences given ego-centric video frames of a virtual robot and a question prompt. To enable training of question answering, we develop a method to automatically generate English-language questions and answers about objects, actions, and the temporal order in which actions occurred during episodes of robot action in the virtual environment. Training one model to both summarize and answer questions enables zero-shot transfer of representations of objects learned through question answering to improved action summarization. % involving objects not seen in training to summarize.", "url": "https://arxiv.org/abs/2306.09922"}, {"metadata": {"arXiv": "2306.09365", "Date": "Wed, 14 Jun 2023 06:46:58 ", "Title": "Fault Detection in Induction Motors using Functional Dimensionality Reduction Methods", "Authors": ["Mar\\'ia Barroso", "Jos\\'e M. Bossio", "Carlos M. Ala\\'iz and \\'Angela Fern\\'andez"], "Categories": "eess.SY cs.AI cs.LG cs.SY"}, "abstract": "The implementation of strategies for fault detection and diagnosis on rotating electrical machines is crucial for the reliability and safety of modern industrial systems. The contribution of this work is a methodology that combines conventional strategy of Motor Current Signature Analysis with functional dimensionality reduction methods, namely Functional Principal Components Analysis and Functional Diffusion Maps, for detecting and classifying fault conditions in induction motors. The results obtained from the proposed scheme are very encouraging, revealing a potential use in the future not only for real-time detection of the presence of a fault in an induction motor, but also in the identification of a greater number of types of faults present through an offline analysis.", "url": "https://arxiv.org/abs/2306.09365"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
