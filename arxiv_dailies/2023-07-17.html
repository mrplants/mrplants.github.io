<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2307.07044", "Date": "Thu, 13 Jul 2023 20:01:26 ", "Title": "AnyStar: Domain randomized universal star-convex 3D instance segmentation", "Authors": ["Neel Dey", "S. Mazdak Abulnaga", "Benjamin Billot", "Esra Abaci Turk", "P. Ellen Grant", "Adrian V. Dalca", "Polina Golland"], "Categories": "cs.CV cs.LG", "Comments": ["Code available at https://github.com/neel-dey/AnyStar"]}, "abstract": "Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or finetuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from unseen datasets. A single network trained on our synthesized data accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence microscopy, mouse cortical nuclei in micro-CT, zebrafish brain nuclei in EM, and placental cotyledons in human fetal MRI, all without any retraining, finetuning, transfer learning, or domain adaptation. Code is available at https://github.com/neel-dey/AnyStar.", "url": "https://arxiv.org/abs/2307.07044"}, {"metadata": {"arXiv": "2307.07063", "Date": "Thu, 13 Jul 2023 21:08:15 ", "Title": "Bootstrapping Vision-Language Learning with Decoupled Language Pre-training", "Authors": ["Yiren Jian", "Chongyang Gao", "Soroush Vosoughi"], "Categories": "cs.CV cs.LG", "Comments": ["The code is available at https://github.com/yiren-jian/BLIText"]}, "abstract": "We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importantly, our framework is modality-agnostic and flexible in terms of architectural design, as validated by its successful application in a video learning task using varied base modules. The code is available at https://github.com/yiren-jian/BLIText", "url": "https://arxiv.org/abs/2307.07063"}, {"metadata": {"arXiv": "2307.07181", "Date": "Fri, 14 Jul 2023 06:21:03 ", "Title": "DISPEL: Domain Generalization via Domain-Specific Liberating", "Authors": ["Chia-Yuan Chang", "Yu-Neng Chuang", "Guanchu Wang", "Mengnan Du", "Zou Na"], "Categories": "cs.CV cs.LG"}, "abstract": "Domain generalization aims to learn a generalization model that can perform well on unseen test domains by only training on limited source domains. However, existing domain generalization approaches often bring in prediction-irrelevant noise or require the collection of domain labels. To address these challenges, we consider the domain generalization problem from a different perspective by categorizing underlying feature groups into domain-shared and domain-specific features. Nevertheless, the domain-specific features are difficult to be identified and distinguished from the input data. In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing fine-grained masking approach that can filter out undefined and indistinguishable domain-specific features in the embedding space. Specifically, DISPEL utilizes a mask generator that produces a unique mask for each input data to filter domain-specific features. The DISPEL framework is highly flexible to be applied to any fine-tuned models. We derive a generalization error bound to guarantee the generalization performance by optimizing a designed objective loss. The experimental results on five benchmarks demonstrate DISPEL outperforms existing methods and can further generalize various algorithms.", "url": "https://arxiv.org/abs/2307.07181"}, {"metadata": {"arXiv": "2307.07246", "Date": "Fri, 14 Jul 2023 09:38:22 ", "Title": "Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training", "Authors": ["Xiaofei Chen", "Yuting He", "Cheng Xue", "Rongjun Ge", "Shuo Li", "Guanyu Yang"], "Categories": "cs.CV cs.LG", "Comments": ["accepted by MICCAI 2023"]}, "abstract": "The foundation models based on pre-training technology have significantly advanced artificial intelligence from theoretical to practical applications. These models have facilitated the feasibility of computer-aided diagnosis for widespread use. Medical contrastive vision-language pre-training, which does not require human annotations, is an effective approach for guiding representation learning using description information in diagnostic reports. However, the effectiveness of pre-training is limited by the large-scale semantic overlap and shifting problems in medical field. To address these issues, we propose the Knowledge-Boosting Contrastive Vision-Language Pre-training framework (KoBo), which integrates clinical knowledge into the learning of vision-language semantic consistency. The framework uses an unbiased, open-set sample-wise knowledge representation to measure negative sample noise and supplement the correspondence between vision-language mutual information and clinical knowledge. Extensive experiments validate the effect of our framework on eight tasks including classification, segmentation, retrieval, and semantic relatedness, achieving comparable or better performance with the zero-shot or few-shot settings. Our code is open on https://github.com/ChenXiaoFei-CS/KoBo.", "url": "https://arxiv.org/abs/2307.07246"}, {"metadata": {"arXiv": "2307.07298", "Date": "Fri, 14 Jul 2023 12:21:11 ", "Title": "3D Shape-Based Myocardial Infarction Prediction Using Point Cloud Classification Networks", "Authors": ["Marcel Beetz", "Yilong Yang", "Abhirup Banerjee", "Lei Li", "Vicente Grau"], "Categories": "cs.CV cs.LG eess.IV", "Comments": ["Accepted at EMBC 2023"]}, "abstract": "Myocardial infarction (MI) is one of the most prevalent cardiovascular diseases with associated clinical decision-making typically based on single-valued imaging biomarkers. However, such metrics only approximate the complex 3D structure and physiology of the heart and hence hinder a better understanding and prediction of MI outcomes. In this work, we investigate the utility of complete 3D cardiac shapes in the form of point clouds for an improved detection of MI events. To this end, we propose a fully automatic multi-step pipeline consisting of a 3D cardiac surface reconstruction step followed by a point cloud classification network. Our method utilizes recent advances in geometric deep learning on point clouds to enable direct and efficient multi-scale learning on high-resolution surface models of the cardiac anatomy. We evaluate our approach on 1068 UK Biobank subjects for the tasks of prevalent MI detection and incident MI prediction and find improvements of ~13% and ~5% respectively over clinical benchmarks. Furthermore, we analyze the role of each ventricle and cardiac phase for 3D shape-based MI detection and conduct a visual analysis of the morphological and physiological patterns typically associated with MI outcomes.", "url": "https://arxiv.org/abs/2307.07298"}, {"metadata": {"arXiv": "2307.07313", "Date": "Fri, 14 Jul 2023 12:46:59 ", "Title": "HEAL-SWIN: A Vision Transformer On The Sphere", "Authors": ["Oscar Carlsson", "Jan E. Gerken", "Hampus Linander", "Heiner Spie{\\ss}", "Fredrik Ohlsson", "Christoffer Petersson", "Daniel Persson"], "Categories": "cs.CV cs.LG", "Comments": ["Main body: 10 pages", "7 figures. Appendices: 4 pages", "2 figures"]}, "abstract": "High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patching and windowing operations of the SWIN transformer, resulting in a one-dimensional representation of the spherical data with minimal computational overhead. We demonstrate the superior performance of our model for semantic segmentation and depth regression tasks on both synthetic and real automotive datasets. Our code is available at https://github.com/JanEGerken/HEAL-SWIN.", "url": "https://arxiv.org/abs/2307.07313"}, {"metadata": {"arXiv": "2307.07378", "Date": "Fri, 14 Jul 2023 14:36:58 ", "Title": "Defect Classification in Additive Manufacturing Using CNN-Based Vision Processing", "Authors": ["Xiao Liu and Alessandra Mileo and Alan F. Smeaton"], "Categories": "cs.CV cs.LG", "Comments": ["4 pages", "accepted at the Irish Machine Vision and Image Processing Conference (IMVIP)", "Galway", "August 2023"]}, "abstract": "The development of computer vision and in-situ monitoring using visual sensors allows the collection of large datasets from the additive manufacturing (AM) process. Such datasets could be used with machine learning techniques to improve the quality of AM. This paper examines two scenarios: first, using convolutional neural networks (CNNs) to accurately classify defects in an image dataset from AM and second, applying active learning techniques to the developed classification model. This allows the construction of a human-in-the-loop mechanism to reduce the size of the data required to train and generate training data.", "url": "https://arxiv.org/abs/2307.07378"}, {"metadata": {"arXiv": "2307.07397", "Date": "Fri, 14 Jul 2023 15:15:45 ", "Title": "Improving Zero-Shot Generalization for CLIP with Synthesized Prompts", "Authors": ["Zhengbo Wang", "Jian Liang", "Ran He", "Nan Xu", "Zilei Wang", "Tieniu Tan"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted by ICCV 2023"]}, "abstract": "With the growing interest in pretrained vision-language models like CLIP, recent research has focused on adapting these models to downstream tasks. Despite achieving promising results, most existing methods require labeled data for all classes, which may not hold in real-world applications due to the long tail and Zipf's law. For example, some classes may lack labeled data entirely, such as emerging concepts. To address this problem, we propose a plug-and-play generative approach called \\textbf{S}ynt\\textbf{H}es\\textbf{I}zed \\textbf{P}rompts~(\\textbf{SHIP}) to improve existing fine-tuning methods. Specifically, we follow variational autoencoders to introduce a generator that reconstructs the visual features by inputting the synthesized prompts and the corresponding class names to the textual encoder of CLIP. In this manner, we easily obtain the synthesized features for the remaining label-only classes. Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled and synthesized features. Extensive experiments on base-to-new generalization, cross-dataset transfer learning, and generalized zero-shot learning demonstrate the superiority of our approach. The code is available at \\url{https://github.com/mrflogs/SHIP}.", "url": "https://arxiv.org/abs/2307.07397"}, {"metadata": {"arXiv": "2307.07487", "Date": "Fri, 14 Jul 2023 17:17:17 ", "Title": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models", "Authors": ["Daiqing Li", "Huan Ling", "Amlan Kar", "David Acuna", "Seung Wook Kim", "Karsten Kreis", "Antonio Torralba", "Sanja Fidler"], "Categories": "cs.CV cs.LG", "Comments": ["Project page: https://research.nvidia.com/labs/toronto-ai/DreamTeacher/"]}, "abstract": "In this work, we introduce a self-supervised feature representation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for specific perception tasks. We investigate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an alternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple generative models, dense prediction benchmarks, and several pre-training regimes. We empirically find that our DreamTeacher significantly outperforms existing self-supervised representation learning approaches across the board. Unsupervised ImageNet pre-training with DreamTeacher leads to significant improvements over ImageNet classification pre-training on downstream datasets, showcasing generative models, and diffusion generative models specifically, as a promising approach to representation learning on large, diverse datasets without requiring manual annotation.", "url": "https://arxiv.org/abs/2307.07487"}, {"metadata": {"arXiv": "2307.07172", "Date": "Fri, 14 Jul 2023 05:51:04 ", "Title": "FedBIAD: Communication-Efficient and Accuracy-Guaranteed Federated Learning with Bayesian Inference-Based Adaptive Dropout", "Authors": ["Jingjing Xue and Min Liu and Sheng Sun and Yuwei Wang and Hui Jiang and Xuefeng Jiang"], "Categories": "cs.DC cs.LG", "Journal-ref": "2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"}, "abstract": "Federated Learning (FL) emerges as a distributed machine learning paradigm without end-user data transmission, effectively avoiding privacy leakage. Participating devices in FL are usually bandwidth-constrained, and the uplink is much slower than the downlink in wireless networks, which causes a severe uplink communication bottleneck. A prominent direction to alleviate this problem is federated dropout, which drops fractional weights of local models. However, existing federated dropout studies focus on random or ordered dropout and lack theoretical support, resulting in unguaranteed performance. In this paper, we propose Federated learning with Bayesian Inference-based Adaptive Dropout (FedBIAD), which regards weight rows of local models as probability distributions and adaptively drops partial weight rows based on importance indicators correlated with the trend of local training loss. By applying FedBIAD, each client adaptively selects a high-quality dropping pattern with accurate approximations and only transmits parameters of non-dropped weight rows to mitigate uplink costs while improving accuracy. Theoretical analysis demonstrates that the convergence rate of the average generalization error of FedBIAD is minimax optimal up to a squared logarithmic factor. Extensive experiments on image classification and next-word prediction show that compared with status quo approaches, FedBIAD provides 2x uplink reduction with an accuracy increase of up to 2.41% even on non-Independent and Identically Distributed (non-IID) data, which brings up to 72% decrease in training time.", "url": "https://arxiv.org/abs/2307.07172"}, {"metadata": {"arXiv": "2307.07199", "Date": "Fri, 14 Jul 2023 07:19:20 ", "Title": "Ed-Fed: A generic federated learning framework with resource-aware client selection for edge devices", "Authors": ["Zitha Sasindran", "Harsha Yelchuri", "T. V. Prabhakar"], "Categories": "cs.DC cs.LG"}, "abstract": "Federated learning (FL) has evolved as a prominent method for edge devices to cooperatively create a unified prediction model while securing their sensitive training data local to the device. Despite the existence of numerous research frameworks for simulating FL algorithms, they do not facilitate comprehensive deployment for automatic speech recognition tasks on heterogeneous edge devices. This is where Ed-Fed, a comprehensive and generic FL framework, comes in as a foundation for future practical FL system research. We also propose a novel resource-aware client selection algorithm to optimise the waiting time in the FL settings. We show that our approach can handle the straggler devices and dynamically set the training time for the selected devices in a round. Our evaluation has shown that the proposed approach significantly optimises waiting time in FL compared to conventional random client selection methods.", "url": "https://arxiv.org/abs/2307.07199"}, {"metadata": {"arXiv": "2307.06966", "Date": "Thu, 13 Jul 2023 09:39:10 ", "Title": "Layerwise Linear Mode Connectivity", "Authors": ["Linara Adilova", "Asja Fischer", "Martin Jaggi"], "Categories": "cs.LG", "Comments": ["HLD 2023: 1st Workshop on High-dimensional Learning Dynamics", "ICML 2023", "Hawaii", "USA"]}, "abstract": "In the federated setup one performs an aggregation of separate local models multiple times during training in order to obtain a stronger global model; most often aggregation is a simple averaging of the parameters. Understanding when and why averaging works in a non-convex setup, such as federated deep learning, is an open challenge that hinders obtaining highly performant global models. On i.i.d.~datasets federated deep learning with frequent averaging is successful. The common understanding, however, is that during the independent training models are drifting away from each other and thus averaging may not work anymore after many local parameter updates. The problem can be seen from the perspective of the loss surface: for points on a non-convex surface the average can become arbitrarily bad. The assumption of local convexity, often used to explain the success of federated averaging, contradicts to the empirical evidence showing that high loss barriers exist between models from the very beginning of the learning, even when training on the same data. Based on the observation that the learning process evolves differently in different layers, we investigate the barrier between models in a layerwise fashion. Our conjecture is that barriers preventing from successful federated training are caused by a particular layer or group of layers.", "url": "https://arxiv.org/abs/2307.06966"}, {"metadata": {"arXiv": "2307.06975", "Date": "Thu, 13 Jul 2023 13:52:41 ", "Title": "Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0", "Authors": ["Luigi Capogrosso", "Alessio Mascolini", "Federico Girella", "Geri Skenderi", "Sebastiano Gaiardelli", "Nicola Dall'Ora", "Francesco Ponzio", "Enrico Fraccaroli", "Santa Di Cataldo", "Sara Vinco", "Enrico Macii", "Franco Fummi", "Marco Cristani"], "Categories": "cs.LG", "Comments": ["Accepted at the 26th Forum on specification and Design Languages (FDL 2023)"]}, "abstract": "Industry 4.0 involves the integration of digital technologies, such as IoT, Big Data, and AI, into manufacturing and industrial processes to increase efficiency and productivity. As these technologies become more interconnected and interdependent, Industry 4.0 systems become more complex, which brings the difficulty of identifying and stopping anomalies that may cause disturbances in the manufacturing process. This paper aims to propose a diffusion-based model for real-time anomaly prediction in Industry 4.0 processes. Using a neuro-symbolic approach, we integrate industrial ontologies in the model, thereby adding formal knowledge on smart manufacturing. Finally, we propose a simple yet effective way of distilling diffusion models through Random Fourier Features for deployment on an embedded system for direct integration into the manufacturing process. To the best of our knowledge, this approach has never been explored before.", "url": "https://arxiv.org/abs/2307.06975"}, {"metadata": {"arXiv": "2307.06978", "Date": "Thu, 13 Jul 2023 14:36:16 ", "Title": "A decision framework for selecting information-transfer strategies in population-based SHM", "Authors": ["Aidan J. Hughes", "Jack Poole", "Nikolaos Dervilis", "Paul Gardner", "Keith Worden"], "Categories": "cs.LG", "Comments": ["12 pages", "2 figures. Author accepted manuscript in Proceedings of the 14th International Workshop on Structural Health Monitoring", "Stanford University", "California", "USA. 2023"]}, "abstract": "Decision-support for the operation and maintenance of structures provides significant motivation for the development and implementation of structural health monitoring (SHM) systems. Unfortunately, the limited availability of labelled training data hinders the development of the statistical models on which these decision-support systems rely. Population-based SHM seeks to mitigate the impact of data scarcity by using transfer learning techniques to share information between individual structures within a population. The current paper proposes a decision framework for selecting transfer strategies based upon a novel concept -- the expected value of information transfer -- such that negative transfer is avoided. By avoiding negative transfer, and by optimising information transfer strategies using the transfer-decision framework, one can reduce the costs associated with operating and maintaining structures, and improve safety.", "url": "https://arxiv.org/abs/2307.06978"}, {"metadata": {"arXiv": "2307.07055", "Date": "Thu, 13 Jul 2023 20:20:40 ", "Title": "Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement", "Authors": ["Hui Yuan", "Kaixuan Huang", "Chengzhuo Ni", "Minshuo Chen", "Mengdi Wang"], "Categories": "cs.LG"}, "abstract": "We explore the methodology and theory of reward-directed generation via conditional diffusion models. Directed generation aims to generate samples with desired properties as measured by a reward function, which has broad applications in generative AI, reinforcement learning, and computational biology. We consider the common learning scenario where the data set consists of unlabeled data along with a smaller set of data with noisy reward labels. Our approach leverages a learned reward function on the smaller data set as a pseudolabeler. From a theoretical standpoint, we show that this directed generator can effectively learn and sample from the reward-conditioned data distribution. Additionally, our model is capable of recovering the latent subspace representation of data. Moreover, we establish that the model generates a new population that moves closer to a user-specified target reward value, where the optimality gap aligns with the off-policy bandit regret in the feature subspace. The improvement in rewards obtained is influenced by the interplay between the strength of the reward signal, the distribution shift, and the cost of off-support extrapolation. We provide empirical results to validate our theory and highlight the relationship between the strength of extrapolation and the quality of generated samples.", "url": "https://arxiv.org/abs/2307.07055"}, {"metadata": {"arXiv": "2307.07072", "Date": "Thu, 13 Jul 2023 21:42:26 ", "Title": "Rician likelihood loss for quantitative MRI using self-supervised deep learning", "Authors": ["Christopher S. Parker", "Anna Schroder", "Sean C. Epstein", "James Cole", "Daniel C. Alexander", "Hui Zhang"], "Categories": "cs.LG cs.CE eess.IV q-bio.QM stat.ML", "Comments": ["16 pages", "6 figures"]}, "abstract": "Purpose: Previous quantitative MR imaging studies using self-supervised deep learning have reported biased parameter estimates at low SNR. Such systematic errors arise from the choice of Mean Squared Error (MSE) loss function for network training, which is incompatible with Rician-distributed MR magnitude signals. To address this issue, we introduce the negative log Rician likelihood (NLR) loss. Methods: A numerically stable and accurate implementation of the NLR loss was developed to estimate quantitative parameters of the apparent diffusion coefficient (ADC) model and intra-voxel incoherent motion (IVIM) model. Parameter estimation accuracy, precision and overall error were evaluated in terms of bias, variance and root mean squared error and compared against the MSE loss over a range of SNRs (5 - 30). Results: Networks trained with NLR loss show higher estimation accuracy than MSE for the ADC and IVIM diffusion coefficients as SNR decreases, with minimal loss of precision or total error. At high effective SNR (high SNR and small diffusion coefficients), both losses show comparable accuracy and precision for all parameters of both models. Conclusion: The proposed NLR loss is numerically stable and accurate across the full range of tested SNRs and improves parameter estimation accuracy of diffusion coefficients using self-supervised deep learning. We expect the development to benefit quantitative MR imaging techniques broadly, enabling more accurate parameter estimation from noisy data.", "url": "https://arxiv.org/abs/2307.07072"}, {"metadata": {"arXiv": "2307.07081", "Date": "Thu, 13 Jul 2023 22:23:05 ", "Title": "Kernel t-distributed stochastic neighbor embedding", "Authors": ["Denis C. Ilie-Ablachim", "Bogdan Dumitrescu", "Cristian Rusu"], "Categories": "cs.LG"}, "abstract": "This paper presents a kernelized version of the t-SNE algorithm, capable of mapping high-dimensional data to a low-dimensional space while preserving the pairwise distances between the data points in a non-Euclidean metric. This can be achieved using a kernel trick only in the high dimensional space or in both spaces, leading to an end-to-end kernelized version. The proposed kernelized version of the t-SNE algorithm can offer new views on the relationships between data points, which can improve performance and accuracy in particular applications, such as classification problems involving kernel methods. The differences between t-SNE and its kernelized version are illustrated for several datasets, showing a neater clustering of points belonging to different classes.", "url": "https://arxiv.org/abs/2307.07081"}, {"metadata": {"arXiv": "2307.07083", "Date": "Thu, 13 Jul 2023 22:35:53 ", "Title": "A Scenario-Based Functional Testing Approach to Improving DNN Performance", "Authors": ["Hong Zhu", "Thi Minh Tam Tran", "Aduen Benjumea and Andrew Bradley"], "Categories": "cs.LG cs.SE", "Comments": ["The paper is accepted to appear in the proceedings of IEEE 17th International Conference on Service-oriented Systems Engineering (IEEE SOSE 2023) as an invited paper of 2023 IEEE CISOSE Congress"]}, "abstract": "This paper proposes a scenario-based functional testing approach for enhancing the performance of machine learning (ML) applications. The proposed method is an iterative process that starts with testing the ML model on various scenarios to identify areas of weakness. It follows by a further testing on the suspected weak scenarios and statistically evaluate the model's performance on the scenarios to confirm the diagnosis. Once the diagnosis of weak scenarios is confirmed by test results, the treatment of the model is performed by retraining the model using a transfer learning technique with the original model as the base and applying a set of training data specifically targeting the treated scenarios plus a subset of training data selected at random from the original train dataset to prevent the so-call catastrophic forgetting effect. Finally, after the treatment, the model is assessed and evaluated again by testing on the treated scenarios as well as other scenarios to check if the treatment is effective and no side effect caused. The paper reports a case study with a real ML deep neural network (DNN) model, which is the perception system of an autonomous racing car. It is demonstrated that the method is effective in the sense that DNN model's performance can be improved. It provides an efficient method of enhancing ML model's performance with much less human and compute resource than retrain from scratch.", "url": "https://arxiv.org/abs/2307.07083"}, {"metadata": {"arXiv": "2307.07093", "Date": "Thu, 13 Jul 2023 23:52:41 ", "Title": "MaxCorrMGNN: A Multi-Graph Neural Network Framework for Generalized Multimodal Fusion of Medical Data for Outcome Prediction", "Authors": ["Niharika S. D'Souza", "Hongzhi Wang", "Andrea Giovannini", "Antonio Foncubierta-Rodriguez", "Kristen L. Beck", "Orest Boyko", "Tanveer Syeda-Mahmood"], "Categories": "cs.LG eess.SP", "Comments": ["To appear in ML4MHD workshop at ICML 2023"]}, "abstract": "With the emergence of multimodal electronic health records, the evidence for an outcome may be captured across multiple modalities ranging from clinical to imaging and genomic data. Predicting outcomes effectively requires fusion frameworks capable of modeling fine-grained and multi-faceted complex interactions between modality features within and across patients. We develop an innovative fusion approach called MaxCorr MGNN that models non-linear modality correlations within and across patients through Hirschfeld-Gebelein-Renyi maximal correlation (MaxCorr) embeddings, resulting in a multi-layered graph that preserves the identities of the modalities and patients. We then design, for the first time, a generalized multi-layered graph neural network (MGNN) for task-informed reasoning in multi-layered graphs, that learns the parameters defining patient-modality graph connectivity and message passing in an end-to-end fashion. We evaluate our model an outcome prediction task on a Tuberculosis (TB) dataset consistently outperforming several state-of-the-art neural, graph-based and traditional fusion techniques.", "url": "https://arxiv.org/abs/2307.07093"}, {"metadata": {"arXiv": "2307.07107", "Date": "Fri, 14 Jul 2023 01:04:18 ", "Title": "Graph Positional and Structural Encoder", "Authors": ["Renming Liu", "Semih Cant\\\"urk", "Olivier Lapointe-Gagn\\'e", "Vincent L\\'etourneau", "Guy Wolf", "Dominique Beaini", "Ladislav Ramp\\'a\\v{s}ek"], "Categories": "cs.LG"}, "abstract": "Positional and structural encodings (PSE) enable better identifiability of nodes within a graph, as in general graphs lack a canonical node ordering. This renders PSEs essential tools for empowering modern GNNs, and in particular graph Transformers. However, designing PSEs that work optimally for a variety of graph prediction tasks is a challenging and unsolved problem. Here, we present the graph positional and structural encoder (GPSE), a first-ever attempt to train a graph encoder that captures rich PSE representations for augmenting any GNN. GPSE can effectively learn a common latent representation for multiple PSEs, and is highly transferable. The encoder trained on a particular graph dataset can be used effectively on datasets drawn from significantly different distributions and even modalities. We show that across a wide range of benchmarks, GPSE-enhanced models can significantly improve the performance in certain tasks, while performing on par with those that employ explicitly computed PSEs in other cases. Our results pave the way for the development of large pre-trained models for extracting graph positional and structural information and highlight their potential as a viable alternative to explicitly computed PSEs as well as to existing self-supervised pre-training approaches.", "url": "https://arxiv.org/abs/2307.07107"}, {"metadata": {"arXiv": "2307.07191", "Date": "Fri, 14 Jul 2023 06:50:02 ", "Title": "Benchmarks and Custom Package for Electrical Load Forecasting", "Authors": ["Zhixian Wang", "Qingsong Wen", "Chaoli Zhang", "Liang Sun", "Leandro Von Krannichfeldt", "and Yi Wang"], "Categories": "cs.LG stat.ML"}, "abstract": "Load forecasting is of great significance in the power industry as it can provide a reference for subsequent tasks such as power grid dispatch, thus bringing huge economic benefits. However, there are many differences between load forecasting and traditional time series forecasting. On the one hand, load forecasting aims to minimize the cost of subsequent tasks such as power grid dispatch, rather than simply pursuing prediction accuracy. On the other hand, the load is largely influenced by many external factors, such as temperature or calendar variables. In addition, the scale of predictions (such as building-level loads and aggregated-level loads) can also significantly impact the predicted results. In this paper, we provide a comprehensive load forecasting archive, which includes load domain-specific feature engineering to help forecasting models better model load data. In addition, different from the traditional loss function which only aims for accuracy, we also provide a method to customize the loss function based on the forecasting error, integrating it into our forecasting framework. Based on this, we conducted extensive experiments on load data at different levels, providing a reference for researchers to compare different load forecasting models.", "url": "https://arxiv.org/abs/2307.07191"}, {"metadata": {"arXiv": "2307.07195", "Date": "Fri, 14 Jul 2023 07:05:17 ", "Title": "Controlling dynamical systems to complex target states using machine learning: next-generation vs. classical reservoir computing", "Authors": ["Alexander Haluszczynski", "Daniel K\\\"oglmayr", "Christoph R\\\"ath"], "Categories": "cs.LG cs.NE cs.SY eess.SY nlin.CD", "Comments": ["IJCNN 2023"]}, "abstract": "Controlling nonlinear dynamical systems using machine learning allows to not only drive systems into simple behavior like periodicity but also to more complex arbitrary dynamics. For this, it is crucial that a machine learning system can be trained to reproduce the target dynamics sufficiently well. On the example of forcing a chaotic parametrization of the Lorenz system into intermittent dynamics, we show first that classical reservoir computing excels at this task. In a next step, we compare those results based on different amounts of training data to an alternative setup, where next-generation reservoir computing is used instead. It turns out that while delivering comparable performance for usual amounts of training data, next-generation RC significantly outperforms in situations where only very limited data is available. This opens even further practical control applications in real world problems where data is restricted.", "url": "https://arxiv.org/abs/2307.07195"}, {"metadata": {"arXiv": "2307.07264", "Date": "Fri, 14 Jul 2023 10:38:30 ", "Title": "On Interpolating Experts and Multi-Armed Bandits", "Authors": ["Houshuang Chen", "Yuchen He", "Chihao Zhang"], "Categories": "cs.LG stat.ML"}, "abstract": "Learning with expert advice and multi-armed bandit are two classic online decision problems which differ on how the information is observed in each round of the game. We study a family of problems interpolating the two. For a vector $\\mathbf{m}=(m_1,\\dots,m_K)\\in \\mathbb{N}^K$, an instance of $\\mathbf{m}$-MAB indicates that the arms are partitioned into $K$ groups and the $i$-th group contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same group are observed. We prove tight minimax regret bounds for $\\mathbf{m}$-MAB and design an optimal PAC algorithm for its pure exploration version, $\\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with as few rounds as possible. We show that the minimax regret of $\\mathbf{m}$-MAB is $\\Theta\\left(\\sqrt{T\\sum_{k=1}^K\\log (m_k+1)}\\right)$ and the minimum number of pulls for an $(\\epsilon,0.05)$-PAC algorithm of $\\mathbf{m}$-BAI is $\\Theta\\left(\\frac{1}{\\epsilon^2}\\cdot \\sum_{k=1}^K\\log (m_k+1)\\right)$. Both our upper bounds and lower bounds for $\\mathbf{m}$-MAB can be extended to a more general setting, namely the bandit with graph feedback, in terms of the clique cover and related graph parameters. As consequences, we obtained tight minimax regret bounds for several families of feedback graphs.", "url": "https://arxiv.org/abs/2307.07264"}, {"metadata": {"arXiv": "2307.07343", "Date": "Fri, 14 Jul 2023 13:46:35 ", "Title": "MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters", "Authors": ["Linkai Luo", "Qiaoling Yang", "Hong Peng", "Yiding Wang", "Ziyang Chen"], "Categories": "cs.LG"}, "abstract": "The selection of model's parameters plays an important role in the application of support vector classification (SVC). The commonly used method of selecting model's parameters is the k-fold cross validation with grid search (CV). It is extremely time-consuming because it needs to train a large number of SVC models. In this paper, a new method is proposed to train SVC with the selection of model's parameters. Firstly, training SVC with the selection of model's parameters is modeled as a minimax optimization problem (MaxMin-L2-SVC-NCH), in which the minimization problem is an optimization problem of finding the closest points between two normal convex hulls (L2-SVC-NCH) while the maximization problem is an optimization problem of finding the optimal model's parameters. A lower time complexity can be expected in MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm is then proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by a projected gradient algorithm (PGA) while the maximization problem is solved by a gradient ascent algorithm with dynamic learning rate. To demonstrate the advantages of the PGA in solving L2-SVC-NCH, we carry out a comparison of the PGA and the famous sequential minimal optimization (SMO) algorithm after a SMO algorithm and some KKT conditions for L2-SVC-NCH are provided. It is revealed that the SMO algorithm is a special case of the PGA. Thus, the PGA can provide more flexibility. The comparative experiments between MaxMin-L2-SVC-NCH and the classical parameter selection models on public datasets show that MaxMin-L2-SVC-NCH greatly reduces the number of models to be trained and the test accuracy is not lost to the classical models. It indicates that MaxMin-L2-SVC-NCH performs better than the other models. We strongly recommend MaxMin-L2-SVC-NCH as a preferred model for SVC task.", "url": "https://arxiv.org/abs/2307.07343"}, {"metadata": {"arXiv": "2307.07344", "Date": "Fri, 14 Jul 2023 13:47:05 ", "Title": "Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks", "Authors": ["Chaoyu Liu", "Zhonghua Qiao", "Chao Li and Carola-Bibiane Sch\\\"onlieb"], "Categories": "cs.LG cs.NA math.NA"}, "abstract": "This paper proposes a novel approach to integrating partial differential equation (PDE)-based evolution models into neural networks through a new type of regularization. Specifically, we propose inverse evolution layers (IELs) based on evolution equations. These layers can achieve specific regularization objectives and endow neural networks' outputs with corresponding properties of the evolution models. Moreover, IELs are straightforward to construct and implement, and can be easily designed for various physical evolutions and neural networks. Additionally, the design process for these layers can provide neural networks with intuitive and mathematical interpretability, thus enhancing the transparency and explainability of the approach. To demonstrate the effectiveness, efficiency, and simplicity of our approach, we present an example of endowing semantic segmentation models with the smoothness property based on the heat diffusion model. To achieve this goal, we design heat-diffusion IELs and apply them to address the challenge of semantic segmentation with noisy labels. The experimental results demonstrate that the heat-diffusion IELs can effectively mitigate the overfitting problem caused by noisy labels.", "url": "https://arxiv.org/abs/2307.07344"}, {"metadata": {"arXiv": "2307.07346", "Date": "Fri, 14 Jul 2023 13:50:00 ", "Title": "A testing-based approach to assess the clusterability of categorical data", "Authors": ["Lianyu Hu", "Junjie Dong", "Mudi Jiang", "Yan Liu", "Zengyou He"], "Categories": "cs.LG stat.AP", "Comments": ["19 pages", "13 figures"]}, "abstract": "The objective of clusterability evaluation is to check whether a clustering structure exists within the data set. As a crucial yet often-overlooked issue in cluster analysis, it is essential to conduct such a test before applying any clustering algorithm. If a data set is unclusterable, any subsequent clustering analysis would not yield valid results. Despite its importance, the majority of existing studies focus on numerical data, leaving the clusterability evaluation issue for categorical data as an open problem. Here we present TestCat, a testing-based approach to assess the clusterability of categorical data in terms of an analytical $p$-value. The key idea underlying TestCat is that clusterable categorical data possess many strongly correlated attribute pairs and hence the sum of chi-squared statistics of all attribute pairs is employed as the test statistic for $p$-value calculation. We apply our method to a set of benchmark categorical data sets, showing that TestCat outperforms those solutions based on existing clusterability evaluation methods for numeric data. To the best of our knowledge, our work provides the first way to effectively recognize the clusterability of categorical data in a statistically sound manner.", "url": "https://arxiv.org/abs/2307.07346"}, {"metadata": {"arXiv": "2307.07389", "Date": "Fri, 14 Jul 2023 14:58:44 ", "Title": "Learning Sparse Neural Networks with Identity Layers", "Authors": ["Mingjian Ni", "Guangyao Chen", "Xiawu Zheng", "Peixi Peng", "Li Yuan", "Yonghong Tian"], "Categories": "cs.LG"}, "abstract": "The sparsity of Deep Neural Networks is well investigated to maximize the performance and reduce the size of overparameterized networks as possible. Existing methods focus on pruning parameters in the training process by using thresholds and metrics. Meanwhile, feature similarity between different layers has not been discussed sufficiently before, which could be rigorously proved to be highly correlated to the network sparsity in this paper. Inspired by interlayer feature similarity in overparameterized models, we investigate the intrinsic link between network sparsity and interlayer feature similarity. Specifically, we prove that reducing interlayer feature similarity based on Centered Kernel Alignment (CKA) improves the sparsity of the network by using information bottleneck theory. Applying such theory, we propose a plug-and-play CKA-based Sparsity Regularization for sparse network training, dubbed CKA-SR, which utilizes CKA to reduce feature similarity between layers and increase network sparsity. In other words, layers of our sparse network tend to have their own identity compared to each other. Experimentally, we plug the proposed CKA-SR into the training process of sparse network training methods and find that CKA-SR consistently improves the performance of several State-Of-The-Art sparse training methods, especially at extremely high sparsity. Code is included in the supplementary materials.", "url": "https://arxiv.org/abs/2307.07389"}, {"metadata": {"arXiv": "2307.07396", "Date": "Fri, 14 Jul 2023 15:12:55 ", "Title": "Visualizing Overlapping Biclusterings and Boolean Matrix Factorizations", "Authors": ["Thibault Marette", "Pauli Miettinen and Stefan Neumann"], "Categories": "cs.LG", "Comments": ["17 pages", "7 figures", "to be published in ECML PKDD 2023"]}, "abstract": "Finding (bi-)clusters in bipartite graphs is a popular data analysis approach. Analysts typically want to visualize the clusters, which is simple as long as the clusters are disjoint. However, many modern algorithms find overlapping clusters, making visualization more complicated. In this paper, we study the problem of visualizing \\emph{a given clustering} of overlapping clusters in bipartite graphs and the related problem of visualizing Boolean Matrix Factorizations. We conceptualize three different objectives that any good visualization should satisfy: (1) proximity of cluster elements, (2) large consecutive areas of elements from the same cluster, and (3) large uninterrupted areas in the visualization, regardless of the cluster membership. We provide objective functions that capture these goals and algorithms that optimize these objective functions. Interestingly, in experiments on real-world datasets, we find that the best trade-off between these competing goals is achieved by a novel heuristic, which locally aims to place rows and columns with similar cluster membership next to each other.", "url": "https://arxiv.org/abs/2307.07396"}, {"metadata": {"arXiv": "2307.07405", "Date": "Fri, 14 Jul 2023 15:31:45 ", "Title": "Performance of $\\ell_1$ Regularization for Sparse Convex Optimization", "Authors": ["Kyriakos Axiotis", "Taisuke Yasuda"], "Categories": "cs.LG cs.DS math.ST stat.ML stat.TH"}, "abstract": "Despite widespread adoption in practice, guarantees for the LASSO and Group LASSO are strikingly lacking in settings beyond statistical problems, and these algorithms are usually considered to be a heuristic in the context of sparse convex optimization on deterministic inputs. We give the first recovery guarantees for the Group LASSO for sparse convex optimization with vector-valued features. We show that if a sufficiently large Group LASSO regularization is applied when minimizing a strictly convex function $l$, then the minimizer is a sparse vector supported on vector-valued features with the largest $\\ell_2$ norm of the gradient. Thus, repeating this procedure selects the same set of features as the Orthogonal Matching Pursuit algorithm, which admits recovery guarantees for any function $l$ with restricted strong convexity and smoothness via weak submodularity arguments. This answers open questions of Tibshirani et al. and Yasuda et al. Our result is the first to theoretically explain the empirical success of the Group LASSO for convex functions under general input instances assuming only restricted strong convexity and smoothness. Our result also generalizes provable guarantees for the Sequential Attention algorithm, which is a feature selection algorithm inspired by the attention mechanism proposed by Yasuda et al. As an application of our result, we give new results for the column subset selection problem, which is well-studied when the loss is the Frobenius norm or other entrywise matrix losses. We give the first result for general loss functions for this problem that requires only restricted strong convexity and smoothness.", "url": "https://arxiv.org/abs/2307.07405"}, {"metadata": {"arXiv": "2307.07406", "Date": "Fri, 14 Jul 2023 15:35:57 ", "Title": "Improved Convergence Analysis and SNR Control Strategies for Federated Learning in the Presence of Noise", "Authors": ["Antesh Upadhyay and Abolfazl Hashemi"], "Categories": "cs.LG cs.IT eess.SP math.IT", "Journal-ref": "in IEEE Access, vol. 11, pp. 63398-63416, 2023", "DOI": "10.1109/ACCESS.2023.3288613"}, "abstract": "We propose an improved convergence analysis technique that characterizes the distributed learning paradigm of federated learning (FL) with imperfect/noisy uplink and downlink communications. Such imperfect communication scenarios arise in the practical deployment of FL in emerging communication systems and protocols. The analysis developed in this paper demonstrates, for the first time, that there is an asymmetry in the detrimental effects of uplink and downlink communications in FL. In particular, the adverse effect of the downlink noise is more severe on the convergence of FL algorithms. Using this insight, we propose improved Signal-to-Noise (SNR) control strategies that, discarding the negligible higher-order terms, lead to a similar convergence rate for FL as in the case of a perfect, noise-free communication channel while incurring significantly less power resources compared to existing solutions. In particular, we establish that to maintain the $O(\\frac{1}{\\sqrt{K}})$ rate of convergence like in the case of noise-free FL, we need to scale down the uplink and downlink noise by $\\Omega({\\sqrt{k}})$ and $\\Omega({k})$ respectively, where $k$ denotes the communication round, $k=1,\\dots, K$. Our theoretical result is further characterized by two major benefits: firstly, it does not assume the somewhat unrealistic assumption of bounded client dissimilarity, and secondly, it only requires smooth non-convex loss functions, a function class better suited for modern machine learning and deep learning models. We also perform extensive empirical analysis to verify the validity of our theoretical findings.", "url": "https://arxiv.org/abs/2307.07406"}, {"metadata": {"arXiv": "2307.07410", "Date": "Thu, 13 Jul 2023 13:27:51 ", "Title": "Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks", "Authors": ["Johan S. Wind", "Vegard Antun", "Anders C. Hansen"], "Categories": "cs.LG math.OC stat.ML", "MSC-class": "90C25, 68T07, 90C17 (Primary) 15A29, 94A08, 46N10 (Secondary)"}, "abstract": "Understanding the implicit regularization imposed by neural network architectures and gradient based optimization methods is a key challenge in deep learning and AI. In this work we provide sharp results for the implicit regularization imposed by the gradient flow of Diagonal Linear Networks (DLNs) in the over-parameterized regression setting and, potentially surprisingly, link this to the phenomenon of phase transitions in generalized hardness of approximation (GHA). GHA generalizes the phenomenon of hardness of approximation from computer science to, among others, continuous and robust optimization. It is well-known that the $\\ell^1$-norm of the gradient flow of DLNs with tiny initialization converges to the objective function of basis pursuit. We improve upon these results by showing that the gradient flow of DLNs with tiny initialization approximates minimizers of the basis pursuit optimization problem (as opposed to just the objective function), and we obtain new and sharp convergence bounds w.r.t.\\ the initialization size. Non-sharpness of our results would imply that the GHA phenomenon would not occur for the basis pursuit optimization problem -- which is a contradiction -- thus implying sharpness. Moreover, we characterize $\\textit{which}$ $\\ell_1$ minimizer of the basis pursuit problem is chosen by the gradient flow whenever the minimizer is not unique. Interestingly, this depends on the depth of the DLN.", "url": "https://arxiv.org/abs/2307.07410"}, {"metadata": {"arXiv": "2307.07412", "Date": "Fri, 14 Jul 2023 15:41:43 ", "Title": "HuCurl: Human-induced Curriculum Discovery", "Authors": ["Mohamed Elgaar", "Hadi Amiri"], "Categories": "cs.LG cs.CL", "Comments": ["In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)"]}, "abstract": "We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as opposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.", "url": "https://arxiv.org/abs/2307.07412"}, {"metadata": {"arXiv": "2307.07477", "Date": "Fri, 14 Jul 2023 16:59:08 ", "Title": "Population Expansion for Training Language Models with Private Federated Learning", "Authors": ["Tatsuki Koga", "Congzheng Song", "Martin Pelikan", "Mona Chitnis"], "Categories": "cs.LG cs.CL cs.CR"}, "abstract": "Federated learning (FL) combined with differential privacy (DP) offers machine learning (ML) training with distributed devices and with a formal privacy guarantee. With a large population of devices, FL with DP produces a performant model in a timely manner. However, for applications with a smaller population, not only does the model utility degrade as the DP noise is inversely proportional to population, but also the training latency increases since waiting for enough clients to become available from a smaller pool is slower. In this work, we thus propose expanding the population based on domain adaptation techniques to speed up the training and improves the final model quality when training with small populations. We empirically demonstrate that our techniques can improve the utility by 13% to 30% on real-world language modeling datasets.", "url": "https://arxiv.org/abs/2307.07477"}, {"metadata": {"arXiv": "2307.07489", "Date": "Fri, 14 Jul 2023 17:21:41 ", "Title": "PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation", "Authors": ["Dapeng Hu", "Jian Liang", "Xinchao Wang", "Chuan-Sheng Foo"], "Categories": "cs.LG cs.CV"}, "abstract": "Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in improving the accuracy of models for unlabeled target domains. However, the calibration of predictive uncertainty in the target domain, a crucial aspect of the safe deployment of UDA models, has received limited attention. The conventional in-domain calibration method, \\textit{temperature scaling} (TempScal), encounters challenges due to domain distribution shifts and the absence of labeled target domain data. Recent approaches have employed importance-weighting techniques to estimate the target-optimal temperature based on re-weighted labeled source data. Nonetheless, these methods require source data and suffer from unreliable density estimates under severe domain shifts, rendering them unsuitable for source-free UDA settings. To overcome these limitations, we propose PseudoCal, a source-free calibration method that exclusively relies on unlabeled target data. Unlike previous approaches that treat UDA calibration as a \\textit{covariate shift} problem, we consider it as an unsupervised calibration problem specific to the target domain. Motivated by the factorization of the negative log-likelihood (NLL) objective in TempScal, we generate a labeled pseudo-target set that captures the structure of the real target. By doing so, we transform the unsupervised calibration problem into a supervised one, enabling us to effectively address it using widely-used in-domain methods like TempScal. Finally, we thoroughly evaluate the calibration performance of PseudoCal by conducting extensive experiments on 10 UDA methods, considering both traditional UDA settings and recent source-free UDA scenarios. The experimental results consistently demonstrate the superior performance of PseudoCal, exhibiting significantly reduced calibration error compared to existing calibration methods.", "url": "https://arxiv.org/abs/2307.07489"}, {"metadata": {"arXiv": "2307.07507", "Date": "Fri, 14 Jul 2023 17:56:48 ", "Title": "MGit: A Model Versioning and Management System", "Authors": ["Wei Hao and Daniel Mendoza and Rafael da Silva and Deepak Narayanan and Amar Phanishaye"], "Categories": "cs.LG cs.DC cs.SE"}, "abstract": "Models derived from other models are extremely common in machine learning (ML) today. For example, transfer learning is used to create task-specific models from \"pre-trained\" models through finetuning. This has led to an ecosystem where models are related to each other, sharing structure and often even parameter values. However, it is hard to manage these model derivatives: the storage overhead of storing all derived models quickly becomes onerous, prompting users to get rid of intermediate models that might be useful for further analysis. Additionally, undesired behaviors in models are hard to track down (e.g., is a bug inherited from an upstream model?). In this paper, we propose a model versioning and management system called MGit that makes it easier to store, test, update, and collaborate on model derivatives. MGit introduces a lineage graph that records provenance and versioning information between models, optimizations to efficiently store model parameters, as well as abstractions over this lineage graph that facilitate relevant testing, updating and collaboration functionality. MGit is able to reduce the lineage graph's storage footprint by up to 7x and automatically update downstream models in response to updates to upstream models.", "url": "https://arxiv.org/abs/2307.07507"}, {"metadata": {"arXiv": "2307.07512", "Date": "Fri, 14 Jul 2023 17:59:53 ", "Title": "Expressive Monotonic Neural Networks", "Authors": ["Ouail Kitouni", "Niklas Nolte", "Michael Williams"], "Categories": "cs.LG", "Comments": ["9 pages", "4 figures", "ICLR 2023 final submission"]}, "abstract": "The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce monotonic dependence, and is highly expressive. We show how the algorithm is used to train powerful, robust, and interpretable discriminators that achieve competitive performance compared to current state-of-the-art methods across various benchmarks, from social applications to the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider.", "url": "https://arxiv.org/abs/2307.07512"}, {"metadata": {"arXiv": "2307.07296", "Date": "Fri, 14 Jul 2023 12:19:46 ", "Title": "Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment", "Authors": ["Kenji Leong"], "Categories": "cs.RO cs.LG", "Comments": ["23 pages", "Journal"]}, "abstract": "Active Simultaneous Localisation and Mapping (SLAM) is a critical problem in autonomous robotics, enabling robots to navigate to new regions while building an accurate model of their surroundings. Visual SLAM is a popular technique that uses virtual elements to enhance the experience. However, existing frontier-based exploration strategies can lead to a non-optimal path in scenarios where there are multiple frontiers with similar distance. This issue can impact the efficiency and accuracy of Visual SLAM, which is crucial for a wide range of robotic applications, such as search and rescue, exploration, and mapping. To address this issue, this research combines both an existing Visual-Graph SLAM known as ExploreORB with reinforcement learning. The proposed algorithm allows the robot to learn and optimize exploration routes through a reward-based system to create an accurate map of the environment with proper frontier selection. Frontier-based exploration is used to detect unexplored areas, while reinforcement learning optimizes the robot's movement by assigning rewards for optimal frontier points. Graph SLAM is then used to integrate the robot's sensory data and build an accurate map of the environment. The proposed algorithm aims to improve the efficiency and accuracy of ExploreORB by optimizing the exploration process of frontiers to build a more accurate map. To evaluate the effectiveness of the proposed approach, experiments will be conducted in various virtual environments using Gazebo, a robot simulation software. Results of these experiments will be compared with existing methods to demonstrate the potential of the proposed approach as an optimal solution for SLAM in autonomous robotics.", "url": "https://arxiv.org/abs/2307.07296"}, {"metadata": {"arXiv": "2307.07059", "Date": "Thu, 13 Jul 2023 20:56:46 ", "Title": "Vertex-based Networks to Accelerate Path Planning Algorithms", "Authors": ["Yuanhang Zhang and Jundong Liu"], "Categories": "cs.AI", "Comments": ["Accepted to IEEE Workshop on Machine Learning for Signal Processing (MLSP'2023)"]}, "abstract": "Path planning plays a crucial role in various autonomy applications, and RRT* is one of the leading solutions in this field. In this paper, we propose the utilization of vertex-based networks to enhance the sampling process of RRT*, leading to more efficient path planning. Our approach focuses on critical vertices along the optimal paths, which provide essential yet sparser abstractions of the paths. We employ focal loss to address the associated data imbalance issue, and explore different masking configurations to determine practical tradeoffs in system performance. Through experiments conducted on randomly generated floor maps, our solutions demonstrate significant speed improvements, achieving over a 400% enhancement compared to the baseline model.", "url": "https://arxiv.org/abs/2307.07059"}, {"metadata": {"arXiv": "2307.07448", "Date": "Tue, 11 Jul 2023 07:01:35 ", "Title": "Depth-bounded Epistemic Logic", "Authors": ["Farid Arthaud (Massachusetts Institute of Technology)", "Martin Rinard (Massachusetts Institute of Technology)"], "Categories": "cs.AI cs.LO cs.MA", "Comments": ["In Proceedings TARK 2023", "arXiv:2307.04005"], "Report-no": "EPTCS 379-7", "ACM-class": "F.4.1; I.2.11", "Journal-ref": "EPTCS 379, 2023, pp. 46-65", "DOI": "10.4204/EPTCS.379.7"}, "abstract": "Epistemic logics model how agents reason about their beliefs and the beliefs of other agents. Existing logics typically assume the ability of agents to reason perfectly about propositions of unbounded modal depth. We present DBEL, an extension of S5 that models agents that can reason about epistemic formulas only up to a specific modal depth. To support explicit reasoning about agent depths, DBEL includes depth atoms Ead (agent a has depth exactly d) and Pad (agent a has depth at least d). We provide a sound and complete axiomatization of DBEL. We extend DBEL to support public announcements for bounded depth agents and show how the resulting DPAL logic generalizes standard axioms from public announcement logic. We present two alternate extensions and identify two undesirable properties, amnesia and knowledge leakage, that these extensions have but DPAL does not. We provide axiomatizations of these logics as well as complexity results for satisfiability and model checking. Finally, we use these logics to illustrate how agents with bounded modal depth reason in the classical muddy children problem, including upper and lower bounds on the depth knowledge necessary for agents to successfully solve the problem.", "url": "https://arxiv.org/abs/2307.07448"}, {"metadata": {"arXiv": "2307.07046", "Date": "Thu, 13 Jul 2023 20:02:07 ", "Title": "A metric learning approach for endoscopic kidney stone identification", "Authors": ["Jorge Gonzalez-Zapata and Francisco Lopez-Tiro and Elias Villalvazo-Avila and Daniel Flores-Araiza and Jacques Hubert and Andres Mendez-Vazquez and Gilberto Ochoa-Ruiz and Christian Daul"], "Categories": "cs.CV cs.AI"}, "abstract": "Several Deep Learning (DL) methods have recently been proposed for an automated identification of kidney stones during an ureteroscopy to enable rapid therapeutic decisions. Even if these DL approaches led to promising results, they are mainly appropriate for kidney stone types for which numerous labelled data are available. However, only few labelled images are available for some rare kidney stone types. This contribution exploits Deep Metric Learning (DML) methods i) to handle such classes with few samples, ii) to generalize well to out of distribution samples, and iii) to cope better with new classes which are added to the database. The proposed Guided Deep Metric Learning approach is based on a novel architecture which was designed to learn data representations in an improved way. The solution was inspired by Few-Shot Learning (FSL) and makes use of a teacher-student approach. The teacher model (GEMINI) generates a reduced hypothesis space based on prior knowledge from the labeled data, and is used it as a guide to a student model (i.e., ResNet50) through a Knowledge Distillation scheme. Extensive tests were first performed on two datasets separately used for the recognition, namely a set of images acquired for the surfaces of the kidney stone fragments, and a set of images of the fragment sections. The proposed DML-approach improved the identification accuracy by 10% and 12% in comparison to DL-methods and other DML-approaches, respectively. Moreover, model embeddings from the two dataset types were merged in an organized way through a multi-view scheme to simultaneously exploit the information of surface and section fragments. Test with the resulting mixed model improves the identification accuracy by at least 3% and up to 30% with respect to DL-models and shallow machine learning methods, respectively.", "url": "https://arxiv.org/abs/2307.07046"}, {"metadata": {"arXiv": "2307.07177", "Date": "Fri, 14 Jul 2023 06:08:30 ", "Title": "TriFormer: A Multi-modal Transformer Framework For Mild Cognitive Impairment Conversion Prediction", "Authors": ["Linfeng Liu", "Junyan Lyu", "Siyu Liu", "Xiaoying Tang", "Shekhar S. Chandra", "Fatima A. Nasrallah"], "Categories": "cs.CV cs.AI"}, "abstract": "The prediction of mild cognitive impairment (MCI) conversion to Alzheimer's disease (AD) is important for early treatment to prevent or slow the progression of AD. To accurately predict the MCI conversion to stable MCI or progressive MCI, we propose Triformer, a novel transformer-based framework with three specialized transformers to incorporate multi-model data. Triformer uses I) an image transformer to extract multi-view image features from medical scans, II) a clinical transformer to embed and correlate multi-modal clinical data, and III) a modality fusion transformer that produces an accurate prediction based on fusing the outputs from the image and clinical transformers. Triformer is evaluated on the Alzheimer's Disease Neuroimaging Initiative (ANDI)1 and ADNI2 datasets and outperforms previous state-of-the-art single and multi-modal methods.", "url": "https://arxiv.org/abs/2307.07177"}, {"metadata": {"arXiv": "2307.07286", "Date": "Fri, 14 Jul 2023 11:52:10 ", "Title": "One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching", "Authors": ["Siyuan Yang", "Jun Liu", "Shijian Lu", "Er Meng Hwa", "Alex C. Kot"], "Categories": "cs.CV cs.AI", "Comments": ["8 pages", "4 figures", "6 tables. Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence"]}, "abstract": "One-shot skeleton action recognition, which aims to learn a skeleton action recognition model with a single training sample, has attracted increasing interest due to the challenge of collecting and annotating large-scale skeleton action data. However, most existing studies match skeleton sequences by comparing their feature vectors directly which neglects spatial structures and temporal orders of skeleton data. This paper presents a novel one-shot skeleton action recognition technique that handles skeleton action recognition via multi-scale spatial-temporal feature matching. We represent skeleton data at multiple spatial and temporal scales and achieve optimal feature matching from two perspectives. The first is multi-scale matching which captures the scale-wise semantic relevance of skeleton data at multiple spatial and temporal scales simultaneously. The second is cross-scale matching which handles different motion magnitudes and speeds by capturing sample-wise relevance across multiple scales. Extensive experiments over three large-scale datasets (NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior one-shot skeleton action recognition, and it outperforms the state-of-the-art consistently by large margins.", "url": "https://arxiv.org/abs/2307.07286"}, {"metadata": {"arXiv": "2307.07469", "Date": "Fri, 14 Jul 2023 16:51:25 ", "Title": "Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition", "Authors": ["Yuhang Wen", "Zixuan Tang", "Yunsheng Pang", "Beichen Ding", "Mengyuan Liu"], "Categories": "cs.CV cs.AI cs.RO", "Comments": ["IROS 2023 Camera-ready version. Project website: https://necolizer.github.io/ISTA-Net/"]}, "abstract": "Recognizing interactive action plays an important role in human-robot interaction and collaboration. Previous methods use late fusion and co-attention mechanism to capture interactive relations, which have limited learning capability or inefficiency to adapt to more interacting entities. With assumption that priors of each entity are already known, they also lack evaluations on a more general setting addressing the diversity of subjects. To address these problems, we propose an Interactive Spatiotemporal Token Attention Network (ISTA-Net), which simultaneously model spatial, temporal, and interactive relations. Specifically, our network contains a tokenizer to partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to represent motions of multiple diverse entities. By extending the entity dimension, ISTs provide better interactive representations. To jointly learn along three dimensions in ISTs, multi-head self-attention blocks integrated with 3D convolutions are designed to capture inter-token correlations. When modeling correlations, a strict entity ordering is usually irrelevant for recognizing interactive actions. To this end, Entity Rearrangement is proposed to eliminate the orderliness in ISTs for interchangeable entities. Extensive experiments on four datasets verify the effectiveness of ISTA-Net by outperforming state-of-the-art methods. Our code is publicly available at https://github.com/Necolizer/ISTA-Net", "url": "https://arxiv.org/abs/2307.07469"}, {"metadata": {"arXiv": "2307.07146", "Date": "Fri, 14 Jul 2023 04:13:11 ", "Title": "Federated Learning-Empowered AI-Generated Content in Wireless Networks", "Authors": ["Xumin Huang", "Peichun Li", "Hongyang Du", "Jiawen Kang", "Dusit Niyato", "Dong In Kim", "Yuan Wu"], "Categories": "cs.DC cs.AI", "Comments": ["8 pages", "3 figures and 2 tables. Submitted to IEEE Network"]}, "abstract": "Artificial intelligence generated content (AIGC) has emerged as a promising technology to improve the efficiency, quality, diversity and flexibility of the content creation process by adopting a variety of generative AI models. Deploying AIGC services in wireless networks has been expected to enhance the user experience. However, the existing AIGC service provision suffers from several limitations, e.g., the centralized training in the pre-training, fine-tuning and inference processes, especially their implementations in wireless networks with privacy preservation. Federated learning (FL), as a collaborative learning framework where the model training is distributed to cooperative data owners without the need for data sharing, can be leveraged to simultaneously improve learning efficiency and achieve privacy protection for AIGC. To this end, we present FL-based techniques for empowering AIGC, and aim to enable users to generate diverse, personalized, and high-quality content. Furthermore, we conduct a case study of FL-aided AIGC fine-tuning by using the state-of-the-art AIGC model, i.e., stable diffusion model. Numerical results show that our scheme achieves advantages in effectively reducing the communication cost and training latency and privacy protection. Finally, we highlight several major research directions and open issues for the convergence of FL and AIGC.", "url": "https://arxiv.org/abs/2307.07146"}, {"metadata": {"arXiv": "2307.07260", "Date": "Fri, 14 Jul 2023 10:21:26 ", "Title": "A Dynamic Points Removal Benchmark in Point Cloud Maps", "Authors": ["Qingwen Zhang", "Daniel Duberg", "Ruoyu Geng", "Mingkai Jia", "Lujia Wang", "Patric Jensfelt"], "Categories": "cs.RO cs.AI", "Comments": ["Code check https://github.com/KTH-RPL/DynamicMap_Benchmark.git ", "7 pages", "accepted by ITSC 2023"]}, "abstract": "In the field of robotics, the point cloud has become an essential map representation. From the perspective of downstream tasks like localization and global path planning, points corresponding to dynamic objects will adversely affect their performance. Existing methods for removing dynamic points in point clouds often lack clarity in comparative evaluations and comprehensive analysis. Therefore, we propose an easy-to-extend unified benchmarking framework for evaluating techniques for removing dynamic points in maps. It includes refactored state-of-art methods and novel metrics to analyze the limitations of these approaches. This enables researchers to dive deep into the underlying reasons behind these limitations. The benchmark makes use of several datasets with different sensor types. All the code and datasets related to our study are publicly available for further development and utilization.", "url": "https://arxiv.org/abs/2307.07260"}, {"metadata": {"arXiv": "2307.06950", "Date": "Mon, 10 Jul 2023 13:06:55 ", "Title": "Pathway toward prior knowledge-integrated machine learning in engineering", "Authors": ["Xia Chen", "Philipp Geyer"], "Categories": "cs.AI cs.LG", "Comments": ["8 pages", "4 figures"]}, "abstract": "Despite the digitalization trend and data volume surge, first-principles models (also known as logic-driven, physics-based, rule-based, or knowledge-based models) and data-driven approaches have existed in parallel, mirroring the ongoing AI debate on symbolism versus connectionism. Research for process development to integrate both sides to transfer and utilize domain knowledge in the data-driven process is rare. This study emphasizes efforts and prevailing trends to integrate multidisciplinary domain professions into machine acknowledgeable, data-driven processes in a two-fold organization: examining information uncertainty sources in knowledge representation and exploring knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm. This approach balances holist and reductionist perspectives in the engineering domain.", "url": "https://arxiv.org/abs/2307.06950"}, {"metadata": {"arXiv": "2307.06951", "Date": "Mon, 10 Jul 2023 20:05:42 ", "Title": "AI For Global Climate Cooperation 2023 Competition Proceedings", "Authors": ["Yoshua Bengio", "Prateek Gupta", "Lu Li", "Soham Phade", "Sunil Srinivasa", "Andrew Williams", "Tianyu Zhang", "Yang Zhang", "Stephan Zheng"], "Categories": "cs.AI cs.LG"}, "abstract": "The international community must collaborate to mitigate climate change and sustain economic growth. However, collaboration is hard to achieve, partly because no global authority can ensure compliance with international climate agreements. Combining AI with climate-economic simulations offers a promising solution to design international frameworks, including negotiation protocols and climate agreements, that promote and incentivize collaboration. In addition, these frameworks should also have policy goals fulfillment, and sustained commitment, taking into account climate-economic dynamics and strategic behaviors. These challenges require an interdisciplinary approach across machine learning, economics, climate science, law, policy, ethics, and other fields. Towards this objective, we organized AI for Global Climate Cooperation, a Mila competition in which teams submitted proposals and analyses of international frameworks, based on (modifications of) RICE-N, an AI-driven integrated assessment model (IAM). In particular, RICE-N supports modeling regional decision-making using AI agents. Furthermore, the IAM then models the climate-economic impact of those decisions into the future. Whereas the first track focused only on performance metrics, the proposals submitted to the second track were evaluated both quantitatively and qualitatively. The quantitative evaluation focused on a combination of (i) the degree of mitigation of global temperature rise and (ii) the increase in economic productivity. On the other hand, an interdisciplinary panel of human experts in law, policy, sociology, economics and environmental science, evaluated the solutions qualitatively. In particular, the panel considered the effectiveness, simplicity, feasibility, ethics, and notions of climate justice of the protocols. In the third track, the participants were asked to critique and improve RICE-N.", "url": "https://arxiv.org/abs/2307.06951"}, {"metadata": {"arXiv": "2307.06963", "Date": "Thu, 13 Jul 2023 07:48:04 ", "Title": "Is Task-Agnostic Explainable AI a Myth?", "Authors": ["Alicja Chaszczewicz"], "Categories": "cs.AI cs.LG", "Comments": ["Accepted to 2023 ICML Workshop on Interpretable ML in Healthcare", "Honolulu", "Hawaii", "USA (oral)"]}, "abstract": "Our work serves as a framework for unifying the challenges of contemporary explainable AI (XAI). We demonstrate that while XAI methods provide supplementary and potentially useful output for machine learning models, researchers and decision-makers should be mindful of their conceptual and technical limitations, which frequently result in these methods themselves becoming black boxes. We examine three XAI research avenues spanning image, textual, and graph data, covering saliency, attention, and graph-type explainers. Despite the varying contexts and timeframes of the mentioned cases, the same persistent roadblocks emerge, highlighting the need for a conceptual breakthrough in the field to address the challenge of compatibility between XAI methods and application tasks.", "url": "https://arxiv.org/abs/2307.06963"}, {"metadata": {"arXiv": "2307.07508", "Date": "Thu, 13 Jul 2023 16:29:25 ", "Title": "Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach", "Authors": ["Edyvalberty Alenquer Cordeiro", "Anselmo Ramalho Pitombeira-Neto"], "Categories": "cs.AI cs.LG math.OC stat.ML", "Comments": ["42 pages", "22 figures"]}, "abstract": "The dynamic vehicle dispatching problem corresponds to deciding which vehicles to assign to requests that arise stochastically over time and space. It emerges in diverse areas, such as in the assignment of trucks to loads to be transported; in emergency systems; and in ride-hailing services. In this paper, we model the problem as a semi-Markov decision process, which allows us to treat time as continuous. In this setting, decision epochs coincide with discrete events whose time intervals are random. We argue that an event-based approach substantially reduces the combinatorial complexity of the decision space and overcomes other limitations of discrete-time models often proposed in the literature. In order to test our approach, we develop a new discrete-event simulator and use double deep q-learning to train our decision agents. Numerical experiments are carried out in realistic scenarios using data from New York City. We compare the policies obtained through our approach with heuristic policies often used in practice. Results show that our policies exhibit better average waiting times, cancellation rates and total service times, with reduction in average waiting times of up to 50% relative to the other tested heuristic policies.", "url": "https://arxiv.org/abs/2307.07508"}, {"metadata": {"arXiv": "2307.07370", "Date": "Fri, 14 Jul 2023 14:25:26 ", "Title": "AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes", "Authors": ["Guoyun Tu", "Ying Liu", "Vladimir Vlassov"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Image captioning is a significant field across computer vision and natural language processing. We propose and present AIC-AB NET, a novel Attribute-Information-Combined Attention-Based Network that combines spatial attention architecture and text attributes in an encoder-decoder. For caption generation, adaptive spatial attention determines which image region best represents the image and whether to attend to the visual features or the visual sentinel. Text attribute information is synchronously fed into the decoder to help image recognition and reduce uncertainty. We have tested and evaluated our AICAB NET on the MS COCO dataset and a new proposed Fashion dataset. The Fashion dataset is employed as a benchmark of single-object images. The results show the superior performance of the proposed model compared to the state-of-the-art baseline and ablated models on both the images from MSCOCO and our single-object images. Our AIC-AB NET outperforms the baseline adaptive attention network by 0.017 (CIDEr score) on the MS COCO dataset and 0.095 (CIDEr score) on the Fashion dataset.", "url": "https://arxiv.org/abs/2307.07370"}, {"metadata": {"arXiv": "2307.06970", "Date": "Thu, 13 Jul 2023 11:10:22 ", "Title": "Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens", "Authors": ["Akshansh Mishra", "Vijaykumar S Jatti"], "Categories": "cs.LG cs.AI math.OC"}, "abstract": "In this study, we investigate the application of supervised machine learning algorithms for estimating the Ultimate Tensile Strength (UTS) of Polylactic Acid (PLA) specimens fabricated using the Fused Deposition Modeling (FDM) process. A total of 31 PLA specimens were prepared, with Infill Percentage, Layer Height, Print Speed, and Extrusion Temperature serving as input parameters. The primary objective was to assess the accuracy and effectiveness of four distinct supervised classification algorithms, namely Logistic Classification, Gradient Boosting Classification, Decision Tree, and K-Nearest Neighbor, in predicting the UTS of the specimens. The results revealed that while the Decision Tree and K-Nearest Neighbor algorithms both achieved an F1 score of 0.71, the KNN algorithm exhibited a higher Area Under the Curve (AUC) score of 0.79, outperforming the other algorithms. This demonstrates the superior ability of the KNN algorithm in differentiating between the two classes of ultimate tensile strength within the dataset, rendering it the most favorable choice for classification in the context of this research. This study represents the first attempt to estimate the UTS of PLA specimens using machine learning-based classification algorithms, and the findings offer valuable insights into the potential of these techniques in improving the performance and accuracy of predictive models in the domain of additive manufacturing.", "url": "https://arxiv.org/abs/2307.06970"}, {"metadata": {"arXiv": "2307.07014", "Date": "Thu, 13 Jul 2023 18:34:14 ", "Title": "Leveraging Factored Action Spaces for Off-Policy Evaluation", "Authors": ["Aaman Rebello (1)", "Shengpu Tang (2)", "Jenna Wiens (2)", "Sonali Parbhoo (1) ((1) Department of Engineering", "Imperial College London", "(2) Division of Computer Science & Engineering", "University of Michigan)"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["Main paper: 8 pages", "7 figures. Appendix: 30 pages", "17 figures. Accepted at ICML 2023 Workshop on Counterfactuals in Minds and Machines", "Honolulu", "Hawaii", "USA. Camera ready version"], "MSC-class": "62D20 (Primary) 62M05, 60J10, 62D05, 62P10 (Secondary)", "ACM-class": "I.2.6; I.2.8; G.3; J.3"}, "abstract": "Off-policy evaluation (OPE) aims to estimate the benefit of following a counterfactual sequence of actions, given data collected from executed sequences. However, existing OPE estimators often exhibit high bias and high variance in problems involving large, combinatorial action spaces. We investigate how to mitigate this issue using factored action spaces i.e. expressing each action as a combination of independent sub-actions from smaller action spaces. This approach facilitates a finer-grained analysis of how actions differ in their effects. In this work, we propose a new family of \"decomposed\" importance sampling (IS) estimators based on factored action spaces. Given certain assumptions on the underlying problem structure, we prove that the decomposed IS estimators have less variance than their original non-decomposed versions, while preserving the property of zero bias. Through simulations, we empirically verify our theoretical results, probing the validity of various assumptions. Provided with a technique that can derive the action space factorisation for a given problem, our work shows that OPE can be improved \"for free\" by utilising this inherent problem structure.", "url": "https://arxiv.org/abs/2307.07014"}, {"metadata": {"arXiv": "2307.07084", "Date": "Thu, 13 Jul 2023 22:52:22 ", "Title": "Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability", "Authors": ["Yanran Wang", "David Boyle"], "Categories": "cs.LG cs.AI cs.RO cs.SY eess.SY", "Comments": ["24 pages", "8 figures", "containing Appendix"]}, "abstract": "Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guaranteed global convergence rates not only in simulation but also in real robot tasks, and empirically verify a reasonable tradeoff between high performance and conservative interpretability.", "url": "https://arxiv.org/abs/2307.07084"}, {"metadata": {"arXiv": "2307.07091", "Date": "Thu, 13 Jul 2023 23:36:55 ", "Title": "Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning", "Authors": ["Marcel Hussing", "Jorge A. Mendez", "Anisha Singrodia", "Cassandra Kent", "Eric Eaton"], "Categories": "cs.LG cs.AI cs.RO"}, "abstract": "Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that current offline RL methods can learn the training tasks to some extent and that compositional methods significantly outperform non-compositional methods. However, current methods are still unable to extract the tasks' compositional structure to generalize to unseen tasks, showing a need for further research in offline compositional RL.", "url": "https://arxiv.org/abs/2307.07091"}, {"metadata": {"arXiv": "2307.07119", "Date": "Fri, 14 Jul 2023 01:50:53 ", "Title": "DataAssist: A Machine Learning Approach to Data Cleaning and Preparation", "Authors": ["Kartikay Goyle", "Quin Xie and Vakul Goyle"], "Categories": "cs.LG cs.AI cs.DB"}, "abstract": "Current automated machine learning (ML) tools are model-centric, focusing on model selection and parameter optimization. However, the majority of the time in data analysis is devoted to data cleaning and wrangling, for which limited tools are available. Here we present DataAssist, an automated data preparation and cleaning platform that enhances dataset quality using ML-informed methods. We show that DataAssist provides a pipeline for exploratory data analysis and data cleaning, including generating visualization for user-selected variables, unifying data annotation, suggesting anomaly removal, and preprocessing data. The exported dataset can be readily integrated with other autoML tools or user-specified model for downstream analysis. Our data-centric tool is applicable to a variety of fields, including economics, business, and forecasting applications saving over 50\\% time of the time spent on data cleansing and preparation.", "url": "https://arxiv.org/abs/2307.07119"}, {"metadata": {"arXiv": "2307.07134", "Date": "Fri, 14 Jul 2023 03:15:56 ", "Title": "Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms", "Authors": ["Qi Liu", "Zheng Gong", "Zhenya Huang", "Chuanren Liu", "Hengshu Zhu", "Zhi Li", "Enhong Chen and Hui Xiong"], "Categories": "cs.LG cs.AI"}, "abstract": "Machine learning algorithms have become ubiquitous in a number of applications (e.g. image classification). However, due to the insufficient measurement of traditional metrics (e.g. the coarse-grained Accuracy of each classifier), substantial gaps are usually observed between the real-world performance of these algorithms and their scores in standardized evaluations. In this paper, inspired by the psychometric theories from human measurement, we propose a task-agnostic evaluation framework Camilla, where a multi-dimensional diagnostic metric Ability is defined for collaboratively measuring the multifaceted strength of each machine learning algorithm. Specifically, given the response logs from different algorithms to data samples, we leverage cognitive diagnosis assumptions and neural networks to learn the complex interactions among algorithms, samples and the skills (explicitly or implicitly pre-defined) of each sample. In this way, both the abilities of each algorithm on multiple skills and some of the sample factors (e.g. sample difficulty) can be simultaneously quantified. We conduct extensive experiments with hundreds of machine learning algorithms on four public datasets, and our experimental results demonstrate that Camilla not only can capture the pros and cons of each algorithm more precisely, but also outperforms state-of-the-art baselines on the metric reliability, rank consistency and rank stability.", "url": "https://arxiv.org/abs/2307.07134"}, {"metadata": {"arXiv": "2307.07167", "Date": "Fri, 14 Jul 2023 05:31:32 ", "Title": "Vulnerability-Aware Instance Reweighting For Adversarial Training", "Authors": ["Olukorede Fakorede", "Ashutosh Kumar Nirala", "Modeste Atsague", "Jin Tian"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Adversarial Training (AT) has been found to substantially improve the robustness of deep learning classifiers against adversarial attacks. AT involves obtaining robustness by including adversarial examples in training a classifier. Most variants of AT algorithms treat every training example equally. However, recent works have shown that better performance is achievable by treating them unequally. In addition, it has been observed that AT exerts an uneven influence on different classes in a training set and unfairly hurts examples corresponding to classes that are inherently harder to classify. Consequently, various reweighting schemes have been proposed that assign unequal weights to robust losses of individual examples in a training set. In this work, we propose a novel instance-wise reweighting scheme. It considers the vulnerability of each natural example and the resulting information loss on its adversarial counterpart occasioned by adversarial attacks. Through extensive experiments, we show that our proposed method significantly improves over existing reweighting schemes, especially against strong white and black-box attacks.", "url": "https://arxiv.org/abs/2307.07167"}, {"metadata": {"arXiv": "2307.07176", "Date": "Fri, 14 Jul 2023 06:00:08 ", "Title": "Safe DreamerV3: Safe Reinforcement Learning with World Models", "Authors": ["Weidong Huang", "Jiaming Ji", "Borong Zhang", "Chunhe Xia", "Yaodong Yang"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.", "url": "https://arxiv.org/abs/2307.07176"}, {"metadata": {"arXiv": "2307.07189", "Date": "Fri, 14 Jul 2023 06:44:43 ", "Title": "Multiplicative update rules for accelerating deep learning training and increasing robustness", "Authors": ["Manos Kirtas", "Nikolaos Passalis", "Anastasios Tefas"], "Categories": "cs.LG cs.AI"}, "abstract": "Even nowadays, where Deep Learning (DL) has achieved state-of-the-art performance in a wide range of research domains, accelerating training and building robust DL models remains a challenging task. To this end, generations of researchers have pursued to develop robust methods for training DL architectures that can be less sensitive to weight distributions, model architectures and loss landscapes. However, such methods are limited to adaptive learning rate optimizers, initialization schemes, and clipping gradients without investigating the fundamental rule of parameters update. Although multiplicative updates have contributed significantly to the early development of machine learning and hold strong theoretical claims, to best of our knowledge, this is the first work that investigate them in context of DL training acceleration and robustness. In this work, we propose an optimization framework that fits to a wide range of optimization algorithms and enables one to apply alternative update rules. To this end, we propose a novel multiplicative update rule and we extend their capabilities by combining it with a traditional additive update term, under a novel hybrid update method. We claim that the proposed framework accelerates training, while leading to more robust models in contrast to traditionally used additive update rule and we experimentally demonstrate their effectiveness in a wide range of task and optimization methods. Such tasks ranging from convex and non-convex optimization to difficult image classification benchmarks applying a wide range of traditionally used optimization methods and Deep Neural Network (DNN) architectures.", "url": "https://arxiv.org/abs/2307.07189"}, {"metadata": {"arXiv": "2307.07250", "Date": "Fri, 14 Jul 2023 09:51:26 ", "Title": "Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning", "Authors": ["Byung-Kwan Lee", "Junho Kim", "Yong Man Ro"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Accepted in ICCV 2023"]}, "abstract": "Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridging a causal perspective into the adversarial vulnerability. Through extensive experiments on various CNN and Transformer architectures, we corroborate that ADML improves adversarial robustness with large margins and relieve the empirical observation.", "url": "https://arxiv.org/abs/2307.07250"}, {"metadata": {"arXiv": "2307.07413", "Date": "Fri, 14 Jul 2023 15:41:53 ", "Title": "Exploiting Counter-Examples for Active Learning with Partial labels", "Authors": ["Fei Zhang", "Yunjie Ye", "Lei Feng", "Zhongwen Rao", "Jieming Zhu", "Marcus Kalander", "Chen Gong", "Jianye Hao", "Bo Han"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["29 pages", "Under review"]}, "abstract": "This paper studies a new problem, \\emph{active learning with partial labels} (ALPL). In this setting, an oracle annotates the query samples with partial labels, relaxing the oracle from the demanding accurate labeling process. To address ALPL, we first build an intuitive baseline that can be seamlessly incorporated into existing AL frameworks. Though effective, this baseline is still susceptible to the \\emph{overfitting}, and falls short of the representative partial-label-based samples during the query process. Drawing inspiration from human inference in cognitive science, where accurate inferences can be explicitly derived from \\emph{counter-examples} (CEs), our objective is to leverage this human-like learning pattern to tackle the \\emph{overfitting} while enhancing the process of selecting representative samples in ALPL. Specifically, we construct CEs by reversing the partial labels for each instance, and then we propose a simple but effective WorseNet to directly learn from this complementary pattern. By leveraging the distribution gap between WorseNet and the predictor, this adversarial evaluation manner could enhance both the performance of the predictor itself and the sample selection process, allowing the predictor to capture more accurate patterns in the data. Experimental results on five real-world datasets and four benchmark datasets show that our proposed method achieves comprehensive improvements over ten representative AL frameworks, highlighting the superiority of WorseNet. The source code will be available at \\url{https://github.com/Ferenas/APLL}.", "url": "https://arxiv.org/abs/2307.07413"}, {"metadata": {"arXiv": "2307.07443", "Date": "Fri, 14 Jul 2023 16:06:42 ", "Title": "Can Large Language Models Empower Molecular Property Prediction?", "Authors": ["Chen Qian", "Huayi Tang", "Zhirui Yang", "Hong Liang", "Yong Liu"], "Categories": "cs.LG cs.AI q-bio.QM"}, "abstract": "Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM model for multiple downstream tasks. The experimental results highlight the superiority of text explanations as molecular representations across multiple benchmark datasets, and confirm the immense potential of LLMs in molecular property prediction tasks. Codes are available at \\url{https://github.com/ChnQ/LLM4Mol}.", "url": "https://arxiv.org/abs/2307.07443"}, {"metadata": {"arXiv": "2307.07457", "Date": "Fri, 14 Jul 2023 16:36:49 ", "Title": "Structured Pruning of Neural Networks for Constraints Learning", "Authors": ["Matteo Cacciola and Antonio Frangioni and Andrea Lodi"], "Categories": "cs.LG cs.AI math.OC"}, "abstract": "In recent years, the integration of Machine Learning (ML) models with Operation Research (OR) tools has gained popularity across diverse applications, including cancer treatment, algorithmic configuration, and chemical process optimization. In this domain, the combination of ML and OR often relies on representing the ML model output using Mixed Integer Programming (MIP) formulations. Numerous studies in the literature have developed such formulations for many ML predictors, with a particular emphasis on Artificial Neural Networks (ANNs) due to their significant interest in many applications. However, ANNs frequently contain a large number of parameters, resulting in MIP formulations that are impractical to solve, thereby impeding scalability. In fact, the ML community has already introduced several techniques to reduce the parameter count of ANNs without compromising their performance, since the substantial size of modern ANNs presents challenges for ML applications as it significantly impacts computational efforts during training and necessitates significant memory resources for storage. In this paper, we showcase the effectiveness of pruning, one of these techniques, when applied to ANNs prior to their integration into MIPs. By pruning the ANN, we achieve significant improvements in the speed of the solution process. We discuss why pruning is more suitable in this context compared to other ML compression techniques, and we identify the most appropriate pruning strategies. To highlight the potential of this approach, we conduct experiments using feed-forward neural networks with multiple layers to construct adversarial examples. Our results demonstrate that pruning offers remarkable reductions in solution times without hindering the quality of the final decision, enabling the resolution of previously unsolvable instances.", "url": "https://arxiv.org/abs/2307.07457"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
