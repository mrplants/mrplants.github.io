<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2306.11754", "Date": "Mon, 19 Jun 2023 14:35:28 ", "Title": "Pre-Pruning and Gradient-Dropping Improve Differentially Private Image Classification", "Authors": ["Kamil Adamczewski", "Yingchen He", "Mijung Park"], "Categories": "cs.CV cs.CR cs.LG", "Comments": ["arXiv admin note: text overlap with arXiv:2303.04612"]}, "abstract": "Scalability is a significant challenge when it comes to applying differential privacy to training deep neural networks. The commonly used DP-SGD algorithm struggles to maintain a high level of privacy protection while achieving high accuracy on even moderately sized models. To tackle this challenge, we take advantage of the fact that neural networks are overparameterized, which allows us to improve neural network training with differential privacy. Specifically, we introduce a new training paradigm that uses \\textit{pre-pruning} and \\textit{gradient-dropping} to reduce the parameter space and improve scalability. The process starts with pre-pruning the parameters of the original network to obtain a smaller model that is then trained with DP-SGD. During training, less important gradients are dropped, and only selected gradients are updated. Our training paradigm introduces a tension between the rates of pre-pruning and gradient-dropping, privacy loss, and classification accuracy. Too much pre-pruning and gradient-dropping reduces the model's capacity and worsens accuracy, while training a smaller model requires less privacy budget for achieving good accuracy. We evaluate the interplay between these factors and demonstrate the effectiveness of our training paradigm for both training from scratch and fine-tuning pre-trained networks on several benchmark image classification datasets. The tools can also be readily incorporated into existing training paradigms.", "url": "https://arxiv.org/abs/2306.11754"}, {"metadata": {"arXiv": "2306.11848", "Date": "Tue, 20 Jun 2023 18:59:27 ", "Title": "Using super-resolution for enhancing visual perception and segmentation performance in veterinary cytology", "Authors": ["Jakub Caputa", "Maciej Wielgosz", "Daria {\\L}ukasik", "Pawe{\\l} Russek", "Jakub Grzeszczyk", "Micha{\\l} Karwatowski", "Szymon Mazurek", "Rafa{\\l} Fr\\k{a}czek", "Anna \\'Smiech", "Ernest Jamro", "Sebastian Koryciak", "Agnieszka D\\k{a}browska-Boruch", "Marcin Pietro\\'n", "Kazimierz Wiatr"], "Categories": "cs.CV cs.LG eess.IV"}, "abstract": "The primary objective of this research was to enhance the quality of semantic segmentation in cytology images by incorporating super-resolution (SR) architectures. An additional contribution was the development of a novel dataset aimed at improving imaging quality in the presence of inaccurate focus. Our experimental results demonstrate that the integration of SR techniques into the segmentation pipeline can lead to a significant improvement of up to 25% in the mean average precision (mAP) segmentation metric. These findings suggest that leveraging SR architectures holds great promise for advancing the state of the art in cytology image analysis.", "url": "https://arxiv.org/abs/2306.11848"}, {"metadata": {"arXiv": "2306.11982", "Date": "Wed, 21 Jun 2023 02:18:27 ", "Title": "Balanced Mixture of SuperNets for Learning the CNN Pooling Architecture", "Authors": ["Mehraveh Javan", "Matthew Toews", "Marco Pedersoli"], "Categories": "cs.CV cs.LG"}, "abstract": "Downsampling layers, including pooling and strided convolutions, are crucial components of the convolutional neural network architecture that determine both the granularity/scale of image feature analysis as well as the receptive field size of a given layer. To fully understand this problem, we analyse the performance of models independently trained with each pooling configurations on CIFAR10, using a ResNet20 network, and show that the position of the downsampling layers can highly influence the performance of a network and predefined downsampling configurations are not optimal. Network Architecture Search (NAS) might be used to optimize downsampling configurations as an hyperparameter. However, we find that common one-shot NAS based on a single SuperNet does not work for this problem. We argue that this is because a SuperNet trained for finding the optimal pooling configuration fully shares its parameters among all pooling configurations. This makes its training hard, because learning some configurations can harm the performance of others. Therefore, we propose a balanced mixture of SuperNets that automatically associates pooling configurations to different weight models and helps to reduce the weight-sharing and inter-influence of pooling configurations on the SuperNet parameters. We evaluate our proposed approach on CIFAR10, CIFAR100, as well as Food101 and show that in all cases, our model outperforms other approaches and improves over the default pooling configurations.", "url": "https://arxiv.org/abs/2306.11982"}, {"metadata": {"arXiv": "2306.12041", "Date": "Wed, 21 Jun 2023 06:18:05 ", "Title": "Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors", "Authors": ["Nicolae-Catalin Ristea", "Florinel-Alin Croitoru", "Radu Tudor Ionescu", "Marius Popescu", "Fahad Shahbaz Khan", "Mubarak Shah"], "Categories": "cs.CV cs.LG"}, "abstract": "We propose an efficient abnormal event detection model based on a lightweight masked auto-encoder (AE) applied at the video frame level. The novelty of the proposed model is threefold. First, we introduce an approach to weight tokens based on motion gradients, thus avoiding learning to reconstruct the static background scene. Second, we integrate a teacher decoder and a student decoder into our architecture, leveraging the discrepancy between the outputs given by the two decoders to improve anomaly detection. Third, we generate synthetic abnormal events to augment the training videos, and task the masked AE model to jointly reconstruct the original frames (without anomalies) and the corresponding pixel-level anomaly maps. Our design leads to an efficient and effective model, as demonstrated by the extensive experiments carried out on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. The empirical results show that our model achieves an excellent trade-off between speed and accuracy, obtaining competitive AUC scores, while processing 1670 FPS. Hence, our model is between 8 and 70 times faster than competing methods. We also conduct an ablation study to justify our design.", "url": "https://arxiv.org/abs/2306.12041"}, {"metadata": {"arXiv": "2306.12070", "Date": "Wed, 21 Jun 2023 07:43:23 ", "Title": "Task-Robust Pre-Training for Worst-Case Downstream Adaptation", "Authors": ["Jianghui Wang", "Cheng Yang", "Xingyu Xie", "Cong Fang", "Zhouchen Lin"], "Categories": "cs.CV cs.LG"}, "abstract": "Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as $\\textit{downstream-task robustness}$. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minimax loss and prove its convergence in the convex setting. In the experiments, we show both on large-scale natural language processing and computer vision datasets our method increases the metrics on worse-case downstream tasks. Additionally, some theoretical explanations for why our loss is beneficial are provided. Specifically, we show fewer samples are inherently required for the most challenging downstream task in some cases.", "url": "https://arxiv.org/abs/2306.12070"}, {"metadata": {"arXiv": "2306.12100", "Date": "Wed, 21 Jun 2023 08:28:51 ", "Title": "Efficient ResNets: Residual Network Design", "Authors": ["Aditya Thakur", "Harish Chauhan", "Nikunj Gupta"], "Categories": "cs.CV cs.LG"}, "abstract": "ResNets (or Residual Networks) are one of the most commonly used models for image classification tasks. In this project, we design and train a modified ResNet model for CIFAR-10 image classification. In particular, we aimed at maximizing the test accuracy on the CIFAR-10 benchmark while keeping the size of our ResNet model under the specified fixed budget of 5 million trainable parameters. Model size, typically measured as the number of trainable parameters, is important when models need to be stored on devices with limited storage capacity (e.g. IoT/edge devices). In this article, we present our residual network design which has less than 5 million parameters. We show that our ResNet achieves a test accuracy of 96.04% on CIFAR-10 which is much higher than ResNet18 (which has greater than 11 million trainable parameters) when equipped with a number of training strategies and suitable ResNet hyperparameters. Models and code are available at https://github.com/Nikunj-Gupta/Efficient_ResNets.", "url": "https://arxiv.org/abs/2306.12100"}, {"metadata": {"arXiv": "2306.12155", "Date": "Wed, 21 Jun 2023 10:07:17 ", "Title": "Joint Dense-Point Representation for Contour-Aware Graph Segmentation", "Authors": ["Kit Mills Bransby", "Greg Slabaugh", "Christos Bourantas", "Qianni Zhang"], "Categories": "cs.CV cs.LG eess.IV", "Comments": ["MICCAI 2023 pre-print"]}, "abstract": "We present a novel methodology that combines graph and dense segmentation techniques by jointly learning both point and pixel contour representations, thereby leveraging the benefits of each approach. This addresses deficiencies in typical graph segmentation methods where misaligned objectives restrict the network from learning discriminative vertex and contour features. Our joint learning strategy allows for rich and diverse semantic features to be encoded, while alleviating common contour stability issues in dense-based approaches, where pixel-level objectives can lead to anatomically implausible topologies. In addition, we identify scenarios where correct predictions that fall on the contour boundary are penalised and address this with a novel hybrid contour distance loss. Our approach is validated on several Chest X-ray datasets, demonstrating clear improvements in segmentation stability and accuracy against a variety of dense- and point-based methods. Our source code is freely available at: www.github.com/kitbransby/Joint_Graph_Segmentation", "url": "https://arxiv.org/abs/2306.12155"}, {"metadata": {"arXiv": "2306.12298", "Date": "Wed, 21 Jun 2023 14:27:31 ", "Title": "StarVQA+: Co-training Space-Time Attention for Video Quality Assessment", "Authors": ["Fengchuang Xing", "Yuan-Gen Wang", "Weixuan Tang", "Guopu Zhu", "Sam Kwong"], "Categories": "cs.CV cs.LG eess.IV"}, "abstract": "Self-attention based Transformer has achieved great success in many computer vision tasks. However, its application to video quality assessment (VQA) has not been satisfactory so far. Evaluating the quality of in-the-wild videos is challenging due to the unknown of pristine reference and shooting distortion. This paper presents a co-trained Space-Time Attention network for the VQA problem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately concatenating the divided space-time attention. Then, to facilitate the training of StarVQA+, we design a vectorized regression loss by encoding the mean opinion score (MOS) to the probability vector and embedding a special token as the learnable variable of MOS, leading to better fitting of human's rating process. Finally, to solve the data hungry problem with Transformer, we propose to co-train the spatial and temporal attention weights using both images and videos. Various experiments are conducted on the de-facto in-the-wild video datasets, including LIVE-Qualcomm, LIVE-VQC, KoNViD-1k, YouTube-UGC, LSVQ, LSVQ-1080p, and DVL2021. Experimental results demonstrate the superiority of the proposed StarVQA+ over the state-of-the-art.", "url": "https://arxiv.org/abs/2306.12298"}, {"metadata": {"arXiv": "2306.12422", "Date": "Wed, 21 Jun 2023 17:59:45 ", "Title": "DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation", "Authors": ["Yukun Huang", "Jianan Wang", "Yukai Shi", "Xianbiao Qi", "Zheng-Jun Zha", "Lei Zhang"], "Categories": "cs.CV cs.GR cs.LG"}, "abstract": "Text-to-image diffusion models pre-trained on billions of image-text pairs have recently enabled text-to-3D content creation by optimizing a randomly initialized Neural Radiance Fields (NeRF) with score distillation. However, the resultant 3D models exhibit two limitations: (a) quality concerns such as saturated color and the Janus problem; (b) extremely low diversity comparing to text-guided image synthesis. In this paper, we show that the conflict between NeRF optimization process and uniform timestep sampling in score distillation is the main reason for these limitations. To resolve this conflict, we propose to prioritize timestep sampling with monotonically non-increasing functions, which aligns NeRF optimization with the sampling process of diffusion model. Extensive experiments show that our simple redesign significantly improves text-to-3D content creation with higher quality and diversity.", "url": "https://arxiv.org/abs/2306.12422"}, {"metadata": {"arXiv": "2306.12185", "Date": "Wed, 21 Jun 2023 11:32:28 ", "Title": "Adaptive DNN Surgery for Selfish Inference Acceleration with On-demand Edge Resource", "Authors": ["Xiang Yang", "Dezhi Chen", "Qi Qi", "Jingyu Wang", "Haifeng Sun", "Jianxin Liao", "Song Guo"], "Categories": "cs.GT cs.LG cs.NI", "Comments": ["Under Review"]}, "abstract": "Deep Neural Networks (DNNs) have significantly improved the accuracy of intelligent applications on mobile devices. DNN surgery, which partitions DNN processing between mobile devices and multi-access edge computing (MEC) servers, can enable real-time inference despite the computational limitations of mobile devices. However, DNN surgery faces a critical challenge: determining the optimal computing resource demand from the server and the corresponding partition strategy, while considering both inference latency and MEC server usage costs. This problem is compounded by two factors: (1) the finite computing capacity of the MEC server, which is shared among multiple devices, leading to inter-dependent demands, and (2) the shift in modern DNN architecture from chains to directed acyclic graphs (DAGs), which complicates potential solutions. In this paper, we introduce a novel Decentralized DNN Surgery (DDS) framework. We formulate the partition strategy as a min-cut and propose a resource allocation game to adaptively schedule the demands of mobile devices in an MEC environment. We prove the existence of a Nash Equilibrium (NE), and develop an iterative algorithm to efficiently reach the NE for each device. Our extensive experiments demonstrate that DDS can effectively handle varying MEC scenarios, achieving up to 1.25$\\times$ acceleration compared to the state-of-the-art algorithm.", "url": "https://arxiv.org/abs/2306.12185"}, {"metadata": {"arXiv": "2306.11800", "Date": "Tue, 20 Jun 2023 18:00:31 ", "Title": "DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization", "Authors": ["Amey Agrawal", "Sameer Reddy", "Satwik Bhattamishra", "Venkata Prabhakara Sarath Nookala", "Vidushi Vashishth", "Kexin Rong", "Alexey Tumanov"], "Categories": "cs.LG"}, "abstract": "With the increase in the scale of Deep Learning (DL) training workloads in terms of compute resources and time consumption, the likelihood of encountering in-training failures rises substantially, leading to lost work and resource wastage. Such failures are typically offset by a checkpointing mechanism, which comes at the cost of storage and network bandwidth overhead. State-of-the-art approaches involve lossy model compression mechanisms, which induce a tradeoff between the resulting model quality (accuracy) and compression ratio. Delta compression is then also used to further reduce the overhead by only storing the difference between consecutive checkpoints. We make a key enabling observation that the sensitivity of model weights to compression varies during training, and different weights benefit from different quantization levels (ranging from retaining full precision to pruning). We propose (1) a non-uniform quantization scheme that leverages this variation, (2) an efficient search mechanism to dynamically adjust to the best quantization configurations, and (3) a quantization-aware delta compression mechanism that rearranges weights to minimize checkpoint differences, thereby maximizing compression. We instantiate these contributions in DynaQuant - a framework for DL workload checkpoint compression. Our experiments show that DynaQuant consistently achieves better tradeoff between accuracy and compression ratios compared to prior works, enabling a compression ratio up to 39x and withstanding up to 10 restores with negligible accuracy impact for fault-tolerant training. DynaQuant achieves at least an order of magnitude reduction in checkpoint storage overhead for training failure recovery as well as transfer learning use cases without any loss of accuracy", "url": "https://arxiv.org/abs/2306.11800"}, {"metadata": {"arXiv": "2306.11830", "Date": "Tue, 20 Jun 2023 18:39:12 ", "Title": "UMM: Unsupervised Mean-difference Maximization", "Authors": ["Jan Sosulski and Michael Tangermann"], "Categories": "cs.LG stat.AP"}, "abstract": "Many brain-computer interfaces make use of brain signals that are elicited in response to a visual, auditory or tactile stimulus, so-called event-related potentials (ERPs). In visual ERP speller applications, sets of letters shown on a screen are flashed randomly, and the participant attends to the target letter they want to spell. When this letter flashes, the resulting ERP is different compared to when any other non-target letter flashes. We propose a new unsupervised approach to detect this attended letter. In each trial, for every available letter our approach makes the hypothesis that it is in fact the attended letter, and calculates the ERPs based on each of these hypotheses. We leverage the fact that only the true hypothesis produces the largest difference between the class means. Note that this unsupervised method does not require any changes to the underlying experimental paradigm and therefore can be employed in almost any ERP-based setup. To deal with limited data, we use a block-Toeplitz regularized covariance matrix that models the background activity. We implemented the proposed novel unsupervised mean-difference maximization (UMM) method and evaluated it in offline replays of brain-computer interface visual speller datasets. For a dataset that used 16 flashes per symbol per trial, UMM correctly classifies 3651 out of 3654 letters ($99.92\\,\\%$) across 25 participants. In another dataset with fewer and shorter trials, 7344 out of 7383 letters ($99.47\\,\\%$) are classified correctly across 54 participants with two sessions each. Even in more challenging datasets obtained from patients with amyotrophic lateral sclerosis ($77.86\\,\\%$) or when using auditory ERPs ($82.52\\,\\%$), the obtained classification rates obtained by UMM are competitive. In addition, UMM provides stable confidence measures which can be used to monitor convergence.", "url": "https://arxiv.org/abs/2306.11830"}, {"metadata": {"arXiv": "2306.11835", "Date": "Tue, 20 Jun 2023 18:45:24 ", "Title": "Topological Parallax: A Geometric Specification for Deep Perception Models", "Authors": ["Abraham D. Smith", "Michael J. Catanzaro", "Gabrielle Angeloro", "Nirav Patel", "Paul Bendich"], "Categories": "cs.LG math.AT stat.ML", "Comments": ["15 pages", "3 pigures. Preprint submitted to NeurIPS 2023"], "MSC-class": "55N31", "ACM-class": "I.2.7"}, "abstract": "For safety and robustness of AI systems, we introduce topological parallax as a theoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. Our proofs and examples show that this geometric similarity between dataset and model is essential to trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between overfitting and generalization in applications of deep-learning. In typical DNN applications, an explicit geometric description of the model is impossible, but parallax can estimate topological features (components, cycles, voids, etc.) in the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset. Thus, parallax indicates whether the model shares similar multiscale geometric features with the dataset. Parallax presents theoretically via topological data analysis [TDA] as a bi-filtered persistence module, and the key properties of this module are stable under perturbation of the reference dataset.", "url": "https://arxiv.org/abs/2306.11835"}, {"metadata": {"arXiv": "2306.11847", "Date": "Tue, 20 Jun 2023 18:56:37 ", "Title": "Decoding Urban-health Nexus: Interpretable Machine Learning Illuminates Cancer Prevalence based on Intertwined City Features", "Authors": ["Chenyue Liu", "Ali Mostafavi"], "Categories": "cs.LG cs.CY"}, "abstract": "This study investigates the interplay among social demographics, built environment characteristics, and environmental hazard exposure features in determining community level cancer prevalence. Utilizing data from five Metropolitan Statistical Areas in the United States: Chicago, Dallas, Houston, Los Angeles, and New York, the study implemented an XGBoost machine learning model to predict the extent of cancer prevalence and evaluate the importance of different features. Our model demonstrates reliable performance, with results indicating that age, minority status, and population density are among the most influential factors in cancer prevalence. We further explore urban development and design strategies that could mitigate cancer prevalence, focusing on green space, developed areas, and total emissions. Through a series of experimental evaluations based on causal inference, the results show that increasing green space and reducing developed areas and total emissions could alleviate cancer prevalence. The study and findings contribute to a better understanding of the interplay among urban features and community health and also show the value of interpretable machine learning models for integrated urban design to promote public health. The findings also provide actionable insights for urban planning and design, emphasizing the need for a multifaceted approach to addressing urban health disparities through integrated urban design strategies.", "url": "https://arxiv.org/abs/2306.11847"}, {"metadata": {"arXiv": "2306.11855", "Date": "Tue, 20 Jun 2023 19:20:18 ", "Title": "A Model-free Closeness-of-influence Test for Features in Supervised Learning", "Authors": ["Mohammad Mehrabi and Ryan A. Rossi"], "Categories": "cs.LG stat.ME"}, "abstract": "Understanding the effect of a feature vector $x \\in \\mathbb{R}^d$ on the response value (label) $y \\in \\mathbb{R}$ is the cornerstone of many statistical learning problems. Ideally, it is desired to understand how a set of collected features combine together and influence the response value, but this problem is notoriously difficult, due to the high-dimensionality of data and limited number of labeled data points, among many others. In this work, we take a new perspective on this problem, and we study the question of assessing the difference of influence that the two given features have on the response value. We first propose a notion of closeness for the influence of features, and show that our definition recovers the familiar notion of the magnitude of coefficients in the parametric model. We then propose a novel method to test for the closeness of influence in general model-free supervised learning problems. Our proposed test can be used with finite number of samples with control on type I error rate, no matter the ground truth conditional law $\\mathcal{L}(Y |X)$. We analyze the power of our test for two general learning problems i) linear regression, and ii) binary classification under mixture of Gaussian models, and show that under the proper choice of score function, an internal component of our test, with sufficient number of samples will achieve full statistical power. We evaluate our findings through extensive numerical simulations, specifically we adopt the datamodel framework (Ilyas, et al., 2022) for CIFAR-10 dataset to identify pairs of training samples with different influence on the trained model via optional black box training mechanisms.", "url": "https://arxiv.org/abs/2306.11855"}, {"metadata": {"arXiv": "2306.11865", "Date": "Tue, 20 Jun 2023 19:51:21 ", "Title": "Unsupervised Deep Unfolded PGD for Transmit Power Allocation in Wireless Systems", "Authors": ["Ramoni Adeogun"], "Categories": "cs.LG eess.SP", "Comments": ["Accepted for IEEE PIMRC 2023"]}, "abstract": "Transmit power control (TPC) is a key mechanism for managing interference, energy utilization, and connectivity in wireless systems. In this paper, we propose a simple low-complexity TPC algorithm based on the deep unfolding of the iterative projected gradient descent (PGD) algorithm into layers of a deep neural network and learning the step-size parameters. An unsupervised learning method with either online learning or offline pretraining is applied for optimizing the weights of the DNN. Performance evaluation in dense device-to-device (D2D) communication scenarios showed that the proposed method can achieve better performance than the iterative algorithm with more than a factor of 2 lower number of iterations.", "url": "https://arxiv.org/abs/2306.11865"}, {"metadata": {"arXiv": "2306.11867", "Date": "Tue, 20 Jun 2023 19:58:58 ", "Title": "Personalized Federated Learning with Feature Alignment and Classifier Collaboration", "Authors": ["Jian Xu", "Xinyi Tong", "Shao-Lun Huang"], "Categories": "cs.LG cs.DC", "Comments": ["ICLR 2023", "fix some typos and add the code link"]}, "abstract": "Data heterogeneity is one of the most challenging issues in federated learning, which motivates a variety of approaches to learn personalized models for participating clients. One such approach in deep neural networks based tasks is employing a shared feature representation and learning a customized classifier head for each client. However, previous works do not utilize the global knowledge during local representation learning and also neglect the fine-grained collaboration between local classifier heads, which limit the model generalization ability. In this work, we conduct explicit local-global feature alignment by leveraging global semantic knowledge for learning a better representation. Moreover, we quantify the benefit of classifier combination for each client as a function of the combining weights and derive an optimization problem for estimating optimal weights. Finally, extensive evaluation results on benchmark datasets with various heterogeneous data scenarios demonstrate the effectiveness of our proposed method. Code is available at https://github.com/JianXu95/FedPAC", "url": "https://arxiv.org/abs/2306.11867"}, {"metadata": {"arXiv": "2306.11903", "Date": "Tue, 20 Jun 2023 21:30:54 ", "Title": "Deep Fusion: Efficient Network Training via Pre-trained Initializations", "Authors": ["Hanna Mazzawi", "Xavi Gonzalvo", "Michael Wunder"], "Categories": "cs.LG"}, "abstract": "In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks is the need for large amounts of computational resources and time. In this paper, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. % We show that Deep Fusion accelerates the training process, reduces computational requirements, and leads to improved generalization performance on a variety of NLP tasks and T5 model sizes. % Our experiments demonstrate that Deep Fusion is a practical and effective approach to reduce the training time and resource consumption while maintaining, or even surpassing, the performance of traditional training methods.", "url": "https://arxiv.org/abs/2306.11903"}, {"metadata": {"arXiv": "2306.11912", "Date": "Tue, 20 Jun 2023 21:51:13 ", "Title": "Copula-Based Deep Survival Models for Dependent Censoring", "Authors": ["Ali Hossein Gharari Foomani", "Michael Cooper", "Russell Greiner", "Rahul G. Krishnan"], "Categories": "cs.LG", "Comments": ["23 pages", "7 figures"]}, "abstract": "A survival dataset describes a set of instances (e.g. patients) and provides, for each, either the time until an event (e.g. death), or the censoring time (e.g. when lost to follow-up - which is a lower bound on the time until the event). We consider the challenge of survival prediction: learning, from such data, a predictive model that can produce an individual survival distribution for a novel instance. Many contemporary methods of survival prediction implicitly assume that the event and censoring distributions are independent conditional on the instance's covariates - a strong assumption that is difficult to verify (as we observe only one outcome for each instance) and which can induce significant bias when it does not hold. This paper presents a parametric model of survival that extends modern non-linear survival analysis by relaxing the assumption of conditional independence. On synthetic and semi-synthetic data, our approach significantly improves estimates of survival distributions compared to the standard that assumes conditional independence in the data.", "url": "https://arxiv.org/abs/2306.11912"}, {"metadata": {"arXiv": "2306.11913", "Date": "Tue, 20 Jun 2023 21:54:13 ", "Title": "Randomized Quantization is All You Need for Differential Privacy in Federated Learning", "Authors": ["Yeojoon Youn", "Zihao Hu", "Juba Ziani", "Jacob Abernethy"], "Categories": "cs.LG cs.CR cs.DC"}, "abstract": "Federated learning (FL) is a common and practical framework for learning a machine model in a decentralized fashion. A primary motivation behind this decentralized approach is data privacy, ensuring that the learner never sees the data of each local source itself. Federated learning then comes with two majors challenges: one is handling potentially complex model updates between a server and a large number of data sources; the other is that de-centralization may, in fact, be insufficient for privacy, as the local updates themselves can reveal information about the sources' data. To address these issues, we consider an approach to federated learning that combines quantization and differential privacy. Absent privacy, Federated Learning often relies on quantization to reduce communication complexity. We build upon this approach and develop a new algorithm called the \\textbf{R}andomized \\textbf{Q}uantization \\textbf{M}echanism (RQM), which obtains privacy through a two-levels of randomization. More precisely, we randomly sub-sample feasible quantization levels, then employ a randomized rounding procedure using these sub-sampled discrete levels. We are able to establish that our results preserve ``Renyi differential privacy'' (Renyi DP). We empirically study the performance of our algorithm and demonstrate that compared to previous work it yields improved privacy-accuracy trade-offs for DP federated learning. To the best of our knowledge, this is the first study that solely relies on randomized quantization without incorporating explicit discrete noise to achieve Renyi DP guarantees in Federated Learning systems.", "url": "https://arxiv.org/abs/2306.11913"}, {"metadata": {"arXiv": "2306.11922", "Date": "Tue, 20 Jun 2023 22:10:40 ", "Title": "No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths", "Authors": ["Charles Guille-Escuret", "Hiroki Naganuma", "Kilian Fatras", "Ioannis Mitliagkas"], "Categories": "cs.LG math.OC"}, "abstract": "Understanding the optimization dynamics of neural networks is necessary for closing the gap between theory and practice. Stochastic first-order optimization algorithms are known to efficiently locate favorable minima in deep neural networks. This efficiency, however, contrasts with the non-convex and seemingly complex structure of neural loss landscapes. In this study, we delve into the fundamental geometric properties of sampled gradients along optimization paths. We focus on two key quantities, which appear in the restricted secant inequality and error bound. Both hold high significance for first-order optimization. Our analysis reveals that these quantities exhibit predictable, consistent behavior throughout training, despite the stochasticity induced by sampling minibatches. Our findings suggest that not only do optimization trajectories never encounter significant obstacles, but they also maintain stable dynamics during the majority of training. These observed properties are sufficiently expressive to theoretically guarantee linear convergence and prescribe learning rate schedules mirroring empirical practices. We conduct our experiments on image classification, semantic segmentation and language modeling across different batch sizes, network architectures, datasets, optimizers, and initialization seeds. We discuss the impact of each factor. Our work provides novel insights into the properties of neural network loss functions, and opens the door to theoretical frameworks more relevant to prevalent practice.", "url": "https://arxiv.org/abs/2306.11922"}, {"metadata": {"arXiv": "2306.11942", "Date": "Tue, 20 Jun 2023 23:39:06 ", "Title": "A Deep Learning Model for Heterogeneous Dataset Analysis -- Application to Winter Wheat Crop Yield Prediction", "Authors": ["Yogesh Bansal", "David Lillis", "Mohand Tahar Kechadi"], "Categories": "cs.LG"}, "abstract": "Western countries rely heavily on wheat, and yield prediction is crucial. Time-series deep learning models, such as Long Short Term Memory (LSTM), have already been explored and applied to yield prediction. Existing literature reported that they perform better than traditional Machine Learning (ML) models. However, the existing LSTM cannot handle heterogeneous datasets (a combination of data which varies and remains static with time). In this paper, we propose an efficient deep learning model that can deal with heterogeneous datasets. We developed the system architecture and applied it to the real-world dataset in the digital agriculture area. We showed that it outperforms the existing ML models.", "url": "https://arxiv.org/abs/2306.11942"}, {"metadata": {"arXiv": "2306.11946", "Date": "Tue, 20 Jun 2023 23:52:39 ", "Title": "Winter Wheat Crop Yield Prediction on Multiple Heterogeneous Datasets using Machine Learning", "Authors": ["Yogesh Bansal", "Dr. David Lillis", "Prof. Mohand Tahar Kechadi"], "Categories": "cs.LG", "Journal-ref": "International Conference on Computational Science and Computational Intelligence (CSCI 2022)", "DOI": "10.1109/CSCI58124.2022.00142"}, "abstract": "Winter wheat is one of the most important crops in the United Kingdom, and crop yield prediction is essential for the nation's food security. Several studies have employed machine learning (ML) techniques to predict crop yield on a county or farm-based level. The main objective of this study is to predict winter wheat crop yield using ML models on multiple heterogeneous datasets, i.e., soil and weather on a zone-based level. Experimental results demonstrated their impact when used alone and in combination. In addition, we employ numerous ML algorithms to emphasize the significance of data quality in any machine-learning strategy.", "url": "https://arxiv.org/abs/2306.11946"}, {"metadata": {"arXiv": "2306.11955", "Date": "Wed, 21 Jun 2023 00:55:02 ", "Title": "TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID Inference using Transformer Nearest-Centroid Embeddings", "Authors": ["Gusseppe Bravo-Rocca", "Peini Liu", "Jordi Guitart", "Ajay Dholakia", "David Ellison"], "Categories": "cs.LG cs.CV", "Comments": ["An early version of this work was presented at CVPR 2023", "LXAI Workshop"]}, "abstract": "Machine Learning (ML) models struggle with data that changes over time or across domains due to factors such as noise, occlusion, illumination, or frequency, unlike humans who can learn from such non independent and identically distributed data. Consequently, a Continual Learning (CL) approach is indispensable, particularly, Domain-Incremental Learning. In this paper, we propose a novel pipeline for identifying tasks in domain-incremental learning scenarios without supervision. The pipeline comprises four steps. First, we obtain base embeddings from the raw data using an existing transformer-based model. Second, we group the embedding densities based on their similarity to obtain the nearest points to each cluster centroid. Third, we train an incremental task classifier using only these few points. Finally, we leverage the lightweight computational requirements of the pipeline to devise an algorithm that decides in an online fashion when to learn a new task using the task classifier and a drift detector. We conduct experiments using the SODA10M real-world driving dataset and several CL strategies. We demonstrate that the performance of these CL strategies with our pipeline can match the ground-truth approach, both in classical experiments assuming task boundaries, and also in more realistic task-agnostic scenarios that require detecting new tasks on-the-fly", "url": "https://arxiv.org/abs/2306.11955"}, {"metadata": {"arXiv": "2306.11957", "Date": "Wed, 21 Jun 2023 00:59:06 ", "Title": "Towards Mitigating Spurious Correlations in the Wild: A Benchmark & a more Realistic Dataset", "Authors": ["Siddharth Joshi", "Yu Yang", "Yihao Xue", "Wenhan Yang and Baharan Mirzasoleiman"], "Categories": "cs.LG", "Comments": ["Package: https://github.com/BigML-CS-UCLA/SpuCo"]}, "abstract": "Deep neural networks often exploit non-predictive features that are spuriously correlated with class labels, leading to poor performance on groups of examples without such features. Despite the growing body of recent works on remedying spurious correlations, the lack of a standardized benchmark hinders reproducible evaluation and comparison of the proposed solutions. To address this, we present SpuCo, a python package with modular implementations of state-of-the-art solutions enabling easy and reproducible evaluation of current methods. Using SpuCo, we demonstrate the limitations of existing datasets and evaluation schemes in validating the learning of predictive features over spurious ones. To overcome these limitations, we propose two new vision datasets: (1) SpuCoMNIST, a synthetic dataset that enables simulating the effect of real world data properties e.g. difficulty of learning spurious feature, as well as noise in the labels and features; (2) SpuCoAnimals, a large-scale dataset curated from ImageNet that captures spurious correlations in the wild much more closely than existing datasets. These contributions highlight the shortcomings of current methods and provide a direction for future research in tackling spurious correlations. SpuCo, containing the benchmark and datasets, can be found at https://github.com/BigML-CS-UCLA/SpuCo, with detailed documentation available at https://spuco.readthedocs.io/en/latest/.", "url": "https://arxiv.org/abs/2306.11957"}, {"metadata": {"arXiv": "2306.11967", "Date": "Wed, 21 Jun 2023 01:43:25 ", "Title": "Complementary Learning Subnetworks for Parameter-Efficient Class-Incremental Learning", "Authors": ["Depeng Li", "Zhigang Zeng"], "Categories": "cs.LG cs.CV", "Comments": ["13 pages", "4 figures. Under review"]}, "abstract": "In the scenario of class-incremental learning (CIL), deep neural networks have to adapt their model parameters to non-stationary data distributions, e.g., the emergence of new classes over time. However, CIL models are challenged by the well-known catastrophic forgetting phenomenon. Typical methods such as rehearsal-based ones rely on storing exemplars of old classes to mitigate catastrophic forgetting, which limits real-world applications considering memory resources and privacy issues. In this paper, we propose a novel rehearsal-free CIL approach that learns continually via the synergy between two Complementary Learning Subnetworks. Our approach involves jointly optimizing a plastic CNN feature extractor and an analytical feed-forward classifier. The inaccessibility of historical data is tackled by holistically controlling the parameters of a well-trained model, ensuring that the decision boundary learned fits new classes while retaining recognition of previously learned classes. Specifically, the trainable CNN feature extractor provides task-dependent knowledge separately without interference; and the final classifier integrates task-specific knowledge incrementally for decision-making without forgetting. In each CIL session, it accommodates new tasks by attaching a tiny set of declarative parameters to its backbone, in which only one matrix per task or one vector per class is kept for knowledge retention. Extensive experiments on a variety of task sequences show that our method achieves competitive results against state-of-the-art methods, especially in accuracy gain, memory cost, training efficiency, and task-order robustness. Furthermore, to make the non-growing backbone (i.e., a model with limited network capacity) suffice to train on more incoming tasks, a graceful forgetting implementation on previously learned trivial tasks is empirically investigated.", "url": "https://arxiv.org/abs/2306.11967"}, {"metadata": {"arXiv": "2306.11985", "Date": "Wed, 21 Jun 2023 02:29:30 ", "Title": "Evaluation of Popular XAI Applied to Clinical Prediction Models: Can They be Trusted?", "Authors": ["Aida Brankovic", "David Cook", "Jessica Rahman", "Wenjie Huang", "Sankalp Khanna"], "Categories": "cs.LG cs.CY"}, "abstract": "The absence of transparency and explainability hinders the clinical adoption of Machine learning (ML) algorithms. Although various methods of explainable artificial intelligence (XAI) have been suggested, there is a lack of literature that delves into their practicality and assesses them based on criteria that could foster trust in clinical environments. To address this gap this study evaluates two popular XAI methods used for explaining predictive models in the healthcare context in terms of whether they (i) generate domain-appropriate representation, i.e. coherent with respect to the application task, (ii) impact clinical workflow and (iii) are consistent. To that end, explanations generated at the cohort and patient levels were analysed. The paper reports the first benchmarking of the XAI methods applied to risk prediction models obtained by evaluating the concordance between generated explanations and the trigger of a future clinical deterioration episode recorded by the data collection system. We carried out an analysis using two Electronic Medical Records (EMR) datasets sourced from Australian major hospitals. The findings underscore the limitations of state-of-the-art XAI methods in the clinical context and their potential benefits. We discuss these limitations and contribute to the theoretical development of trustworthy XAI solutions where clinical decision support guides the choice of intervention by suggesting the pattern or drivers for clinical deterioration in the future.", "url": "https://arxiv.org/abs/2306.11985"}, {"metadata": {"arXiv": "2306.11987", "Date": "Wed, 21 Jun 2023 02:45:01 ", "Title": "Training Transformers with 4-bit Integers", "Authors": ["Haocheng Xi", "Changhao Li", "Jianfei Chen", "and Jun Zhu"], "Categories": "cs.LG cs.NE", "Comments": ["9 pages", "8 figures"]}, "abstract": "Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and image classification. Unlike previous 4-bit training methods, our algorithm can be implemented on the current generation of GPUs. Our prototypical linear operator implementation is up to 2.2 times faster than the FP16 counterparts and speeds up the training by up to 35.1%.", "url": "https://arxiv.org/abs/2306.11987"}, {"metadata": {"arXiv": "2306.12014", "Date": "Wed, 21 Jun 2023 04:34:27 ", "Title": "3HAN: A Deep Neural Network for Fake News Detection", "Authors": ["Sneha Singhania", "Nigel Fernandez", "Shrisha Rao"], "Categories": "cs.LG cs.CL cs.SI", "Comments": ["Published as a conference paper at ICONIP 2017"], "DOI": "10.1007/978-3-319-70096-0_59"}, "abstract": "The rapid spread of fake news is a serious problem calling for AI solutions. We employ a deep learning based automated detector through a three level hierarchical attention network (3HAN) for fast, accurate detection of fake news. 3HAN has three levels, one each for words, sentences, and the headline, and constructs a news vector: an effective representation of an input news article, by processing an article in an hierarchical bottom-up manner. The headline is known to be a distinguishing feature of fake news, and furthermore, relatively few words and sentences in an article are more important than the rest. 3HAN gives a differential importance to parts of an article, on account of its three layers of attention. By experiments on a large real-world data set, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike some other deep learning models, 3HAN provides an understandable output through the attention weights given to different parts of an article, which can be visualized through a heatmap to enable further manual fact checking.", "url": "https://arxiv.org/abs/2306.12014"}, {"metadata": {"arXiv": "2306.12026", "Date": "Wed, 21 Jun 2023 05:26:28 ", "Title": "Continual Learners are Incremental Model Generalizers", "Authors": ["Jaehong Yoon", "Sung Ju Hwang", "Yue Cao"], "Categories": "cs.LG cs.CV", "Comments": ["ICML 2023"]}, "abstract": "Motivated by the efficiency and rapid convergence of pre-trained models for solving downstream tasks, this paper extensively studies the impact of Continual Learning (CL) models as pre-trainers. In both supervised and unsupervised CL, we find that the transfer quality of the representation often increases gradually without noticeable degradation in fine-tuning performance. This is because CL models can learn improved task-general features when easily forgetting task-specific knowledge. Based on this observation, we suggest a new unsupervised CL framework with masked modeling, which aims to capture fluent task-generic representation during training. Furthermore, we propose a new fine-tuning scheme, GLobal Attention Discretization (GLAD), that preserves rich task-generic representation during solving downstream tasks. The model fine-tuned with GLAD achieves competitive performance and can also be used as a good pre-trained model itself. We believe this paper breaks the barriers between pre-training and fine-tuning steps and leads to a sustainable learning framework in which the continual learner incrementally improves model generalization, yielding better transfer to unseen tasks.", "url": "https://arxiv.org/abs/2306.12026"}, {"metadata": {"arXiv": "2306.12033", "Date": "Wed, 21 Jun 2023 05:48:51 ", "Title": "End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection", "Authors": ["Jaemin Yoo", "Lingxiao Zhao", "and Leman Akoglu"], "Categories": "cs.LG cs.CV"}, "abstract": "Self-supervised learning (SSL) has emerged as a promising paradigm that presents self-generated supervisory signals to real-world problems, bypassing the extensive manual labeling burden. SSL is especially attractive for unsupervised tasks such as anomaly detection, where labeled anomalies are often nonexistent and costly to obtain. While self-supervised anomaly detection (SSAD) has seen a recent surge of interest, the literature has failed to treat data augmentation as a hyperparameter. Meanwhile, recent works have reported that the choice of augmentation has significant impact on detection performance. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised Anomaly Detection), the first systematic approach to SSAD in regards to rigorously tuning augmentation. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between the augmented training data and the (unlabeled) test data. In principle we adopt transduction, quantifying the extent to which augmentation mimics the true anomaly-generating mechanism, in contrast to augmenting data with arbitrary pseudo anomalies without regard to test data. Second, we present new differentiable augmentation functions, allowing data augmentation hyperparameter(s) to be tuned end-to-end via our proposed validation loss. Experiments on two testbeds with semantic class anomalies and subtle industrial defects show that systematically tuning augmentation offers significant performance gains over current practices.", "url": "https://arxiv.org/abs/2306.12033"}, {"metadata": {"arXiv": "2306.12079", "Date": "Wed, 21 Jun 2023 07:55:29 ", "Title": "FLGo: A Fully Customizable Federated Learning Platform", "Authors": ["Zheng Wang", "Xiaoliang Fan", "Zhaopeng Peng", "Xueheng Li", "Ziqi Yang", "Mingkuan Feng", "Zhicheng Yang", "Xiao Liu", "and Cheng Wang"], "Categories": "cs.LG cs.DC"}, "abstract": "Federated learning (FL) has found numerous applications in healthcare, finance, and IoT scenarios. Many existing FL frameworks offer a range of benchmarks to evaluate the performance of FL under realistic conditions. However, the process of customizing simulations to accommodate application-specific settings, data heterogeneity, and system heterogeneity typically remains unnecessarily complicated. This creates significant hurdles for traditional ML researchers in exploring the usage of FL, while also compromising the shareability of codes across FL frameworks. To address this issue, we propose a novel lightweight FL platform called FLGo, to facilitate cross-application FL studies with a high degree of shareability. Our platform offers 40+ benchmarks, 20+ algorithms, and 2 system simulators as out-of-the-box plugins. We also provide user-friendly APIs for quickly customizing new plugins that can be readily shared and reused for improved reproducibility. Finally, we develop a range of experimental tools, including parallel acceleration, experiment tracker and analyzer, and parameters auto-tuning. FLGo is maintained at \\url{flgo-xmu.github.io}.", "url": "https://arxiv.org/abs/2306.12079"}, {"metadata": {"arXiv": "2306.12086", "Date": "Wed, 21 Jun 2023 08:05:05 ", "Title": "What Constitutes Good Contrastive Learning in Time-Series Forecasting?", "Authors": ["Chiyu Zhang", "Qi Yan", "Lili Meng", "Tristan Sylvain"], "Categories": "cs.LG"}, "abstract": "In recent years, the introduction of self-supervised contrastive learning (SSCL) has demonstrated remarkable improvements in representation learning across various domains, including natural language processing and computer vision. By leveraging the inherent benefits of self-supervision, SSCL enables the pre-training of representation models using vast amounts of unlabeled data. Despite these advances, there remains a significant gap in understanding the impact of different SSCL strategies on time series forecasting performance, as well as the specific benefits that SSCL can bring. This paper aims to address these gaps by conducting a comprehensive analysis of the effectiveness of various training variables, including different SSCL algorithms, learning strategies, model architectures, and their interplay. Additionally, to gain deeper insights into the improvements brought about by SSCL in the context of time-series forecasting, a qualitative analysis of the empirical receptive field is performed. Through our experiments, we demonstrate that the end-to-end training of a Transformer model using the Mean Squared Error (MSE) loss and SSCL emerges as the most effective approach in time series forecasting. Notably, the incorporation of the contrastive objective enables the model to prioritize more pertinent information for forecasting, such as scale and periodic relationships. These findings contribute to a better understanding of the benefits of SSCL in time series forecasting and provide valuable insights for future research in this area.", "url": "https://arxiv.org/abs/2306.12086"}, {"metadata": {"arXiv": "2306.12088", "Date": "Wed, 21 Jun 2023 08:07:07 ", "Title": "An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning", "Authors": ["Cheng Yang", "Xue Yang", "Dongxian Wu", "Xiaohu Tang"], "Categories": "cs.LG"}, "abstract": "Communication overhead is one of the major challenges in Federated Learning(FL). A few classical schemes assume the server can extract the auxiliary information about training data of the participants from the local models to construct a central dummy dataset. The server uses the dummy dataset to finetune aggregated global model to achieve the target test accuracy in fewer communication rounds. In this paper, we summarize the above solutions into a data-based communication-efficient FL framework. The key of the proposed framework is to design an efficient extraction module(EM) which ensures the dummy dataset has a positive effect on finetuning aggregated global model. Different from the existing methods that use generator to design EM, our proposed method, FedINIBoost borrows the idea of gradient match to construct EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two steps for each participant at each communication round. Then the server aggregates all the proxy datasets to form a central dummy dataset, which is used to finetune aggregated global model. Extensive experiments verify the superiority of our method compared with the existing classical method, FedAVG, FedProx, Moon and FedFTG. Moreover, FedINIBoost plays a significant role in finetuning the performance of aggregated global model at the initial stage of FL.", "url": "https://arxiv.org/abs/2306.12088"}, {"metadata": {"arXiv": "2306.12091", "Date": "Wed, 21 Jun 2023 08:11:40 ", "Title": "Structure-Aware DropEdge Towards Deep Graph Convolutional Networks", "Authors": ["Jiaqi Han", "Wenbing Huang", "Yu Rong", "Tingyang Xu", "Fuchun Sun", "Junzhou Huang"], "Categories": "cs.LG", "Comments": ["Accepted to IEEE TNNLS"], "Journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2023", "DOI": "10.1109/TNNLS.2023.3288484"}, "abstract": "It has been discovered that Graph Convolutional Networks (GCNs) encounter a remarkable drop in performance when multiple layers are piled up. The main factor that accounts for why deep GCNs fail lies in over-smoothing, which isolates the network output from the input with the increase of network depth, weakening expressivity and trainability. In this paper, we start by investigating refined measures upon DropEdge -- an existing simple yet effective technique to relieve over-smoothing. We term our method as DropEdge++ for its two structure-aware samplers in contrast to DropEdge: layer-dependent sampler and feature-dependent sampler. Regarding the layer-dependent sampler, we interestingly find that increasingly sampling edges from the bottom layer yields superior performance than the decreasing counterpart as well as DropEdge. We theoretically reveal this phenomenon with Mean-Edge-Number (MEN), a metric closely related to over-smoothing. For the feature-dependent sampler, we associate the edge sampling probability with the feature similarity of node pairs, and prove that it further correlates the convergence subspace of the output layer with the input features. Extensive experiments on several node classification benchmarks, including both full- and semi- supervised tasks, illustrate the efficacy of DropEdge++ and its compatibility with a variety of backbones by achieving generally better performance over DropEdge and the no-drop version.", "url": "https://arxiv.org/abs/2306.12091"}, {"metadata": {"arXiv": "2306.12093", "Date": "Wed, 21 Jun 2023 08:13:41 ", "Title": "Edge Devices Inference Performance Comparison", "Authors": ["R. Tobiasz", "G. Wilczy\\'nski", "P. Graszka", "N. Czechowski", "S. {\\L}uczak"], "Categories": "cs.LG cs.CV", "ACM-class": "I.2.10; B.8.0"}, "abstract": "In this work, we investigate the inference time of the MobileNet family, EfficientNet V1 and V2 family, VGG models, Resnet family, and InceptionV3 on four edge platforms. Specifically NVIDIA Jetson Nano, Intel Neural Stick, Google Coral USB Dongle, and Google Coral PCIe. Our main contribution is a thorough analysis of the aforementioned models in multiple settings, especially as a function of input size, the presence of the classification head, its size, and the scale of the model. Since throughout the industry, those architectures are mainly utilized as feature extractors we put our main focus on analyzing them as such. We show that Google platforms offer the fastest average inference time, especially for newer models like MobileNet or EfficientNet family, while Intel Neural Stick is the most universal accelerator allowing to run most architectures. These results should provide guidance for engineers in the early stages of AI edge systems development. All of them are accessible at https://bulletprove.com/research/edge_inference_results.csv", "url": "https://arxiv.org/abs/2306.12093"}, {"metadata": {"arXiv": "2306.12105", "Date": "Wed, 21 Jun 2023 08:43:29 ", "Title": "Mass-Producing Failures of Multimodal Systems with Language Models", "Authors": ["Shengbang Tong", "Erik Jones", "Jacob Steinhardt"], "Categories": "cs.LG cs.CL cs.SE", "Comments": ["Under Review"]}, "abstract": "Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures -- generalizable, natural-language descriptions of patterns of model failures. To uncover systematic failures, MultiMon scrapes a corpus for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g., \"ignores quantifiers\") of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g., \"a shelf with a few/many books\"). Because CLIP is the backbone for most state-of-the-art multimodal systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-driving cars. We see MultiMon as a step towards evaluation that autonomously explores the long tail of potential system failures. Code for MULTIMON is available at https://github.com/tsb0601/MultiMon.", "url": "https://arxiv.org/abs/2306.12105"}, {"metadata": {"arXiv": "2306.12139", "Date": "Wed, 21 Jun 2023 09:35:50 ", "Title": "Spatial Heterophily Aware Graph Neural Networks", "Authors": ["Congxi Xiao", "Jingbo Zhou", "Jizhou Huang", "Tong Xu", "Hui Xiong"], "Categories": "cs.LG cs.SI", "Comments": ["Accepted by KDD 2023"], "DOI": "10.1145/3580305.3599510"}, "abstract": "Graph Neural Networks (GNNs) have been broadly applied in many urban applications upon formulating a city as an urban graph whose nodes are urban objects like regions or points of interest. Recently, a few enhanced GNN architectures have been developed to tackle heterophily graphs where connected nodes are dissimilar. However, urban graphs usually can be observed to possess a unique spatial heterophily property; that is, the dissimilarity of neighbors at different spatial distances can exhibit great diversity. This property has not been explored, while it often exists. To this end, in this paper, we propose a metric, named Spatial Diversity Score, to quantitatively measure the spatial heterophily and show how it can influence the performance of GNNs. Indeed, our experimental investigation clearly shows that existing heterophilic GNNs are still deficient in handling the urban graph with high spatial diversity score. This, in turn, may degrade their effectiveness in urban applications. Along this line, we propose a Spatial Heterophily Aware Graph Neural Network (SHGNN), to tackle the spatial diversity of heterophily of urban graphs. Based on the key observation that spatially close neighbors on the urban graph present a more similar mode of difference to the central node, we first design a rotation-scaling spatial aggregation module, whose core idea is to properly group the spatially close neighbors and separately process each group with less diversity inside. Then, a heterophily-sensitive spatial interaction module is designed to adaptively capture the commonality and diverse dissimilarity in different spatial groups. Extensive experiments on three real-world urban datasets demonstrate the superiority of our SHGNN over several its competitors.", "url": "https://arxiv.org/abs/2306.12139"}, {"metadata": {"arXiv": "2306.12190", "Date": "Wed, 21 Jun 2023 11:35:59 ", "Title": "Quantifying lottery tickets under label noise: accuracy, calibration, and complexity", "Authors": ["Viplove Arora", "Daniele Irto", "Sebastian Goldt", "Guido Sanguinetti"], "Categories": "cs.LG"}, "abstract": "Pruning deep neural networks is a widely used strategy to alleviate the computational burden in machine learning. Overwhelming empirical evidence suggests that pruned models retain very high accuracy even with a tiny fraction of parameters. However, relatively little work has gone into characterising the small pruned networks obtained, beyond a measure of their accuracy. In this paper, we use the sparse double descent approach to identify univocally and characterise pruned models associated with classification tasks. We observe empirically that, for a given task, iterative magnitude pruning (IMP) tends to converge to networks of comparable sizes even when starting from full networks with sizes ranging over orders of magnitude. We analyse the best pruned models in a controlled experimental setup and show that their number of parameters reflects task difficulty and that they are much better than full networks at capturing the true conditional probability distribution of the labels. On real data, we similarly observe that pruned models are less prone to overconfident predictions. Our results suggest that pruned models obtained via IMP not only have advantageous computational properties but also provide a better representation of uncertainty in learning.", "url": "https://arxiv.org/abs/2306.12190"}, {"metadata": {"arXiv": "2306.12194", "Date": "Wed, 21 Jun 2023 11:42:23 ", "Title": "Split Learning in 6G Edge Networks", "Authors": ["Zheng Lin", "Guanqiao Qu", "Xianhao Chen", "and Kaibin Huang"], "Categories": "cs.LG cs.DC cs.NI", "Comments": ["7 pages", "6 figures"]}, "abstract": "With the proliferation of distributed edge computing resources, the 6G mobile network will evolve into a network for connected intelligence. Along this line, the proposal to incorporate federated learning into the mobile edge has gained considerable interest in recent years. However, the deployment of federated learning faces substantial challenges as massive resource-limited IoT devices can hardly support on-device model training. This leads to the emergence of split learning (SL) which enables servers to handle the major training workload while still enhancing data privacy. In this article, we offer a brief overview of key advancements in SL and articulate its seamless integration with wireless edge networks. We begin by illustrating the tailored 6G architecture to support edge SL. Then, we examine the critical design issues for edge SL, including innovative resource-efficient learning frameworks and resource management strategies under a single edge server. Additionally, we expand the scope to multi-edge scenarios, exploring multi-edge collaboration and mobility management from a networking perspective. Finally, we discuss open problems for edge SL, including convergence analysis, asynchronous SL and U-shaped SL.", "url": "https://arxiv.org/abs/2306.12194"}, {"metadata": {"arXiv": "2306.12212", "Date": "Wed, 21 Jun 2023 12:11:02 ", "Title": "MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates", "Authors": ["Yuchang Sun and Yuyi Mao and Jun Zhang"], "Categories": "cs.LG cs.DC"}, "abstract": "Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modifies each received model update based on the previous ones. The proposed modification of the received model updates is able to mimic the imaginary central update irrespective of the dropout clients. The theoretical analysis of MimiC shows that the divergence between the aggregated update and the central update diminishes with a proper choice of the learning rates, leading to its convergence. Simulation results further demonstrate that MimiC maintains stable convergence performance in the presence of client dropouts and learns better models than the baseline methods.", "url": "https://arxiv.org/abs/2306.12212"}, {"metadata": {"arXiv": "2306.12231", "Date": "Wed, 21 Jun 2023 12:44:52 ", "Title": "Predicting protein variants with equivariant graph neural networks", "Authors": ["Antonia Boca", "Simon Mathis"], "Categories": "cs.LG q-bio.BM", "Comments": ["4 pages", "2 figures", "accepted to the 2023 ICML Workshop on Computational Biology"]}, "abstract": "Pre-trained models have been successful in many protein engineering tasks. Most notably, sequence-based models have achieved state-of-the-art performance on protein fitness prediction while structure-based models have been used experimentally to develop proteins with enhanced functions. However, there is a research gap in comparing structure- and sequence-based methods for predicting protein variants that are better than the wildtype protein. This paper aims to address this gap by conducting a comparative study between the abilities of equivariant graph neural networks (EGNNs) and sequence-based approaches to identify promising amino-acid mutations. The results show that our proposed structural approach achieves a competitive performance to sequence-based methods while being trained on significantly fewer molecules. Additionally, we find that combining assay labelled data with structure pre-trained models yields similar trends as with sequence pre-trained models.", "url": "https://arxiv.org/abs/2306.12231"}, {"metadata": {"arXiv": "2306.12251", "Date": "Wed, 21 Jun 2023 13:16:10 ", "Title": "GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection", "Authors": ["Jianheng Tang", "Fengrui Hua", "Ziqi Gao", "Peilin Zhao", "Jia Li"], "Categories": "cs.LG"}, "abstract": "With a long history of traditional Graph Anomaly Detection (GAD) algorithms and recently popular Graph Neural Networks (GNNs), it is still not clear (1) how they perform under a standard comprehensive setting, (2) whether GNNs outperform traditional algorithms such as tree ensembles, and (3) their efficiency on large-scale graphs. In response, we present GADBench -- a comprehensive benchmark for supervised anomalous node detection on static graphs. GADBench provides a thorough comparison across 23 distinct models on ten real-world GAD datasets ranging from thousands to millions of nodes ($\\sim$6M). Our main finding is that tree ensembles with simple neighborhood aggregation outperform all other baselines, including the latest GNNs tailored for the GAD task. By making GADBench available as an open-source tool, we offer pivotal insights into the current advancements of GAD and establish a solid foundation for future research. Our code is available at https://github.com/squareRoot3/GADBench.", "url": "https://arxiv.org/abs/2306.12251"}, {"metadata": {"arXiv": "2306.12285", "Date": "Wed, 21 Jun 2023 14:13:56 ", "Title": "Resilient Sparse Array Radar with the Aid of Deep Learning", "Authors": ["Aya Mostafa Ahmed", "Udaya S.K.P. Miriya Thanthrige", "Aydin Sezgin and Fulvio Gini"], "Categories": "cs.LG eess.SP", "Comments": ["Accepted to be published in 2023 IEEE 97th Vehicular Technology Conference: VTC2023-Spring", "2023"], "Journal-ref": "2023 IEEE 97th Vehicular Technology Conference (VTC2023-Spring)"}, "abstract": "In this paper, we address the problem of direction of arrival (DOA) estimation for multiple targets in the presence of sensor failures in a sparse array. Generally, sparse arrays are known with very high-resolution capabilities, where N physical sensors can resolve up to $\\mathcal{O}(N^2)$ uncorrelated sources. However, among the many configurations introduced in the literature, the arrays that provide the largest hole-free co-array are the most susceptible to sensor failures. We propose here two machine learning (ML) methods to mitigate the effect of sensor failures and maintain the DOA estimation performance and resolution. The first method enhances the conventional spatial smoothing using deep neural network (DNN), while the second one is an end-to-end data-driven method. Numerical results show that both approaches can significantly improve the performance of MRA with two failed sensors. The data-driven method can maintain the performance of the array with no failures at high signal-tonoise ratio (SNR). Moreover, both approaches can even perform better than the original array at low SNR thanks to the denoising effect of the proposed DNN", "url": "https://arxiv.org/abs/2306.12285"}, {"metadata": {"arXiv": "2306.12306", "Date": "Wed, 21 Jun 2023 14:36:03 ", "Title": "Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift", "Authors": ["Florian Seligmann", "Philipp Becker", "Michael Volpp", "Gerhard Neumann"], "Categories": "cs.LG", "Comments": ["Code at https://github.com/Feuermagier/Beyond_Deep_Ensembles"]}, "abstract": "Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or under-confident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, where training from scratch is prohibitively expensive. Finally, given the recent success of Deep Ensembles, we extend popular single-mode posterior approximations to multiple modes by the use of ensembles. While we find that ensembling single-mode approximations generally improves the generalization capability and calibration of the models by a significant margin, we also identify a failure mode of ensembles when finetuning large transformer-based language models. In this setting, variational inference based approaches such as last-layer Bayes By Backprop outperform other methods in terms of accuracy by a large margin, while modern approximate inference algorithms such as SWAG achieve the best calibration.", "url": "https://arxiv.org/abs/2306.12306"}, {"metadata": {"arXiv": "2306.12314", "Date": "Wed, 21 Jun 2023 14:53:33 ", "Title": "Introspective Action Advising for Interpretable Transfer Learning", "Authors": ["Joseph Campbell", "Yue Guo", "Fiona Xie", "Simon Stepputtis", "Katia Sycara"], "Categories": "cs.LG", "Comments": ["Accepted to CoLLAs 2023"]}, "abstract": "Transfer learning can be applied in deep reinforcement learning to accelerate the training of a policy in a target task by transferring knowledge from a policy learned in a related source task. This is commonly achieved by copying pretrained weights from the source policy to the target policy prior to training, under the constraint that they use the same model architecture. However, not only does this require a robust representation learned over a wide distribution of states -- often failing to transfer between specialist models trained over single tasks -- but it is largely uninterpretable and provides little indication of what knowledge is transferred. In this work, we propose an alternative approach to transfer learning between tasks based on action advising, in which a teacher trained in a source task actively guides a student's exploration in a target task. Through introspection, the teacher is capable of identifying when advice is beneficial to the student and should be given, and when it is not. Our approach allows knowledge transfer between policies agnostic of the underlying representations, and we empirically show that this leads to improved convergence rates in Gridworld and Atari environments while providing insight into what knowledge is transferred.", "url": "https://arxiv.org/abs/2306.12314"}, {"metadata": {"arXiv": "2306.12330", "Date": "Wed, 21 Jun 2023 15:17:39 ", "Title": "ProtoGate: Prototype-based Neural Networks with Local Feature Selection for Tabular Biomedical Data", "Authors": ["Xiangjian Jiang", "Andrei Margeloiu", "Nikola Simidjievski", "Mateja Jamnik"], "Categories": "cs.LG", "Comments": ["Early version presented at the 3rd Interpretable Machine Learning in Healthcare (IMLH) workshop", "2023"]}, "abstract": "Tabular biomedical data poses challenges in machine learning because it is often high-dimensional and typically low-sample-size. Previous research has attempted to address these challenges via feature selection approaches, which can lead to unstable performance on real-world data. This suggests that current methods lack appropriate inductive biases that capture patterns common to different samples. In this paper, we propose ProtoGate, a prototype-based neural model that introduces an inductive bias by attending to both homogeneity and heterogeneity across samples. ProtoGate selects features in a global-to-local manner and leverages them to produce explainable predictions via an interpretable prototype-based model. We conduct comprehensive experiments to evaluate the performance of ProtoGate on synthetic and real-world datasets. Our results show that exploiting the homogeneous and heterogeneous patterns in the data can improve prediction accuracy while prototypes imbue interpretability.", "url": "https://arxiv.org/abs/2306.12330"}, {"metadata": {"arXiv": "2306.12344", "Date": "Wed, 21 Jun 2023 15:41:34 ", "Title": "An efficient, provably exact algorithm for the 0-1 loss linear classification problem", "Authors": ["Xi He", "Max A. Little"], "Categories": "cs.LG cs.DS stat.ML", "Comments": ["15 pages"]}, "abstract": "Algorithms for solving the linear classification problem have a long history, dating back at least to 1936 with linear discriminant analysis. For linearly separable data, many algorithms can obtain the exact solution to the corresponding 0-1 loss classification problem efficiently, but for data which is not linearly separable, it has been shown that this problem, in full generality, is NP-hard. Alternative approaches all involve approximations of some kind, including the use of surrogates for the 0-1 loss (for example, the hinge or logistic loss) or approximate combinatorial search, none of which can be guaranteed to solve the problem exactly. Finding efficient algorithms to obtain an exact i.e. globally optimal solution for the 0-1 loss linear classification problem with fixed dimension, remains an open problem. In research we report here, we detail the construction of a new algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss classification problem exactly in polynomial time. To our knowledge, this is the first, rigorously-proven polynomial time algorithm for this long-standing problem.", "url": "https://arxiv.org/abs/2306.12344"}, {"metadata": {"arXiv": "2306.12356", "Date": "Wed, 21 Jun 2023 16:04:03 ", "Title": "Provably Efficient Representation Learning with Tractable Planning in Low-Rank POMDP", "Authors": ["Jiacheng Guo", "Zihao Li", "Huazheng Wang", "Mengdi Wang", "Zhuoran Yang", "Xuezhou Zhang"], "Categories": "cs.LG"}, "abstract": "In this paper, we study representation learning in partially observable Markov Decision Processes (POMDPs), where the agent learns a decoder function that maps a series of high-dimensional raw observations to a compact representation and uses it for more efficient exploration and planning. We focus our attention on the sub-classes of \\textit{$\\gamma$-observable} and \\textit{decodable POMDPs}, for which it has been shown that statistically tractable learning is possible, but there has not been any computationally efficient algorithm. We first present an algorithm for decodable POMDPs that combines maximum likelihood estimation (MLE) and optimism in the face of uncertainty (OFU) to perform representation learning and achieve efficient sample complexity, while only calling supervised learning computational oracles. We then show how to adapt this algorithm to also work in the broader class of $\\gamma$-observable POMDPs.", "url": "https://arxiv.org/abs/2306.12356"}, {"metadata": {"arXiv": "2306.12370", "Date": "Wed, 21 Jun 2023 16:26:14 ", "Title": "PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning", "Authors": ["Neeratyoy Mallik and Edward Bergman and Carl Hvarfner and Danny Stoll and Maciej Janowski and Marius Lindauer and Luigi Nardi and Frank Hutter"], "Categories": "cs.LG"}, "abstract": "Hyperparameters of Deep Learning (DL) pipelines are crucial for their downstream performance. While a large number of methods for Hyperparameter Optimization (HPO) have been developed, their incurred costs are often untenable for modern DL. Consequently, manual experimentation is still the most prevalent approach to optimize hyperparameters, relying on the researcher's intuition, domain knowledge, and cheap preliminary explorations. To resolve this misalignment between HPO algorithms and DL researchers, we propose PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency across a range of DL benchmarks and show its gains under informative expert input and robustness against poor expert beliefs", "url": "https://arxiv.org/abs/2306.12370"}, {"metadata": {"arXiv": "2306.12371", "Date": "Wed, 21 Jun 2023 16:26:59 ", "Title": "Optimistic Active Exploration of Dynamical Systems", "Authors": ["Bhavya Sukhija", "Lenart Treven", "Cansu Sancaktar", "Sebastian Blaes", "Stelian Coros", "Andreas Krause"], "Categories": "cs.LG cs.RO cs.SY eess.SY"}, "abstract": "Reinforcement learning algorithms commonly seek to optimize policies for solving one particular task. How should we explore an unknown dynamical system such that the estimated model allows us to solve multiple downstream tasks in a zero-shot manner? In this paper, we address this challenge, by developing an algorithm -- OPAX -- for active exploration. OPAX uses well-calibrated probabilistic models to quantify the epistemic uncertainty about the unknown dynamics. It optimistically -- w.r.t. to plausible dynamics -- maximizes the information gain between the unknown dynamics and state observations. We show how the resulting optimization problem can be reduced to an optimal control problem that can be solved at each episode using standard approaches. We analyze our algorithm for general models, and, in the case of Gaussian process dynamics, we give a sample complexity bound and show that the epistemic uncertainty converges to zero. In our experiments, we compare OPAX with other heuristic active exploration approaches on several environments. Our experiments show that OPAX is not only theoretically sound but also performs well for zero-shot planning on novel downstream tasks.", "url": "https://arxiv.org/abs/2306.12371"}, {"metadata": {"arXiv": "2306.12377", "Date": "Wed, 21 Jun 2023 16:42:02 ", "Title": "Geometric Algorithms for $k$-NN Poisoning", "Authors": ["Diego Ihara Centurion", "Karine Chubarian", "Bohan Fan", "Francesco Sgherzi", "Thiruvenkadam S Radhakrishnan", "Anastasios Sidiropoulos", "Angelo Straight"], "Categories": "cs.LG cs.CG cs.CR", "Comments": ["14 pages", "1 figure"]}, "abstract": "We propose a label poisoning attack on geometric data sets against $k$-nearest neighbor classification. We provide an algorithm that can compute an $\\varepsilon n$-additive approximation of the optimal poisoning in $n\\cdot 2^{2^{O(d+k/\\varepsilon)}}$ time for a given data set $X \\in \\mathbb{R}^d$, where $|X| = n$. Our algorithm achieves its objectives through the application of multi-scale random partitions.", "url": "https://arxiv.org/abs/2306.12377"}, {"metadata": {"arXiv": "2306.12380", "Date": "Wed, 21 Jun 2023 16:51:50 ", "Title": "On the Validation of Gibbs Algorithms: Training Datasets, Test Datasets and their Aggregation", "Authors": ["Samir M. Perlaza", "I\\~naki Esnaola", "Gaetan Bisson", "H. Vincent Poor"], "Categories": "cs.LG cs.IT math.IT math.PR math.ST stat.TH", "Comments": ["In Proc. IEEE International Symposium on Information Theory (ISIT)", "Taipei", "Taiwan", "Jun.", "2023. arXiv admin note: text overlap with arXiv:2211.06617"]}, "abstract": "The dependence on training data of the Gibbs algorithm (GA) is analytically characterized. By adopting the expected empirical risk as the performance metric, the sensitivity of the GA is obtained in closed form. In this case, sensitivity is the performance difference with respect to an arbitrary alternative algorithm. This description enables the development of explicit expressions involving the training errors and test errors of GAs trained with different datasets. Using these tools, dataset aggregation is studied and different figures of merit to evaluate the generalization capabilities of GAs are introduced. For particular sizes of such datasets and parameters of the GAs, a connection between Jeffrey's divergence, training and test errors is established.", "url": "https://arxiv.org/abs/2306.12380"}, {"metadata": {"arXiv": "2306.12383", "Date": "Wed, 21 Jun 2023 17:03:22 ", "Title": "Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms", "Authors": ["Qian Yu", "Yining Wang", "Baihe Huang", "Qi Lei", "Jason D. Lee"], "Categories": "cs.LG stat.ML"}, "abstract": "In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called energy allocation, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributions, which are enabled by a truncation method.", "url": "https://arxiv.org/abs/2306.12383"}, {"metadata": {"arXiv": "2306.12413", "Date": "Wed, 21 Jun 2023 17:51:32 ", "Title": "Addressing Discontinuous Root-Finding for Subsequent Differentiability in Machine Learning, Inverse Problems, and Control", "Authors": ["Daniel Johnson", "Ronald Fedkiw"], "Categories": "cs.LG"}, "abstract": "There are many physical processes that have inherent discontinuities in their mathematical formulations. This paper is motivated by the specific case of collisions between two rigid or deformable bodies and the intrinsic nature of that discontinuity. The impulse response to a collision is discontinuous with the lack of any response when no collision occurs, which causes difficulties for numerical approaches that require differentiability which are typical in machine learning, inverse problems, and control. We theoretically and numerically demonstrate that the derivative of the collision time with respect to the parameters becomes infinite as one approaches the barrier separating colliding from not colliding, and use lifting to complexify the solution space so that solutions on the other side of the barrier are directly attainable as precise values. Subsequently, we mollify the barrier posed by the unbounded derivatives, so that one can tunnel back and forth in a smooth and reliable fashion facilitating the use of standard numerical approaches. Moreover, we illustrate that standard approaches fail in numerous ways mostly due to a lack of understanding of the mathematical nature of the problem (e.g. typical backpropagation utilizes many rules of differentiation, but ignores L'Hopital's rule).", "url": "https://arxiv.org/abs/2306.12413"}, {"metadata": {"arXiv": "2306.11740", "Date": "Sat, 17 Jun 2023 11:29:09 ", "Title": "A survey on deep learning approaches for data integration in autonomous driving system", "Authors": ["Xi Zhu", "Likang Wang", "Caifa Zhou", "Xiya Cao", "Yue Gong", "Lei Chen"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["23 pages", "1 figure", "plan to submit to IEEE Trans. on ITS"]}, "abstract": "The perception module of self-driving vehicles relies on a multi-sensor system to understand its environment. Recent advancements in deep learning have led to the rapid development of approaches that integrate multi-sensory measurements to enhance perception capabilities. This paper surveys the latest deep learning integration techniques applied to the perception module in autonomous driving systems, categorizing integration approaches based on \"what, how, and when to integrate.\" A new taxonomy of integration is proposed, based on three dimensions: multi-view, multi-modality, and multi-frame. The integration operations and their pros and cons are summarized, providing new insights into the properties of an \"ideal\" data integration approach that can alleviate the limitations of existing methods. After reviewing hundreds of relevant papers, this survey concludes with a discussion of the key features of an optimal data integration approach.", "url": "https://arxiv.org/abs/2306.11740"}, {"metadata": {"arXiv": "2306.12392", "Date": "Wed, 21 Jun 2023 17:26:11 ", "Title": "One-shot Imitation Learning via Interaction Warping", "Authors": ["Ondrej Biza", "Skye Thompson", "Kishore Reddy Pagidi", "Abhinav Kumar", "Elise van der Pol", "Robin Walters", "Thomas Kipf", "Jan-Willem van de Meent", "Lawson L.S. Wong", "Robert Platt"], "Categories": "cs.RO cs.LG"}, "abstract": "Imitation learning of robot policies from few demonstrations is crucial in open-ended applications. We propose a new method, Interaction Warping, for learning SE(3) robotic manipulation policies from a single demonstration. We infer the 3D mesh of each object in the environment using shape warping, a technique for aligning point clouds across object instances. Then, we represent manipulation actions as keypoints on objects, which can be warped with the shape of the object. We show successful one-shot imitation learning on three simulated and real-world object re-arrangement tasks. We also demonstrate the ability of our method to predict object meshes and robot grasps in the wild.", "url": "https://arxiv.org/abs/2306.12392"}, {"metadata": {"arXiv": "2306.12129", "Date": "Wed, 21 Jun 2023 09:19:33 ", "Title": "Machine Learning Based Compensation for Inconsistencies in Knitted Force Sensors", "Authors": ["Roland Aigner and Andreas St\\\"ockl"], "Categories": "eess.SY cs.LG cs.SY eess.SP"}, "abstract": "Knitted sensors frequently suffer from inconsistencies due to innate effects such as offset, relaxation, and drift. These properties, in combination, make it challenging to reliably map from sensor data to physical actuation. In this paper, we demonstrate a method for counteracting this by applying processing using a minimal artificial neural network (ANN) in combination with straightforward pre-processing. We apply a number of exponential smoothing filters on a re-sampled sensor signal, to produce features that preserve different levels of historical sensor data and, in combination, represent an adequate state of previous sensor actuation. By training a three-layer ANN with a total of 8 neurons, we manage to significantly improve the mapping between sensor reading and actuation force. Our findings also show that our technique translates to sensors of reasonably different composition in terms of material and structure, and it can furthermore be applied to related physical features such as strain.", "url": "https://arxiv.org/abs/2306.12129"}, {"metadata": {"arXiv": "2306.12361", "Date": "Wed, 21 Jun 2023 16:14:21 ", "Title": "Sigma-point Kalman Filter with Nonlinear Unknown Input Estimation via Optimization and Data-driven Approach for Dynamic Systems", "Authors": ["Junn Yong Loo", "Ze Yang Ding", "Vishnu Monn Baskaran", "Surya Girinatha Nurzaman", "and Chee Pin Tan"], "Categories": "eess.SY cs.LG cs.SY"}, "abstract": "Most works on joint state and unknown input (UI) estimation require the assumption that the UIs are linear; this is potentially restrictive as it does not hold in many intelligent autonomous systems. To overcome this restriction and circumvent the need to linearize the system, we propose a derivative-free Unknown Input Sigma-point Kalman Filter (SPKF-nUI) where the SPKF is interconnected with a general nonlinear UI estimator that can be implemented via nonlinear optimization and data-driven approaches. The nonlinear UI estimator uses the posterior state estimate which is less susceptible to state prediction error. In addition, we introduce a joint sigma-point transformation scheme to incorporate both the state and UI uncertainties in the estimation of SPKF-nUI. An in-depth stochastic stability analysis proves that the proposed SPKF-nUI yields exponentially converging estimation error bounds under reasonable assumptions. Finally, two case studies are carried out on a simulation-based rigid robot and a physical soft robot, i.e., robots made of soft materials with complex dynamics to validate effectiveness of the proposed filter on nonlinear dynamic systems. Our results demonstrate that the proposed SPKF-nUI achieves the lowest state and UI estimation errors when compared to the existing nonlinear state-UI filters.", "url": "https://arxiv.org/abs/2306.12361"}, {"metadata": {"arXiv": "2306.11755", "Date": "Mon, 19 Jun 2023 14:38:06 ", "Title": "On Identifiability of Conditional Causal Effects", "Authors": ["Yaroslav Kivva", "Jalal Etesami", "Negar Kiyavash"], "Categories": "cs.AI math.ST stat.TH"}, "abstract": "We address the problem of identifiability of an arbitrary conditional causal effect given both the causal graph and a set of any observational and/or interventional distributions of the form $Q[S]:=P(S|do(V\\setminus S))$, where $V$ denotes the set of all observed variables and $S\\subseteq V$. We call this problem conditional generalized identifiability (c-gID in short) and prove the completeness of Pearl's $do$-calculus for the c-gID problem by providing sound and complete algorithm for the c-gID problem. This work revisited the c-gID problem in Lee et al. [2020], Correa et al. [2021] by adding explicitly the positivity assumption which is crucial for identifiability. It extends the results of [Lee et al., 2019, Kivva et al., 2022] on general identifiability (gID) which studied the problem for unconditional causal effects and Shpitser and Pearl [2006b] on identifiability of conditional causal effects given merely the observational distribution $P(\\mathbf{V})$ as our algorithm generalizes the algorithms proposed in [Kivva et al., 2022] and [Shpitser and Pearl, 2006b].", "url": "https://arxiv.org/abs/2306.11755"}, {"metadata": {"arXiv": "2306.11759", "Date": "Tue, 20 Jun 2023 07:51:14 ", "Title": "Deep Learning Accelerator in Loop Reliability Evaluation for Autonomous Driving", "Authors": ["Haitong Huang", "Cheng Liu"], "Categories": "cs.AI cs.AR cs.RO", "Comments": ["2 pages", "2 figures"]}, "abstract": "The reliability of deep learning accelerators (DLAs) used in autonomous driving systems has significant impact on the system safety. However, the DLA reliability is usually evaluated with low-level metrics like mean square errors of the output which remains rather different from the high-level metrics like total distance traveled before failure in autonomous driving. As a result, the high-level reliability metrics evaluated at the post-silicon stage may still lead to DLA design revision and result in expensive reliable DLA design iterations targeting at autonomous driving. To address the problem, we proposed a DLA-in-loop reliability evaluation platform to enable system reliability evaluation at the early DLA design stage.", "url": "https://arxiv.org/abs/2306.11759"}, {"metadata": {"arXiv": "2306.11767", "Date": "Tue, 20 Jun 2023 12:06:41 ", "Title": "A Graphical Modeling Language for Artificial Intelligence Applications in Automation Systems", "Authors": ["Marvin Schieseck", "Philip Topalis", "Alexander Fay"], "Categories": "cs.AI cs.SY eess.SY"}, "abstract": "Artificial Intelligence (AI) applications in automation systems are usually distributed systems whose development and integration involve several experts. Each expert uses its own domain-specific modeling language and tools to model the system elements. An interdisciplinary graphical modeling language that enables the modeling of an AI application as an overall system comprehensible to all disciplines does not yet exist. As a result, there is often a lack of interdisciplinary system understanding, leading to increased development, integration, and maintenance efforts. This paper therefore presents a graphical modeling language that enables consistent and understandable modeling of AI applications in automation systems at system level. This makes it possible to subdivide individual subareas into domain specific subsystems and thus reduce the existing efforts.", "url": "https://arxiv.org/abs/2306.11767"}, {"metadata": {"arXiv": "2306.11739", "Date": "Sat, 17 Jun 2023 03:25:13 ", "Title": "Multi-view 3D Object Reconstruction and Uncertainty Modelling with Neural Shape Prior", "Authors": ["Ziwei Liao", "Steven L. Waslander"], "Categories": "cs.CV cs.AI cs.RO", "Comments": ["12 pages", "8 figures"]}, "abstract": "3D object reconstruction is important for semantic scene understanding. It is challenging to reconstruct detailed 3D shapes from monocular images directly due to a lack of depth information, occlusion and noise. Most current methods generate deterministic object models without any awareness of the uncertainty of the reconstruction. We tackle this problem by leveraging a neural object representation which learns an object shape distribution from large dataset of 3d object models and maps it into a latent space. We propose a method to model uncertainty as part of the representation and define an uncertainty-aware encoder which generates latent codes with uncertainty directly from individual input images. Further, we propose a method to propagate the uncertainty in the latent code to SDF values and generate a 3d object mesh with local uncertainty for each mesh component. Finally, we propose an incremental fusion method under a Bayesian framework to fuse the latent codes from multi-view observations. We evaluate the system in both synthetic and real datasets to demonstrate the effectiveness of uncertainty-based fusion to improve 3D object reconstruction accuracy.", "url": "https://arxiv.org/abs/2306.11739"}, {"metadata": {"arXiv": "2306.12113", "Date": "Wed, 21 Jun 2023 08:55:45 ", "Title": "Lightweight wood panel defect detection method incorporating attention mechanism and feature fusion network", "Authors": ["Yongxin Cao", "Fanghua Liu", "Lai Jiang", "Cheng Bao", "You Miao and Yang Chen"], "Categories": "cs.CV cs.AI"}, "abstract": "In recent years, deep learning has made significant progress in wood panel defect detection. However, there are still challenges such as low detection , slow detection speed, and difficulties in deploying embedded devices on wood panel surfaces. To overcome these issues, we propose a lightweight wood panel defect detection method called YOLOv5-LW, which incorporates attention mechanisms and a feature fusion network.Firstly, to enhance the detection capability of acceptable defects, we introduce the Multi-scale Bi-directional Feature Pyramid Network (MBiFPN) as a feature fusion network. The MBiFPN reduces feature loss, enriches local and detailed features, and improves the model's detection capability for acceptable defects.Secondly, to achieve a lightweight design, we reconstruct the ShuffleNetv2 network model as the backbone network. This reconstruction reduces the number of parameters and computational requirements while maintaining performance. We also introduce the Stem Block and Spatial Pyramid Pooling Fast (SPPF) models to compensate for any accuracy loss resulting from the lightweight design, ensuring the model's detection capabilities remain intact while being computationally efficient.Thirdly, we enhance the backbone network by incorporating Efficient Channel Attention (ECA), which improves the network's focus on key information relevant to defect detection. By attending to essential features, the model becomes more proficient in accurately identifying and localizing defects.We validate the proposed method using a self-developed wood panel defect dataset.The experimental results demonstrate the effectiveness of the improved YOLOv5-LW method. Compared to the original model, our approach achieves a 92.8\\% accuracy rate, reduces the number of parameters by 27.78\\%, compresses computational volume by 41.25\\%, improves detection inference speed by 10.16\\%", "url": "https://arxiv.org/abs/2306.12113"}, {"metadata": {"arXiv": "2306.12156", "Date": "Wed, 21 Jun 2023 10:08:29 ", "Title": "Fast Segment Anything", "Authors": ["Xu Zhao", "Wenchao Ding", "Yongqi An", "Yinglong Du", "Tao Yu", "Min Li", "Ming Tang", "Jinqiao Wang"], "Categories": "cs.CV cs.AI", "Comments": ["Technical Report. The code is released at https://github.com/CASIA-IVA-Lab/FastSAM"]}, "abstract": "The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.", "url": "https://arxiv.org/abs/2306.12156"}, {"metadata": {"arXiv": "2306.11862", "Date": "Tue, 20 Jun 2023 19:42:30 ", "Title": "Proactive Human-Robot Co-Assembly: Leveraging Human Intention Prediction and Robust Safe Control", "Authors": ["Ruixuan Liu", "Rui Chen", "Abulikemu Abuduweili", "Changliu Liu"], "Categories": "cs.RO cs.AI"}, "abstract": "Human-robot collaboration (HRC) is one key component to achieving flexible manufacturing to meet the different needs of customers. However, it is difficult to build intelligent robots that can proactively assist humans in a safe and efficient way due to several challenges.First, it is challenging to achieve efficient collaboration due to diverse human behaviors and data scarcity. Second, it is difficult to ensure interactive safety due to uncertainty in human behaviors. This paper presents an integrated framework for proactive HRC. A robust intention prediction module, which leverages prior task information and human-in-the-loop training, is learned to guide the robot for efficient collaboration. The proposed framework also uses robust safe control to ensure interactive safety under uncertainty. The developed framework is applied to a co-assembly task using a Kinova Gen3 robot. The experiment demonstrates that our solution is robust to environmental changes as well as different human preferences and behaviors. In addition, it improves task efficiency by approximately 15-20%. Moreover, the experiment demonstrates that our solution can guarantee interactive safety during proactive collaboration.", "url": "https://arxiv.org/abs/2306.11862"}, {"metadata": {"arXiv": "2306.11897", "Date": "Tue, 20 Jun 2023 21:19:13 ", "Title": "Reinforcement Learning-based Virtual Fixtures for Teleoperation of Hydraulic Construction Machine", "Authors": ["Hyung Joo Lee and Sigrid Brell-Cokcan"], "Categories": "cs.RO cs.AI", "Report-no": "Accepted at IEEE ROMAN 2023"}, "abstract": "The utilization of teleoperation is a crucial aspect of the construction industry, as it enables operators to control machines safely from a distance. However, remote operation of these machines at a joint level using individual joysticks necessitates extensive training for operators to achieve proficiency due to their multiple degrees of freedom. Additionally, verifying the machine resulting motion is only possible after execution, making optimal control challenging. In addressing this issue, this study proposes a reinforcement learning-based approach to optimize task performance. The control policy acquired through learning is used to provide instructions on efficiently controlling and coordinating multiple joints. To evaluate the effectiveness of the proposed framework, a user study is conducted with a Brokk 170 construction machine by assessing its performance in a typical construction task involving inserting a chisel into a borehole. The effectiveness of the proposed framework is evaluated by comparing the performance of participants in the presence and absence of virtual fixtures. This study results demonstrate the proposed framework potential in enhancing the teleoperation process in the construction industry.", "url": "https://arxiv.org/abs/2306.11897"}, {"metadata": {"arXiv": "2306.11846", "Date": "Tue, 20 Jun 2023 18:56:25 ", "Title": "Discovering Causality for Efficient Cooperation in Multi-Agent Environments", "Authors": ["Rafael Pina", "Varuna De Silva", "Corentin Artaud"], "Categories": "cs.AI cs.LG cs.MA stat.ME"}, "abstract": "In cooperative Multi-Agent Reinforcement Learning (MARL) agents are required to learn behaviours as a team to achieve a common goal. However, while learning a task, some agents may end up learning sub-optimal policies, not contributing to the objective of the team. Such agents are called lazy agents due to their non-cooperative behaviours that may arise from failing to understand whether they caused the rewards. As a consequence, we observe that the emergence of cooperative behaviours is not necessarily a byproduct of being able to solve a task as a team. In this paper, we investigate the applications of causality in MARL and how it can be applied in MARL to penalise these lazy agents. We observe that causality estimations can be used to improve the credit assignment to the agents and show how it can be leveraged to improve independent learning in MARL. Furthermore, we investigate how Amortized Causal Discovery can be used to automate causality detection within MARL environments. The results demonstrate that causality relations between individual observations and the team reward can be used to detect and punish lazy agents, making them develop more intelligent behaviours. This results in improvements not only in the overall performances of the team but also in their individual capabilities. In addition, results show that Amortized Causal Discovery can be used efficiently to find causal relations in MARL.", "url": "https://arxiv.org/abs/2306.11846"}, {"metadata": {"arXiv": "2306.11890", "Date": "Thu, 15 Jun 2023 20:08:16 ", "Title": "Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy", "Authors": ["Wolfgang M. Pernice", "Michael Doron", "Alex Quach", "Aditya Pratapa", "Sultan Kenjeyev", "Nicholas De Veaux", "Michio Hirano", "Juan C. Caicedo"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Accepted at CVPR 2023 CVMI"]}, "abstract": "Real-world deployment of computer vision systems, including in the discovery processes of biomedical research, requires causal representations that are invariant to contextual nuisances and generalize to new data. Leveraging the internal replicate structure of two novel single-cell fluorescent microscopy datasets, we propose generally applicable tests to assess the extent to which models learn causal representations across increasingly challenging levels of OOD-generalization. We show that despite seemingly strong performance, as assessed by other established metrics, both naive and contemporary baselines designed to ward against confounding, collapse on these tests. We introduce a new method, Interventional Style Transfer (IST), that substantially improves OOD generalization by generating interventional training distributions in which spurious correlations between biological causes and nuisances are mitigated. We publish our code and datasets.", "url": "https://arxiv.org/abs/2306.11890"}, {"metadata": {"arXiv": "2306.12150", "Date": "Wed, 21 Jun 2023 09:53:37 ", "Title": "Benchmark data to study the influence of pre-training on explanation performance in MR image classification", "Authors": ["Marta Oliveira", "Rick Wilming", "Benedict Clark", "C\\'eline Budding", "Fabian Eitel", "Kerstin Ritter", "Stefan Haufe"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Under review"]}, "abstract": "Convolutional Neural Networks (CNNs) are frequently and successfully used in medical prediction tasks. They are often used in combination with transfer learning, leading to improved performance when training data for the task are scarce. The resulting models are highly complex and typically do not provide any insight into their predictive mechanisms, motivating the field of 'explainable' artificial intelligence (XAI). However, previous studies have rarely quantitatively evaluated the 'explanation performance' of XAI methods against ground-truth data, and transfer learning and its influence on objective measures of explanation performance has not been investigated. Here, we propose a benchmark dataset that allows for quantifying explanation performance in a realistic magnetic resonance imaging (MRI) classification task. We employ this benchmark to understand the influence of transfer learning on the quality of explanations. Experimental results show that popular XAI methods applied to the same underlying model differ vastly in performance, even when considering only correctly classified examples. We further observe that explanation performance strongly depends on the task used for pre-training and the number of CNN layers pre-trained. These results hold after correcting for a substantial correlation between explanation and classification performance.", "url": "https://arxiv.org/abs/2306.12150"}, {"metadata": {"arXiv": "2306.11758", "Date": "Tue, 20 Jun 2023 06:46:54 ", "Title": "MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing", "Authors": ["Haitong Huang", "Cheng Liu", "Xinghua Xue", "Ying Wang", "Huawei Li", "Xiaowei Li"], "Categories": "cs.LG cs.AI cs.AR", "Comments": ["8 pages", "11 figures", "source code is on https://github.com/fffasttime/MRFI"]}, "abstract": "To ensure resilient neural network processing on even unreliable hardware, comprehensive reliability analysis against various hardware faults is generally required before the deep neural network models are deployed, and efficient error injection tools are highly demanded. However, most existing fault injection tools remain rather limited to basic fault injection to neurons and fail to provide fine-grained vulnerability analysis capability. In addition, many of the fault injection tools still need to change the neural network models and make the fault injection closely coupled with normal neural network processing, which further complicates the use of the fault injection tools and slows down the fault simulation. In this work, we propose MRFI, a highly configurable multi-resolution fault injection tool for deep neural networks. It enables users to modify an independent fault configuration file rather than neural network models for the fault injection and vulnerability analysis. Particularly, it integrates extensive fault analysis functionalities from different perspectives and enables multi-resolution investigation of the vulnerability of neural networks. In addition, it does not modify the major neural network computing framework of PyTorch. Hence, it allows parallel processing on GPUs naturally and exhibits fast fault simulation according to our experiments.", "url": "https://arxiv.org/abs/2306.11758"}, {"metadata": {"arXiv": "2306.11816", "Date": "Tue, 20 Jun 2023 18:19:17 ", "Title": "Learning to Generate Better Than Your LLM", "Authors": ["Jonathan D. Chang", "Kiante Brantley", "Rajkumar Ramamurthy", "Dipendra Misra", "Wen Sun"], "Categories": "cs.LG cs.AI cs.CL", "Comments": ["23 pages", "5 figures", "7 tables", "4 algorithms"]}, "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. On CommonGen, we not only outperform our SL baselines but also improve beyond PPO across a variety of lexical and semantic metrics beyond the one we optimized for. Notably, on the IMDB dataset, we show that our GPT-2 based policy outperforms the zero-shot GPT-3 oracle, indicating that our algorithms can learn from a powerful, black-box GPT-3 oracle with a simpler, cheaper, and publicly available GPT-2 model while gaining performance.", "url": "https://arxiv.org/abs/2306.11816"}, {"metadata": {"arXiv": "2306.11827", "Date": "Tue, 20 Jun 2023 18:37:21 ", "Title": "Any Deep ReLU Network is Shallow", "Authors": ["Mattia Jacopo Villani", "Nandi Schoots"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["12 pages including bibliography and appendix"]}, "abstract": "We constructively prove that every deep ReLU network can be rewritten as a functionally identical three-layer network with weights valued in the extended reals. Based on this proof, we provide an algorithm that, given a deep ReLU network, finds the explicit weights of the corresponding shallow network. The resulting shallow network is transparent and used to generate explanations of the model s behaviour.", "url": "https://arxiv.org/abs/2306.11827"}, {"metadata": {"arXiv": "2306.11885", "Date": "Tue, 20 Jun 2023 20:58:33 ", "Title": "Reward Shaping via Diffusion Process in Reinforcement Learning", "Authors": ["Peeyush Kumar"], "Categories": "cs.LG cs.AI", "Comments": ["Reinforcement Learning", "MDP", "Reward Shaping", "Diffusion Process", "Drift Model", "Stochastic Thermodynamics", "Information theoretic", "Entropy"]}, "abstract": "Reinforcement Learning (RL) models have continually evolved to navigate the exploration - exploitation trade-off in uncertain Markov Decision Processes (MDPs). In this study, I leverage the principles of stochastic thermodynamics and system dynamics to explore reward shaping via diffusion processes. This provides an elegant framework as a way to think about exploration-exploitation trade-off. This article sheds light on relationships between information entropy, stochastic system dynamics, and their influences on entropy production. This exploration allows us to construct a dual-pronged framework that can be interpreted as either a maximum entropy program for deriving efficient policies or a modified cost optimization program accounting for informational costs and benefits. This work presents a novel perspective on the physical nature of information and its implications for online learning in MDPs, consequently providing a better understanding of information-oriented formulations in RL.", "url": "https://arxiv.org/abs/2306.11885"}, {"metadata": {"arXiv": "2306.11898", "Date": "Tue, 20 Jun 2023 21:19:57 ", "Title": "Unexplainable Explanations: Towards Interpreting tSNE and UMAP Embeddings", "Authors": ["Andrew Draganov and Simon Dohn"], "Categories": "cs.LG cs.AI"}, "abstract": "It has become standard to explain neural network latent spaces with attraction/repulsion dimensionality reduction (ARDR) methods like tSNE and UMAP. This relies on the premise that structure in the 2D representation is consistent with the structure in the model's latent space. However, this is an unproven assumption -- we are unaware of any convergence guarantees for ARDR algorithms. We work on closing this question by relating ARDR methods to classical dimensionality reduction techniques. Specifically, we show that one can fully recover a PCA embedding by applying attractions and repulsions onto a randomly initialized dataset. We also show that, with a small change, Locally Linear Embeddings (LLE) can reproduce ARDR embeddings. Finally, we formalize a series of conjectures that, if true, would allow one to attribute structure in the 2D embedding back to the input distribution.", "url": "https://arxiv.org/abs/2306.11898"}, {"metadata": {"arXiv": "2306.11915", "Date": "Tue, 20 Jun 2023 22:01:37 ", "Title": "Structure-Aware Robustness Certificates for Graph Classification", "Authors": ["Pierre Osselin", "Henry Kenlay and Xiaowen Dong"], "Categories": "cs.LG cs.AI cs.SI stat.ML", "Comments": ["9 pages", "6 figures (15 pages", "10 figures including references and appendices)"]}, "abstract": "Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demonstrate the benefits of these certificates in both synthetic and real-world experiments.", "url": "https://arxiv.org/abs/2306.11915"}, {"metadata": {"arXiv": "2306.11918", "Date": "Tue, 20 Jun 2023 22:06:14 ", "Title": "Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback", "Authors": ["Hang Wang", "Sen Lin", "Junshan Zhang"], "Categories": "cs.LG cs.AI", "Comments": ["NeurIPS 2021"]}, "abstract": "The ensemble method is a promising way to mitigate the overestimation issue in Q-learning, where multiple function approximators are used to estimate the action values. It is known that the estimation bias hinges heavily on the ensemble size (i.e., the number of Q-function approximators used in the target), and that determining the `right' ensemble size is highly nontrivial, because of the time-varying nature of the function approximation errors during the learning process. To tackle this challenge, we first derive an upper bound and a lower bound on the estimation bias, based on which the ensemble size is adapted to drive the bias to be nearly zero, thereby coping with the impact of the time-varying approximation errors accordingly. Motivated by the theoretic findings, we advocate that the ensemble method can be combined with Model Identification Adaptive Control (MIAC) for effective ensemble size adaptation. Specifically, we devise Adaptive Ensemble Q-learning (AdaEQ), a generalized ensemble method with two key steps: (a) approximation error characterization which serves as the feedback for flexibly controlling the ensemble size, and (b) ensemble size adaptation tailored towards minimizing the estimation bias. Extensive experiments are carried out to show that AdaEQ can improve the learning performance than the existing methods for the MuJoCo benchmark.", "url": "https://arxiv.org/abs/2306.11918"}, {"metadata": {"arXiv": "2306.11941", "Date": "Tue, 20 Jun 2023 23:38:24 ", "Title": "Efficient Dynamics Modeling in Interactive Environments with Koopman Theory", "Authors": ["Arnab Kumar Mondal", "Siba Smarak Panigrahi", "Sai Rajeswar", "Kaleem Siddiqi", "Siamak Ravanbakhsh"], "Categories": "cs.LG cs.AI", "Comments": ["18 pages", "3 figures"]}, "abstract": "The accurate modeling of dynamics in interactive environments is critical for successful long-range prediction. Such a capability could advance Reinforcement Learning (RL) and Planning algorithms, but achieving it is challenging. Inaccuracies in model estimates can compound, resulting in increased errors over long horizons. We approach this problem from the lens of Koopman theory, where the nonlinear dynamics of the environment can be linearized in a high-dimensional latent space. This allows us to efficiently parallelize the sequential problem of long-range prediction using convolution, while accounting for the agent's action at every time step. Our approach also enables stability analysis and better control over gradients through time. Taken together, these advantages result in significant improvement over the existing approaches, both in the efficiency and the accuracy of modeling dynamics over extended horizons. We also report promising experimental results in dynamics modeling for the scenarios of both model-based planning and model-free RL.", "url": "https://arxiv.org/abs/2306.11941"}, {"metadata": {"arXiv": "2306.11971", "Date": "Wed, 21 Jun 2023 01:53:01 ", "Title": "AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization", "Authors": ["Maziar Gomrokchi", "Owen Levin", "Jeffrey Roach", "Jonah White"], "Categories": "cs.LG cs.AI"}, "abstract": "We introduce \\env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within the environment, we demonstrate the challenges imposed by sparsity and non-stationarity on agent convergence and performance. We hope these challenges further encourage discourse and development around effective strategies for managing real-world uncertainties.", "url": "https://arxiv.org/abs/2306.11971"}, {"metadata": {"arXiv": "2306.12059", "Date": "Wed, 21 Jun 2023 07:01:38 ", "Title": "EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations", "Authors": ["Yi-Lun Liao", "Brandon Wood", "Abhishek Das", "Tess Smidt"], "Categories": "cs.LG cs.AI physics.comp-ph"}, "abstract": "Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\\%$ on forces, $4\\%$ on energies, offers better speed-accuracy trade-offs, and $2\\times$ reduction in DFT calculations needed for computing adsorption energies.", "url": "https://arxiv.org/abs/2306.12059"}, {"metadata": {"arXiv": "2306.12077", "Date": "Wed, 21 Jun 2023 07:52:07 ", "Title": "Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers", "Authors": ["Kai Lagemann", "Christian Lagemann", "Sach Mukherjee"], "Categories": "cs.LG cs.AI"}, "abstract": "We propose a method for learning dynamical systems from high-dimensional empirical data that combines variational autoencoders and (spatio-)temporal attention within a framework designed to enforce certain scientifically-motivated invariances. We focus on the setting in which data are available from multiple different instances of a system whose underlying dynamical model is entirely unknown at the outset. The approach rests on a separation into an instance-specific encoding (capturing initial conditions, constants etc.) and a latent dynamics model that is itself universal across all instances/realizations of the system. The separation is achieved in an automated, data-driven manner and only empirical data are required as inputs to the model. The approach allows effective inference of system behaviour at any continuous time but does not require an explicit neural ODE formulation, which makes it efficient and highly scalable. We study behaviour through simple theoretical analyses and extensive experiments on synthetic and real-world datasets. The latter investigate learning the dynamics of complex systems based on finite data and show that the proposed approach can outperform state-of-the-art neural-dynamical models. We study also more general inductive bias in the context of transfer to data obtained under entirely novel system interventions. Overall, our results provide a promising new framework for efficiently learning dynamical models from heterogeneous data with potential applications in a wide range of fields including physics, medicine, biology and engineering.", "url": "https://arxiv.org/abs/2306.12077"}, {"metadata": {"arXiv": "2306.12161", "Date": "Wed, 21 Jun 2023 10:17:55 ", "Title": "Adversarial Attacks Neutralization via Data Set Randomization", "Authors": ["Mouna Rabhi and Roberto Di Pietro"], "Categories": "cs.LG cs.AI cs.CR"}, "abstract": "Adversarial attacks on deep-learning models pose a serious threat to their reliability and security. Existing defense mechanisms are narrow addressing a specific type of attack or being vulnerable to sophisticated attacks. We propose a new defense mechanism that, while being focused on image-based classifiers, is general with respect to the cited category. It is rooted on hyperspace projection. In particular, our solution provides a pseudo-random projection of the original dataset into a new dataset. The proposed defense mechanism creates a set of diverse projected datasets, where each projected dataset is used to train a specific classifier, resulting in different trained classifiers with different decision boundaries. During testing, it randomly selects a classifier to test the input. Our approach does not sacrifice accuracy over legitimate input. Other than detailing and providing a thorough characterization of our defense mechanism, we also provide a proof of concept of using four optimization-based adversarial attacks (PGD, FGSM, IGSM, and C\\&W) and a generative adversarial attack testing them on the MNIST dataset. Our experimental results show that our solution increases the robustness of deep learning models against adversarial attacks and significantly reduces the attack success rate by at least 89% for optimization attacks and 78% for generative attacks. We also analyze the relationship between the number of used hyperspaces and the efficacy of the defense mechanism. As expected, the two are positively correlated, offering an easy-to-tune parameter to enforce the desired level of security. The generality and scalability of our solution and adaptability to different attack scenarios, combined with the excellent achieved results, other than providing a robust defense against adversarial attacks on deep learning networks, also lay the groundwork for future research in the field.", "url": "https://arxiv.org/abs/2306.12161"}, {"metadata": {"arXiv": "2306.12215", "Date": "Wed, 21 Jun 2023 12:15:57 ", "Title": "Automated Machine Learning for Remaining Useful Life Predictions", "Authors": ["Marc-Andr\\'e Z\\\"oller", "Fabian Mauthe", "Peter Zeiler", "Marius Lindauer", "Marco F. Huber"], "Categories": "cs.LG cs.AI", "Comments": ["Manuscript accepted at IEEE SMC 2023"]}, "abstract": "Being able to predict the remaining useful life (RUL) of an engineering system is an important task in prognostics and health management. Recently, data-driven approaches to RUL predictions are becoming prevalent over model-based approaches since no underlying physical knowledge of the engineering system is required. Yet, this just replaces required expertise of the underlying physics with machine learning (ML) expertise, which is often also not available. Automated machine learning (AutoML) promises to build end-to-end ML pipelines automatically enabling domain experts without ML expertise to create their own models. This paper introduces AutoRUL, an AutoML-driven end-to-end approach for automatic RUL predictions. AutoRUL combines fine-tuned standard regression methods to an ensemble with high predictive power. By evaluating the proposed method on eight real-world and synthetic datasets against state-of-the-art hand-crafted models, we show that AutoML provides a viable alternative to hand-crafted data-driven RUL predictions. Consequently, creating RUL predictions can be made more accessible for domain experts using AutoML by eliminating ML expertise from data-driven model construction.", "url": "https://arxiv.org/abs/2306.12215"}, {"metadata": {"arXiv": "2306.12230", "Date": "Wed, 21 Jun 2023 12:43:55 ", "Title": "Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training", "Authors": ["Aleksandra I. Nowak", "Bram Grooten", "Decebal Constantin Mocanu", "Jacek Tabor"], "Categories": "cs.LG cs.AI cs.CV stat.ML"}, "abstract": "Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their effect on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based pruning. The code is provided at https://github.com/alooow/fantastic_weights_paper", "url": "https://arxiv.org/abs/2306.12230"}, {"metadata": {"arXiv": "2306.12384", "Date": "Wed, 21 Jun 2023 17:06:54 ", "Title": "Probing the limit of hydrologic predictability with the Transformer network", "Authors": ["Jiangtao Liu", "Yuchen Bian and Chaopeng Shen"], "Categories": "cs.LG cs.AI physics.geo-ph"}, "abstract": "For a number of years since its introduction to hydrology, recurrent neural networks like long short-term memory (LSTM) have proven remarkably difficult to surpass in terms of daily hydrograph metrics on known, comparable benchmarks. Outside of hydrology, Transformers have now become the model of choice for sequential prediction tasks, making it a curious architecture to investigate. Here, we first show that a vanilla Transformer architecture is not competitive against LSTM on the widely benchmarked CAMELS dataset, and lagged especially for the high-flow metrics due to short-term processes. However, a recurrence-free variant of Transformer can obtain mixed comparisons with LSTM, producing the same Kling-Gupta efficiency coefficient (KGE), along with other metrics. The lack of advantages for the Transformer is linked to the Markovian nature of the hydrologic prediction problem. Similar to LSTM, the Transformer can also merge multiple forcing dataset to improve model performance. While the Transformer results are not higher than current state-of-the-art, we still learned some valuable lessons: (1) the vanilla Transformer architecture is not suitable for hydrologic modeling; (2) the proposed recurrence-free modification can improve Transformer performance so future work can continue to test more of such modifications; and (3) the prediction limits on the dataset should be close to the current state-of-the-art model. As a non-recurrent model, the Transformer may bear scale advantages for learning from bigger datasets and storing knowledge. This work serves as a reference point for future modifications of the model.", "url": "https://arxiv.org/abs/2306.12384"}, {"metadata": {"arXiv": "2306.11886", "Date": "Tue, 20 Jun 2023 20:59:10 ", "Title": "SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling", "Authors": ["Jesse Zhang and Karl Pertsch and Jiahui Zhang and Joseph J. Lim"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["29 pages", "18 figures"]}, "abstract": "Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/sprint.", "url": "https://arxiv.org/abs/2306.11886"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
