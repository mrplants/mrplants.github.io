<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <script>
        var papers = [{"id": "2306.04032", "date": "Tue, 6 Jun 2023 21:49:56 GMT", "title": "BokehOrNot: Transforming Bokeh Effect with Image Transformer and Lens\n\u00a0Metadata Embedding\n", "authors": ["Zhihao Yang", "Wenyi Lian", "Siyuan Lai\n"], "categories": ["cs.CV", "cs.LG", "eess.IV\n"], "abstract": "Bokeh effect is an optical phenomenon that offers a pleasant visual experience, typically generated by high-end cameras with wide aperture lenses. The task of bokeh effect transformation aims to produce a desired effect in one set of lenses and apertures based on another combination. Current models are limited in their ability to render a specific set of bokeh effects, primarily transformations from sharp to blur. In this paper, we propose a novel universal method for embedding lens metadata into the model and introducing a loss calculation method using alpha masks from the newly released Bokeh Effect Transformation Dataset(BETD) [3]. Based on the above techniques, we propose the BokehOrNot model, which is capable of producing both blur-to-sharp and sharp-to-blur bokeh effect with various combinations of lenses and aperture sizes. Our proposed model outperforms current leading bokeh rendering and image restoration models and renders visually natural bokeh effects. Our code is available at: https://github.com/indicator0/bokehornot.", "link": "https://arxiv.org/abs/2306.04032"}, {"id": "2306.03928", "date": "Tue, 6 Jun 2023 18:00:09 GMT", "title": "Designing Decision Support Systems Using Counterfactual Prediction Sets\n", "authors": ["Eleni Straitouri and Manuel Gomez Rodriguez\n"], "categories": ["cs.LG", "cs.HC\n"], "abstract": "Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not require, nor assumes, an expert model. Our methodology leverages the nested structure of the prediction sets provided by any conformal predictor and a natural counterfactual monotonicity assumption on the experts' predictions over the prediction sets to achieve an exponential improvement in regret in comparison with vanilla bandit algorithms. We conduct a large-scale human subject study ($n = 2{,}751$) to verify our counterfactual monotonicity assumption and compare our methodology to several competitive baselines. The results suggest that decision support systems that limit experts' level of agency may be practical and may offer greater performance than those allowing experts to always exercise their own agency.", "link": "https://arxiv.org/abs/2306.03928"}, {"id": "2306.03938", "date": "Mon, 5 Jun 2023 13:11:33 GMT", "title": "Learning Causal Mechanisms through Orthogonal Neural Networks\n", "authors": ["Peyman Sheikholharam Mashhadi", "Slawomir Nowaczyk\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "A fundamental feature of human intelligence is the ability to infer high-level abstractions from low-level sensory data. An essential component of such inference is the ability to discover modularized generative mechanisms. Despite many efforts to use statistical learning and pattern recognition for finding disentangled factors, arguably human intelligence remains unmatched in this area. In this paper, we investigate a problem of learning, in a fully unsupervised manner, the inverse of a set of independent mechanisms from distorted data points. We postulate, and justify this claim with experimental results, that an important weakness of existing machine learning solutions lies in the insufficiency of cross-module diversification. Addressing this crucial discrepancy between human and machine intelligence is an important challenge for pattern recognition systems. To this end, our work proposes an unsupervised method that discovers and disentangles a set of independent mechanisms from unlabeled data, and learns how to invert them. A number of experts compete against each other for individual data points in an adversarial setting: one that best inverses the (unknown) generative mechanism is the winner. We demonstrate that introducing an orthogonalization layer into the expert architectures enforces additional diversity in the outputs, leading to significantly better separability. Moreover, we propose a procedure for relocating data points between experts to further prevent any one from claiming multiple mechanisms. We experimentally illustrate that these techniques allow discovery and modularization of much less pronounced transformations, in addition to considerably faster convergence.", "link": "https://arxiv.org/abs/2306.03938"}, {"id": "2306.03949", "date": "Tue, 6 Jun 2023 18:27:20 GMT", "title": "Partial Inference in Structured Prediction\n", "authors": ["Chuyang Ke", "Jean Honorio\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "In this paper, we examine the problem of partial inference in the context of structured prediction. Using a generative model approach, we consider the task of maximizing a score function with unary and pairwise potentials in the space of labels on graphs. Employing a two-stage convex optimization algorithm for label recovery, we analyze the conditions under which a majority of the labels can be recovered. We introduce a novel perspective on the Karush-Kuhn-Tucker (KKT) conditions and primal and dual construction, and provide statistical and topological requirements for partial recovery with provable guarantees.", "link": "https://arxiv.org/abs/2306.03949"}, {"id": "2306.03982", "date": "Tue, 6 Jun 2023 19:35:09 GMT", "title": "Globally injective and bijective neural operators\n", "authors": ["Takashi Furuya", "Michael Puthawala", "Matti Lassas", "Maarten V. de Hoop\n"], "categories": ["cs.LG", "stat.ML\nComments:", "39", "pages\n"], "abstract": "Recently there has been great interest in operator learning, where networks learn operators between function spaces from an essentially infinite-dimensional perspective. In this work we present results for when the operators learned by these networks are injective and surjective. As a warmup, we combine prior work in both the finite-dimensional ReLU and operator learning setting by giving sharp conditions under which ReLU layers with linear neural operators are injective. We then consider the case the case when the activation function is pointwise bijective and obtain sufficient conditions for the layer to be injective. We remark that this question, while trivial in the finite-rank case, is subtler in the infinite-rank case and is proved using tools from Fredholm theory. Next, we prove that our supplied injective neural operators are universal approximators and that their implementation, with finite-rank neural networks, are still injective. This ensures that injectivity is not `lost' in the transcription from analytical operators to their finite-rank implementation with networks. Finally, we conclude with an increase in abstraction and consider general conditions when subnetworks, which may be many layers deep, are injective and surjective and provide an exact inversion from a `linearization.' This section uses general arguments from Fredholm theory and Leray-Schauder degree theory for non-linear integral equations to analyze the mapping properties of neural operators in function spaces. These results apply to subnetworks formed from the layers considered in this work, under natural conditions. We believe that our work has applications in Bayesian UQ where injectivity enables likelihood estimation and in inverse problems where surjectivity and injectivity corresponds to existence and uniqueness, respectively.", "link": "https://arxiv.org/abs/2306.03982"}, {"id": "2306.04001", "date": "Tue, 6 Jun 2023 20:28:37 GMT", "title": "One-Dimensional Deep Image Prior for Curve Fitting of S-Parameters from\n\u00a0Electromagnetic Solvers\n", "authors": ["Sriram Ravula", "Varun Gorti", "Bo Deng", "Swagato Chakraborty", "James\n\u00a0Pingenot", "Bhyrav Mutnury", "Doug Wallace", "Doug Winterberg", "Adam Klivans,\n\u00a0Alexandros G. Dimakis\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "A key problem when modeling signal integrity for passive filters and interconnects in IC packages is the need for multiple S-parameter measurements within a desired frequency band to obtain adequate resolution. These samples are often computationally expensive to obtain using electromagnetic (EM) field solvers. Therefore, a common approach is to select a small subset of the necessary samples and use an appropriate fitting mechanism to recreate a densely-sampled broadband representation. We present the first deep generative model-based approach to fit S-parameters from EM solvers using one-dimensional Deep Image Prior (DIP). DIP is a technique that optimizes the weights of a randomly-initialized convolutional neural network to fit a signal from noisy or under-determined measurements. We design a custom architecture and propose a novel regularization inspired by smoothing splines that penalizes discontinuous jumps. We experimentally compare DIP to publicly available and proprietary industrial implementations of Vector Fitting (VF), the industry-standard tool for fitting S-parameters. Relative to publicly available implementations of VF, our method shows superior performance on nearly all test examples using only 5-15% of the frequency samples. Our method is also competitive to proprietary VF tools and often outperforms them for challenging input instances.", "link": "https://arxiv.org/abs/2306.04001"}, {"id": "2306.04039", "date": "Tue, 6 Jun 2023 22:08:42 GMT", "title": "Revisiting Neural Retrieval on Accelerators\n", "authors": ["Jiaqi Zhai", "Zhaojie Gong", "Yueming Wang", "Xiao Sun", "Zheng Yan", "Fu Li,\n\u00a0Xing Liu\n"], "categories": ["cs.LG", "cs.IR\nComments:", "To", "appear", "in", "the", "29th", "ACM", "SIGKDD", "Conference", "on", "Knowledge", "Discovery\n\u00a0and", "Data", "Mining", "(KDD", "2023)\nDOI:", "10.1145/3580305.3599897\n"], "abstract": "Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose \\textit{mixture of logits} (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, \\textit{h-indexer}, we are able to scale up MoL to 100M corpus on a single GPU with latency comparable to MIPS baselines. On public datasets, our approach leads to uplifts of up to 77.3\\% in hit rate (HR). Experiments on a large recommendation surface at Meta showed strong metric gains and reduced popularity bias, validating the proposed approach's performance and improved generalization.", "link": "https://arxiv.org/abs/2306.04039"}, {"id": "2306.04049", "date": "Tue, 6 Jun 2023 22:35:16 GMT", "title": "One-sided Matrix Completion from Two Observations Per Row\n", "authors": ["Steven Cao", "Percy Liang", "Gregory Valiant\n"], "categories": ["cs.LG", "cs.DS", "stat.ML\nComments:", "ICML", "2023\n"], "abstract": "Given only a few observed entries from a low-rank matrix $X$, matrix completion is the problem of imputing the missing entries, and it formalizes a wide range of real-world settings that involve estimating missing data. However, when there are too few observed entries to complete the matrix, what other aspects of the underlying matrix can be reliably recovered? We study one such problem setting, that of \"one-sided\" matrix completion, where our goal is to recover the right singular vectors of $X$, even in the regime where recovering the left singular vectors is impossible, which arises when there are more rows than columns and very few observations. We propose a natural algorithm that involves imputing the missing values of the matrix $X^TX$ and show that even with only two observations per row in $X$, we can provably recover $X^TX$ as long as we have at least $\\Omega(r^2 d \\log d)$ rows, where $r$ is the rank and $d$ is the number of columns. We evaluate our algorithm on one-sided recovery of synthetic data and low-coverage genome sequencing. In these settings, our algorithm substantially outperforms standard matrix completion and a variety of direct factorization methods.", "link": "https://arxiv.org/abs/2306.04049"}, {"id": "2306.04066", "date": "Tue, 6 Jun 2023 23:54:01 GMT", "title": "Intelligent sampling for surrogate modeling, hyperparameter\n\u00a0optimization, and data analysis\n", "authors": ["Chandrika Kamath\n"], "categories": ["cs.LG", "stat.CO\nComments:", "4", "Tables,", "18", "Figures\nReport-no:", "LLNL-TR-829837\nJournal-ref:", "Machine", "Learning", "with", "Applications,", "Volume", "9,", "September", "2022\nDOI:", "10.1016/j.mlwa.2022.100373\n"], "abstract": "Sampling techniques are used in many fields, including design of experiments, image processing, and graphics. The techniques in each field are designed to meet the constraints specific to that field such as uniform coverage of the range of each dimension or random samples that are at least a certain distance apart from each other. When an application imposes new constraints, for example, by requiring samples in a non-rectangular domain or the addition of new samples to an existing set, a common solution is to modify the algorithm currently in use, often with less than satisfactory results. As an alternative, we propose the concept of intelligent sampling, where we devise algorithms specifically tailored to meet our sampling needs, either by creating new algorithms or by modifying suitable algorithms from other fields. Surprisingly, both qualitative and quantitative comparisons indicate that some relatively simple algorithms can be easily modified to meet the many sampling requirements of surrogate modeling, hyperparameter optimization, and data analysis; these algorithms outperform their more sophisticated counterparts currently in use, resulting in better use of time and computer resources.", "link": "https://arxiv.org/abs/2306.04066"}, {"id": "2306.04096", "date": "Wed, 7 Jun 2023 01:36:37 GMT", "title": "A novel deeponet model for learning moving-solution operators with\n\u00a0applications to earthquake hypocenter localization\n", "authors": ["Ehsan Haghighat", "Umair bin Waheed", "George Karniadakis\n"], "categories": ["cs.LG", "cs.CE", "physics.comp-ph\n"], "abstract": "Seismicity induced by human activities poses a significant threat to public safety, emphasizing the need for accurate and timely earthquake hypocenter localization. In this study, we introduce X-DeepONet, a novel variant of deep operator networks (DeepONets), for learning moving-solution operators of parametric partial differential equations (PDEs), with application to real-time earthquake localization. Leveraging the power of neural operators, X-DeepONet learns to estimate traveltime fields associated with earthquake sources by incorporating information from seismic arrival times and velocity models. Similar to the DeepONet, X-DeepONet includes a trunk net and a branch net. Additionally, we introduce a root network that not only takes the standard DeepONet's multiplication operator as input, it also takes addition and subtraction operators. We show that for problems with moving fields, the standard multiplication operation of DeepONet is insufficient to capture field relocation, while addition and subtraction operators along with the eXtended root significantly improve its accuracy both under data-driven (supervised) and physics-informed (unsupervised) training. We demonstrate the effectiveness of X-DeepONet through various experiments, including scenarios with variable velocity models and arrival times. The results show remarkable accuracy in earthquake localization, even for heterogeneous and complex velocity models. The proposed framework also exhibits excellent generalization capabilities and robustness against noisy arrival times. The method provides a computationally efficient approach for quantifying uncertainty in hypocenter locations resulting from traveltime pick errors and velocity model variations. Our results underscore X-DeepONet's potential to improve seismic monitoring systems, aiding the development of early warning systems for seismic hazard mitigation.", "link": "https://arxiv.org/abs/2306.04096"}, {"id": "2306.04098", "date": "Wed, 7 Jun 2023 01:43:09 GMT", "title": "Phoenix: A Federated Generative Diffusion Model\n", "authors": ["Fiona Victoria Stanley Jothiraj and Afra Mashhadi\n"], "categories": ["cs.LG", "cs.CV\n"], "abstract": "Generative AI has made impressive strides in enabling users to create diverse and realistic visual content such as images, videos, and audio. However, training generative models on large centralized datasets can pose challenges in terms of data privacy, security, and accessibility. Federated learning (FL) is an approach that uses decentralized techniques to collaboratively train a shared deep learning model while retaining the training data on individual edge devices to preserve data privacy. This paper proposes a novel method for training a Denoising Diffusion Probabilistic Model (DDPM) across multiple data sources using FL techniques. Diffusion models, a newly emerging generative model, show promising results in achieving superior quality images than Generative Adversarial Networks (GANs). Our proposed method Phoenix is an unconditional diffusion model that leverages strategies to improve the data diversity of generated samples even when trained on data with statistical heterogeneity or Non-IID (Non-Independent and Identically Distributed) data. We demonstrate how our approach outperforms the default diffusion model in an FL setting. These results indicate that high-quality samples can be generated by maintaining data diversity, preserving privacy, and reducing communication between data sources, offering exciting new possibilities in the field of generative AI.", "link": "https://arxiv.org/abs/2306.04098"}, {"id": "2306.04111", "date": "Wed, 7 Jun 2023 02:33:20 GMT", "title": "Quasi-Newton Updating for Large-Scale Distributed Learning\n", "authors": ["Wu Shuyuan", "Huang Danyang", "Wang Hansheng\n"], "categories": ["cs.LG", "cs.DC", "stat.ME\nComments:", "56", "pages,", "3", "figures\n"], "abstract": "Distributed computing is critically important for modern statistical analysis. Herein, we develop a distributed quasi-Newton (DQN) framework with excellent statistical, computation, and communication efficiency. In the DQN method, no Hessian matrix inversion or communication is needed. This considerably reduces the computation and communication complexity of the proposed method. Notably, related existing methods only analyze numerical convergence and require a diverging number of iterations to converge. However, we investigate the statistical properties of the DQN method and theoretically demonstrate that the resulting estimator is statistically efficient over a small number of iterations under mild conditions. Extensive numerical analyses demonstrate the finite sample performance.", "link": "https://arxiv.org/abs/2306.04111"}, {"id": "2306.04118", "date": "Wed, 7 Jun 2023 03:20:44 GMT", "title": "M$^3$Fair: Mitigating Bias in Healthcare Data through Multi-Level and\n\u00a0Multi-Sensitive-Attribute Reweighting Method\n", "authors": ["Yinghao Zhu", "Jingkun An", "Enshen Zhou", "Lu An", "Junyi Gao", "Hao Li", "Haoran\n\u00a0Feng", "Bo Hou", "Wen Tang", "Chengwei Pan", "Liantao Ma\n"], "categories": ["cs.LG", "cs.AI\nComments:", "4", "pages,", "1", "table,", "Beijing", "Health", "Data", "Science", "Summit", "2023\n"], "abstract": "In the data-driven artificial intelligence paradigm, models heavily rely on large amounts of training data. However, factors like sampling distribution imbalance can lead to issues of bias and unfairness in healthcare data. Sensitive attributes, such as race, gender, age, and medical condition, are characteristics of individuals that are commonly associated with discrimination or bias. In healthcare AI, these attributes can play a significant role in determining the quality of care that individuals receive. For example, minority groups often receive fewer procedures and poorer-quality medical care than white individuals in US. Therefore, detecting and mitigating bias in data is crucial to enhancing health equity. Bias mitigation methods include pre-processing, in-processing, and post-processing. Among them, Reweighting (RW) is a widely used pre-processing method that performs well in balancing machine learning performance and fairness performance. RW adjusts the weights for samples within each (group, label) combination, where these weights are utilized in loss functions. However, RW is limited to considering only a single sensitive attribute when mitigating bias and assumes that each sensitive attribute is equally important. This may result in potential inaccuracies when addressing intersectional bias. To address these limitations, we propose M3Fair, a multi-level and multi-sensitive-attribute reweighting method by extending the RW method to multiple sensitive attributes at multiple levels. Our experiments on real-world datasets show that the approach is effective, straightforward, and generalizable in addressing the healthcare fairness issues.", "link": "https://arxiv.org/abs/2306.04118"}, {"id": "2306.04125", "date": "Wed, 7 Jun 2023 03:44:50 GMT", "title": "Multimodal Fusion Interactions: A Study of Human and Automatic\n\u00a0Quantification\n", "authors": ["Paul Pu Liang", "Yun Cheng", "Ruslan Salakhutdinov", "Louis-Philippe Morency\n"], "categories": ["cs.LG", "cs.CL", "cs.HC\n"], "abstract": "Multimodal fusion of multiple heterogeneous and interconnected signals is a fundamental challenge in almost all multimodal problems and applications. In order to perform multimodal fusion, we need to understand the types of interactions that modalities can exhibit: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how human annotators can be leveraged to annotate two categorizations of multimodal interactions: (1) partial labels, where different randomly assigned annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator is tasked to annotate the label given the first modality before giving them the second modality and asking them to explicitly reason about how their answer changes, before proposing an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions on the task, uniqueness: the extent to which one modality enables a task prediction that the other does not, and synergy: the extent to which only both modalities enable one to make a prediction about the task that one would not otherwise make using either modality individually. Through extensive experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying interactions in multimodal datasets.", "link": "https://arxiv.org/abs/2306.04125"}, {"id": "2306.04139", "date": "Wed, 7 Jun 2023 04:26:41 GMT", "title": "A Survey on Generative Diffusion Models for Structured Data\n", "authors": ["Heejoon Koo\n"], "categories": ["cs.LG", "cs.AI\nComments:", "Work", "in", "progress\n"], "abstract": "In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its review on structured data modelling via diffusion models, compared to other data modalities such as computer vision and natural language processing. Hence, in this paper, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works using structured data in both data-driven general tasks and domain-specific applications. Thereafter, we analyse and discuss the limitations and challenges shown in existing works and suggest potential research directions. We hope this review serves as a catalyst for the research community, promoting the developments in generative diffusion models for structured data.", "link": "https://arxiv.org/abs/2306.04139"}, {"id": "2306.04178", "date": "Wed, 7 Jun 2023 06:15:12 GMT", "title": "Optimal Transport Model Distributional Robustness\n", "authors": ["Van-Anh Nguyen", "Trung Le", "Anh Tuan Bui", "Thanh-Toan Do", "and Dinh Phung\n"], "categories": ["cs.LG", "cs.CG\n"], "abstract": "Distributional robustness is a promising framework for training deep learning models that are less vulnerable to adversarial examples and data distribution shifts. Previous works have mainly focused on exploiting distributional robustness in data space. In this work, we explore an optimal transport-based distributional robustness framework on model spaces. Specifically, we examine a model distribution in a Wasserstein ball of a given center model distribution that maximizes the loss. We have developed theories that allow us to learn the optimal robust center model distribution. Interestingly, through our developed theories, we can flexibly incorporate the concept of sharpness awareness into training a single model, ensemble models, and Bayesian Neural Networks by considering specific forms of the center model distribution, such as a Dirac delta distribution over a single model, a uniform distribution over several models, and a general Bayesian Neural Network. Furthermore, we demonstrate that sharpness-aware minimization (SAM) is a specific case of our framework when using a Dirac delta distribution over a single model, while our framework can be viewed as a probabilistic extension of SAM. We conduct extensive experiments to demonstrate the usefulness of our framework in the aforementioned settings, and the results show remarkable improvements in our approaches to the baselines.", "link": "https://arxiv.org/abs/2306.04178"}, {"id": "2306.04201", "date": "Wed, 7 Jun 2023 07:15:08 GMT", "title": "Improving Hyperparameter Learning under Approximate Inference in\n\u00a0Gaussian Process Models\n", "authors": ["Rui Li", "ST John", "Arno Solin\n"], "categories": ["cs.LG", "stat.ML\nComments:", "International", "Conference", "on", "Machine", "Learning", "(ICML)", "2023\n"], "abstract": "Approximate inference in Gaussian process (GP) models with non-conjugate likelihoods gets entangled with the learning of the model hyperparameters. We improve hyperparameter learning in GP models and focus on the interplay between variational inference (VI) and the learning target. While VI's lower bound to the marginal likelihood is a suitable objective for inferring the approximate posterior, we show that a direct approximation of the marginal likelihood as in Expectation Propagation (EP) is a better learning objective for hyperparameter optimization. We design a hybrid training procedure to bring the best of both worlds: it leverages conjugate-computation VI for inference and uses an EP-like marginal likelihood approximation for hyperparameter learning. We compare VI, EP, Laplace approximation, and our proposed training procedure and empirically demonstrate the effectiveness of our proposal across a wide range of data sets.", "link": "https://arxiv.org/abs/2306.04201"}, {"id": "2306.04212", "date": "Wed, 7 Jun 2023 07:37:01 GMT", "title": "Migrate Demographic Group For Fair GNNs\n", "authors": ["YanMing Hu", "TianChi Liao", "JiaLong Chen", "Chuan Chen", "Jing Bian", "and\n\u00a0ZiBin Zheng\n"], "categories": ["cs.LG", "cs.CY\n"], "abstract": "Graph Neural networks (GNNs) have been applied in many scenarios due to the superior performance of graph learning. However, fairness is always ignored when designing GNNs. As a consequence, biased information in training data can easily affect vanilla GNNs, causing biased results toward particular demographic groups (divided by sensitive attributes, such as race and age). There have been efforts to address the fairness issue. However, existing fair techniques generally divide the demographic groups by raw sensitive attributes and assume that are fixed. The biased information correlated with raw sensitive attributes will run through the training process regardless of the implemented fair techniques. It is urgent to resolve this problem for training fair GNNs. To tackle this problem, we propose a brand new framework, FairMigration, which can dynamically migrate the demographic groups instead of keeping that fixed with raw sensitive attributes. FairMigration is composed of two training stages. In the first stage, the GNNs are initially optimized by personalized self-supervised learning, and the demographic groups are adjusted dynamically. In the second stage, the new demographic groups are frozen and supervised learning is carried out under the constraints of new demographic groups and adversarial training. Extensive experiments reveal that FairMigration balances model performance and fairness well.", "link": "https://arxiv.org/abs/2306.04212"}, {"id": "2306.04214", "date": "Wed, 7 Jun 2023 07:40:04 GMT", "title": "DualHGNN: A Dual Hypergraph Neural Network for Semi-Supervised Node\n\u00a0Classification based on Multi-View Learning and Density Awareness\n", "authors": ["Jianpeng Liao", "Jun Yan and Qian Tao\n"], "categories": ["cs.LG", "cs.CV\nComments:", "This", "work", "has", "been", "accepted", "by", "2023", "International", "Joint", "Conference", "on\n\u00a0Neural", "Networks", "(IJCNN", "2023).", "arXiv", "admin", "note:", "text", "overlap", "with\n\u00a0arXiv:2201.11511\n"], "abstract": "Graph-based semi-supervised node classification has been shown to become a state-of-the-art approach in many applications with high research value and significance. Most existing methods are only based on the original intrinsic or artificially established graph structure which may not accurately reflect the \"true\" correlation among data and are not optimal for semi-supervised node classification in the downstream graph neural networks. Besides, while existing graph-based methods mostly utilize the explicit graph structure, some implicit information, for example, the density information, can also provide latent information that can be further exploited. To address these limitations, this paper proposes the Dual Hypergraph Neural Network (DualHGNN), a new dual connection model integrating both hypergraph structure learning and hypergraph representation learning simultaneously in a unified architecture. The DualHGNN first leverages a multi-view hypergraph learning network to explore the optimal hypergraph structure from multiple views, constrained by a consistency loss proposed to improve its generalization. Then, DualHGNN employs a density-aware hypergraph attention network to explore the high-order semantic correlation among data points based on the density-aware attention mechanism. Extensive experiments are conducted in various benchmark datasets, and the results demonstrate the effectiveness of the proposed approach.", "link": "https://arxiv.org/abs/2306.04214"}, {"id": "2306.04220", "date": "Wed, 7 Jun 2023 07:51:05 GMT", "title": "Look Beneath the Surface: Exploiting Fundamental Symmetry for\n\u00a0Sample-Efficient Offline RL\n", "authors": ["Peng Cheng", "Xianyuan Zhan", "Zhihao Wu", "Wenjia Zhang", "Shoucheng Song,\n\u00a0Han Wang", "Youfang Lin", "Li Jiang\n"], "categories": ["cs.LG", "cs.AI\nComments:", "The", "first", "two", "authors", "contributed", "equally\n"], "abstract": "Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. These can be readily used to construct a new offline RL algorithm (TSRL) with less conservative policy constraints and a reliable latent space data augmentation procedure. Based on extensive experiments, we find TSRL achieves great performance on small benchmark datasets with as few as 1% of the original samples, which significantly outperforms the recent offline RL algorithms in terms of data efficiency and generalizability.", "link": "https://arxiv.org/abs/2306.04220"}, {"id": "2306.04226", "date": "Wed, 7 Jun 2023 08:05:46 GMT", "title": "Normalization Layers Are All That Sharpness-Aware Minimization Needs\n", "authors": ["Maximilian Mueller", "Tiffany Vlaar", "David Rolnick", "Matthias Hein\n"], "categories": ["cs.LG", "cs.CV\n"], "abstract": "Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima and has been shown to enhance generalization performance in various settings. In this work we show that perturbing only the affine normalization parameters (comprising less than 0.1% of the total parameters) in the adversarial step of SAM outperforms perturbing all of the parameters. This finding generalizes to different SAM variants and both ResNet (Batch Normalization) and Vision Transformer (Layer Normalization) architectures. We consider alternative sparse perturbation approaches and find that these do not achieve similar performance enhancement at such extreme sparsity levels, showing that this behaviour is unique to the normalization layers. Although our findings reaffirm the effectiveness of SAM in improving generalization performance, they cast doubt on whether this is solely caused by reduced sharpness. The code for our experiments is publicly available at https://github.com/mueller-mp/SAM-ON.", "link": "https://arxiv.org/abs/2306.04226"}, {"id": "2306.04228", "date": "Wed, 7 Jun 2023 08:06:50 GMT", "title": "Data Mining for Faster, Interpretable Solutions to Inverse Problems: A\n\u00a0Case Study Using Additive Manufacturing\n", "authors": ["Chandrika Kamath", "Juliette Franzman", "Ravi Ponmalai\n"], "categories": ["cs.LG", "cs.NA", "math.NA\nComments:", "16", "figures", "and", "4", "tables\nReport-no:", "LLNL-TR-818722\nJournal-ref:", "Machine", "Learning", "with", "Applications,", "Volume", "6,", "December", "2021\nDOI:", "10.1016/j.mlwa.2021.100122\n"], "abstract": "Solving inverse problems, where we find the input values that result in desired values of outputs, can be challenging. The solution process is often computationally expensive and it can be difficult to interpret the solution in high-dimensional input spaces. In this paper, we use a problem from additive manufacturing to address these two issues with the intent of making it easier to solve inverse problems and exploit their results. First, focusing on Gaussian process surrogates that are used to solve inverse problems, we describe how a simple modification to the idea of tapering can substantially speed up the surrogate without losing accuracy in prediction. Second, we demonstrate that Kohonen self-organizing maps can be used to visualize and interpret the solution to the inverse problem in the high-dimensional input space. For our data set, as not all input dimensions are equally important, we show that using weighted distances results in a better organized map that makes the relationships among the inputs obvious.", "link": "https://arxiv.org/abs/2306.04228"}, {"id": "2306.04288", "date": "Wed, 7 Jun 2023 09:40:18 GMT", "title": "Revising deep learning methods in parking lot occupancy detection\n", "authors": ["Anastasia Martynova", "Mikhail Kuznetsov", "Vadim Porvatov", "Vladislav\n\u00a0Tishin", "Andrey Kuznetsov", "Natalia Semenova", "Ksenia Kuznetsova\n"], "categories": ["cs.LG", "cs.CV\n"], "abstract": "Parking guidance systems have recently become a popular trend as a part of the smart cities' paradigm of development. The crucial part of such systems is the algorithm allowing drivers to search for available parking lots across regions of interest. The classic approach to this task is based on the application of neural network classifiers to camera records. However, existing systems demonstrate a lack of generalization ability and appropriate testing regarding specific visual conditions. In this study, we extensively evaluate state-of-the-art parking lot occupancy detection algorithms, compare their prediction quality with the recently emerged vision transformers, and propose a new pipeline based on EfficientNet architecture. Performed computational experiments have demonstrated the performance increase in the case of our model, which was evaluated on 5 different datasets.", "link": "https://arxiv.org/abs/2306.04288"}, {"id": "2306.04319", "date": "Wed, 7 Jun 2023 10:32:53 GMT", "title": "CaptAinGlove: Capacitive and Inertial Fusion-Based Glove for Real-Time\n\u00a0on Edge Hand Gesture Recognition for Drone Control\n", "authors": ["Hymalai Bello", "Sungho Suh", "Daniel Gei{\\ss}ler", "Lala Ray", "Bo Zhou and\n\u00a0Paul Lukowicz\n"], "categories": ["cs.LG", "cs.HC", "cs.RO\n"], "abstract": "We present CaptAinGlove, a textile-based, low-power (1.15Watts), privacy-conscious, real-time on-the-edge (RTE) glove-based solution with a tiny memory footprint (2MB), designed to recognize hand gestures used for drone control. We employ lightweight convolutional neural networks as the backbone models and a hierarchical multimodal fusion to reduce power consumption and improve accuracy. The system yields an F1-score of 80% for the offline evaluation of nine classes; eight hand gesture commands and null activity. For the RTE, we obtained an F1-score of 67% (one user).", "link": "https://arxiv.org/abs/2306.04319"}, {"id": "2306.04423", "date": "Wed, 7 Jun 2023 13:30:43 GMT", "title": "On Computing Optimal Tree Ensembles\n", "authors": ["Christian Komusiewicz", "Pascal Kunz", "Frank Sommer and Manuel Sorge\n"], "categories": ["cs.LG", "cs.DS\nComments:", "Accepted", "at", "ICML", "2023\n"], "abstract": "Random forests and, more generally, (decision\\nobreakdash-)tree ensembles are widely used methods for classification and regression. Recent algorithmic advances allow to compute decision trees that are optimal for various measures such as their size or depth. We are not aware of such research for tree ensembles and aim to contribute to this area. Mainly, we provide two novel algorithms and corresponding lower bounds. First, we are able to carry over and substantially improve on tractability results for decision trees, obtaining a $(6\\delta D S)^S \\cdot poly$-time algorithm, where $S$ is the number of cuts in the tree ensemble, $D$ the largest domain size, and $\\delta$ is the largest number of features in which two examples differ. To achieve this, we introduce the witness-tree technique which also seems promising for practice. Second, we show that dynamic programming, which has been successful for decision trees, may also be viable for tree ensembles, providing an $\\ell^n \\cdot poly$-time algorithm, where $\\ell$ is the number of trees and $n$ the number of examples. Finally, we compare the number of cuts necessary to classify training data sets for decision trees and tree ensembles, showing that ensembles may need exponentially fewer cuts for increasing number of trees.", "link": "https://arxiv.org/abs/2306.04423"}, {"id": "2306.04425", "date": "Wed, 7 Jun 2023 13:31:57 GMT", "title": "Towards High-Performance Exploratory Data Analysis (EDA) Via Stable\n\u00a0Equilibrium Point\n", "authors": ["Yuxuan Song", "Yongyu Wang\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Exploratory data analysis (EDA) is a vital procedure for data science projects. In this work, we introduce a stable equilibrium point (SEP) - based framework for improving the efficiency and solution quality of EDA. By exploiting the SEPs to be the representative points, our approach aims to generate high-quality clustering and data visualization for large-scale data sets. A very unique property of the proposed method is that the SEPs will directly encode the clustering properties of data sets. Compared with prior state-of-the-art clustering and data visualization methods, the proposed methods allow substantially improving computing efficiency and solution quality for large-scale data analysis tasks.", "link": "https://arxiv.org/abs/2306.04425"}, {"id": "2306.04444", "date": "Wed, 7 Jun 2023 14:07:35 GMT", "title": "Fast Optimal Locally Private Mean Estimation via Random Projections\n", "authors": ["Hilal Asi", "Vitaly Feldman", "Jelani Nelson", "Huy L. Nguyen", "Kunal Talwar\n"], "categories": ["cs.LG", "cs.CR", "stat.ML\n"], "abstract": "We study the problem of locally private mean estimation of high-dimensional vectors in the Euclidean ball. Existing algorithms for this problem either incur sub-optimal error or have high communication and/or run-time complexity. We propose a new algorithmic framework, ProjUnit, for private mean estimation that yields algorithms that are computationally efficient, have low communication complexity, and incur optimal error up to a $1+o(1)$-factor. Our framework is deceptively simple: each randomizer projects its input to a random low-dimensional subspace, normalizes the result, and then runs an optimal algorithm such as PrivUnitG in the lower-dimensional space. In addition, we show that, by appropriately correlating the random projection matrices across devices, we can achieve fast server run-time. We mathematically analyze the error of the algorithm in terms of properties of the random projections, and study two instantiations. Lastly, our experiments for private mean estimation and private federated learning demonstrate that our algorithms empirically obtain nearly the same utility as optimal ones while having significantly lower communication and computational cost.", "link": "https://arxiv.org/abs/2306.04444"}, {"id": "2306.04454", "date": "Wed, 7 Jun 2023 14:28:42 GMT", "title": "Training-Free Neural Active Learning with Initialization-Robustness\n\u00a0Guarantees\n", "authors": ["Apivich Hemachandra", "Zhongxiang Dai", "Jasraj Singh", "See-Kiong Ng and\n\u00a0Bryan Kian Hsiang Low\n"], "categories": ["cs.LG", "cs.AI\nComments:", "Accepted", "to", "40th", "International", "Conference", "on", "Machine", "Learning", "(ICML\n\u00a02023),", "41", "pages\n"], "abstract": "Existing neural active learning algorithms have aimed to optimize the predictive performance of neural networks (NNs) by selecting data for labelling. However, other than a good predictive performance, being robust against random parameter initializations is also a crucial requirement in safety-critical applications. To this end, we introduce our expected variance with Gaussian processes (EV-GP) criterion for neural active learning, which is theoretically guaranteed to select data points which lead to trained NNs with both (a) good predictive performances and (b) initialization robustness. Importantly, our EV-GP criterion is training-free, i.e., it does not require any training of the NN during data selection, which makes it computationally efficient. We empirically demonstrate that our EV-GP criterion is highly correlated with both initialization robustness and generalization performance, and show that it consistently outperforms baseline methods in terms of both desiderata, especially in situations with limited initial data or large batch sizes.", "link": "https://arxiv.org/abs/2306.04454"}, {"id": "2306.04498", "date": "Wed, 7 Jun 2023 15:05:53 GMT", "title": "Optimal Fair Multi-Agent Bandits\n", "authors": ["Amir Leshem\n"], "categories": ["cs.LG", "cs.DC\nComments:", "17", "pages,", "3", "figures\n"], "abstract": "In this paper, we study the problem of fair multi-agent multi-arm bandit learning when agents do not communicate with each other, except collision information, provided to agents accessing the same arm simultaneously. We provide an algorithm with regret $O\\left(N^3 \\log N \\log T \\right)$ (assuming bounded rewards, with unknown bound). This significantly improves previous results which had regret of order $O(\\log T \\log\\log T)$ and exponential dependence on the number of agents. The result is attained by using a distributed auction algorithm to learn the sample-optimal matching, a new type of exploitation phase whose length is derived from the observed samples, and a novel order-statistics-based regret analysis. Simulation results present the dependence of the regret on $\\log T$.", "link": "https://arxiv.org/abs/2306.04498"}, {"id": "2306.04502", "date": "Wed, 7 Jun 2023 15:10:01 GMT", "title": "Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal\n", "authors": ["Anastasiia Sedova", "Lena Zellinger", "Benjamin Roth\n"], "categories": ["cs.LG", "cs.AI\nComments:", "Accepted", "for", "ECML", "PKDD", "2023\n"], "abstract": "An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis supports our initial hypothesis: permanent hard outlier removal is not always what model benefits the most from.", "link": "https://arxiv.org/abs/2306.04502"}, {"id": "2306.04519", "date": "Wed, 7 Jun 2023 15:29:46 GMT", "title": "Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks\n", "authors": ["Emilie Gr\\'egoire", "Hafeez Chaudhary and Sam Verboven\n"], "categories": ["cs.LG", "cs.AI\nComments:", "16", "pages,", "7", "figures\nACM-class:", "I.2.6\n"], "abstract": "Multi-task learning (MTL) can improve the generalization performance of neural networks by sharing representations with related tasks. Nonetheless, MTL can also degrade performance through harmful interference between tasks. Recent work has pursued task-specific loss weighting as a solution for this interference. However, existing algorithms treat tasks as atomic, lacking the ability to explicitly separate harmful and helpful signals beyond the task level. To this end, we propose SLGrad, a sample-level weighting algorithm for multi-task learning with auxiliary tasks. Through sample-specific task weights, SLGrad reshapes the task distributions during training to eliminate harmful auxiliary signals and augment useful task signals. Substantial generalization performance gains are observed on (semi-) synthetic datasets and common supervised multi-task problems.", "link": "https://arxiv.org/abs/2306.04519"}, {"id": "2306.04539", "date": "Wed, 7 Jun 2023 15:44:53 GMT", "title": "Multimodal Learning Without Labeled Multimodal Data: Guarantees and\n\u00a0Applications\n", "authors": ["Paul Pu Liang", "Chun Kai Ling", "Yun Cheng", "Alex Obolenskiy", "Yudong Liu,\n\u00a0Rohan Pandey", "Alex Wilf", "Louis-Philippe Morency", "Ruslan Salakhutdinov\n"], "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.IT", "math.IT", "stat.ML\nComments:", "Code", "available", "at:", "https://github.com/pliang279/PID\n"], "abstract": "In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.", "link": "https://arxiv.org/abs/2306.04539"}, {"id": "2306.04556", "date": "Wed, 7 Jun 2023 16:03:55 GMT", "title": "StudentEval: A Benchmark of Student-Written Prompts for Large Language\n\u00a0Models of Code\n", "authors": ["Hannah McLean Babe", "Sydney Nguyen", "Yangtian Zi", "Arjun Guha", "Molly Q\n\u00a0Feldman", "Carolyn Jane Anderson\n"], "categories": ["cs.LG", "cs.HC\n"], "abstract": "Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code LLMs.", "link": "https://arxiv.org/abs/2306.04556"}, {"id": "2306.04581", "date": "Wed, 7 Jun 2023 16:33:52 GMT", "title": "Divide and Repair: Using Options to Improve Performance of Imitation\n\u00a0Learning Against Adversarial Demonstrations\n", "authors": ["Prithviraj Dasgupta\n"], "categories": ["cs.LG", "cs.AI\nComments:", "33", "pages,", "4", "figures,", "3", "tables\nACM-class:", "I.2.3\n"], "abstract": "We consider the problem of learning to perform a task from demonstrations given by teachers or experts, when some of the experts' demonstrations might be adversarial and demonstrate an incorrect way to perform the task. We propose a novel technique that can identify parts of demonstrated trajectories that have not been significantly modified by the adversary and utilize them for learning, using temporally extended policies or options. We first define a trajectory divergence measure based on the spatial and temporal features of demonstrated trajectories to detect and discard parts of the trajectories that have been significantly modified by an adversarial expert, and, could degrade the learner's performance, if used for learning, We then use an options-based algorithm that partitions trajectories and learns only from the parts of trajectories that have been determined as admissible. We provide theoretical results of our technique to show that repairing partial trajectories improves the sample efficiency of the demonstrations without degrading the learner's performance. We then evaluate the proposed algorithm for learning to play an Atari-like, computer-based game called LunarLander in the presence of different types and degrees of adversarial attacks of demonstrated trajectories. Our experimental results show that our technique can identify adversarially modified parts of the demonstrated trajectories and successfully prevent the learning performance from degrading due to adversarial demonstrations.", "link": "https://arxiv.org/abs/2306.04581"}, {"id": "2306.04590", "date": "Wed, 7 Jun 2023 16:40:51 GMT", "title": "Proximity-Informed Calibration for Deep Neural Networks\n", "authors": ["Miao Xiong", "Ailin Deng", "Pang Wei Koh", "Jiaying Wu", "Shen Li", "Jianqing\n\u00a0Xu", "Bryan Hooi\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, a plug-and-play algorithm with a theoretical guarantee to adjust sample confidence based on proximity. To further quantify the effectiveness of calibration algorithms in mitigating proximity bias, we introduce proximity-informed expected calibration error (PIECE) with theoretical analysis. We show that ProCal is effective in addressing proximity bias and improving calibration on balanced, long-tail, and distribution-shift settings under four metrics over various model architectures.", "link": "https://arxiv.org/abs/2306.04590"}, {"id": "2306.04620", "date": "Wed, 7 Jun 2023 17:48:29 GMT", "title": "Goal-conditioned GFlowNets for Controllable Multi-Objective Molecular\n\u00a0Design\n", "authors": ["Julien Roy", "Pierre-Luc Bacon", "Christopher Pal and Emmanuel Bengio\n"], "categories": ["cs.LG", "q-bio.BM\nComments:", "14", "pages\n"], "abstract": "In recent years, in-silico molecular design has received much attention from the machine learning community. When designing a new compound for pharmaceutical applications, there are usually multiple properties of such molecules that need to be optimised: binding energy to the target, synthesizability, toxicity, EC50, and so on. While previous approaches have employed a scalarization scheme to turn the multi-objective problem into a preference-conditioned single objective, it has been established that this kind of reduction may produce solutions that tend to slide towards the extreme points of the objective space when presented with a problem that exhibits a concave Pareto front. In this work we experiment with an alternative formulation of goal-conditioned molecular generation to obtain a more controllable conditional model that can uniformly explore solutions along the entire Pareto front.", "link": "https://arxiv.org/abs/2306.04620"}, {"id": "2306.04621", "date": "Wed, 7 Jun 2023 17:50:59 GMT", "title": "Align, Distill, and Augment Everything All at Once for Imbalanced\n\u00a0Semi-Supervised Learning\n", "authors": ["Emanuel Sanchez Aimar and Hannah Helgesen and Michael Felsberg and\n\u00a0Marco Kuhlmann\n"], "categories": ["cs.LG", "cs.CV\nComments:", "Under", "review,", "12", "pages\n"], "abstract": "Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partition. This last facet comes in as a response to the commonly-overlooked fact that disjoint partitions of labeled and unlabeled data prevent the benefits of strong data augmentation on the labeled set. Our overall framework requires no additional training cycles, so it will align, distill, and augment everything all at once (ADALLO). Our extensive evaluations of ADALLO on imbalanced SSL benchmark datasets, including CIFAR10-LT, CIFAR100-LT, and STL10-LT with varying degrees of class imbalance, amount of labeled data, and distribution mismatch, demonstrate significant improvements in the performance of imbalanced SSL under large distribution mismatch, as well as competitiveness with state-of-the-art methods when the labeled and unlabeled data follow the same marginal distribution. Our code will be released upon paper acceptance.", "link": "https://arxiv.org/abs/2306.04621"}, {"id": "2306.04622", "date": "Wed, 7 Jun 2023 17:52:29 GMT", "title": "Yet Another Algorithm for Supervised Principal Component Analysis:\n\u00a0Supervised Linear Centroid-Encoder\n", "authors": ["Tomojit Ghosh", "Michael Kirby\n"], "categories": ["cs.LG", "cs.CV\nComments:", "A", "novel", "algorithm", "for", "supervised", "PCA.", "22", "pages", "(including", "2", "reference\n\u00a0pages),", "8", "figures", "and", "mathematical", "analysis", "of", "the", "proposed", "algorithm.", "The\n\u00a0article", "is", "under", "review", "now\n"], "abstract": "We propose a new supervised dimensionality reduction technique called Supervised Linear Centroid-Encoder (SLCE), a linear counterpart of the nonlinear Centroid-Encoder (CE) \\citep{ghosh2022supervised}. SLCE works by mapping the samples of a class to its class centroid using a linear transformation. The transformation is a projection that reconstructs a point such that its distance from the corresponding class centroid, i.e., centroid-reconstruction loss, is minimized in the ambient space. We derive a closed-form solution using an eigendecomposition of a symmetric matrix. We did a detailed analysis and presented some crucial mathematical properties of the proposed approach. %We also provide an iterative solution approach based solving the optimization problem using a descent method. We establish a connection between the eigenvalues and the centroid-reconstruction loss. In contrast to Principal Component Analysis (PCA) which reconstructs a sample in the ambient space, the transformation of SLCE uses the instances of a class to rebuild the corresponding class centroid. Therefore the proposed method can be considered a form of supervised PCA. Experimental results show the performance advantage of SLCE over other supervised methods.", "link": "https://arxiv.org/abs/2306.04622"}, {"id": "2306.04634", "date": "Wed, 7 Jun 2023 17:58:48 GMT", "title": "On the Reliability of Watermarks for Large Language Models\n", "authors": ["John Kirchenbauer", "Jonas Geiping", "Yuxin Wen", "Manli Shu", "Khalid\n\u00a0Saifullah", "Kezhi Kong", "Kasun Fernando", "Aniruddha Saha", "Micah Goldblum and Tom\n\u00a0Goldstein\n"], "categories": ["cs.LG", "cs.CL", "cs.CR\nComments:", "14", "pages", "in", "the", "main", "body.", "Code", "is", "available", "at\n\u00a0https://github.com/jwkirchenbauer/lm-watermarking\n"], "abstract": "Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight our human study, where we investigate the reliability of watermarking when faced with human paraphrasing. We compare watermark-based detection to other detection strategies, finding overall that watermarking is a reliable solution, especially because of its sample complexity - for all attacks we consider, the watermark evidence compounds the more examples are given, and the watermark is eventually detected.", "link": "https://arxiv.org/abs/2306.04634"}, {"id": "2306.04050", "date": "Tue, 6 Jun 2023 22:42:00 GMT", "title": "LLMZip: Lossless Text Compression using Large Language Models\n", "authors": ["Chandra Shekhara Kaushik Valmeekam", "Krishna Narayanan", "Dileep\n\u00a0Kalathil", "Jean-Francois Chamberland", "Srinivas Shakkottai\n"], "categories": ["cs.IT", "cs.LG", "math.IT\nComments:", "7", "pages,", "4", "figures,", "4", "tables,", "preprint\n"], "abstract": "We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in \\cite{cover1978convergent}, \\cite{lutati2023focus}. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.", "link": "https://arxiv.org/abs/2306.04050"}, {"id": "2306.04054", "date": "Tue, 6 Jun 2023 23:04:22 GMT", "title": "RescueSpeech: A German Corpus for Speech Recognition in Search and\n\u00a0Rescue Domain\n", "authors": ["Sangeet Sagar", "Mirco Ravanelli", "Bernd Kiefer", "Ivana Kruijff Korbayova,\n\u00a0Josef van Genabith\n"], "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP\n"], "abstract": "Despite recent advancements in speech recognition, there are still difficulties in accurately transcribing conversational and emotional speech in noisy and reverberant acoustic environments. This poses a particular challenge in the search and rescue (SAR) domain, where transcribing conversations among rescue team members is crucial to support real-time decision-making. The scarcity of speech data and associated background noise in SAR scenarios make it difficult to deploy robust speech recognition systems. To address this issue, we have created and made publicly available a German speech dataset called RescueSpeech. This dataset includes real speech recordings from simulated rescue exercises. Additionally, we have released competitive training recipes and pre-trained models. Our study indicates that the current level of performance achieved by state-of-the-art methods is still far from being acceptable.", "link": "https://arxiv.org/abs/2306.04054"}, {"id": "2306.04174", "date": "Wed, 7 Jun 2023 05:55:45 GMT", "title": "End-to-End Learning for Stochastic Optimization: A Bayesian Perspective\n", "authors": ["Yves Rychener", "Daniel Kuhn Tobias Sutter\n"], "categories": ["math.OC", "cs.LG", "stat.ML\nComments:", "Accepted", "at", "ICML", "2023\n"], "abstract": "We develop a principled approach to end-to-end learning in stochastic optimization. First, we show that the standard end-to-end learning algorithm admits a Bayesian interpretation and trains a posterior Bayes action map. Building on the insights of this analysis, we then propose new end-to-end learning algorithms for training decision maps that output solutions of empirical risk minimization and distributionally robust optimization problems, two dominant modeling paradigms in optimization under uncertainty. Numerical results for a synthetic newsvendor problem illustrate the key differences between alternative training schemes. We also investigate an economic dispatch problem based on real data to showcase the impact of the neural network architecture of the decision maps on their test performance.", "link": "https://arxiv.org/abs/2306.04174"}, {"id": "2306.04190", "date": "Wed, 7 Jun 2023 06:58:38 GMT", "title": "An ASR-Based Tutor for Learning to Read: How to Optimize Feedback to\n\u00a0First Graders\n", "authors": ["Yu Bai", "Cristian Tejedor-Garcia", "Ferdy Hubers", "Catia Cucchiarini,\n\u00a0Helmer Strik\n"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS\nComments:", "Published", "(double-blind", "peer-reviewed)", "on", "SPECOM", "2021\nJournal-ref:", "In:", "Karpov", "A.,", "Potapova", "R.", "(eds)", "Speech", "and", "Computer.", "SPECOM", "2021.\n\u00a0Lecture", "Notes", "in", "Computer", "Science,", "vol", "12997.", "Springer,", "Cham\nDOI:", "10.1007/978-3-030-87802-3_6\n"], "abstract": "The interest in employing automatic speech recognition (ASR) in applications for reading practice has been growing in recent years. In a previous study, we presented an ASR-based Dutch reading tutor application that was developed to provide instantaneous feedback to first-graders learning to read. We saw that ASR has potential at this stage of the reading process, as the results suggested that pupils made progress in reading accuracy and fluency by using the software. In the current study, we used children's speech from an existing corpus (JASMIN) to develop two new ASR systems, and compared the results to those of the previous study. We analyze correct/incorrect classification of the ASR systems using human transcripts at word level, by means of evaluation measures such as Cohen's Kappa, Matthews Correlation Coefficient (MCC), precision, recall and F-measures. We observe improvements for the newly developed ASR systems regarding the agreement with human-based judgment and correct rejection (CR). The accuracy of the ASR systems varies for different reading tasks and word types. Our results suggest that, in the current configuration, it is difficult to classify isolated words. We discuss these results, possible ways to improve our systems and avenues for future research.", "link": "https://arxiv.org/abs/2306.04190"}, {"id": "2306.04374", "date": "Wed, 7 Jun 2023 12:14:16 GMT", "title": "Label Aware Speech Representation Learning For Language Identification\n", "authors": ["Shikhar Vashishth", "Shikhar Bharadwaj", "Sriram Ganapathy", "Ankur Bapna,\n\u00a0Min Ma", "Wei Han", "Vera Axelrod", "Partha Talukdar\n"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS\nComments:", "Accepted", "at", "Interspeech", "2023\n"], "abstract": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.", "link": "https://arxiv.org/abs/2306.04374"}, {"id": "2306.03937", "date": "Tue, 6 Jun 2023 18:02:02 GMT", "title": "Guiding The Last Layer in Federated Learning with Pre-Trained Models\n", "authors": ["Gwen Legate", "Nicolas Bernier", "Lucas Caccia", "Edouard Oyallon", "Eugene\n\u00a0Belilovsky\n"], "categories": ["cs.AI", "cs.LG\n"], "abstract": "Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yield rapid convergence and improved generalization in the federated setting. We demonstrate the potential our method has to reduce communication and compute costs while achieving better model performance.", "link": "https://arxiv.org/abs/2306.03937"}, {"id": "2306.04018", "date": "Tue, 6 Jun 2023 21:19:03 GMT", "title": "PyTrial: A Comprehensive Platform for Artificial Intelligence for Drug\n\u00a0Development\n", "authors": ["Zifeng Wang and Brandon Theodorou and Tianfan Fu and Cao Xiao and\n\u00a0Jimeng Sun\n"], "categories": ["cs.AI", "q-bio.QM\n"], "abstract": "Drug development is a complex process that aims to test the efficacy and safety of candidate drugs in the human body for regulatory approval via clinical trials. Recently, machine learning has emerged as a vital tool for drug development, offering new opportunities to improve the efficiency and success rates of the process. To facilitate the research and development of artificial intelligence (AI) for drug development, we developed a Python package, namely PyTrial, that implements various clinical trial tasks supported by AI algorithms. To be specific, PyTrial implements 6 essential drug development tasks, including patient outcome prediction, trial site selection, trial outcome prediction, patient-trial matching, trial similarity search, and synthetic data generation. In PyTrial, all tasks are defined by four steps: load data, model definition, model training, and model evaluation, which can be done with a couple of lines of code. In addition, the modular API design allows practitioners to extend the framework to new algorithms and tasks easily. PyTrial is featured for a unified API, detailed documentation, and interactive examples with preprocessed benchmark data for all implemented algorithms. This package can be installed through Python Package Index (PyPI) and is publicly available at https://github.com/RyanWangZf/PyTrial.", "link": "https://arxiv.org/abs/2306.04018"}, {"id": "2306.04090", "date": "Wed, 7 Jun 2023 01:23:38 GMT", "title": "Professional Basketball Player Behavior Synthesis via Planning with\n\u00a0Diffusion\n", "authors": ["Xiusi Chen", "Wei-Yao Wang", "Ziniu Hu", "Curtis Chou", "Lam Hoang", "Kun Jin,\n\u00a0Mingyan Liu", "P. Jeffrey Brantingham", "Wei Wang\n"], "categories": ["cs.AI", "cs.MA\n"], "abstract": "Dynamically planning in multi-agent systems has been explored to improve decision-making in various domains. Professional basketball serves as a compelling example of a dynamic spatio-temporal game, encompassing both concealed strategic policies and decision-making. However, processing the diverse on-court signals and navigating the vast space of potential actions and outcomes makes it difficult for existing approaches to swiftly identify optimal strategies in response to evolving circumstances. In this study, we first formulate the sequential decision-making process as a conditional trajectory generation process. We further introduce PLAYBEST (PLAYer BEhavior SynThesis), a method for enhancing player decision-making. We extend the state-of-the-art generative model, diffusion probabilistic model, to learn challenging multi-agent environmental dynamics from historical National Basketball Association (NBA) player motion tracking data. To incorporate data-driven strategies, an auxiliary value function is trained using the play-by-play data with corresponding rewards acting as the plan guidance. To accomplish reward-guided trajectory generation, conditional sampling is introduced to condition the diffusion model on the value function and conduct classifier-guided sampling. We validate the effectiveness of PLAYBEST via comprehensive simulation studies from real-world data, contrasting the generated trajectories and play strategies with those employed by professional basketball teams. Our results reveal that the model excels at generating high-quality basketball trajectories that yield efficient plays, surpassing conventional planning techniques in terms of adaptability, flexibility, and overall performance. Moreover, the synthesized play strategies exhibit a remarkable alignment with professional tactics, highlighting the model's capacity to capture the intricate dynamics of basketball games.", "link": "https://arxiv.org/abs/2306.04090"}, {"id": "2306.04123", "date": "Wed, 7 Jun 2023 03:38:03 GMT", "title": "Retrosynthesis Prediction with Local Template Retrieval\n", "authors": ["Shufang Xie", "Rui Yan", "Junliang Guo", "Yingce Xia", "Lijun Wu", "Tao Qin\n"], "categories": ["cs.AI", "cs.LG\nComments:", "AAAI-2023", "camera", "ready\n"], "abstract": "Retrosynthesis, which predicts the reactants of a given target molecule, is an essential task for drug discovery. In recent years, the machine learing based retrosynthesis methods have achieved promising results. In this work, we introduce RetroKNN, a local reaction template retrieval method to further boost the performance of template-based systems with non-parametric retrieval. We first build an atom-template store and a bond-template store that contain the local templates in the training data, then retrieve from these templates with a k-nearest-neighbor (KNN) search during inference. The retrieved templates are combined with neural network predictions as the final output. Furthermore, we propose a lightweight adapter to adjust the weights when combing neural network and KNN predictions conditioned on the hidden representation and the retrieved templates. We conduct comprehensive experiments on two widely used benchmarks, the USPTO-50K and USPTO-MIT. Especially for the top-1 accuracy, we improved 7.1% on the USPTO-50K dataset and 12.0% on the USPTO-MIT dataset. These results demonstrate the effectiveness of our method.", "link": "https://arxiv.org/abs/2306.04123"}, {"id": "2306.04292", "date": "Wed, 7 Jun 2023 09:46:38 GMT", "title": "Dear XAI Community, We Need to Talk! Fundamental Misconceptions in\n\u00a0Current XAI Research\n", "authors": ["Timo Freiesleben and Gunnar K\\\"onig\n"], "categories": ["cs.AI", "stat.ML\nComments:", "A", "revised", "version", "of", "this", "preprint", "has", "been", "accepted", "at", "the", "World", "XAI\n\u00a0Conference.", "It", "will", "be", "referenced", "as", "soon", "as", "it", "is", "published\n"], "abstract": "Despite progress in the field, significant parts of current XAI research are still not on solid conceptual, ethical, or methodological grounds. Unfortunately, these unfounded parts are not on the decline but continue to grow. Many explanation techniques are still proposed without clarifying their purpose. Instead, they are advertised with ever more fancy-looking heatmaps or only seemingly relevant benchmarks. Moreover, explanation techniques are motivated with questionable goals, such as building trust, or rely on strong assumptions about the 'concepts' that deep learning algorithms learn. In this paper, we highlight and discuss these and other misconceptions in current XAI research. We also suggest steps to make XAI a more substantive area of research.", "link": "https://arxiv.org/abs/2306.04292"}, {"id": "2306.04308", "date": "Wed, 7 Jun 2023 10:14:17 GMT", "title": "Personality testing of GPT-3: Limited temporal reliability, but\n\u00a0highlighted social desirability of GPT-3's personality instruments results\n", "authors": ["Bojana Bodroza (1)", "Bojana M. Dinic (1) and Ljubisa Bojic (2) ((1)\n\u00a0Department of Psychology", "Faculty of Philosophy", "University of Novi Sad,\n\u00a0Serbia", "(2) Digital Society Lab", "Institute for Philosophy and Social Theory,\n\u00a0University of Belgrade", "Serbia)\n"], "categories": ["cs.AI", "cs.CL", "cs.HC\nComments:", "18", "pages,", "1", "table\n"], "abstract": "To assess the potential applications and limitations of chatbot GPT-3 Davinci-003, this study explored the temporal reliability of personality questionnaires applied to the chatbot and its personality profile. Psychological questionnaires were administered to the chatbot on two separate occasions, followed by a comparison of the responses to human normative data. The findings revealed varying levels of agreement in the chatbot's responses over time, with some scales displaying excellent while others demonstrated poor agreement. Overall, Davinci-003 displayed a socially desirable and pro-social personality profile, particularly in the domain of communion. However, the underlying basis of the chatbot's responses, whether driven by conscious self-reflection or predetermined algorithms, remains uncertain.", "link": "https://arxiv.org/abs/2306.04308"}, {"id": "2306.04422", "date": "Wed, 7 Jun 2023 13:30:24 GMT", "title": "Social robots to improve therapeutic adherence in pediatric asthma\n", "authors": ["Laura Montalbano", "Agnese Augello", "Giovanni Pilato", "Stefania La Grutta\n"], "categories": ["cs.AI", "cs.HC", "cs.RO\n"], "abstract": "In chronic diseases, obtaining a correct diagnosis and providing the most appropriate treatments often is not enough to guarantee an improvement of the clinical condition of a patient. Poor adherence to medical prescriptions constitutes one of the main causes preventing achievement of therapeutic goals. This is generally true especially for certain diseases and specific target patients, such as children. An engaging and entertaining technology can be exploited in support of clinical practices to achieve better health outcomes. Our assumption is that a gamified session with a humanoid robot, compared to the usual methodologies for therapeutic education, can be more incisive in learning the correct inhalation procedure in children affected by asthma. In this perspective, we describe an interactive module implemented on the Pepper robotic platform and the setting of a study that was planned in 2020 to be held at the Pneumoallergology Pediatric clinic of CNR in Palermo. The study was canceled due to the pandemic and the subsequent and permanent closure of the clinic. Our long-term goal is to assess, by means of a qualitative-quantitative survey plan, the impact of such an educational action, evaluating possible improvement in the adherence to the treatment.", "link": "https://arxiv.org/abs/2306.04422"}, {"id": "2306.04440", "date": "Wed, 7 Jun 2023 13:58:45 GMT", "title": "Dual policy as self-model for planning\n", "authors": ["Jaesung Yoo", "Fernanda de la Torre", "Robert Guangyu Yang\n"], "categories": ["cs.AI", "cs.LG\n"], "abstract": "Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster inference than using model-free policy, promotes better exploration, and could learn a comprehensive understanding of its own behaviors, at the cost of distilling a new network apart from the model-free policy.", "link": "https://arxiv.org/abs/2306.04440"}, {"id": "2306.04449", "date": "Mon, 5 Jun 2023 17:30:07 GMT", "title": "Neural Networks from Biological to Artificial and Vice Versa\n", "authors": ["Abdullatif Baba\n"], "categories": ["cs.AI", "q-bio.QM\n"], "abstract": "In this paper, we examine how deep learning can be utilized to investigate neural health and the difficulties in interpreting neurological analyses within algorithmic models. The key contribution of this paper is the investigation of the impact of a dead neuron on the performance of artificial neural networks (ANNs). Therefore, we conduct several tests using different training algorithms and activation functions to identify the precise influence of the training process on neighboring neurons and the overall performance of the ANN in such cases. The aim is to assess the potential application of the findings in the biological domain, the expected results may have significant implications for the development of effective treatment strategies for neurological disorders. Successive training phases that incorporate visual and acoustic data derived from past social and familial experiences could be suggested to achieve this goal. Finally, we explore the conceptual analogy between the Adam optimizer and the learning process of the brain by delving into the specifics of both systems while acknowledging their fundamental differences.", "link": "https://arxiv.org/abs/2306.04449"}, {"id": "2306.04484", "date": "Wed, 7 Jun 2023 14:53:12 GMT", "title": "Artificial Intelligence can facilitate selfish decisions by altering the\n\u00a0appearance of interaction partners\n", "authors": ["Nils K\\\"obis", "Philipp Lorenz-Spreen", "Tamer Ajaj", "Jean-Francois\n\u00a0Bonnefon", "Ralph Hertwig", "Iyad Rahwan\n"], "categories": ["cs.AI", "cs.HC\n"], "abstract": "The increasing prevalence of image-altering filters on social media and video conferencing technologies has raised concerns about the ethical and psychological implications of using Artificial Intelligence (AI) to manipulate our perception of others. In this study, we specifically investigate the potential impact of blur filters, a type of appearance-altering technology, on individuals' behavior towards others. Our findings consistently demonstrate a significant increase in selfish behavior directed towards individuals whose appearance is blurred, suggesting that blur filters can facilitate moral disengagement through depersonalization. These results emphasize the need for broader ethical discussions surrounding AI technologies that modify our perception of others, including issues of transparency, consent, and the awareness of being subject to appearance manipulation by others. We also emphasize the importance of anticipatory experiments in informing the development of responsible guidelines and policies prior to the widespread adoption of such technologies.", "link": "https://arxiv.org/abs/2306.04484"}, {"id": "2306.04528", "date": "Wed, 7 Jun 2023 15:37:00 GMT", "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models\n\u00a0on Adversarial Prompts\n", "authors": ["Kaijie Zhu", "Jindong Wang", "Jiaheng Zhou", "Zichen Wang", "Hao Chen", "Yidong\n\u00a0Wang", "Linyi Yang", "Wei Ye", "Neil Zhenqiang Gong", "Yue Zhang", "Xing Xie\n"], "categories": ["cs.AI", "cs.LG\nComments:", "Technical", "report;", "23", "pages\n"], "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4,032 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our findings demonstrate that contemporary LLMs are vulnerable to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. We make our code, prompts, and methodologies to generate adversarial prompts publicly accessible, thereby enabling and encouraging collaborative exploration in this pivotal field: https://github.com/microsoft/promptbench.", "link": "https://arxiv.org/abs/2306.04528"}, {"id": "2306.04546", "date": "Wed, 7 Jun 2023 15:50:15 GMT", "title": "Querying Circumscribed Description Logic Knowledge Bases\n", "authors": ["Carsten Lutz", "Quentin Mani\\`ere", "Robin Nolte\n"], "categories": ["cs.AI", "cs.CC", "cs.LO\nComments:", "42", "pages", "-", "Extended", "version", "of", "a", "paper", "accepted", "at", "KR", "2023\n"], "abstract": "Circumscription is one of the main approaches for defining non-monotonic description logics (DLs). While the decidability and complexity of traditional reasoning tasks such as satisfiability of circumscribed DL knowledge bases (KBs) is well understood, for evaluating conjunctive queries (CQs) and unions thereof (UCQs), not even decidability had been established. In this paper, we prove decidability of (U)CQ evaluation on circumscribed DL KBs and obtain a rather complete picture of both the combined complexity and the data complexity, for DLs ranging from ALCHIO via EL to various versions of DL-Lite. We also study the much simpler atomic queries (AQs).", "link": "https://arxiv.org/abs/2306.04546"}, {"id": "2306.04563", "date": "Wed, 7 Jun 2023 16:10:21 GMT", "title": "ChatGPT is fun, but it is not funny! Humor is still challenging Large\n\u00a0Language Models\n", "authors": ["Sophie Jentzsch", "Kristian Kersting\n"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG\n"], "abstract": "Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI's ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny? We put ChatGPT's sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional explanations for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the classification of jokes. ChatGPT has not solved computational humor yet but it can be a big leap toward \"funny\" machines.", "link": "https://arxiv.org/abs/2306.04563"}, {"id": "2306.03954", "date": "Tue, 6 Jun 2023 18:30:51 GMT", "title": "Recognition of Handwritten Japanese Characters Using Ensemble of\n\u00a0Convolutional Neural Networks\n", "authors": ["Angel I. Solis", "Justin Zarkovacki", "John Ly and Adham Atyabi\n"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG\n"], "abstract": "The Japanese writing system is complex, with three character types of Hiragana, Katakana, and Kanji. Kanji consists of thousands of unique characters, further adding to the complexity of character identification and literature understanding. Being able to translate handwritten Japanese characters into digital text is useful for data analysis, translation, learning and cultural preservation. In this study, a machine learning approach to analyzing and recognizing handwritten Japanese characters (Kanji) is proposed. The study used an ensemble of three convolutional neural networks (CNNs) for recognizing handwritten Kanji characters and utilized four datasets of MNIST, K-MNIST, Kuzushiji-49 (K49) and the top 150 represented classes in the Kuzushiji-Kanji (K-Kanji) dataset for its performance evaluation. The results indicate feasibility of using proposed CNN-ensemble architecture for recognizing handwritten characters, achieving 99.4%, 96.4%, 95.0% and 96.4% classification accuracy on MNIST, K-MNIS, K49, and K-Kanji datasets respectively.", "link": "https://arxiv.org/abs/2306.03954"}, {"id": "2306.03993", "date": "Tue, 6 Jun 2023 20:08:54 GMT", "title": "Real-Time Online Unsupervised Domain Adaptation for Real-World Person\n\u00a0Re-identification\n", "authors": ["Christopher Neff", "Armin Danesh Pazho", "Hamed Tabkhi\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\n"], "abstract": "Following the popularity of Unsupervised Domain Adaptation (UDA) in person re-identification, the recently proposed setting of Online Unsupervised Domain Adaptation (OUDA) attempts to bridge the gap towards practical applications by introducing a consideration of streaming data. However, this still falls short of truly representing real-world applications. This paper defines the setting of Real-world Real-time Online Unsupervised Domain Adaptation (R$^2$OUDA) for Person Re-identification. The R$^2$OUDA setting sets the stage for true real-world real-time OUDA, bringing to light four major limitations found in real-world applications that are often neglected in current research: system generated person images, subset distribution selection, time-based data stream segmentation, and a segment-based time constraint. To address all aspects of this new R$^2$OUDA setting, this paper further proposes Real-World Real-Time Online Streaming Mutual Mean-Teaching (R$^2$MMT), a novel multi-camera system for real-world person re-identification. Taking a popular person re-identification dataset, R$^2$MMT was used to construct over 100 data subsets and train more than 3000 models, exploring the breadth of the R$^2$OUDA setting to understand the training time and accuracy trade-offs and limitations for real-world applications. R$^2$MMT, a real-world system able to respect the strict constraints of the proposed R$^2$OUDA setting, achieves accuracies within 0.1% of comparable OUDA methods that cannot be applied directly to real-world applications.", "link": "https://arxiv.org/abs/2306.03993"}, {"id": "2306.04047", "date": "Tue, 6 Jun 2023 22:32:49 GMT", "title": "Active Sparse Conversations for Improved Audio-Visual Embodied\n\u00a0Navigation\n", "authors": ["Xiulong Liu", "Sudipta Paul", "Moitreya Chatterjee", "Anoop Cherian\n"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG\n"], "abstract": "Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructions. Key to generating the agent's questions is our novel TrajectoryNet that forecasts the most likely next steps to the goal and a QuestionNet that uses these steps to produce a question. All the policies are learned end-to-end via the RL setup, with penalties to enforce sparsity in receiving navigation instructions from the oracle. To evaluate the performance of CAVEN, we present extensive experiments on the SoundSpaces framework for the task of semantic audio-visual navigation. Our results show that CAVEN achieves upto 12% gain in performance over competing methods, especially in localizing new sound sources, even in the presence of auditory distractions.", "link": "https://arxiv.org/abs/2306.04047"}, {"id": "2306.04633", "date": "Wed, 7 Jun 2023 17:57:45 GMT", "title": "Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast\n\u00a0Contrastive Fusion\n", "authors": ["Yash Bhalgat", "Iro Laina", "Jo\\~ao F. Henriques", "Andrew Zisserman", "Andrea\n\u00a0Vedaldi\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\n"], "abstract": "Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as on our newly created Messy Rooms dataset, demonstrating the effectiveness and scalability of our slow-fast clustering method.", "link": "https://arxiv.org/abs/2306.04633"}, {"id": "2306.04027", "date": "Tue, 6 Jun 2023 21:44:23 GMT", "title": "Intervention Generalization: A View from Factor Graph Models\n", "authors": ["Gecia Bravo-Hermsdorff", "David S. Watson", "Jialin Yu", "Jakob Zeitler", "and\n\u00a0Ricardo Silva\n"], "categories": ["stat.ML", "cs.AI", "cs.LG\n"], "abstract": "One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated $\\textit{interventional factor model}$ (IFM) may not always be informative, but it conveniently abstracts away a need for explicit unmeasured confounding and feedback mechanisms, leading to directly testable claims. We derive necessary and sufficient conditions for causal effect identifiability with IFMs using data from a collection of experimental settings, and implement practical algorithms for generalizing expected outcomes to novel conditions never observed in the data.", "link": "https://arxiv.org/abs/2306.04027"}, {"id": "2306.04163", "date": "Wed, 7 Jun 2023 05:26:38 GMT", "title": "Enhancing Virtual Assistant Intelligence: Precise Area Targeting for\n\u00a0Instance-level User Intents beyond Metadata\n", "authors": ["Mengyu Chen", "Zhenchang Xing", "Jieshan Chen", "Chunyang Chen and Qinghua\n\u00a0Lu\n"], "categories": ["cs.HC", "cs.AI", "cs.CV\n"], "abstract": "Virtual assistants have been widely used by mobile phone users in recent years. Although their capabilities of processing user intents have been developed rapidly, virtual assistants in most platforms are only capable of handling pre-defined high-level tasks supported by extra manual efforts of developers. However, instance-level user intents containing more detailed objectives with complex practical situations, are yet rarely studied so far. In this paper, we explore virtual assistants capable of processing instance-level user intents based on pixels of application screens, without the requirements of extra extensions on the application side. We propose a novel cross-modal deep learning pipeline, which understands the input vocal or textual instance-level user intents, predicts the targeting operational area, and detects the absolute button area on screens without any metadata of applications. We conducted a user study with 10 participants to collect a testing dataset with instance-level user intents. The testing dataset is then utilized to evaluate the performance of our model, which demonstrates that our model is promising with the achievement of 64.43% accuracy on our testing dataset.", "link": "https://arxiv.org/abs/2306.04163"}, {"id": "2306.04366", "date": "Wed, 7 Jun 2023 11:59:45 GMT", "title": "Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing\n\u00a0Based on GCN Trustworthiness Prediction\n", "authors": ["Zhongwei Zhan", "Yingjie Wang", "Peiyong Duan", "Akshita Maradapu Vera\n\u00a0Venkata Sai", "Zhaowei Liu", "Chaocan Xiang", "Xiangrong Tong", "Weilong Wang,\n\u00a0Zhipeng Cai\n"], "categories": ["cs.SI", "cs.AI", "cs.LG\n"], "abstract": "Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed using workers' abilities, trust values, and distance weights, transforming the worker recruitment problem into a Maximum Weight Average Subgraph Problem (MWASP). A Tabu Search Recruitment (TSR) algorithm is proposed to rationally recruit a balanced multi-objective optimal task utility worker set for each task. Extensive simulation experiments on four real-world datasets demonstrate the effectiveness of the proposed strategy, outperforming other strategies.", "link": "https://arxiv.org/abs/2306.04366"}, {"id": "2306.04376", "date": "Wed, 7 Jun 2023 12:17:34 GMT", "title": "Label Shift Quantification with Robustness Guarantees via Distribution\n\u00a0Feature Matching\n", "authors": ["Bastien Dussap", "Gilles Blanchard", "Badr-Eddine Ch\\'erief-Abdellatif\n"], "categories": ["stat.ML", "cs.AI", "cs.LG\nComments:", "Accepted", "at", "the", "European", "Conference", "on", "Machine", "Learning", "and\n\u00a0Principles", "and", "Practice", "of", "Knowledge", "Discovery", "in", "Databases", "(ECML)", "2023\n"], "abstract": "Quantification learning deals with the task of estimating the target label distribution under label shift. In this paper, we first present a unifying framework, distribution feature matching (DFM), that recovers as particular instances various estimators introduced in previous literature. We derive a general performance bound for DFM procedures, improving in several key aspects upon previous bounds derived in particular cases. We then extend this analysis to study robustness of DFM procedures in the misspecified setting under departure from the exact label shift hypothesis, in particular in the case of contamination of the target by an unknown distribution. These theoretical findings are confirmed by a detailed numerical study on simulated and real-world datasets. We also introduce an efficient, scalable and robust version of kernel-based DFM using the Random Fourier Feature principle.", "link": "https://arxiv.org/abs/2306.04376"}, {"id": "2306.04384", "date": "Wed, 7 Jun 2023 12:31:07 GMT", "title": "Multilingual Clinical NER: Translation or Cross-lingual Transfer?\n", "authors": ["Xavier Fontaine", "F\\'elix Gaschi", "Parisa Rastin and Yannick Toussaint\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\nComments:", "23", "pages,", "Proceedings", "of", "the", "5th", "Clinical", "Natural", "Language", "Processing\n\u00a0Workshop\n"], "abstract": "Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medical dataset (Frei and Kramer, 2021), we show that translation-based methods can achieve similar performance to CLT but require more care in their design. And while they can take advantage of monolingual clinical language models, those do not guarantee better results than large general-purpose multilingual models, whether with cross-lingual transfer or translation.", "link": "https://arxiv.org/abs/2306.04384"}, {"id": "2306.04640", "date": "Wed, 7 Jun 2023 17:59:57 GMT", "title": "ModuleFormer: Learning Modular Large Language Models From Uncurated Data\n", "authors": ["Yikang Shen", "Zheyu Zhang", "Tianyou Cao", "Shawn Tan", "Zhenfang Chen,\n\u00a0Chuang Gan\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\n"], "abstract": "Large Language Models (LLMs) have achieved remarkable results. But existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike the previous SMoE-based modular language model [Gururangan et al., 2021], which requires domain-labeled data to learn domain-specific experts, ModuleFormer can induce modularity from uncurated data with its new load balancing and load concentration losses. ModuleFormer is a modular architecture that includes two different types of modules, new stick-breaking attention heads, and feedforward experts. Different modules are sparsely activated conditions on the input token during training and inference. In our experiment, we found that the modular architecture enables three important abilities for large pre-trained language models: 1) Efficiency, since ModuleFormer only activates a subset of its modules for each input token, thus it could achieve the same performance as dense LLMs with more than two times throughput; 2) Extendability, ModuleFormer is more immune to catastrophic forgetting than dense LLMs and can be easily extended with new modules to learn new knowledge that is not included in the training data; 3) Specialisation, finetuning ModuleFormer could specialize a subset of modules to the finetuning task, and the task-unrelated modules could be easily pruned for a lightweight deployment.", "link": "https://arxiv.org/abs/2306.04640"}, {"id": "2306.03976", "date": "Tue, 6 Jun 2023 19:18:46 GMT", "title": "Explainable AI using expressive Boolean formulas\n", "authors": ["Gili Rosenberg", "J. Kyle Brubaker", "Martin J. A. Schuetz", "Grant Salton,\n\u00a0Zhihuai Zhu", "Elton Yechao Zhu", "Serdar Kad{\\i}o\\u{g}lu", "Sima E. Borujeni,\n\u00a0Helmut G. Katzgraber\n"], "categories": ["cs.AI", "cs.LG", "math.OC", "quant-ph\nComments:", "28", "pages,", "16", "figures,", "4", "tables\n"], "abstract": "We propose and implement an interpretable machine learning classification model for Explainable AI (XAI) based on expressive Boolean formulas. Potential applications include credit scoring and diagnosis of medical conditions. The Boolean formula defines a rule with tunable complexity (or interpretability), according to which input data are classified. Such a formula can include any operator that can be applied to one or more Boolean variables, thus providing higher expressivity compared to more rigid rule-based and tree-based approaches. The classifier is trained using native local optimization techniques, efficiently searching the space of feasible formulas. Shallow rules can be determined by fast Integer Linear Programming (ILP) or Quadratic Unconstrained Binary Optimization (QUBO) solvers, potentially powered by special purpose hardware or quantum devices. We combine the expressivity and efficiency of the native local optimizer with the fast operation of these devices by executing non-local moves that optimize over subtrees of the full Boolean formula. We provide extensive numerical benchmarking results featuring several baselines on well-known public datasets. Based on the results, we find that the native local rule classifier is generally competitive with the other classifiers. The addition of non-local moves achieves similar results with fewer iterations, and therefore using specialized or quantum hardware could lead to a speedup by fast proposal of non-local moves.", "link": "https://arxiv.org/abs/2306.03976"}, {"id": "2306.04021", "date": "Tue, 6 Jun 2023 21:27:08 GMT", "title": "Energy-Based Models for Cross-Modal Localization using Convolutional\n\u00a0Transformers\n", "authors": ["Alan Wu and Michael S. Ryoo\n"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO\nComments:", "ICRA", "2023\n"], "abstract": "We present a novel framework using Energy-Based Models (EBMs) for localizing a ground vehicle mounted with a range sensor against satellite imagery in the absence of GPS. Lidar sensors have become ubiquitous on autonomous vehicles for describing its surrounding environment. Map priors are typically built using the same sensor modality for localization purposes. However, these map building endeavors using range sensors are often expensive and time-consuming. Alternatively, we leverage the use of satellite images as map priors, which are widely available, easily accessible, and provide comprehensive coverage. We propose a method using convolutional transformers that performs accurate metric-level localization in a cross-modal manner, which is challenging due to the drastic difference in appearance between the sparse range sensor readings and the rich satellite imagery. We train our model end-to-end and demonstrate our approach achieving higher accuracy than the state-of-the-art on KITTI, Pandaset, and a custom dataset.", "link": "https://arxiv.org/abs/2306.04021"}, {"id": "2306.04396", "date": "Wed, 7 Jun 2023 12:56:56 GMT", "title": "Improving Diffusion-based Image Translation using Asymmetric Gradient\n\u00a0Guidance\n", "authors": ["Gihyun Kwon", "Jong Chul Ye\n"], "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML\n"], "abstract": "Diffusion models have shown significant progress in image translation tasks recently. However, due to their stochastic nature, there's often a trade-off between style transformation and content preservation. Current strategies aim to disentangle style and content, preserving the source image's structure while successfully transitioning from a source to a target domain under text or one-shot image conditions. Yet, these methods often require computationally intense fine-tuning of diffusion models or additional neural networks. To address these challenges, here we present an approach that guides the reverse process of diffusion sampling by applying asymmetric gradient guidance. This results in quicker and more stable image manipulation for both text-guided and image-guided image translation. Our model's adaptability allows it to be implemented with both image- and latent-diffusion models. Experiments show that our method outperforms various state-of-the-art models in image translation tasks.", "link": "https://arxiv.org/abs/2306.04396"}, {"id": "2306.03929", "date": "Tue, 6 Jun 2023 18:00:29 GMT", "title": "Finding Counterfactually Optimal Action Sequences in Continuous State\n\u00a0Spaces\n", "authors": ["Stratis Tsirtsis", "Manuel Gomez-Rodriguez\n"], "categories": ["cs.LG", "cs.AI", "cs.CY\n"], "abstract": "Humans performing tasks that involve taking a series of multiple dependent actions over time often learn from experience by reflecting on specific cases and points in time, where different actions could have led to significantly better outcomes. While recent machine learning methods to retrospectively analyze sequential decision making processes promise to aid decision makers in identifying such cases, they have focused on environments with finitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to fill this gap. We start by formally characterizing a sequence of discrete actions and continuous states using finite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this characterization, we formalize the problem of finding counterfactually optimal action sequences and show that, in general, we cannot expect to solve it in polynomial time. Then, we develop a search method based on the $A^*$ algorithm that, under a natural form of Lipschitz continuity of the environment's dynamics, is guaranteed to return the optimal solution to the problem. Experiments on real clinical data show that our method is very efficient in practice, and it has the potential to offer interesting insights for sequential decision making tasks.", "link": "https://arxiv.org/abs/2306.03929"}, {"id": "2306.03962", "date": "Tue, 6 Jun 2023 18:45:05 GMT", "title": "PILLAR: How to make semi-private learning more effective\n", "authors": ["Francesco Pinto", "Yaxi Hu", "Fanny Yang", "Amartya Sanyal\n"], "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ML\n"], "abstract": "In Semi-Supervised Semi-Private (SP) learning, the learner has access to both public unlabelled and private labelled data. We propose a computationally efficient algorithm that, under mild assumptions on the data, provably achieves significantly lower private labelled sample complexity and can be efficiently run on real-world datasets. For this purpose, we leverage the features extracted by networks pre-trained on public (labelled or unlabelled) data, whose distribution can significantly differ from the one on which SP learning is performed. To validate its empirical effectiveness, we propose a wide variety of experiments under tight privacy constraints (\\(\\epsilon=0.1\\)) and with a focus on low-data regimes. In all of these settings, our algorithm exhibits significantly improved performance over available baselines that use similar amounts of public data.", "link": "https://arxiv.org/abs/2306.03962"}, {"id": "2306.04026", "date": "Tue, 6 Jun 2023 21:41:31 GMT", "title": "Your Value Function is a Control Barrier Function: Verification of\n\u00a0Learned Policies using Control Theory\n", "authors": ["Daniel C.H. Tan and Fernando Acero and Robert McCarthy and Dimitrios\n\u00a0Kanoulas and Zhibin Alex Li\n"], "categories": ["cs.LG", "cs.AI", "cs.RO\n"], "abstract": "Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.", "link": "https://arxiv.org/abs/2306.04026"}, {"id": "2306.04037", "date": "Tue, 6 Jun 2023 22:04:45 GMT", "title": "Quantitative Analysis of Primary Attribution Explainable Artificial\n\u00a0Intelligence Methods for Remote Sensing Image Classification\n", "authors": ["Akshatha Mohan and Joshua Peeples\n"], "categories": ["cs.LG", "cs.AI", "cs.CV\nComments:", "4", "pages,", "3", "figures,", "Accepted", "to", "2023", "IGARSS", "Community-Contributed\n\u00a0Sessions", "-", "Opening", "the", "Black", "Box:", "Explainable", "AI/ML", "in", "Remote", "Sensing\n\u00a0Analysis\n"], "abstract": "We present a comprehensive analysis of quantitatively evaluating explainable artificial intelligence (XAI) techniques for remote sensing image classification. Our approach leverages state-of-the-art machine learning approaches to perform remote sensing image classification across multiple modalities. We investigate the results of the models qualitatively through XAI methods. Additionally, we compare the XAI methods quantitatively through various categories of desired properties. Through our analysis, we offer insights and recommendations for selecting the most appropriate XAI method(s) to gain a deeper understanding of the models' decision-making processes. The code for this work is publicly available.", "link": "https://arxiv.org/abs/2306.04037"}, {"id": "2306.04040", "date": "Tue, 6 Jun 2023 22:11:13 GMT", "title": "FedVal: Different good or different bad in federated learning\n", "authors": ["Viktor Valadi", "Xinchi Qiu", "Pedro Porto Buarque de Gusm\\~ao", "Nicholas\n\u00a0D. Lane", "Mina Alibeigi\n"], "categories": ["cs.LG", "cs.AI", "cs.CR\nComments:", "To", "appear", "in", "the", "proceedings", "of", "the", "USENIX", "Security", "Symposium", "2023\n"], "abstract": "Federated learning (FL) systems are susceptible to attacks from malicious actors who might attempt to corrupt the training model through various poisoning attacks. FL also poses new challenges in addressing group bias, such as ensuring fair performance for different demographic groups. Traditional methods used to address such biases require centralized access to the data, which FL systems do not have. In this paper, we present a novel approach FedVal for both robustness and fairness that does not require any additional information from clients that could raise privacy concerns and consequently compromise the integrity of the FL system. To this end, we propose an innovative score function based on a server-side validation method that assesses client updates and determines the optimal aggregation balance between locally-trained models. Our research shows that this approach not only provides solid protection against poisoning attacks but can also be used to reduce group bias and subsequently promote fairness while maintaining the system's capability for differential privacy. Extensive experiments on the CIFAR-10, FEMNIST, and PUMS ACSIncome datasets in different configurations demonstrate the effectiveness of our method, resulting in state-of-the-art performances. We have proven robustness in situations where 80% of participating clients are malicious. Additionally, we have shown a significant increase in accuracy for underrepresented labels from 32% to 53%, and increase in recall rate for underrepresented features from 19% to 50%.", "link": "https://arxiv.org/abs/2306.04040"}, {"id": "2306.04107", "date": "Wed, 7 Jun 2023 02:16:36 GMT", "title": "BeMap: Balanced Message Passing for Fair Graph Neural Network\n", "authors": ["Xiao Lin", "Jian Kang", "Weilin Cong", "Hanghang Tong\n"], "categories": ["cs.LG", "cs.AI", "cs.SI\nComments:", "13", "pages,", "4", "figures\n"], "abstract": "Graph Neural Network (GNN) has shown strong empirical performance in many downstream tasks by iteratively aggregating information from the local neighborhood of each node, i.e., message passing. However, concrete evidence has revealed that a graph neural network could be biased against certain demographic groups, which calls for the consideration of algorithmic fairness. Despite the increasing efforts in ensuring algorithmic fairness on graph neural networks, they often do not explicitly consider the induced bias caused by message passing in GNN during training. In this paper, we first investigate the problem of bias amplification in message passing. We empirically and theoretically demonstrate that message passing could amplify the bias when the 1-hop neighbors from different demographic groups are unbalanced. Guided by such analyses, we propose BeMap, a fair message passing method, that leverages a balance-aware sampling strategy to balance the number of the 1-hop neighbors of each node among different demographic groups. Extensive experiments on node classification demonstrate the efficacy of our proposed BeMap method in mitigating bias while maintaining classification accuracy.", "link": "https://arxiv.org/abs/2306.04107"}, {"id": "2306.04120", "date": "Wed, 7 Jun 2023 03:28:47 GMT", "title": "MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY\n\u00a0Estimation\n", "authors": ["Tony Tohme", "Mohsen Sadr", "Kamal Youcef-Toumi", "Nicolas G.\n\u00a0Hadjiconstantinou\n"], "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH\n"], "abstract": "We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method in each iteration of the random search is linear with the number of samples and quadratic with the number of basis functions. We validate the proposed MESSY estimation method against other benchmark methods for the case of a bi-modal and a discontinuous density, as well as a density at the limit of physical realizability. We find that the addition of a symbolic search for basis functions improves the accuracy of the estimation at a reasonable additional computational cost. Our results suggest that the proposed method outperforms existing density recovery methods in the limit of a small to moderate number of samples by providing a low-bias and tractable symbolic description of the unknown density at a reasonable computational cost.", "link": "https://arxiv.org/abs/2306.04120"}, {"id": "2306.04251", "date": "Wed, 7 Jun 2023 08:44:51 GMT", "title": "Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards\n\u00a0Simpler Subnetworks\n", "authors": ["Feng Chen", "Daniel Kunin", "Atsushi Yamamura", "Surya Ganguli\n"], "categories": ["cs.LG", "cs.AI", "stat.ML\nComments:", "30", "pages,", "10", "figures\n"], "abstract": "In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss. We observe empirically the existence of attractive invariant sets in trained deep neural networks, implying that SGD dynamics often collapses to simple subnetworks with either vanishing or redundant neurons. We further demonstrate how this simplifying process of stochastic collapse benefits generalization in a linear teacher-student framework. Finally, through this analysis, we mechanistically explain why early training with large learning rates for extended periods benefits subsequent generalization.", "link": "https://arxiv.org/abs/2306.04251"}, {"id": "2306.04265", "date": "Wed, 7 Jun 2023 09:05:56 GMT", "title": "Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised\n\u00a0Learning\n", "authors": ["Jianfei Li", "Ruigang Zheng", "Han Feng", "Xiaosheng Zhuang\n"], "categories": ["cs.LG", "cs.AI", "math.FA\n"], "abstract": "The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.", "link": "https://arxiv.org/abs/2306.04265"}, {"id": "2306.04400", "date": "Wed, 7 Jun 2023 12:58:52 GMT", "title": "A Fair Classifier Embracing Triplet Collapse\n", "authors": ["A. Martzloff (1)", "N. Posocco (2)", "Q. Ferr\\'e (1) ((1) Euranova,\n\u00a0Marseille", "France", "(2) Euranova", "Mont-Saint-Guibert", "Belgique)\n"], "categories": ["cs.LG", "cs.AI", "cs.CY\nComments:", "9", "pages,", "7", "figures,", "CAp2023\nACM-class:", "I.2.6;", "I.5.1;", "K.4.2\n"], "abstract": "In this paper, we study the behaviour of the triplet loss and show that it can be exploited to limit the biases created and perpetuated by machine learning models. Our fair classifier uses the collapse of the triplet loss when its margin is greater than the maximum distance between two points in the latent space, in the case of stochastic triplet selection.", "link": "https://arxiv.org/abs/2306.04400"}, {"id": "2306.04406", "date": "Wed, 7 Jun 2023 13:04:34 GMT", "title": "Generalized Teacher Forcing for Learning Chaotic Dynamics\n", "authors": ["Florian Hess", "Zahra Monfared", "Manuel Brenner", "Daniel Durstewitz\n"], "categories": ["cs.LG", "cs.AI", "math.DS", "nlin.CD\nComments:", "To", "be", "published", "in", "the", "Proceedings", "of", "the", "40th", "International\n\u00a0Conference", "on", "Machine", "Learning", "(ICML", "2023)\n"], "abstract": "Chaotic dynamical systems (DS) are ubiquitous in nature and society. Often we are interested in reconstructing such systems from observed time series for prediction or mechanistic insight, where by reconstruction we mean learning geometrical and invariant temporal properties of the system in question (like attractors). However, training reconstruction algorithms like recurrent neural networks (RNNs) on such systems by gradient-descent based techniques faces severe challenges. This is mainly due to exploding gradients caused by the exponential divergence of trajectories in chaotic systems. Moreover, for (scientific) interpretability we wish to have as low dimensional reconstructions as possible, preferably in a model which is mathematically tractable. Here we report that a surprisingly simple modification of teacher forcing leads to provably strictly all-time bounded gradients in training on chaotic systems, and, when paired with a simple architectural rearrangement of a tractable RNN design, piecewise-linear RNNs (PLRNNs), allows for faithful reconstruction in spaces of at most the dimensionality of the observed system. We show on several DS that with these amendments we can reconstruct DS better than current SOTA algorithms, in much lower dimensions. Performance differences were particularly compelling on real world data with which most other methods severely struggled. This work thus led to a simple yet powerful DS reconstruction algorithm which is highly interpretable at the same time.", "link": "https://arxiv.org/abs/2306.04406"}, {"id": "2306.04488", "date": "Wed, 7 Jun 2023 14:58:15 GMT", "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating\n\u00a0weights fine-tuned on diverse rewards\n", "authors": ["Alexandre Rame", "Guillaume Couairon", "Mustafa Shukor", "Corentin Dancette,\n\u00a0Jean-Baptiste Gaya", "Laure Soulier and Matthieu Cord\n"], "categories": ["cs.LG", "cs.AI", "cs.CV\n"], "abstract": "Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.", "link": "https://arxiv.org/abs/2306.04488"}, {"id": "2306.04505", "date": "Wed, 7 Jun 2023 15:12:16 GMT", "title": "Hardness of Deceptive Certificate Selection\n", "authors": ["Stephan W\\\"aldchen\n"], "categories": ["cs.LG", "cs.AI", "cs.CC\nComments:", "15", "pages,", "3", "figures\nMSC-class:", "68T01,", "91A06\nACM-class:", "I.2.0\n"], "abstract": "Recent progress towards theoretical interpretability guarantees for AI has been made with classifiers that are based on interactive proof systems. A prover selects a certificate from the datapoint and sends it to a verifier who decides the class. In the context of machine learning, such a certificate can be a feature that is informative of the class. For a setup with high soundness and completeness, the exchanged certificates must have a high mutual information with the true class of the datapoint. However, this guarantee relies on a bound on the Asymmetric Feature Correlation of the dataset, a property that so far is difficult to estimate for high-dimensional data. It was conjectured in W\\\"aldchen et al. that it is computationally hard to exploit the AFC, which is what we prove here. We consider a malicious prover-verifier duo that aims to exploit the AFC to achieve high completeness and soundness while using uninformative certificates. We show that this task is $\\mathsf{NP}$-hard and cannot be approximated better than $\\mathcal{O}(m^{1/8 - \\epsilon})$, where $m$ is the number of possible certificates, for $\\epsilon>0$ under the Dense-vs-Random conjecture. This is some evidence that AFC should not prevent the use of interactive classification for real-world tasks, as it is computationally hard to be exploited.", "link": "https://arxiv.org/abs/2306.04505"}, {"id": "2306.04542", "date": "Wed, 7 Jun 2023 15:46:47 GMT", "title": "On the Design Fundamentals of Diffusion Models: A Survey\n", "authors": ["Ziyi Chang", "George A. Koulieris", "Hubert P. H. Shum\n"], "categories": ["cs.LG", "cs.AI", "cs.CV\n"], "abstract": "Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.", "link": "https://arxiv.org/abs/2306.04542"}, {"id": "2306.04566", "date": "Wed, 7 Jun 2023 16:13:16 GMT", "title": "Recent applications of machine learning, remote sensing, and iot\n\u00a0approaches in yield prediction: a critical review\n", "authors": ["Fatima Zahra Bassine", "Terence Epule Epule", "Ayoub Kechchour", "Abdelghani\n\u00a0Chehbouni\n"], "categories": ["cs.LG", "cs.AI", "cs.NI", "cs.SE\nComments:", "35", "pages,", "12", "figures,", "14", "tables\n"], "abstract": "The integration of remote sensing and machine learning in agriculture is transforming the industry by providing insights and predictions through data analysis. This combination leads to improved yield prediction and water management, resulting in increased efficiency, better yields, and more sustainable agricultural practices. Achieving the United Nations' Sustainable Development Goals, especially \"zero hunger,\" requires the investigation of crop yield and precipitation gaps, which can be accomplished through, the usage of artificial intelligence (AI), machine learning (ML), remote sensing (RS), and the internet of things (IoT). By integrating these technologies, a robust agricultural mobile or web application can be developed, providing farmers and decision-makers with valuable information and tools for improving crop management and increasing efficiency. Several studies have investigated these new technologies and their potential for diverse tasks such as crop monitoring, yield prediction, irrigation management, etc. Through a critical review, this paper reviews relevant articles that have used RS, ML, cloud computing, and IoT in crop yield prediction. It reviews the current state-of-the-art in this field by critically evaluating different machine-learning approaches proposed in the literature for crop yield prediction and water management. It provides insights into how these methods can improve decision-making in agricultural production systems. This work will serve as a compendium for those interested in yield prediction in terms of primary literature but, most importantly, what approaches can be used for real-time and robust prediction.", "link": "https://arxiv.org/abs/2306.04566"}, {"id": "2306.04595", "date": "Wed, 7 Jun 2023 16:49:03 GMT", "title": "Generalization Across Observation Shifts in Reinforcement Learning\n", "authors": ["Anuj Mahajan and Amy Zhang\n"], "categories": ["cs.LG", "cs.AI", "cs.RO\n"], "abstract": "Learning policies which are robust to changes in the environment are critical for real world deployment of Reinforcement Learning agents. They are also necessary for achieving good generalization across environment shifts. We focus on bisimulation metrics, which provide a powerful means for abstracting task relevant components of the observation and learning a succinct representation space for training the agent using reinforcement learning. In this work, we extend the bisimulation framework to also account for context dependent observation shifts. Specifically, we focus on the simulator based learning setting and use alternate observations to learn a representation space which is invariant to observation shifts using a novel bisimulation based objective. This allows us to deploy the agent to varying observation settings during test time and generalize to unseen scenarios. We further provide novel theoretical bounds for simulator fidelity and performance transfer guarantees for using a learnt policy to unseen shifts. Empirical analysis on the high-dimensional image based control domains demonstrates the efficacy of our method.", "link": "https://arxiv.org/abs/2306.04595"}, {"id": "2306.04637", "date": "Wed, 7 Jun 2023 17:59:31 GMT", "title": "Transformers as Statisticians: Provable In-Context Learning with\n\u00a0In-Context Algorithm Selection\n", "authors": ["Yu Bai", "Fan Chen", "Huan Wang", "Caiming Xiong", "Song Mei\n"], "categories": ["cs.LG", "cs.AI", "cs.CL", "math.ST", "stat.ML", "stat.TH\n"], "abstract": "Neural sequence models based on the transformer architecture have demonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they can perform new tasks when prompted with training and test examples, without any parameter update to the model. This work first provides a comprehensive statistical theory for transformers to perform ICL. Concretely, we show that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions. Using an efficient implementation of in-context gradient descent as the underlying mechanism, our transformer constructions admit mild size bounds, and can be learned with polynomially many pretraining sequences. Building on these ``base'' ICL algorithms, intriguingly, we show that transformers can implement more complex ICL procedures involving \\emph{in-context algorithm selection}, akin to what a statistician can do in real life -- A \\emph{single} transformer can adaptively select different base ICL algorithms -- or even perform qualitatively different tasks -- on different input sequences, without any explicit prompting of the right algorithm or task. We both establish this in theory by explicit constructions, and also observe this phenomenon experimentally. In theory, we construct two general mechanisms for algorithm selection with concrete examples: pre-ICL testing, and post-ICL validation. As an example, we use the post-ICL validation mechanism to construct a transformer that can perform nearly Bayes-optimal ICL on a challenging task -- noisy linear models with mixed noise levels. Experimentally, we demonstrate the strong in-context algorithm selection capabilities of standard transformer architectures.", "link": "https://arxiv.org/abs/2306.04637"}, {"id": "2306.03933", "date": "Tue, 6 Jun 2023 18:01:03 GMT", "title": "High-dimensional and Permutation Invariant Anomaly Detection\n", "authors": ["Vinicius Mikuni", "Benjamin Nachman\n"], "categories": ["hep-ph", "cs.AI", "cs.LG", "hep-ex\nComments:", "7", "pages,", "5", "figures\n"], "abstract": "Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.", "link": "https://arxiv.org/abs/2306.03933"}, {"id": "2306.04339", "date": "Wed, 7 Jun 2023 11:10:10 GMT", "title": "Unpaired Deep Learning for Pharmacokinetic Parameter Estimation from\n\u00a0Dynamic Contrast-Enhanced MRI\n", "authors": ["Gyutaek Oh", "Won-Jin Moon", "and Jong Chul Ye\n"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "physics.med-ph\n"], "abstract": "DCE-MRI provides information about vascular permeability and tissue perfusion through the acquisition of pharmacokinetic parameters. However, traditional methods for estimating these pharmacokinetic parameters involve fitting tracer kinetic models, which often suffer from computational complexity and low accuracy due to noisy arterial input function (AIF) measurements. Although some deep learning approaches have been proposed to tackle these challenges, most existing methods rely on supervised learning that requires paired input DCE-MRI and labeled pharmacokinetic parameter maps. This dependency on labeled data introduces significant time and resource constraints, as well as potential noise in the labels, making supervised learning methods often impractical. To address these limitations, here we present a novel unpaired deep learning method for estimating both pharmacokinetic parameters and the AIF using a physics-driven CycleGAN approach. Our proposed CycleGAN framework is designed based on the underlying physics model, resulting in a simpler architecture with a single generator and discriminator pair. Crucially, our experimental results indicate that our method, which does not necessitate separate AIF measurements, produces more reliable pharmacokinetic parameters than other techniques.", "link": "https://arxiv.org/abs/2306.04339"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.link + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
