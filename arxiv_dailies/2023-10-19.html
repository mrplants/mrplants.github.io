<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2310.11513", "Date": "Tue, 17 Oct 2023 18:20:03 ", "Title": "GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment", "Authors": ["Dhruba Ghosh", "Hanna Hajishirzi", "Ludwig Schmidt"], "Categories": "cs.CV cs.LG"}, "abstract": "Recent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. However, most current automated evaluation metrics like FID or CLIPScore only offer a holistic measure of image quality or image-text alignment, and are unsuited for fine-grained or instance-level analysis. In this paper, we introduce GenEval, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. We then evaluate several open-source text-to-image models and analyze their relative generative capabilities on our benchmark. We find that recent models demonstrate significant improvement on these tasks, though they are still lacking in complex capabilities such as spatial relations and attribute binding. Finally, we demonstrate how GenEval might be used to help discover existing failure modes, in order to inform development of the next generation of text-to-image models. Our code to run the GenEval framework is publicly available at https://github.com/djghosh13/geneval.", "url": "https://arxiv.org/abs/2310.11513"}, {"metadata": {"arXiv": "2310.11605", "Date": "Tue, 17 Oct 2023 21:59:45 ", "Title": "DIAR: Deep Image Alignment and Reconstruction using Swin Transformers", "Authors": ["Monika Kwiatkowski", "Simon Matern", "Olaf Hellwich"], "Categories": "cs.CV cs.LG"}, "abstract": "When taking images of some occluded content, one is often faced with the problem that every individual image frame contains unwanted artifacts, but a collection of images contains all relevant information if properly aligned and aggregated. In this paper, we attempt to build a deep learning pipeline that simultaneously aligns a sequence of distorted images and reconstructs them. We create a dataset that contains images with image distortions, such as lighting, specularities, shadows, and occlusion. We create perspective distortions with corresponding ground-truth homographies as labels. We use our dataset to train Swin transformer models to analyze sequential image data. The attention maps enable the model to detect relevant image content and differentiate it from outliers and artifacts. We further explore using neural feature maps as alternatives to classical key point detectors. The feature maps of trained convolutional layers provide dense image descriptors that can be used to find point correspondences between images. We utilize this to compute coarse image alignments and explore its limitations.", "url": "https://arxiv.org/abs/2310.11605"}, {"metadata": {"arXiv": "2310.11758", "Date": "Wed, 18 Oct 2023 07:31:35 ", "Title": "Domain-Generalized Face Anti-Spoofing with Unknown Attacks", "Authors": ["Zong-Wei Hong", "Yu-Chen Lin", "Hsuan-Tung Liu", "Yi-Ren Yeh", "Chu-Song Chen"], "Categories": "cs.CV cs.LG", "Comments": ["IEEE International Conference on Image Processing (ICIP 2023)"], "DOI": "10.1109/ICIP49359.2023.10223078"}, "abstract": "Although face anti-spoofing (FAS) methods have achieved remarkable performance on specific domains or attack types, few studies have focused on the simultaneous presence of domain changes and unknown attacks, which is closer to real application scenarios. To handle domain-generalized unknown attacks, we introduce a new method, DGUA-FAS, which consists of a Transformer-based feature extractor and a synthetic unknown attack sample generator (SUASG). The SUASG network simulates unknown attack samples to assist the training of the feature extractor. Experimental results show that our method achieves superior performance on domain generalization FAS with known or unknown attacks.", "url": "https://arxiv.org/abs/2310.11758"}, {"metadata": {"arXiv": "2310.11864", "Date": "Wed, 18 Oct 2023 10:26:56 ", "Title": "VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization", "Authors": ["Hongliang Zhong", "Jingbo Zhang", "Jing Liao"], "Categories": "cs.CV cs.GR cs.LG", "Comments": ["Under Review. Project Page: https://jtbzhl.github.io/VQ-NeRF.github.io/"]}, "abstract": "We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy to predict the number of materials in a scene, which reduces redundancy in the material segmentation process. To improve usability, we also develop an interactive interface to further assist material editing. We evaluate our model on both computer-generated and real-world scenes, demonstrating its superior performance. To the best of our knowledge, our model is the first to enable discrete material editing in 3D scenes.", "url": "https://arxiv.org/abs/2310.11864"}, {"metadata": {"arXiv": "2310.11867", "Date": "Wed, 18 Oct 2023 10:32:39 ", "Title": "Evaluating the Fairness of Discriminative Foundation Models in Computer Vision", "Authors": ["Junaid Ali", "Matthaeus Kleindessner", "Florian Wenzel", "Kailash Budhathoki", "Volkan Cevher and Chris Russell"], "Categories": "cs.CV cs.CY cs.LG", "Comments": ["Accepted at AIES'23"], "DOI": "10.1145/3600211.3604720"}, "abstract": "We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes over ten diverse datasets. We find that fair PCA, a post-processing method for fair representations, works very well for debiasing in most of the aforementioned tasks while incurring only minor loss of performance. However, different debiasing approaches vary in their effectiveness depending on the task. Hence, one should choose the debiasing approach depending on the specific use case.", "url": "https://arxiv.org/abs/2310.11867"}, {"metadata": {"arXiv": "2310.11566", "Date": "Tue, 17 Oct 2023 20:25:40 ", "Title": "Partially Observable Stochastic Games with Neural Perception Mechanisms", "Authors": ["Rui Yan", "Gabriel Santos", "Gethin Norman", "David Parker and Marta Kwiatkowska"], "Categories": "cs.GT cs.LG", "Comments": ["41 pages", "5 figures"]}, "abstract": "Stochastic games are a well established model for multi-agent sequential decision making under uncertainty. In reality, though, agents have only partial observability of their environment, which makes the problem computationally challenging, even in the single-agent setting of partially observable Markov decision processes. Furthermore, in practice, agents increasingly perceive their environment using data-driven approaches such as neural networks trained on continuous data. To tackle this problem, we propose the model of neuro-symbolic partially-observable stochastic games (NS-POSGs), a variant of continuous-space concurrent stochastic games that explicitly incorporates perception mechanisms. We focus on a one-sided setting, comprising a partially-informed agent with discrete, data-driven observations and a fully-informed agent with continuous observations. We present a new point-based method, called one-sided NS-HSVI, for approximating values of one-sided NS-POSGs and implement it based on the popular particle-based beliefs, showing that it has closed forms for computing values of interest. We provide experimental results to demonstrate the practical applicability of our method for neural networks whose preimage is in polyhedral form.", "url": "https://arxiv.org/abs/2310.11566"}, {"metadata": {"arXiv": "2310.11479", "Date": "Tue, 17 Oct 2023 10:24:25 ", "Title": "On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction", "Authors": ["Seohyeon Cha", "Honggu Kang", "and Joonhyuk Kang"], "Categories": "cs.LG stat.ML"}, "abstract": "Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing $\\textit{valid}$ prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as $\\textit{inefficiency}$, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is $\\textit{well-calibrated}$ only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP framework. We empirically demonstrate the existence of temperatures that result in more efficient prediction sets. Furthermore, we conduct an analysis to identify the factors contributing to inefficiency and offer valuable insights into the relationship between CP performance and model calibration.", "url": "https://arxiv.org/abs/2310.11479"}, {"metadata": {"arXiv": "2310.11515", "Date": "Tue, 17 Oct 2023 18:27:27 ", "Title": "Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs", "Authors": ["Yu-Heng Hung", "Ping-Chun Hsieh", "Akshay Mete", "P. R. Kumar"], "Categories": "cs.LG"}, "abstract": "We consider the infinite-horizon linear Markov Decision Processes (MDPs), where the transition probabilities of the dynamic model can be linearly parameterized with the help of a predefined low-dimensional feature mapping. While the existing regression-based approaches have been theoretically shown to achieve nearly-optimal regret, they are computationally rather inefficient due to the need for a large number of optimization runs in each time step, especially when the state and action spaces are large. To address this issue, we propose to solve linear MDPs through the lens of Value-Biased Maximum Likelihood Estimation (VBMLE), which is a classic model-based exploration principle in the adaptive control literature for resolving the well-known closed-loop identification problem of Maximum Likelihood Estimation. We formally show that (i) VBMLE enjoys $\\widetilde{O}(d\\sqrt{T})$ regret, where $T$ is the time horizon and $d$ is the dimension of the model parameter, and (ii) VBMLE is computationally more efficient as it only requires solving one optimization problem in each time step. In our regret analysis, we offer a generic convergence result of MLE in linear MDPs through a novel supermartingale construct and uncover an interesting connection between linear MDPs and online learning, which could be of independent interest. Finally, the simulation results show that VBMLE significantly outperforms the benchmark method in terms of both empirical regret and computation time.", "url": "https://arxiv.org/abs/2310.11515"}, {"metadata": {"arXiv": "2310.11558", "Date": "Tue, 17 Oct 2023 20:09:41 ", "Title": "Online Algorithms with Uncertainty-Quantified Predictions", "Authors": ["Bo Sun", "Jerry Huang", "Nicolas Christianson", "Mohammad Hajiesmaili", "Adam Wierman"], "Categories": "cs.LG cs.DS"}, "abstract": "Online algorithms with predictions have become a trending topic in the field of beyond worst-case analysis of algorithms. These algorithms incorporate predictions about the future to obtain performance guarantees that are of high quality when the predictions are good, while still maintaining bounded worst-case guarantees when predictions are arbitrarily poor. In general, the algorithm is assumed to be unaware of the prediction's quality. However, recent developments in the machine learning literature have studied techniques for providing uncertainty quantification on machine-learned predictions, which describes how certain a model is about its quality. This paper examines the question of how to optimally utilize uncertainty-quantified predictions in the design of online algorithms. In particular, we consider predictions augmented with uncertainty quantification describing the likelihood of the ground truth falling in a certain range, designing online algorithms with these probabilistic predictions for two classic online problems: ski rental and online search. In each case, we demonstrate that non-trivial modifications to algorithm design are needed to fully leverage the probabilistic predictions. Moreover, we consider how to utilize more general forms of uncertainty quantification, proposing a framework based on online learning that learns to exploit uncertainty quantification to make optimal decisions in multi-instance settings.", "url": "https://arxiv.org/abs/2310.11558"}, {"metadata": {"arXiv": "2310.11607", "Date": "Tue, 17 Oct 2023 22:00:42 ", "Title": "TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification", "Authors": ["Nicholas Botzer", "David Vasquez", "Tim Weninger", "Issam Laradji"], "Categories": "cs.LG", "Comments": ["9 pages", "6 figures", "4 tables"]}, "abstract": "The ability to detect intent in dialogue systems has become increasingly important in modern technology. These systems often generate a large amount of unlabeled data, and manually labeling this data requires substantial human effort. Semi-supervised methods attempt to remedy this cost by using a model trained on a few labeled examples and then by assigning pseudo-labels to further a subset of unlabeled examples that has a model prediction confidence higher than a certain threshold. However, one particularly perilous consequence of these methods is the risk of picking an imbalanced set of examples across classes, which could lead to poor labels. In the present work, we describe Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling approach based on distance in the embedding space while maintaining a balanced set of pseudo-labeled examples across classes through a ranking-based approach. Experiments on several datasets show that TK-KNN outperforms existing models, particularly when labeled data is scarce on popular datasets such as CLINC150 and Banking77. Code is available at https://github.com/ServiceNow/tk-knn", "url": "https://arxiv.org/abs/2310.11607"}, {"metadata": {"arXiv": "2310.11609", "Date": "Tue, 17 Oct 2023 22:05:11 ", "Title": "Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance", "Authors": ["Austin Cheng", "Alston Lo", "Santiago Miret", "Brooks Pate", "Al\\'an Aspuru-Guzik"], "Categories": "cs.LG astro-ph.GA physics.chem-ph"}, "abstract": "Structure determination is necessary to identify unknown organic molecules, such as those in natural products, forensic samples, the interstellar medium, and laboratory syntheses. Rotational spectroscopy enables structure determination by providing accurate 3D information about small organic molecules via their moments of inertia. Using these moments, Kraitchman analysis determines isotopic substitution coordinates, which are the unsigned $|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance, including carbon, nitrogen, and oxygen. While unsigned substitution coordinates can verify guesses of structures, the missing $+/-$ signs make it challenging to determine the actual structure from the substitution coordinates alone. To tackle this inverse problem, we develop KREED (Kraitchman REflection-Equivariant Diffusion), a generative diffusion model that infers a molecule's complete 3D structure from its molecular formula, moments of inertia, and unsigned substitution coordinates of heavy atoms. KREED's top-1 predictions identify the correct 3D structure with >98% accuracy on the QM9 and GEOM datasets when provided with substitution coordinates of all heavy atoms with natural isotopic abundance. When substitution coordinates are restricted to only a subset of carbons, accuracy is retained at 91% on QM9 and 32% on GEOM. On a test set of experimentally measured substitution coordinates gathered from the literature, KREED predicts the correct all-atom 3D structure in 25 of 33 cases, demonstrating experimental applicability for context-free 3D structure determination with rotational spectroscopy.", "url": "https://arxiv.org/abs/2310.11609"}, {"metadata": {"arXiv": "2310.11611", "Date": "Tue, 17 Oct 2023 22:08:01 ", "Title": "In defense of parameter sharing for model-compression", "Authors": ["Aditya Desai", "Anshumali Shrivastava"], "Categories": "cs.LG"}, "abstract": "When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training. In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent capacity advantage that RPS enjoys over sparse models. Theoretically, we establish RPS as a superior technique in terms of memory-efficient representation when compared to pruning for linear models. This paper argues in favor of paradigm shift towards RPS based models. During our rigorous evaluation of RPS, we identified issues in the state-of-the-art RPS technique ROAST, specifically regarding stability (ROAST's sensitivity to initialization hyperparameters, often leading to divergence) and Pareto-continuity (ROAST's inability to recover the accuracy of the original model at zero compression). We provably address both of these issues. We refer to the modified RPS, which incorporates our improvements, as STABLE-RPS.", "url": "https://arxiv.org/abs/2310.11611"}, {"metadata": {"arXiv": "2310.11612", "Date": "Tue, 17 Oct 2023 22:10:17 ", "Title": "Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks", "Authors": ["Yimu Wang", "Xiangru Jian", "Bo Xue"], "Categories": "cs.LG", "Comments": ["Accepted by EMNLP 2023"]}, "abstract": "In this work, we present a post-processing solution to address the hubness problem in cross-modal retrieval, a phenomenon where a small number of gallery data points are frequently retrieved, resulting in a decline in retrieval performance. We first theoretically demonstrate the necessity of incorporating both the gallery and query data for addressing hubness as hubs always exhibit high similarity with gallery and query data. Second, building on our theoretical results, we propose a novel framework, Dual Bank Normalization (DBNorm). While previous work has attempted to alleviate hubness by only utilizing the query samples, DBNorm leverages two banks constructed from the query and gallery samples to reduce the occurrence of hubs during inference. Next, to complement DBNorm, we introduce two novel methods, dual inverted softmax and dual dynamic inverted softmax, for normalizing similarity based on the two banks. Specifically, our proposed methods reduce the similarity between hubs and queries while improving the similarity between non-hubs and queries. Finally, we present extensive experimental results on diverse language-grounded benchmarks, including text-image, text-video, and text-audio, demonstrating the superior performance of our approaches compared to previous methods in addressing hubness and boosting retrieval performance. Our code is available at https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.", "url": "https://arxiv.org/abs/2310.11612"}, {"metadata": {"arXiv": "2310.11654", "Date": "Wed, 18 Oct 2023 01:54:48 ", "Title": "Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features", "Authors": ["Hangbin Lee", "Il Do Ha", "Changha Hwang", "Youngjo Lee"], "Categories": "cs.LG stat.ML"}, "abstract": "There is a growing interest in subject-specific predictions using deep neural networks (DNNs) because real-world data often exhibit correlations, which has been typically overlooked in traditional DNN frameworks. In this paper, we propose a novel hierarchical likelihood learning framework for introducing gamma random effects into the Poisson DNN, so as to improve the prediction performance by capturing both nonlinear effects of input variables and subject-specific cluster effects. The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function. This approach enables a fast end-to-end algorithm for handling clustered count data, which often involve high-cardinality categorical features. Furthermore, state-of-the-art network architectures can be easily implemented into the proposed h-likelihood framework. As an example, we introduce multi-head attention layer and a sparsemax function, which allows feature selection in high-dimensional settings. To enhance practical performance and learning efficiency, we present an adjustment procedure for prediction of random parameters and a method-of-moments estimator for pretraining of variance component. Various experiential studies and real data analyses confirm the advantages of our proposed methods.", "url": "https://arxiv.org/abs/2310.11654"}, {"metadata": {"arXiv": "2310.11693", "Date": "Wed, 18 Oct 2023 03:43:11 ", "Title": "AUC-mixup: Deep AUC Maximization with Mixup", "Authors": ["Jianzhi Xv", "Gang Li and Tianbao Yang"], "Categories": "cs.LG eess.IV", "Comments": ["3 pages", "4 figures"]}, "abstract": "While deep AUC maximization (DAM) has shown remarkable success on imbalanced medical tasks, e.g., chest X-rays classification and skin lesions classification, it could suffer from severe overfitting when applied to small datasets due to its aggressive nature of pushing prediction scores of positive data away from that of negative data. This paper studies how to improve generalization of DAM by mixup data augmentation -- an approach that is widely used for improving generalization of the cross-entropy loss based deep learning methods. %For overfitting issues arising from limited data, the common approach is to employ mixup data augmentation to boost the models' generalization performance by enriching the training data. However, AUC is defined over positive and negative pairs, which makes it challenging to incorporate mixup data augmentation into DAM algorithms. To tackle this challenge, we employ the AUC margin loss and incorporate soft labels into the formulation to effectively learn from data generated by mixup augmentation, which is referred to as the AUC-mixup loss. Our experimental results demonstrate the effectiveness of the proposed AUC-mixup methods on imbalanced benchmark and medical image datasets compared to standard DAM training methods.", "url": "https://arxiv.org/abs/2310.11693"}, {"metadata": {"arXiv": "2310.11707", "Date": "Wed, 18 Oct 2023 04:39:25 ", "Title": "Learning under Label Proportions for Text Classification", "Authors": ["Jatin Chauhan", "Xiaoxuan Wang", "Wei Wang"], "Categories": "cs.LG", "Comments": ["accepted as long paper in Findings of EMNLP 2023"]}, "abstract": "We present one of the preliminary NLP works under the challenging setup of Learning from Label Proportions (LLP), where the data is provided in an aggregate form called bags and only the proportion of samples in each class as the ground truth. This setup is inline with the desired characteristics of training models under Privacy settings and Weakly supervision. By characterizing some irregularities of the most widely used baseline technique DLLP, we propose a novel formulation that is also robust. This is accompanied with a learnability result that provides a generalization bound under LLP. Combining this formulation with a self-supervised objective, our method achieves better results as compared to the baselines in almost 87% of the experimental configurations which include large scale models for both long and short range texts across multiple metrics.", "url": "https://arxiv.org/abs/2310.11707"}, {"metadata": {"arXiv": "2310.11714", "Date": "Wed, 18 Oct 2023 05:06:04 ", "Title": "On the Evaluation of Generative Models in Distributed Learning Tasks", "Authors": ["Zixiao Wang", "Farzan Farnia", "Zhenghao Lin", "Yunheng Shen", "Bei Yu"], "Categories": "cs.LG", "Comments": ["17 pages", "10 figures"]}, "abstract": "The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsistent, which can lead to different optimal generative models according to the two aggregate scores. Next, we consider the kernel inception distance (KID) and similarly define the KID-avg and KID-all aggregations. Unlike the FID case, we prove that KID-all and KID-avg result in the same rankings of generative models. We perform several numerical experiments on standard image datasets and training schemes to support our theoretical findings on the evaluation of generative models in distributed learning problems.", "url": "https://arxiv.org/abs/2310.11714"}, {"metadata": {"arXiv": "2310.11732", "Date": "Wed, 18 Oct 2023 06:07:28 ", "Title": "Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting", "Authors": ["Guande He", "Peng Cui", "Jianfei Chen", "Wenbo Hu", "Jun Zhu"], "Categories": "cs.LG cs.CL"}, "abstract": "Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc calibration methods for aligned LMs and propose an easy-to-implement and sample-efficient method to calibrate aligned LMs. We hope our findings could provide insights into the design of more reliable alignment processes for LMs.", "url": "https://arxiv.org/abs/2310.11732"}, {"metadata": {"arXiv": "2310.11739", "Date": "Wed, 18 Oct 2023 06:45:49 ", "Title": "Unintended Memorization in Large ASR Models, and How to Mitigate It", "Authors": ["Lun Wang", "Om Thakkar", "Rajiv Mathews"], "Categories": "cs.LG cs.SD eess.AS"}, "abstract": "It is well-known that neural networks can unintentionally memorize their training examples, causing privacy concerns. However, auditing memorization in large non-auto-regressive automatic speech recognition (ASR) models has been challenging due to the high compute cost of existing methods such as hardness calibration. In this work, we design a simple auditing method to measure memorization in large ASR models without the extra compute overhead. Concretely, we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. Hence, accurate predictions only for sped-up training examples can serve as clear evidence for memorization, and the corresponding accuracy can be used to measure memorization. Using the proposed method, we showcase memorization in the state-of-the-art ASR models. To mitigate memorization, we tried gradient clipping during training to bound the influence of any individual example on the final model. We empirically show that clipping each example's gradient can mitigate memorization for sped-up training examples with up to 16 repetitions in the training set. Furthermore, we show that in large-scale distributed training, clipping the average gradient on each compute core maintains neutral model quality and compute cost while providing strong privacy protection.", "url": "https://arxiv.org/abs/2310.11739"}, {"metadata": {"arXiv": "2310.11762", "Date": "Wed, 18 Oct 2023 07:39:05 ", "Title": "A Quasi-Wasserstein Loss for Learning Graph Neural Networks", "Authors": ["Minjie Cheng and Hongteng Xu"], "Categories": "cs.LG"}, "abstract": "When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a \"Quasi-Wasserstein\" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transport. When predicting node labels, our model combines the output of the GNN with the residual component provided by the optimal label transport, leading to a new transductive prediction paradigm. Experiments show that the proposed QW loss applies to various GNNs and helps to improve their performance in node-level classification and regression tasks.", "url": "https://arxiv.org/abs/2310.11762"}, {"metadata": {"arXiv": "2310.11787", "Date": "Wed, 18 Oct 2023 08:27:09 ", "Title": "NeuroCUT: A Neural Approach for Robust Graph Partitioning", "Authors": ["Rishi Shah", "Krishnanshu Jain", "Sahil Manchanda", "Sourav Medya and Sayan Ranu"], "Categories": "cs.LG"}, "abstract": "Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning based framework over node representations derived from a graph neural network, NeuroCut can accommodate any optimization objective, even those encompassing non-differentiable functions. Through empirical evaluation, we demonstrate that NeuroCut excels in identifying high-quality partitions, showcases strong generalization across a wide spectrum of partitioning objectives, and exhibits resilience to topological modifications.", "url": "https://arxiv.org/abs/2310.11787"}, {"metadata": {"arXiv": "2310.11789", "Date": "Wed, 18 Oct 2023 08:28:43 ", "Title": "Adversarial Training for Physics-Informed Neural Networks", "Authors": ["Yao Li", "Shengzhu Shi", "Zhichang Guo", "Boying Wu"], "Categories": "cs.LG cs.NA math.NA"}, "abstract": "Physics-informed neural networks have shown great promise in solving partial differential equations. However, due to insufficient robustness, vanilla PINNs often face challenges when solving complex PDEs, especially those involving multi-scale behaviors or solutions with sharp or oscillatory characteristics. To address these issues, based on the projected gradient descent adversarial attack, we proposed an adversarial training strategy for PINNs termed by AT-PINNs. AT-PINNs enhance the robustness of PINNs by fine-tuning the model with adversarial samples, which can accurately identify model failure locations and drive the model to focus on those regions during training. AT-PINNs can also perform inference with temporal causality by selecting the initial collocation points around temporal initial values. We implement AT-PINNs to the elliptic equation with multi-scale coefficients, Poisson equation with multi-peak solutions, Burgers equation with sharp solutions and the Allen-Cahn equation. The results demonstrate that AT-PINNs can effectively locate and reduce failure regions. Moreover, AT-PINNs are suitable for solving complex PDEs, since locating failure regions through adversarial attacks is independent of the size of failure regions or the complexity of the distribution.", "url": "https://arxiv.org/abs/2310.11789"}, {"metadata": {"arXiv": "2310.11829", "Date": "Wed, 18 Oct 2023 09:31:21 ", "Title": "Towards Graph Foundation Models: A Survey and Beyond", "Authors": ["Jiawei Liu", "Cheng Yang", "Zhiyuan Lu", "Junze Chen", "Yibo Li", "Mengmei Zhang", "Ting Bai", "Yuan Fang", "Lichao Sun", "Philip S. Yu", "and Chuan Shi"], "Categories": "cs.LG"}, "abstract": "Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their reliance on graph neural networks and large language models. Beyond providing a comprehensive overview of the current landscape of graph foundation models, this article also discusses potential research directions for this evolving field.", "url": "https://arxiv.org/abs/2310.11829"}, {"metadata": {"arXiv": "2310.11840", "Date": "Wed, 18 Oct 2023 09:46:01 ", "Title": "On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning", "Authors": ["Rohan Subramani and Marcus Williams and Max Heitmann and Halfdan Holm and Charlie Griffin and Joar Skalse"], "Categories": "cs.LG"}, "abstract": "To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimise with current techniques. For example, we prove that each of Regularised RL, Outer Nonlinear Markov Rewards, Reward Machines, Linear Temporal Logic, and Limit Average Rewards can express an objective that the others cannot. Our findings have implications for both policy optimisation and reward learning. Firstly, we identify expressivity limitations which are important to consider when specifying objectives in practice. Secondly, our results highlight the need for future research which adapts reward learning to work with a variety of formalisms, since many existing reward learning methods implicitly assume that desired objectives can be expressed with Markovian rewards. Our work contributes towards a more cohesive understanding of the costs and benefits of different RL objective-specification formalisms.", "url": "https://arxiv.org/abs/2310.11840"}, {"metadata": {"arXiv": "2310.11845", "Date": "Wed, 18 Oct 2023 09:51:59 ", "Title": "Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning", "Authors": ["Yufei Kuang", "Xijun Li", "Jie Wang", "Fangzhou Zhu", "Meng Lu", "Zhihai Wang", "Jia Zeng", "Houqiang Li", "Yongdong Zhang", "Feng Wu"], "Categories": "cs.LG"}, "abstract": "Large-scale LP problems from industry usually contain much redundancy that severely hurts the efficiency and reliability of solving LPs, making presolve (i.e., the problem simplification module) one of the most critical components in modern LP solvers. However, how to design high-quality presolve routines -- that is, the program determining (P1) which presolvers to select, (P2) in what order to execute, and (P3) when to stop -- remains a highly challenging task due to the extensive requirements on expert knowledge and the large search space. Due to the sequential decision property of the task and the lack of expert demonstrations, we propose a simple and efficient reinforcement learning (RL) framework -- namely, reinforcement learning for presolve (RL4Presolve) -- to tackle (P1)-(P3) simultaneously. Specifically, we formulate the routine design task as a Markov decision process and propose an RL framework with adaptive action sequences to generate high-quality presolve routines efficiently. Note that adaptive action sequences help learn complex behaviors efficiently and adapt to various benchmarks. Experiments on two solvers (open-source and commercial) and eight benchmarks (real-world and synthetic) demonstrate that RL4Presolve significantly and consistently improves the efficiency of solving large-scale LPs, especially on benchmarks from industry. Furthermore, we optimize the hard-coded presolve routines in LP solvers by extracting rules from learned policies for simple and efficient deployment to Huawei's supply chain. The results show encouraging economic and academic potential for incorporating machine learning to modern solvers.", "url": "https://arxiv.org/abs/2310.11845"}, {"metadata": {"arXiv": "2310.11865", "Date": "Wed, 18 Oct 2023 10:28:29 ", "Title": "Effective and Efficient Federated Tree Learning on Hybrid Data", "Authors": ["Qinbin Li", "Chulin Xie", "Xiaojun Xu", "Xiaoyuan Liu", "Ce Zhang", "Bo Li", "Bingsheng He", "Dawn Song"], "Categories": "cs.LG"}, "abstract": "Federated learning has emerged as a promising distributed learning paradigm that facilitates collaborative learning among multiple parties without transferring raw data. However, most existing federated learning studies focus on either horizontal or vertical data settings, where the data of different parties are assumed to be from the same feature or sample space. In practice, a common scenario is the hybrid data setting, where data from different parties may differ both in the features and samples. To address this, we propose HybridTree, a novel federated learning approach that enables federated tree learning on hybrid data. We observe the existence of consistent split rules in trees. With the help of these split rules, we theoretically show that the knowledge of parties can be incorporated into the lower layers of a tree. Based on our theoretical analysis, we propose a layer-level solution that does not need frequent communication traffic to train a tree. Our experiments demonstrate that HybridTree can achieve comparable accuracy to the centralized setting with low computational and communication overhead. HybridTree can achieve up to 8 times speedup compared with the other baselines.", "url": "https://arxiv.org/abs/2310.11865"}, {"metadata": {"arXiv": "2310.11866", "Date": "Wed, 18 Oct 2023 10:29:58 ", "Title": "Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function", "Authors": ["Liu Liu", "Xuanqing Liu", "Cho-Jui Hsieh", "and Dacheng Tao"], "Categories": "cs.LG", "Comments": ["arXiv admin note: text overlap with arXiv:1809.09853"]}, "abstract": "Trust-region (TR) and adaptive regularization using cubics (ARC) have proven to have some very appealing theoretical properties for non-convex optimization by concurrently computing function value, gradient, and Hessian matrix to obtain the next search direction and the adjusted parameters. Although stochastic approximations help largely reduce the computational cost, it is challenging to theoretically guarantee the convergence rate. In this paper, we explore a family of stochastic TR and ARC methods that can simultaneously provide inexact computations of the Hessian matrix, gradient, and function values. Our algorithms require much fewer propagations overhead per iteration than TR and ARC. We prove that the iteration complexity to achieve $\\epsilon$-approximate second-order optimality is of the same order as the exact computations demonstrated in previous studies. Additionally, the mild conditions on inexactness can be met by leveraging a random sampling technology in the finite-sum minimization problem. Numerical experiments with a non-convex problem support these findings and demonstrate that, with the same or a similar number of iterations, our algorithms require less computational overhead per iteration than current second-order methods.", "url": "https://arxiv.org/abs/2310.11866"}, {"metadata": {"arXiv": "2310.11875", "Date": "Wed, 18 Oct 2023 10:49:29 ", "Title": "Fractional Concepts in Neural Networks: Enhancing Activation and Loss Functions", "Authors": ["Zahra Alijani", "Vojtech Molek"], "Categories": "cs.LG cs.CV", "Comments": ["12 pages", "6 figures", "submitted to Neurocomputing journal"]}, "abstract": "The paper presents a method for using fractional concepts in a neural network to modify the activation and loss functions. The methodology allows the neural network to define and optimize its activation functions by determining the fractional derivative order of the training process as an additional hyperparameter. This will enable neurons in the network to adjust their activation functions to match input data better and reduce output errors, potentially improving the network's overall performance.", "url": "https://arxiv.org/abs/2310.11875"}, {"metadata": {"arXiv": "2310.11876", "Date": "Wed, 18 Oct 2023 10:56:57 ", "Title": "SQ Lower Bounds for Learning Mixtures of Linear Classifiers", "Authors": ["Ilias Diakonikolas", "Daniel M. Kane and Yuxin Sun"], "Categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "Comments": ["To appear in NeurIPS 2023"]}, "abstract": "We study the problem of learning mixtures of linear classifiers under Gaussian covariates. Given sample access to a mixture of $r$ distributions on $\\mathbb{R}^n$ of the form $(\\mathbf{x},y_{\\ell})$, $\\ell\\in [r]$, where $\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{I}_n)$ and $y_\\ell=\\mathrm{sign}(\\langle\\mathbf{v}_\\ell,\\mathbf{x}\\rangle)$ for an unknown unit vector $\\mathbf{v}_\\ell$, the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible, even for the special case of uniform mixtures. In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\\mathrm{poly}(1/\\Delta) \\log(r)}$, where $\\Delta$ is a lower bound on the pairwise $\\ell_2$-separation between the $\\mathbf{v}_\\ell$'s. The key technical ingredient underlying our result is a new construction of spherical designs that may be of independent interest.", "url": "https://arxiv.org/abs/2310.11876"}, {"metadata": {"arXiv": "2310.11880", "Date": "Wed, 18 Oct 2023 11:06:06 ", "Title": "Online Convex Optimization with Switching Cost and Delayed Gradients", "Authors": ["Spandan Senapati", "Rahul Vaze"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["35 pages", "accepted at IFIP Performance'23", "appears in Elsevier Performance Evaluation"], "DOI": "doi.org/10.1016/j.peva.2023.102371"}, "abstract": "We consider the online convex optimization (OCO) problem with quadratic and linear switching cost in the limited information setting, where an online algorithm can choose its action using only gradient information about the previous objective function. For $L$-smooth and $\\mu$-strongly convex objective functions, we propose an online multiple gradient descent (OMGD) algorithm and show that its competitive ratio for the OCO problem with quadratic switching cost is at most $4(L + 5) + \\frac{16(L + 5)}{\\mu}$. The competitive ratio upper bound for OMGD is also shown to be order-wise tight in terms of $L,\\mu$. In addition, we show that the competitive ratio of any online algorithm is $\\max\\{\\Omega(L), \\Omega(\\frac{L}{\\sqrt{\\mu}})\\}$ in the limited information setting when the switching cost is quadratic. We also show that the OMGD algorithm achieves the optimal (order-wise) dynamic regret in the limited information setting. For the linear switching cost, the competitive ratio upper bound of the OMGD algorithm is shown to depend on both the path length and the squared path length of the problem instance, in addition to $L, \\mu$, and is shown to be order-wise, the best competitive ratio any online algorithm can achieve. Consequently, we conclude that the optimal competitive ratio for the quadratic and linear switching costs are fundamentally different in the limited information setting.", "url": "https://arxiv.org/abs/2310.11880"}, {"metadata": {"arXiv": "2310.11897", "Date": "Wed, 18 Oct 2023 11:33:22 ", "Title": "Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning", "Authors": ["Yen-Ju Chen", "Nai-Chieh Huang", "Ping-Chun Hsieh"], "Categories": "cs.LG", "Comments": ["51 pages", "8 figures"]}, "abstract": "Policy gradient methods have recently been shown to enjoy global convergence at a $\\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \\textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-concave regime, where APG could benefit significantly from the momentum, within finite iterations. By means of numerical validation, we confirm that APG exhibits $\\tilde{O}(1/t^2)$ rate as well as show that APG could significantly improve the convergence behavior over the standard policy gradient.", "url": "https://arxiv.org/abs/2310.11897"}, {"metadata": {"arXiv": "2310.11952", "Date": "Wed, 18 Oct 2023 13:26:52 ", "Title": "Recasting Continual Learning as Sequence Modeling", "Authors": ["Soochan Lee", "Jaehyeon Son", "Gunhee Kim"], "Categories": "cs.LG", "Comments": ["NeurIPS 2023"]}, "abstract": "In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.", "url": "https://arxiv.org/abs/2310.11952"}, {"metadata": {"arXiv": "2310.11966", "Date": "Wed, 18 Oct 2023 13:45:17 ", "Title": "Flexible Payload Configuration for Satellites using Machine Learning", "Authors": ["Marcele O. K. Mendonca", "Flor G. Ortiz-Gomez", "Jorge Querol", "Eva Lagunas", "Juan A. V\\'asquez Peralvo", "Victor Monzon Baeza", "Symeon Chatzinotas and Bjorn Ottersten"], "Categories": "cs.LG cs.SY eess.SY", "Comments": ["in review for conference"]}, "abstract": "Satellite communications, essential for modern connectivity, extend access to maritime, aeronautical, and remote areas where terrestrial networks are unfeasible. Current GEO systems distribute power and bandwidth uniformly across beams using multi-beam footprints with fractional frequency reuse. However, recent research reveals the limitations of this approach in heterogeneous traffic scenarios, leading to inefficiencies. To address this, this paper presents a machine learning (ML)-based approach to Radio Resource Management (RRM). We treat the RRM task as a regression ML problem, integrating RRM objectives and constraints into the loss function that the ML algorithm aims at minimizing. Moreover, we introduce a context-aware ML metric that evaluates the ML model's performance but also considers the impact of its resource allocation decisions on the overall performance of the communication system.", "url": "https://arxiv.org/abs/2310.11966"}, {"metadata": {"arXiv": "2310.11984", "Date": "Wed, 18 Oct 2023 14:10:47 ", "Title": "From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers", "Authors": ["Shaoxiong Duan and Yining Shi"], "Categories": "cs.LG cs.CL"}, "abstract": "Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.", "url": "https://arxiv.org/abs/2310.11984"}, {"metadata": {"arXiv": "2310.11985", "Date": "Wed, 18 Oct 2023 14:11:41 ", "Title": "A Finite-Horizon Approach to Active Level Set Estimation", "Authors": ["Phillip Kearns", "Bruno Jedynak", "John Lipor"], "Categories": "cs.LG cs.RO stat.ML"}, "abstract": "We consider the problem of active learning in the context of spatial sampling for level set estimation (LSE), where the goal is to localize all regions where a function of interest lies above/below a given threshold as quickly as possible. We present a finite-horizon search procedure to perform LSE in one dimension while optimally balancing both the final estimation error and the distance traveled for a fixed number of samples. A tuning parameter is used to trade off between the estimation accuracy and distance traveled. We show that the resulting optimization problem can be solved in closed form and that the resulting policy generalizes existing approaches to this problem. We then show how this approach can be used to perform level set estimation in higher dimensions under the popular Gaussian process model. Empirical results on synthetic data indicate that as the cost of travel increases, our method's ability to treat distance nonmyopically allows it to significantly improve on the state of the art. On real air quality data, our approach achieves roughly one fifth the estimation error at less than half the cost of competing algorithms.", "url": "https://arxiv.org/abs/2310.11985"}, {"metadata": {"arXiv": "2310.11989", "Date": "Wed, 18 Oct 2023 14:20:55 ", "Title": "Image Clustering with External Guidance", "Authors": ["Yunfan Li", "Peng Hu", "Dezhong Peng", "Jiancheng Lv", "Jianping Fan", "Xi Peng"], "Categories": "cs.LG"}, "abstract": "The core of clustering is incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering, even though it seems irrelevant to the given data. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance the feature discriminability. Then, to improve image clustering performance, TAC collaborates text and image modalities by mutually distilling cross-modal neighborhood information. Experiments demonstrate that TAC achieves state-of-the-art performance on five widely used and three more challenging image clustering benchmarks, including the full ImageNet-1K dataset.", "url": "https://arxiv.org/abs/2310.11989"}, {"metadata": {"arXiv": "2310.11991", "Date": "Wed, 18 Oct 2023 14:22:36 ", "Title": "Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation", "Authors": ["Floris Holstege", "Bram Wouters", "Noud van Giersbergen", "Cees Diks"], "Categories": "cs.LG stat.ML", "Comments": ["Preprint. Under Review. 33 pages"]}, "abstract": "Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods", "url": "https://arxiv.org/abs/2310.11991"}, {"metadata": {"arXiv": "2310.12001", "Date": "Wed, 18 Oct 2023 14:32:20 ", "Title": "Bayesian Flow Networks in Continual Learning", "Authors": ["Mateusz Pyla", "Kamil Deja", "Bart{\\l}omiej Twardowski", "Tomasz Trzci\\'nski"], "Categories": "cs.LG cs.CV stat.ML", "Comments": ["Submitted to NeurIPS 2023 Workshop on Diffusion Models"]}, "abstract": "Bayesian Flow Networks (BFNs) has been recently proposed as one of the most promising direction to universal generative modelling, having ability to learn any of the data type. Their power comes from the expressiveness of neural networks and Bayesian inference which make them suitable in the context of continual learning. We delve into the mechanics behind BFNs and conduct the experiments to empirically verify the generative capabilities on non-stationary data.", "url": "https://arxiv.org/abs/2310.12001"}, {"metadata": {"arXiv": "2310.12032", "Date": "Wed, 18 Oct 2023 15:16:24 ", "Title": "Exact and efficient solutions of the LMC Multitask Gaussian Process model", "Authors": ["Olivier Truffinet (CEA Saclay)", "Karim Ammar (CEA Saclay)", "Jean-Philippe Argaud (EDF R&D)", "Bertrand Bouriquet (EDF)"], "Categories": "cs.LG stat.ML", "Comments": ["21 pages", "5 figures", "submitted to AISTATS"], "ACM-class": "I.2.6"}, "abstract": "The Linear Model of Co-regionalization (LMC) is a very general model of multitask gaussian process for regression or classification. While its expressivity and conceptual simplicity are appealing, naive implementations have cubic complexity in the number of datapoints and number of tasks, making approximations mandatory for most applications. However, recent work has shown that under some conditions the latent processes of the model can be decoupled, leading to a complexity that is only linear in the number of said processes. We here extend these results, showing from the most general assumptions that the only condition necessary to an efficient exact computation of the LMC is a mild hypothesis on the noise model. We introduce a full parametrization of the resulting \\emph{projected LMC} model, and an expression of the marginal likelihood enabling efficient optimization. We perform a parametric study on synthetic data to show the excellent performance of our approach, compared to an unrestricted exact LMC and approximations of the latter. Overall, the projected LMC appears as a credible and simpler alternative to state-of-the art models, which greatly facilitates some computations such as leave-one-out cross-validation and fantasization.", "url": "https://arxiv.org/abs/2310.12032"}, {"metadata": {"arXiv": "2310.12033", "Date": "Wed, 18 Oct 2023 15:17:10 ", "Title": "Conformal Drug Property Prediction with Density Estimation under Covariate Shift", "Authors": ["Siddhartha Laghuvarapu and Zhen Lin and Jimeng Sun"], "Categories": "cs.LG stat.ML", "Comments": ["Accepted at NeurIPS 2023"]}, "abstract": "In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift. In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, we demonstrate the ability of CoDrug to provide valid prediction sets and its utility in addressing the distribution shift arising from de novo drug design models. On average, using CoDrug can reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.", "url": "https://arxiv.org/abs/2310.12033"}, {"metadata": {"arXiv": "2310.12055", "Date": "Wed, 18 Oct 2023 15:42:53 ", "Title": "Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning", "Authors": ["Ali Baheri"], "Categories": "cs.LG cs.SY eess.SY math.OC"}, "abstract": "In inverse reinforcement learning (IRL), the central objective is to infer underlying reward functions from observed expert behaviors in a way that not only explains the given data but also generalizes to unseen scenarios. This ensures robustness against reward ambiguity where multiple reward functions can equally explain the same expert behaviors. While significant efforts have been made in addressing this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation. This paper harnesses the optimal transport (OT) theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying reward ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle reward ambiguity in high-dimensional settings.", "url": "https://arxiv.org/abs/2310.12055"}, {"metadata": {"arXiv": "2310.12083", "Date": "Wed, 18 Oct 2023 16:24:23 ", "Title": "Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait", "Authors": ["Markus Gambietz", "Marlies Nitschke", "J\\\"org Miehling", "Anne Koelewijn"], "Categories": "cs.LG"}, "abstract": "Objective: As metabolic cost is a primary factor influencing humans' gait, we want to deepen our understanding of metabolic energy expenditure models. Therefore, this paper identifies the parameters and input variables, such as muscle or joint states, that contribute to accurate metabolic cost estimations. Methods: We explored the parameters of four metabolic energy expenditure models in a Monte Carlo sensitivity analysis. Then, we analysed the model parameters by their calculated sensitivity indices, physiological context, and the resulting metabolic rates during the gait cycle. The parameter combination with the highest accuracy in the Monte Carlo simulations represented a quasi-optimized model. In the second step, we investigated the importance of input parameters and variables by analysing the accuracy of neural networks trained with different input features. Results: Power-related parameters were most influential in the sensitivity analysis and the neural network-based feature selection. We observed that the quasi-optimized models produced negative metabolic rates, contradicting muscle physiology. Neural network-based models showed promising abilities but have been unable to match the accuracy of traditional metabolic energy expenditure models. Conclusion: We showed that power-related metabolic energy expenditure model parameters and inputs are most influential during gait. Furthermore, our results suggest that neural network-based metabolic energy expenditure models are viable. However, bigger datasets are required to achieve better accuracy. Significance: As there is a need for more accurate metabolic energy expenditure models, we explored which musculoskeletal parameters are essential when developing a model to estimate metabolic energy.", "url": "https://arxiv.org/abs/2310.12083"}, {"metadata": {"arXiv": "2310.12095", "Date": "Wed, 18 Oct 2023 16:38:23 ", "Title": "On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields", "Authors": ["Nicola Rares Franco", "Daniel Fraulin", "Andrea Manzoni and Paolo Zunino"], "Categories": "cs.LG cs.NA math.NA"}, "abstract": "Deep Learning is having a remarkable impact on the design of Reduced Order Models (ROMs) for Partial Differential Equations (PDEs), where it is exploited as a powerful tool for tackling complex problems for which classical methods might fail. In this respect, deep autoencoders play a fundamental role, as they provide an extremely flexible tool for reducing the dimensionality of a given problem by leveraging on the nonlinear capabilities of neural networks. Indeed, starting from this paradigm, several successful approaches have already been developed, which are here referred to as Deep Learning-based ROMs (DL-ROMs). Nevertheless, when it comes to stochastic problems parameterized by random fields, the current understanding of DL-ROMs is mostly based on empirical evidence: in fact, their theoretical analysis is currently limited to the case of PDEs depending on a finite number of (deterministic) parameters. The purpose of this work is to extend the existing literature by providing some theoretical insights about the use of DL-ROMs in the presence of stochasticity generated by random fields. In particular, we derive explicit error bounds that can guide domain practitioners when choosing the latent dimension of deep autoencoders. We evaluate the practical usefulness of our theory by means of numerical experiments, showing how our analysis can significantly impact the performance of DL-ROMs.", "url": "https://arxiv.org/abs/2310.12095"}, {"metadata": {"arXiv": "2310.12107", "Date": "Wed, 18 Oct 2023 17:01:32 ", "Title": "An Online Learning Theory of Brokerage", "Authors": ["Nata\\v{s}a Boli\\'c", "Tommaso Cesari", "Roberto Colomboni"], "Categories": "cs.LG"}, "abstract": "We investigate brokerage between traders from an online learning perspective. At any round $t$, two traders arrive with their private valuations, and the broker proposes a trading price. Unlike other bilateral trade problems already studied in the online learning literature, we focus on the case where there are no designated buyer and seller roles: each trader will attempt to either buy or sell depending on the current price of the good. We assume the agents' valuations are drawn i.i.d. from a fixed but unknown distribution. If the distribution admits a density bounded by some constant $M$, then, for any time horizon $T$: $\\bullet$ If the agents' valuations are revealed after each interaction, we provide an algorithm achieving regret $M \\log T$ and show this rate is optimal, up to constant factors. $\\bullet$ If only their willingness to sell or buy at the proposed price is revealed after each interaction, we provide an algorithm achieving regret $\\sqrt{M T}$ and show this rate is optimal, up to constant factors. Finally, if we drop the bounded density assumption, we show that the optimal rate degrades to $\\sqrt{T}$ in the first case, and the problem becomes unlearnable in the second.", "url": "https://arxiv.org/abs/2310.12107"}, {"metadata": {"arXiv": "2310.12109", "Date": "Wed, 18 Oct 2023 17:06:22 ", "Title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture", "Authors": ["Daniel Y. Fu", "Simran Arora", "Jessica Grogan", "Isys Johnson", "Sabri Eyuboglu", "Armin W. Thomas", "Benjamin Spector", "Michael Poli", "Atri Rudra", "Christopher R\\'e"], "Categories": "cs.LG", "Comments": ["NeurIPS 2023 (Oral)"]}, "abstract": "Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.", "url": "https://arxiv.org/abs/2310.12109"}, {"metadata": {"arXiv": "2310.12121", "Date": "Wed, 18 Oct 2023 17:21:01 ", "Title": "Automatic prediction of mortality in patients with mental illness using electronic health records", "Authors": ["Sean Kim and Samuel Kim"], "Categories": "cs.LG"}, "abstract": "Mental disorders impact the lives of millions of people globally, not only impeding their day-to-day lives but also markedly reducing life expectancy. This paper addresses the persistent challenge of predicting mortality in patients with mental diagnoses using predictive machine-learning models with electronic health records (EHR). Data from patients with mental disease diagnoses were extracted from the well-known clinical MIMIC-III data set utilizing demographic, prescription, and procedural information. Four machine learning algorithms (Logistic Regression, Random Forest, Support Vector Machine, and K-Nearest Neighbors) were used, with results indicating that Random Forest and Support Vector Machine models outperformed others, with AUC scores of 0.911. Feature importance analysis revealed that drug prescriptions, particularly Morphine Sulfate, play a pivotal role in prediction. We applied a variety of machine learning algorithms to predict 30-day mortality followed by feature importance analysis. This study can be used to assist hospital workers in identifying at-risk patients to reduce excess mortality.", "url": "https://arxiv.org/abs/2310.12121"}, {"metadata": {"arXiv": "2310.12143", "Date": "Wed, 18 Oct 2023 17:54:29 ", "Title": "Simple Mechanisms for Representing, Indexing and Manipulating Concepts", "Authors": ["Yuanzhi Li", "Raghu Meka", "Rina Panigrahy", "Kulin Shah"], "Categories": "cs.LG cs.CL stat.ML", "Comments": ["19 pages"]}, "abstract": "Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.", "url": "https://arxiv.org/abs/2310.12143"}, {"metadata": {"arXiv": "2310.11590", "Date": "Tue, 17 Oct 2023 21:12:32 ", "Title": "Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios", "Authors": ["Qiping Zhang", "Nathan Tsoi", "Booyeon Choi", "Jie Tan", "Hao-Tien Lewis Chiang", "Marynel V\\'azquez"], "Categories": "cs.RO cs.LG"}, "abstract": "Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Also, when evaluating results as binary classification (rather than multiclass classification), the F1-Score of human predictions and machine learning models more than doubles, showing that both are better at telling the directionality of robot performance than predicting exact performance ratings. Based on our findings, we provide guidelines for implementing these predictions models in real-world navigation scenarios.", "url": "https://arxiv.org/abs/2310.11590"}, {"metadata": {"arXiv": "2310.12077", "Date": "Wed, 18 Oct 2023 16:13:35 ", "Title": "One-Shot Imitation Learning: A Pose Estimation Perspective", "Authors": ["Pietro Vitiello", "Kamil Dreczkowski", "Edward Johns"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["Published at the 7th Conference on Robot Learning (CoRL 2023). For more details please visit https://www.robot-learning.uk/pose-estimation-perspective"]}, "abstract": "In this paper, we study imitation learning under the challenging setting of: (1) only a single demonstration, (2) no further data collection, and (3) no prior task or object knowledge. We show how, with these constraints, imitation learning can be formulated as a combination of trajectory transfer and unseen object pose estimation. To explore this idea, we provide an in-depth study on how state-of-the-art unseen object pose estimators perform for one-shot imitation learning on ten real-world tasks, and we take a deep dive into the effects that camera calibration, pose estimation error, and spatial generalisation have on task success rates. For videos, please visit https://www.robot-learning.uk/pose-estimation-perspective.", "url": "https://arxiv.org/abs/2310.12077"}, {"metadata": {"arXiv": "2310.11690", "Date": "Wed, 18 Oct 2023 03:36:10 ", "Title": "Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance", "Authors": ["Yang Li", "Jiting Cao", "Yan Xu", "Lipeng Zhu", "Zhao Yang Dong"], "Categories": "eess.SY cs.LG cs.SY", "Comments": ["Accepted by Renewable and Sustainable Energy Reviews"]}, "abstract": "Most existing data-driven power system short-term voltage stability assessment (STVSA) approaches presume class-balanced input data. However, in practical applications, the occurrence of short-term voltage instability following a disturbance is minimal, leading to a significant class imbalance problem and a consequent decline in classifier performance. This work proposes a Transformer-based STVSA method to address this challenge. By utilizing the basic Transformer architecture, a stability assessment Transformer (StaaT) is developed {as a classification model to reflect the correlation between the operational states of the system and the resulting stability outcomes}. To combat the negative impact of imbalanced datasets, this work employs a conditional Wasserstein generative adversarial network with gradient penalty (CWGAN-GP) for synthetic data generation, aiding in the creation of a balanced, representative training set for the classifier. Semi-supervised clustering learning is implemented to enhance clustering quality, addressing the lack of a unified quantitative criterion for short-term voltage stability. {Numerical tests on the IEEE 39-bus test system extensively demonstrate that the proposed method exhibits robust performance under class imbalances up to 100:1 and noisy environments, and maintains consistent effectiveness even with an increased penetration of renewable energy}. Comparative results reveal that the CWGAN-GP generates more balanced datasets than traditional oversampling methods and that the StaaT outperforms other deep learning algorithms. This study presents a compelling solution for real-world STVSA applications that often face class imbalance and data noise challenges.", "url": "https://arxiv.org/abs/2310.11690"}, {"metadata": {"arXiv": "2310.12144", "Date": "Wed, 18 Oct 2023 17:55:12 ", "Title": "Dynamic financial processes identification using sparse regressive reservoir computers", "Authors": ["Fredy Vides", "Idelfonso B. R. Nogueira", "Lendy Banegas", "Evelyn Flores"], "Categories": "eess.SY cs.LG cs.SY math.OC", "Comments": ["The content of this publication represents the opinion of the researchers affiliated with the Department of Statistics and Research", "but not the official opinion of the CNBS"]}, "abstract": "In this document, we present key findings in structured matrix approximation theory, with applications to the regressive representation of dynamic financial processes. Initially, we explore a comprehensive approach involving generic nonlinear time delay embedding for time series data extracted from a financial or economic system under examination. Subsequently, we employ sparse least-squares and structured matrix approximation methods to discern approximate representations of the output coupling matrices. These representations play a pivotal role in establishing the regressive models corresponding to the recursive structures inherent in a given financial system. The document further introduces prototypical algorithms that leverage the aforementioned techniques. These algorithms are demonstrated through applications in approximate identification and predictive simulation of dynamic financial and economic processes, encompassing scenarios that may or may not exhibit chaotic behavior.", "url": "https://arxiv.org/abs/2310.12144"}, {"metadata": {"arXiv": "2310.11614", "Date": "Tue, 17 Oct 2023 22:28:13 ", "Title": "Learning a Hierarchical Planner from Humans in Multiple Generations", "Authors": ["Leonardo Hernandez Cano", "Yewen Pu", "Robert D. Hawkins", "Josh Tenenbaum", "Armando Solar-Lezama"], "Categories": "cs.AI", "Comments": ["First two authors contributed equally"]}, "abstract": "A typical way in which a machine acquires knowledge from humans is by programming. Compared to learning from demonstrations or experiences, programmatic learning allows the machine to acquire a novel skill as soon as the program is written, and, by building a library of programs, a machine can quickly learn how to perform complex tasks. However, as programs often take their execution contexts for granted, they are brittle when the contexts change, making it difficult to adapt complex programs to new contexts. We present natural programming, a library learning system that combines programmatic learning with a hierarchical planner. Natural programming maintains a library of decompositions, consisting of a goal, a linguistic description of how this goal decompose into sub-goals, and a concrete instance of its decomposition into sub-goals. A user teaches the system via curriculum building, by identifying a challenging yet not impossible goal along with linguistic hints on how this goal may be decomposed into sub-goals. The system solves for the goal via hierarchical planning, using the linguistic hints to guide its probability distribution in proposing the right plans. The system learns from this interaction by adding newly found decompositions in the successful search into its library. Simulated studies and a human experiment (n=360) on a controlled environment demonstrate that natural programming can robustly compose programs learned from different users and contexts, adapting faster and solving more complex tasks when compared to programmatic baselines.", "url": "https://arxiv.org/abs/2310.11614"}, {"metadata": {"arXiv": "2310.11709", "Date": "Wed, 18 Oct 2023 04:54:22 ", "Title": "Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT", "Authors": ["Zhen Zhang", "Bingqiao Luo", "Shengliang Lu", "Bingsheng He"], "Categories": "cs.AI", "Comments": ["Accepted by NeurIPS 2023", "Datasets and Benchmarks Track"]}, "abstract": "Numerous studies have been conducted to investigate the properties of large-scale temporal graphs. Despite the ubiquity of these graphs in real-world scenarios, it's usually impractical for us to obtain the whole real-time graphs due to privacy concerns and technical limitations. In this paper, we introduce the concept of {\\it Live Graph Lab} for temporal graphs, which enables open, dynamic and real transaction graphs from blockchains. Among them, Non-fungible tokens (NFTs) have become one of the most prominent parts of blockchain over the past several years. With more than \\$40 billion market capitalization, this decentralized ecosystem produces massive, anonymous and real transaction activities, which naturally forms a complicated transaction network. However, there is limited understanding about the characteristics of this emerging NFT ecosystem from a temporal graph analysis perspective. To mitigate this gap, we instantiate a live graph with NFT transaction network and investigate its dynamics to provide new observations and insights. Specifically, through downloading and parsing the NFT transaction activities, we obtain a temporal graph with more than 4.5 million nodes and 124 million edges. Then, a series of measurements are presented to understand the properties of the NFT ecosystem. Through comparisons with social, citation, and web networks, our analyses give intriguing findings and point out potential directions for future exploration. Finally, we also study machine learning models in this live graph to enrich the current datasets and provide new opportunities for the graph community. The source codes and dataset are available at https://livegraphlab.github.io.", "url": "https://arxiv.org/abs/2310.11709"}, {"metadata": {"arXiv": "2310.11723", "Date": "Wed, 18 Oct 2023 05:42:51 ", "Title": "Uncertainty in Automated Ontology Matching: Lessons Learned from an Empirical Experimentation", "Authors": ["In\\`es Osman", "Salvatore F. Pileggi", "Sadok Ben Yahia"], "Categories": "cs.AI"}, "abstract": "Data integration is considered a classic research field and a pressing need within the information science community. Ontologies play a critical role in such a process by providing well-consolidated support to link and semantically integrate datasets via interoperability. This paper approaches data integration from an application perspective, looking at techniques based on ontology matching. An ontology-based process may only be considered adequate by assuming manual matching of different sources of information. However, since the approach becomes unrealistic once the system scales up, automation of the matching process becomes a compelling need. Therefore, we have conducted experiments on actual data with the support of existing tools for automatic ontology matching from the scientific community. Even considering a relatively simple case study (i.e., the spatio-temporal alignment of global indicators), outcomes clearly show significant uncertainty resulting from errors and inaccuracies along the automated matching process. More concretely, this paper aims to test on real-world data a bottom-up knowledge-building approach, discuss the lessons learned from the experimental results of the case study, and draw conclusions about uncertainty and uncertainty management in an automated ontology matching process. While the most common evaluation metrics clearly demonstrate the unreliability of fully automated matching solutions, properly designed semi-supervised approaches seem to be mature for a more generalized application.", "url": "https://arxiv.org/abs/2310.11723"}, {"metadata": {"arXiv": "2310.11731", "Date": "Wed, 18 Oct 2023 06:07:10 ", "Title": "Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning", "Authors": ["Jianlan Luo", "Perry Dong", "Jeffrey Wu", "Aviral Kumar", "Xinyang Geng", "Sergey Levine"], "Categories": "cs.AI"}, "abstract": "The offline reinforcement learning (RL) paradigm provides a general recipe to convert static behavior datasets into policies that can perform better than the policy that collected the data. While policy constraints, conservatism, and other methods for mitigating distributional shifts have made offline reinforcement learning more effective, the continuous action setting often necessitates various approximations for applying these techniques. Many of these challenges are greatly alleviated in discrete action settings, where offline RL constraints and regularizers can often be computed more precisely or even exactly. In this paper, we propose an adaptive scheme for action quantization. We use a VQ-VAE to learn state-conditioned action quantization, avoiding the exponential blowup that comes with na\\\"ive discretization of the action space. We show that several state-of-the-art offline RL methods such as IQL, CQL, and BRAC improve in performance on benchmarks when combined with our proposed discretization scheme. We further validate our approach on a set of challenging long-horizon complex robotic manipulation tasks in the Robomimic environment, where our discretized offline RL algorithms are able to improve upon their continuous counterparts by 2-3x. Our project page is at https://saqrl.github.io/", "url": "https://arxiv.org/abs/2310.11731"}, {"metadata": {"arXiv": "2310.11798", "Date": "Wed, 18 Oct 2023 08:38:42 ", "Title": "Auction-Based Scheduling", "Authors": ["Guy Avni", "Kaushik Mallik", "Suman Sadhukhan"], "Categories": "cs.AI"}, "abstract": "Many sequential decision-making tasks require satisfaction of multiple, partially contradictory objectives. Existing approaches are monolithic, namely all objectives are fulfilled using a single policy, which is a function that selects a sequence of actions. We present auction-based scheduling, a modular framework for multi-objective decision-making problems. Each objective is fulfilled using a separate policy, and the policies can be independently created, modified, and replaced. Understandably, different policies with conflicting goals may choose conflicting actions at a given time. In order to resolve conflicts, and compose policies, we employ a novel auction-based mechanism. We allocate a bounded budget to each policy, and at each step, the policies simultaneously bid from their available budgets for the privilege of being scheduled and choosing an action. Policies express their scheduling urgency using their bids and the bounded budgets ensure long-run scheduling fairness. We lay the foundations of auction-based scheduling using path planning problems on finite graphs with two temporal objectives. We present decentralized algorithms to synthesize a pair of policies, their initially allocated budgets, and bidding strategies. We consider three categories of decentralized synthesis problems, parameterized by the assumptions that the policies make on each other: (a) strong synthesis, with no assumptions and strongest guarantees, (b) assume-admissible synthesis, with weakest rationality assumptions, and (c) assume-guarantee synthesis, with explicit contract-based assumptions. For reachability objectives, we show that, surprisingly, decentralized assume-admissible synthesis is always possible when the out-degrees of all vertices are at most two.", "url": "https://arxiv.org/abs/2310.11798"}, {"metadata": {"arXiv": "2310.11818", "Date": "Wed, 18 Oct 2023 09:21:37 ", "Title": "IntentDial: An Intent Graph based Multi-Turn Dialogue System with Reasoning Path Visualization", "Authors": ["Zengguang Hao and Jie Zhang and Binxia Xu and Yafang Wang and Gerard de Melo and Xiaolong Li"], "Categories": "cs.AI", "Comments": ["4pages", "5 figures"]}, "abstract": "Intent detection and identification from multi-turn dialogue has become a widely explored technique in conversational agents, for example, voice assistants and intelligent customer services. The conventional approaches typically cast the intent mining process as a classification task. Although neural classifiers have proven adept at such classification tasks, the issue of neural network models often impedes their practical deployment in real-world settings. We present a novel graph-based multi-turn dialogue system called , which identifies a user's intent by identifying intent elements and a standard query from a dynamically constructed and extensible intent graph using reinforcement learning. In addition, we provide visualization components to monitor the immediate reasoning path for each turn of a dialogue, which greatly facilitates further improvement of the system.", "url": "https://arxiv.org/abs/2310.11818"}, {"metadata": {"arXiv": "2310.11841", "Date": "Wed, 18 Oct 2023 09:46:05 ", "Title": "Classification Aggregation without Unanimity", "Authors": ["Olivier Cailloux", "Matthieu Hervouin", "Ali I. Ozkes", "M. Remzi Sanver"], "Categories": "cs.AI cs.MA"}, "abstract": "A classification is a surjective mapping from a set of objects to a set of categories. A classification aggregation function aggregates every vector of classifications into a single one. We show that every citizen sovereign and independent classification aggregation function is essentially a dictatorship. This impossibility implies an earlier result of Maniquet and Mongin (2016), who show that every unanimous and independent classification aggregation function is a dictatorship. The relationship between the two impossibilities is reminiscent to the relationship between Wilson's and Arrow's impossibilities in preference aggregation. Moreover, while the Maniquet-Mongin impossibility rests on the existence of at least three categories, we propose an alternative proof technique that covers the case of two categories, except when the number of objects is also two. We also identify all independent and unanimous classification aggregation functions for the case of two categories and two objects.", "url": "https://arxiv.org/abs/2310.11841"}, {"metadata": {"arXiv": "2310.11846", "Date": "Wed, 18 Oct 2023 09:53:27 ", "Title": "Masked Pretraining for Multi-Agent Decision Making", "Authors": ["Jie Liu", "Yinmin Zhang", "Chuming Li", "Chao Yang", "Yaodong Yang", "Yu Liu", "Wanli Ouyang"], "Categories": "cs.AI", "Comments": ["17 pages"]}, "abstract": "Building a single generalist agent with zero-shot capability has recently sparked significant advancements in decision-making. However, extending this capability to multi-agent scenarios presents challenges. Most current works struggle with zero-shot capabilities, due to two challenges particular to the multi-agent settings: a mismatch between centralized pretraining and decentralized execution, and varying agent numbers and action spaces, making it difficult to create generalizable representations across diverse downstream tasks. To overcome these challenges, we propose a \\textbf{Mask}ed pretraining framework for \\textbf{M}ulti-\\textbf{a}gent decision making (MaskMA). This model, based on transformer architecture, employs a mask-based collaborative learning strategy suited for decentralized execution with partial observation. Moreover, MaskMA integrates a generalizable action representation by dividing the action space into actions toward self-information and actions related to other entities. This flexibility allows MaskMA to tackle tasks with varying agent numbers and thus different action spaces. Extensive experiments in SMAC reveal MaskMA, with a single model pretrained on 11 training maps, can achieve an impressive 77.8% zero-shot win rate on 60 unseen test maps by decentralized execution, while also performing effectively on other types of downstream tasks (\\textit{e.g.,} varied policies collaboration and ad hoc team play).", "url": "https://arxiv.org/abs/2310.11846"}, {"metadata": {"arXiv": "2310.11986", "Date": "Wed, 18 Oct 2023 14:13:58 ", "Title": "Sociotechnical Safety Evaluation of Generative AI Systems", "Authors": ["Laura Weidinger", "Maribeth Rauh", "Nahema Marchal", "Arianna Manzini", "Lisa Anne Hendricks", "Juan Mateos-Garcia", "Stevie Bergman", "Jackie Kay", "Conor Griffin", "Ben Bariach", "Iason Gabriel", "Verena Rieser", "William Isaac"], "Categories": "cs.AI cs.CL cs.CY", "Comments": ["main paper p.1-29", "5 figures", "2 tables"]}, "abstract": "Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. Sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative AI systems.", "url": "https://arxiv.org/abs/2310.11986"}, {"metadata": {"arXiv": "2310.12081", "Date": "Wed, 18 Oct 2023 16:16:53 ", "Title": "DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework", "Authors": ["Haoran Cheng", "Dixin Luo", "Hongteng Xu"], "Categories": "cs.AI"}, "abstract": "Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs. Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM. Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities. Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results. This method can be implemented as computing the HOT distance between the two graphs -- each matching result is an optimal transport plan associated with the Gromov-Wasserstein (GW) distance between two relational matrices, and the weights of all matching results are the elements of an upper-level optimal transport plan defined on the matrix sets. We propose a bi-level optimization algorithm to compute the HOT distance in a differentiable way, making the significance of the relational matrices adjustable. Experiments on various graph matching tasks demonstrate the superiority and robustness of our method compared to state-of-the-art approaches.", "url": "https://arxiv.org/abs/2310.12081"}, {"metadata": {"arXiv": "2310.12103", "Date": "Wed, 18 Oct 2023 16:46:16 ", "Title": "Quality Diversity through Human Feedback", "Authors": ["Li Ding", "Jenny Zhang", "Jeff Clune", "Lee Spector", "Joel Lehman"], "Categories": "cs.AI cs.NE"}, "abstract": "Reinforcement learning from human feedback (RLHF) has exhibited the potential to enhance the performance of foundation models for qualitative tasks. Despite its promise, its efficacy is often restricted when conceptualized merely as a mechanism to maximize learned reward models of averaged human preferences, especially in areas such as image generation which demand diverse model responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking diverse, high-quality solutions, are often constrained by the dependency on manually defined diversity metrics. Interestingly, such limitations of RLHF and QD can be overcome by blending insights from both. This paper introduces Quality Diversity through Human Feedback (QDHF), which employs human feedback for inferring diversity metrics, expanding the applicability of QD algorithms. Empirical results reveal that QDHF outperforms existing QD methods regarding automatic diversity discovery, and matches the search capabilities of QD with human-constructed metrics. Notably, when deployed for a latent space illumination task, QDHF markedly enhances the diversity of images generated by a Diffusion model. The study concludes with an in-depth analysis of QDHF's sample efficiency and the quality of its derived diversity metrics, emphasizing its promise for enhancing exploration and diversity in optimization for complex, open-ended tasks.", "url": "https://arxiv.org/abs/2310.12103"}, {"metadata": {"arXiv": "2310.11482", "Date": "Tue, 17 Oct 2023 13:06:39 ", "Title": "Rethinking Class-incremental Learning in the Era of Large Pre-trained Models via Test-Time Adaptation", "Authors": ["Imad Eddine Marouf", "Subhankar Roy", "Enzo Tartaglione", "St\\'ephane Lathuili\\`ere"], "Categories": "cs.CV cs.AI", "Comments": ["8 pages,5 figures"]}, "abstract": "Class-incremental learning (CIL) is a challenging task that involves continually learning to categorize classes into new tasks without forgetting previously learned information. The advent of the large pre-trained models (PTMs) has fast-tracked the progress in CIL due to the highly transferable PTM representations, where tuning a small set of parameters results in state-of-the-art performance when compared with the traditional CIL methods that are trained from scratch. However, repeated fine-tuning on each task destroys the rich representations of the PTMs and further leads to forgetting previous tasks. To strike a balance between the stability and plasticity of PTMs for CIL, we propose a novel perspective of eliminating training on every new task and instead performing test-time adaptation (TTA) directly on the test instances. Concretely, we propose \"Test-Time Adaptation for Class-Incremental Learning\" (TTACIL) that first fine-tunes Layer Norm parameters of the PTM on each test instance for learning task-specific features, and then resets them back to the base model to preserve stability. As a consequence, TTACIL does not undergo any forgetting, while benefiting each task with the rich PTM features. Additionally, by design, our method is robust to common data corruptions. Our TTACIL outperforms several state-of-the-art CIL methods when evaluated on multiple CIL benchmarks under both clean and corrupted data.", "url": "https://arxiv.org/abs/2310.11482"}, {"metadata": {"arXiv": "2310.11595", "Date": "Tue, 17 Oct 2023 21:43:42 ", "Title": "WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks", "Authors": ["Jun Xia", "Zhihao Yue", "Yingbo Zhou", "Zhiwei Ling", "Xian Wei", "Mingsong Chen"], "Categories": "cs.CV cs.AI"}, "abstract": "Due to the popularity of Artificial Intelligence (AI) technology, numerous backdoor attacks are designed by adversaries to mislead deep neural network predictions by manipulating training samples and training processes. Although backdoor attacks are effective in various real scenarios, they still suffer from the problems of both low fidelity of poisoned samples and non-negligible transfer in latent space, which make them easily detectable by existing backdoor detection algorithms. To overcome the weakness, this paper proposes a novel frequency-based backdoor attack method named WaveAttack, which obtains image high-frequency features through Discrete Wavelet Transform (DWT) to generate backdoor triggers. Furthermore, we introduce an asymmetric frequency obfuscation method, which can add an adaptive residual in the training and inference stage to improve the impact of triggers and further enhance the effectiveness of WaveAttack. Comprehensive experimental results show that WaveAttack not only achieves higher stealthiness and effectiveness, but also outperforms state-of-the-art (SOTA) backdoor attack methods in the fidelity of images by up to 28.27\\% improvement in PSNR, 1.61\\% improvement in SSIM, and 70.59\\% reduction in IS. Our code is available at https://anonymous.4open.science/r/AnonymousRep-701D.", "url": "https://arxiv.org/abs/2310.11595"}, {"metadata": {"arXiv": "2310.12007", "Date": "Wed, 18 Oct 2023 14:40:52 ", "Title": "KI-PMF: Knowledge Integrated Plausible Motion Forecasting", "Authors": ["Abhishek Vivekanandan", "Ahmed Abouelazm", "Philip Sch\\\"orner", "J. Marius Z\\\"ollner"], "Categories": "cs.RO cs.AI cs.CV"}, "abstract": "Accurately forecasting the motion of traffic actors is crucial for the deployment of autonomous vehicles at a large scale. Current trajectory forecasting approaches primarily concentrate on optimizing a loss function with a specific metric, which can result in predictions that do not adhere to physical laws or violate external constraints. Our objective is to incorporate explicit knowledge priors that allow a network to forecast future trajectories in compliance with both the kinematic constraints of a vehicle and the geometry of the driving environment. To achieve this, we introduce a non-parametric pruning layer and attention layers to integrate the defined knowledge priors. Our proposed method is designed to ensure reachability guarantees for traffic actors in both complex and dynamic situations. By conditioning the network to follow physical laws, we can obtain accurate and safe predictions, essential for maintaining autonomous vehicles' safety and efficiency in real-world settings.In summary, this paper presents concepts that prevent off-road predictions for safe and reliable motion forecasting by incorporating knowledge priors into the training process.", "url": "https://arxiv.org/abs/2310.12007"}, {"metadata": {"arXiv": "2310.11667", "Date": "Wed, 18 Oct 2023 02:27:01 ", "Title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents", "Authors": ["Xuhui Zhou", "Hao Zhu", "Leena Mathur", "Ruohong Zhang", "Haofei Yu", "Zhengyang Qi", "Louis-Philippe Morency", "Yonatan Bisk", "Daniel Fried", "Graham Neubig", "Maarten Sap"], "Categories": "cs.AI cs.CL cs.LG", "Comments": ["Preprint", "43 pages. The first two authors contribute equally"]}, "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.", "url": "https://arxiv.org/abs/2310.11667"}, {"metadata": {"arXiv": "2310.11884", "Date": "Wed, 18 Oct 2023 11:08:02 ", "Title": "From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks", "Authors": ["Jae Hee Lee", "Sergio Lanza and Stefan Wermter"], "Categories": "cs.AI cs.CL cs.CV cs.LG cs.NE", "Comments": ["Submitted to Neurosymbolic Artificial Intelligence (https://neurosymbolic-ai-journal.com/paper/neural-activations-concepts-survey-explaining-concepts-neural-networks)"]}, "abstract": "In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.", "url": "https://arxiv.org/abs/2310.11884"}, {"metadata": {"arXiv": "2310.12036", "Date": "Wed, 18 Oct 2023 15:21:28 ", "Title": "A General Theoretical Paradigm to Understand Learning from Human Preferences", "Authors": ["Mohammad Gheshlaghi Azar and Mark Rowland and Bilal Piot and Daniel Guo and Daniele Calandriello and Michal Valko and R\\'emi Munos"], "Categories": "cs.AI cs.LG stat.ML"}, "abstract": "The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation. In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $\\Psi$PO) and to identify their potential pitfalls. We then consider another special case for $\\Psi$PO by setting $\\Psi$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.", "url": "https://arxiv.org/abs/2310.12036"}, {"metadata": {"arXiv": "2310.11700", "Date": "Wed, 18 Oct 2023 04:15:39 ", "Title": "Runner re-identification from single-view video in the open-world setting", "Authors": ["Tomohiro Suzuki", "Kazushi Tsutsui", "Kazuya Takeda", "Keisuke Fujii"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["18 pages", "8 figures"]}, "abstract": "In many sports, player re-identification is crucial for automatic video processing and analysis. However, most of the current studies on player re-identification in multi- or single-view sports videos focus on re-identification in the closed-world setting using labeled image dataset, and player re-identification in the open-world setting for automatic video analysis is not well developed. In this paper, we propose a runner re-identification system that directly processes single-view video to address the open-world setting. In the open-world setting, we cannot use labeled dataset and have to process video directly. The proposed system automatically processes raw video as input to identify runners, and it can identify runners even when they are framed out multiple times. For the automatic processing, we first detect the runners in the video using the pre-trained YOLOv8 and the fine-tuned EfficientNet. We then track the runners using ByteTrack and detect their shoes with the fine-tuned YOLOv8. Finally, we extract the image features of the runners using an unsupervised method using the gated recurrent unit autoencoder model. To improve the accuracy of runner re-identification, we use dynamic features of running sequence images. We evaluated the system on a running practice video dataset and showed that the proposed method identified runners with higher accuracy than one of the state-of-the-art models in unsupervised re-identification. We also showed that our unsupervised running dynamic feature extractor was effective for runner re-identification. Our runner re-identification system can be useful for the automatic analysis of running videos.", "url": "https://arxiv.org/abs/2310.11700"}, {"metadata": {"arXiv": "2310.12031", "Date": "Wed, 18 Oct 2023 15:15:13 ", "Title": "SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment", "Authors": ["Tatiana Zemskova", "Margarita Kichik", "Dmitry Yudin", "Aleksei Staroverov", "Aleksandr Panov"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["14 pages", "6 figures"]}, "abstract": "This paper presents an adaptive transformer model named SegmATRon for embodied image semantic segmentation. Its distinctive feature is the adaptation of model weights during inference on several images using a hybrid multicomponent loss function. We studied this model on datasets collected in the photorealistic Habitat and the synthetic AI2-THOR Simulators. We showed that obtaining additional images using the agent's actions in an indoor environment can improve the quality of semantic segmentation. The code of the proposed approach and datasets are publicly available at https://github.com/wingrune/SegmATRon.", "url": "https://arxiv.org/abs/2310.12031"}, {"metadata": {"arXiv": "2310.12128", "Date": "Wed, 18 Oct 2023 17:37:10 ", "Title": "DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning", "Authors": ["Abhay Zala", "Han Lin", "Jaemin Cho", "Mohit Bansal"], "Categories": "cs.CV cs.AI cs.CL cs.LG", "Comments": ["Project page: https://diagrammerGPT.github.io/"]}, "abstract": "Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planner-auditor feedback loop) which describe all the entities (objects and text labels), their relationships (arrows or lines), and their bounding box layouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a text label rendering module to generate diagrams following the diagram plans. To benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a densely annotated diagram dataset built on top of the AI2D dataset. We show quantitatively and qualitatively that our DiagrammerGPT framework produces more accurate diagrams, outperforming existing T2I models. We also provide comprehensive analysis including open-domain diagram generation, vector graphic diagram generation in different platforms, human-in-the-loop diagram plan editing, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our work can inspire further research on diagram generation via T2I models and LLMs.", "url": "https://arxiv.org/abs/2310.12128"}, {"metadata": {"arXiv": "2310.11518", "Date": "Tue, 17 Oct 2023 18:33:21 ", "Title": "Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability", "Authors": ["Revan MacQueen", "James R. Wright"], "Categories": "cs.GT cs.AI cs.LG", "Comments": ["To appear at NeurIPS 2023"]}, "abstract": "Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multi-player games. We show that in games that approximately decompose into a set of two-player constant-sum games (called polymatrix games) where global $\\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each subgame, any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify a structural property of multi-player games that enable performance guarantees for the strategies produced by a broad class of self-play algorithms. We demonstrate our findings through experiments on Leduc poker.", "url": "https://arxiv.org/abs/2310.11518"}, {"metadata": {"arXiv": "2310.11465", "Date": "Fri, 13 Oct 2023 13:25:16 ", "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis", "Authors": ["Abdullah Al Imran", "Md Sakib Hossain Shovon", "M. F. Mridha"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "This study presents a large multi-modal Bangla YouTube clickbait dataset consisting of 253,070 data points collected through an automated process using the YouTube API and Python web automation frameworks. The dataset contains 18 diverse features categorized into metadata, primary content, engagement statistics, and labels for individual videos from 58 Bangla YouTube channels. A rigorous preprocessing step has been applied to denoise, deduplicate, and remove bias from the features, ensuring unbiased and reliable analysis. As the largest and most robust clickbait corpus in Bangla to date, this dataset provides significant value for natural language processing and data science researchers seeking to advance modeling of clickbait phenomena in low-resource languages. Its multi-modal nature allows for comprehensive analyses of clickbait across content, user interactions, and linguistic dimensions to develop more sophisticated detection methods with cross-linguistic applications.", "url": "https://arxiv.org/abs/2310.11465"}, {"metadata": {"arXiv": "2310.11466", "Date": "Sat, 14 Oct 2023 08:43:42 ", "Title": "Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction", "Authors": ["Yufei Huang", "Siyuan Li", "Jin Su", "Lirong Wu", "Odin Zhang", "Haitao Lin", "Jingqi Qi", "Zihan Liu", "Zhangyang Gao", "Jiangbin Zheng", "Stan.ZQ.Li"], "Categories": "cs.LG cs.AI q-bio.QM"}, "abstract": "Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem, we identify a Protein 3D Graph Structure Learning Problem for Robust Protein Property Prediction (PGSL-RP3), collect benchmark datasets, and present a protein Structure embedding Alignment Optimization framework (SAO) to mitigate the problem of structure embedding bias between the predicted and experimental protein structures. Extensive experiments have shown that our framework is model-agnostic and effective in improving the property prediction of both predicted structures and experimental structures. The benchmark datasets and codes will be released to benefit the community.", "url": "https://arxiv.org/abs/2310.11466"}, {"metadata": {"arXiv": "2310.11470", "Date": "Wed, 24 May 2023 13:38:38 ", "Title": "Classic machine learning methods", "Authors": ["Johann Faouzi and Olivier Colliot"], "Categories": "cs.LG cs.AI"}, "abstract": "In this chapter, we present the main classic machine learning methods. A large part of the chapter is devoted to supervised learning techniques for classification and regression, including nearest-neighbor methods, linear and logistic regressions, support vector machines and tree-based algorithms. We also describe the problem of overfitting as well as strategies to overcome it. We finally provide a brief overview of unsupervised learning methods, namely for clustering and dimensionality reduction.", "url": "https://arxiv.org/abs/2310.11470"}, {"metadata": {"arXiv": "2310.11477", "Date": "Tue, 17 Oct 2023 07:50:52 ", "Title": "Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function", "Authors": ["Khoa Tran", "Lam Pham", "Hai-Canh Vu"], "Categories": "cs.LG cs.AI"}, "abstract": "This paper presents a comprehensive analysis of motor bearing fault detection (MBFD), which involves the task of identifying faults in a motor bearing based on its vibration. To this end, we first propose and evaluate various machine learning based systems for the MBFD task. Furthermore, we propose three deep learning based systems for the MBFD task, each of which explores one of the following training strategies: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning based systems and deep learning based systems are evaluated, compared, and then they are used to identify the best model for the MBFD task. We conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn University (PU). The experimental results on different datasets highlight two main contributions of this study. First, we prove that deep learning based systems are more effective than machine learning based systems for the MBFD task. Second, we achieve a robust and general deep learning based system with a novel loss function for the MBFD task on several benchmark datasets, demonstrating its potential for real-life MBFD applications.", "url": "https://arxiv.org/abs/2310.11477"}, {"metadata": {"arXiv": "2310.11478", "Date": "Tue, 17 Oct 2023 09:36:22 ", "Title": "ASP: Automatic Selection of Proxy dataset for efficient AutoML", "Authors": ["Peng Yao", "Chao Liao", "Jiyuan Jia", "Jianchao Tan", "Bin Chen", "Chengru Song", "Di Zhang"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["This paper was actually finished in 2021"]}, "abstract": "Deep neural networks have gained great success due to the increasing amounts of data, and diverse effective neural network designs. However, it also brings a heavy computing burden as the amount of training data is proportional to the training time. In addition, a well-behaved model requires repeated trials of different structure designs and hyper-parameters, which may take a large amount of time even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms and neural architecture search (NAS) algorithms. In this paper, we propose an Automatic Selection of Proxy dataset framework (ASP) aimed to dynamically find the informative proxy subsets of training data at each epoch, reducing the training data size as well as saving the AutoML processing time. We verify the effectiveness and generalization of ASP on CIFAR10, CIFAR100, ImageNet16-120, and ImageNet-1k, across various public model benchmarks. The experiment results show that ASP can obtain better results than other data selection methods at all selection ratios. ASP can also enable much more efficient AutoML processing with a speedup of 2x-20x while obtaining better architectures and better hyper-parameters compared to utilizing the entire dataset.", "url": "https://arxiv.org/abs/2310.11478"}, {"metadata": {"arXiv": "2310.11523", "Date": "Tue, 17 Oct 2023 18:41:57 ", "Title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models", "Authors": ["Siyan Zhao", "John Dang", "Aditya Grover"], "Categories": "cs.LG cs.AI cs.CL", "Comments": ["24 pages", "12 figures"]}, "abstract": "Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users. Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods.", "url": "https://arxiv.org/abs/2310.11523"}, {"metadata": {"arXiv": "2310.11531", "Date": "Tue, 17 Oct 2023 19:01:08 ", "Title": "Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach", "Authors": ["Dengwang Tang", "Rahul Jain", "Botao Hao", "Zheng Wen"], "Categories": "cs.LG cs.AI cs.SY eess.SY stat.ML", "Comments": ["22 pages"], "MSC-class": "93E35"}, "abstract": "In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\\tilde{O}(\\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.", "url": "https://arxiv.org/abs/2310.11531"}, {"metadata": {"arXiv": "2310.11550", "Date": "Tue, 17 Oct 2023 19:43:37 ", "Title": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback", "Authors": ["Haolin Liu", "Chen-Yu Wei", "Julian Zimmert"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, ensures a regret of $\\widetilde{\\mathcal{O}}\\left(\\sqrt{K}\\right)$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, which is based on the policy optimization framework, guarantees a regret of $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{3}{4}} \\right)$ and is computationally efficient. Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023] with $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{4}{5}}+poly\\left(\\frac{1}{\\lambda_{\\min}}\\right) \\right)$ regret, for some problem-dependent constant $\\lambda_{\\min}$ that can be arbitrarily close to zero, and a computationally efficient algorithm by Sherman et al. [2023b] with $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{6}{7}} \\right)$ regret.", "url": "https://arxiv.org/abs/2310.11550"}, {"metadata": {"arXiv": "2310.11569", "Date": "Tue, 17 Oct 2023 20:30:16 ", "Title": "When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting", "Authors": ["Harshavardhan Kamarthi", "Lingkai Kong", "Alexander Rodr\\'iguez", "Chao Zhang", "B. Aditya Prakash"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at KDD 2023", "16 pages", "4 figures Updated Conference version of 2206.07940 with added baselines. arXiv admin note: text overlap with arXiv:2206.07940"]}, "abstract": "Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regularization to learn from hierarchical relations for entire forecast distribution that enables robust and calibrated forecasts as well as adapt to datasets of varying hierarchical consistency. On evaluating PROFHiT over wide range of datasets, we observed 41-88% better performance in accuracy and significantly better calibration. Due to modeling the coherency over full distribution, we observed that PROFHiT can robustly provide reliable forecasts even if up to 10% of input time-series data is missing where other methods' performance severely degrade by over 70%.", "url": "https://arxiv.org/abs/2310.11569"}, {"metadata": {"arXiv": "2310.11594", "Date": "Tue, 17 Oct 2023 21:38:41 ", "Title": "Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning", "Authors": ["Taejin Kim", "Jiarui Li", "Shubhranshu Singh", "Nikhil Madaan", "Carlee Joe-Wong"], "Categories": "cs.LG cs.AI", "Comments": ["8 pages", "6 main pages of text", "4 figures", "2 tables. Made for a Neurips workshop on backdoor attacks"]}, "abstract": "In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks. Our findings inform strategies for enhancing ARU to counter current defensive measures and highlight the limitations of existing defenses, offering insights into bolstering defenses against ARU.", "url": "https://arxiv.org/abs/2310.11594"}, {"metadata": {"arXiv": "2310.11664", "Date": "Wed, 18 Oct 2023 02:19:12 ", "Title": "Hetero$^2$Net: Heterophily-aware Representation Learning on Heterogenerous Graphs", "Authors": ["Jintang Li", "Zheng Wei", "Jiawang Dan", "Jing Zhou", "Yuchang Zhu", "Ruofan Wu", "Baokun Wang", "Zhang Zhen", "Changhua Meng", "Hong Jin", "Zibin Zheng", "Liang Chen"], "Categories": "cs.LG cs.AI", "Comments": ["Preprint"]}, "abstract": "Real-world graphs are typically complex, exhibiting heterogeneity in the global structure, as well as strong heterophily within local neighborhoods. While a growing body of literature has revealed the limitations of common graph neural networks (GNNs) in handling homogeneous graphs with heterophily, little work has been conducted on investigating the heterophily properties in the context of heterogeneous graphs. To bridge this research gap, we identify the heterophily in heterogeneous graphs using metapaths and propose two practical metrics to quantitatively describe the levels of heterophily. Through in-depth investigations on several real-world heterogeneous graphs exhibiting varying levels of heterophily, we have observed that heterogeneous graph neural networks (HGNNs), which inherit many mechanisms from GNNs designed for homogeneous graphs, fail to generalize to heterogeneous graphs with heterophily or low level of homophily. To address the challenge, we present Hetero$^2$Net, a heterophily-aware HGNN that incorporates both masked metapath prediction and masked label prediction tasks to effectively and flexibly handle both homophilic and heterophilic heterogeneous graphs. We evaluate the performance of Hetero$^2$Net on five real-world heterogeneous graph benchmarks with varying levels of heterophily. The results demonstrate that Hetero$^2$Net outperforms strong baselines in the semi-supervised node classification task, providing valuable insights into effectively handling more complex heterogeneous graphs.", "url": "https://arxiv.org/abs/2310.11664"}, {"metadata": {"arXiv": "2310.11676", "Date": "Wed, 18 Oct 2023 02:59:57 ", "Title": "PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection", "Authors": ["Junjun Pan", "Yixin Liu", "Yizhen Zheng", "Shirui Pan"], "Categories": "cs.LG cs.AI"}, "abstract": "Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies - reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreover, through rigorous evaluations of five real-world datasets, our method demonstrated robustness and effectiveness. Notably, when validated on the ACM dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training speed, and sharply reduce memory usage compared to the most efficient baseline.", "url": "https://arxiv.org/abs/2310.11676"}, {"metadata": {"arXiv": "2310.11677", "Date": "Wed, 18 Oct 2023 03:00:15 ", "Title": "Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes", "Authors": ["Washim Uddin Mondal and Vaneet Aggarwal"], "Categories": "cs.LG cs.AI"}, "abstract": "We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\\mathcal{O}({\\epsilon^{-2}})$ sample complexity and $\\mathcal{O}(\\epsilon^{-1})$ iteration complexity with general parameterization where $\\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\\log(\\frac{1}{\\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\\mathcal{O}(\\epsilon^{-\\frac{1}{2}})$ and simultaneously matches their state-of-the-art iteration complexity.", "url": "https://arxiv.org/abs/2310.11677"}, {"metadata": {"arXiv": "2310.11678", "Date": "Wed, 18 Oct 2023 03:00:59 ", "Title": "Using Experience Classification for Training Non-Markovian Tasks", "Authors": ["Ruixuan Miao", "Xu Lu", "Cong Tian", "Bin Yu", "Zhenhua Duan"], "Categories": "cs.LG cs.AI cs.FL cs.LO"}, "abstract": "Unlike the standard Reinforcement Learning (RL) model, many real-world tasks are non-Markovian, whose rewards are predicated on state history rather than solely on the current state. Solving a non-Markovian task, frequently applied in practical applications such as autonomous driving, financial trading, and medical diagnosis, can be quite challenging. We propose a novel RL approach to achieve non-Markovian rewards expressed in temporal logic LTL$_f$ (Linear Temporal Logic over Finite Traces). To this end, an encoding of linear complexity from LTL$_f$ into MDPs (Markov Decision Processes) is introduced to take advantage of advanced RL algorithms. Then, a prioritized experience replay technique based on the automata structure (semantics equivalent to LTL$_f$ specification) is utilized to improve the training process. We empirically evaluate several benchmark problems augmented with non-Markovian tasks to demonstrate the feasibility and effectiveness of our approach.", "url": "https://arxiv.org/abs/2310.11678"}, {"metadata": {"arXiv": "2310.11684", "Date": "Wed, 18 Oct 2023 03:17:51 ", "Title": "Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning", "Authors": ["Bhargav Ganguly and Vaneet Aggarwal"], "Categories": "cs.LG cs.AI quant-ph"}, "abstract": "This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\\tilde{\\mathcal{O}}(1)$, a significant improvement over the $\\tilde{\\mathcal{O}}(\\sqrt{T})$ bound exhibited by classical counterparts.", "url": "https://arxiv.org/abs/2310.11684"}, {"metadata": {"arXiv": "2310.11730", "Date": "Wed, 18 Oct 2023 05:59:41 ", "Title": "Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation", "Authors": ["Bo Yan", "Yang Cao", "Haoyu Wang", "Wenchuan Yang", "Junping Du", "Chuan Shi"], "Categories": "cs.LG cs.AI cs.CR cs.DC", "Comments": ["Submit to WWW 2024"]}, "abstract": "Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's high-order patterns from shared HINs. To recover the broken meta-path based semantics caused by distributed data storage and satisfy the proposed privacy, we elaborately design a semantic-preserving user interactions publishing method, which locally perturbs user's high-order patterns as well as related user-item interactions for publishing. After that, we propose a HGNN model for recommendation, which conducts node- and semantic-level aggregations to capture recovered semantics. Extensive experiments on three datasets demonstrate our model outperforms existing methods by a large margin (up to 34% in HR@10 and 42% in NDCG@10) under an acceptable privacy budget.", "url": "https://arxiv.org/abs/2310.11730"}, {"metadata": {"arXiv": "2310.11815", "Date": "Wed, 18 Oct 2023 09:14:19 ", "Title": "Conservative Predictions on Noisy Financial Data", "Authors": ["Omkar Nabar", "Gautam Shroff"], "Categories": "cs.LG cs.AI cs.CE", "Comments": ["Accepted at ACM ICAIF 2023"], "DOI": "10.1145/3604237.3626859"}, "abstract": "Price movements in financial markets are well known to be very noisy. As a result, even if there are, on occasion, exploitable patterns that could be picked up by machine-learning algorithms, these are obscured by feature and label noise rendering the predictions less useful, and risky in practice. Traditional rule-learning techniques developed for noisy data, such as CN2, would seek only high precision rules and refrain from making predictions where their antecedents did not apply. We apply a similar approach, where a model abstains from making a prediction on data points that it is uncertain on. During training, a cascade of such models are learned in sequence, similar to rule lists, with each model being trained only on data on which the previous model(s) were uncertain. Similar pruning of data takes place at test-time, with (higher accuracy) predictions being made albeit only on a fraction (support) of test-time data. In a financial prediction setting, such an approach allows decisions to be taken only when the ensemble model is confident, thereby reducing risk. We present results using traditional MLPs as well as differentiable decision trees, on synthetic data as well as real financial market data, to predict fixed-term returns using commonly used features. We submit that our approach is likely to result in better overall returns at a lower level of risk. In this context we introduce an utility metric to measure the average gain per trade, as well as the return adjusted for downside risk, both of which are improved significantly by our approach.", "url": "https://arxiv.org/abs/2310.11815"}, {"metadata": {"arXiv": "2310.11950", "Date": "Wed, 18 Oct 2023 13:24:05 ", "Title": "Too Good To Be True: performance overestimation in (re)current practices for Human Activity Recognition", "Authors": ["Andr\\'es Tello", "Victoria Degeler and Alexander Lazovik"], "Categories": "cs.LG cs.AI"}, "abstract": "Today, there are standard and well established procedures within the Human Activity Recognition (HAR) pipeline. However, some of these conventional approaches lead to accuracy overestimation. In particular, sliding windows for data segmentation followed by standard random k-fold cross validation, produce biased results. An analysis of previous literature and present-day studies, surprisingly, shows that these are common approaches in state-of-the-art studies on HAR. It is important to raise awareness in the scientific community about this problem, whose negative effects are being overlooked. Otherwise, publications of biased results lead to papers that report lower accuracies, with correct unbiased methods, harder to publish. Several experiments with different types of datasets and different types of classification models allow us to exhibit the problem and show it persists independently of the method or dataset.", "url": "https://arxiv.org/abs/2310.11950"}, {"metadata": {"arXiv": "2310.11959", "Date": "Wed, 18 Oct 2023 13:39:07 ", "Title": "A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis", "Authors": ["Shuhan Zhong", "Sizhe Song", "Guanyao Li", "Weipeng Zhuo", "Yang Liu", "S.-H. Gary Chan"], "Categories": "cs.LG cs.AI"}, "abstract": "Time series data, often characterized by unique composition and complex multi-scale temporal variations, requires special consideration of decomposition and multi-scale modeling in its analysis. Existing deep learning methods on this best fit to only univariate time series, and have not sufficiently accounted for sub-series level modeling and decomposition completeness. To address this, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer which learns to explicitly decompose the input time series into different components, and represents the components in different layers. To handle multi-scale temporal patterns and inter-channel dependencies, we propose a novel temporal patching approach to model the time series as multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and inter-patch variations and channel-wise correlations. In addition, we propose a loss function to constrain both the magnitude and autocorrelation of the decomposition residual for decomposition completeness. Through extensive experiments on various real-world datasets for five common time series analysis tasks (long- and short-term forecasting, imputation, anomaly detection, and classification), we demonstrate that MSD-Mixer consistently achieves significantly better performance in comparison with other state-of-the-art task-general and task-specific approaches.", "url": "https://arxiv.org/abs/2310.11959"}, {"metadata": {"arXiv": "2310.11971", "Date": "Wed, 18 Oct 2023 13:54:15 ", "Title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning", "Authors": ["Rui Zheng", "Wei Shen", "Yuan Hua", "Wenbin Lai", "Shihan Dou", "Yuhao Zhou", "Zhiheng Xi", "Xiao Wang", "Haoran Huang", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "Categories": "cs.LG cs.AI"}, "abstract": "The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.", "url": "https://arxiv.org/abs/2310.11971"}, {"metadata": {"arXiv": "2310.12052", "Date": "Wed, 18 Oct 2023 15:37:19 ", "Title": "Machine Learning-based Nutrient Application's Timeline Recommendation for Smart Agriculture: A Large-Scale Data Mining Approach", "Authors": ["Usama Ikhlaq", "Tahar Kechadi"], "Categories": "cs.LG cs.AI", "Comments": ["Research articles have: 6 Pages", "6 Figures", "and 3 Tables | ACKNOWLEDGMENT: CONSUS is funded under Science Foundation Ireland's Strategic Partnerships Programme (16/SPP/3296) and is co-funded by Origin Enterprises Plc"], "ACM-class": "I.2.6; J.3; H.2.8"}, "abstract": "This study addresses the vital role of data analytics in monitoring fertiliser applications in crop cultivation. Inaccurate fertiliser application decisions can lead to costly consequences, hinder food production, and cause environmental harm. We propose a solution to predict nutrient application by determining required fertiliser quantities for an entire season. The proposed solution recommends adjusting fertiliser amounts based on weather conditions and soil characteristics to promote cost-effective and environmentally friendly agriculture. The collected dataset is high-dimensional and heterogeneous. Our research examines large-scale heterogeneous datasets in the context of the decision-making process, encompassing data collection and analysis. We also study the impact of fertiliser applications combined with weather data on crop yield, using the winter wheat crop as a case study. By understanding local contextual and geographic factors, we aspire to stabilise or even reduce the demand for agricultural nutrients while enhancing crop development. The proposed approach is proven to be efficient and scalable, as it is validated using a real-world and large dataset.", "url": "https://arxiv.org/abs/2310.12052"}, {"metadata": {"arXiv": "2310.12063", "Date": "Wed, 18 Oct 2023 15:53:20 ", "Title": "Black-Box Training Data Identification in GANs via Detector Networks", "Authors": ["Lukman Olagoke", "Salil Vadhan", "Seth Neel"], "Categories": "cs.LG cs.AI"}, "abstract": "Since their inception Generative Adversarial Networks (GANs) have been popular generative models across images, audio, video, and tabular data. In this paper we study whether given access to a trained GAN, as well as fresh samples from the underlying distribution, if it is possible for an attacker to efficiently identify if a given point is a member of the GAN's training data. This is of interest for both reasons related to copyright, where a user may want to determine if their copyrighted data has been used to train a GAN, and in the study of data privacy, where the ability to detect training set membership is known as a membership inference attack. Unlike the majority of prior work this paper investigates the privacy implications of using GANs in black-box settings, where the attack only has access to samples from the generator, rather than access to the discriminator as well. We introduce a suite of membership inference attacks against GANs in the black-box setting and evaluate our attacks on image GANs trained on the CIFAR10 dataset and tabular GANs trained on genomic data. Our most successful attack, called The Detector, involve training a second network to score samples based on their likelihood of being generated by the GAN, as opposed to a fresh sample from the distribution. We prove under a simple model of the generator that the detector is an approximately optimal membership inference attack. Across a wide range of tabular and image datasets, attacks, and GAN architectures, we find that adversaries can orchestrate non-trivial privacy attacks when provided with access to samples from the generator. At the same time, the attack success achievable against GANs still appears to be lower compared to other generative and discriminative models; this leaves the intriguing open question of whether GANs are in fact more private, or if it is a matter of developing stronger attacks.", "url": "https://arxiv.org/abs/2310.12063"}, {"metadata": {"arXiv": "2310.12126", "Date": "Wed, 18 Oct 2023 17:35:15 ", "Title": "SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks", "Authors": ["Mohammadreza Salehi", "Sachin Mehta", "Aditya Kusupati", "Ali Farhadi", "Hannaneh Hajishirzi"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.", "url": "https://arxiv.org/abs/2310.12126"}, {"metadata": {"arXiv": "2310.12145", "Date": "Wed, 18 Oct 2023 17:56:24 ", "Title": "Fairer and More Accurate Tabular Models Through NAS", "Authors": ["Richeek Das", "Samuel Dooley"], "Categories": "cs.LG cs.AI cs.CY stat.ML"}, "abstract": "Making models algorithmically fairer in tabular data has been long studied, with techniques typically oriented towards fixes which usually take a neural model with an undesirable outcome and make changes to how the data are ingested, what the model weights are, or how outputs are processed. We employ an emergent and different strategy where we consider updating the model's architecture and training hyperparameters to find an entirely new model with better outcomes from the beginning of the debiasing procedure. In this work, we propose using multi-objective Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) in the first application to the very challenging domain of tabular data. We conduct extensive exploration of architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer) across diverse datasets, demonstrating the dependence of accuracy and fairness metrics of model predictions on hyperparameter combinations. We show that models optimized solely for accuracy with NAS often fail to inherently address fairness concerns. We propose a novel approach that jointly optimizes architectural and training hyperparameters in a multi-objective constraint of both accuracy and fairness. We produce architectures that consistently Pareto dominate state-of-the-art bias mitigation methods either in fairness, accuracy or both, all of this while being Pareto-optimal over hyperparameters achieved through single-objective (accuracy) optimization runs. This research underscores the promise of automating fairness and accuracy optimization in deep learning models.", "url": "https://arxiv.org/abs/2310.12145"}, {"metadata": {"arXiv": "2310.12153", "Date": "Wed, 18 Oct 2023 17:59:45 ", "Title": "Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing", "Authors": ["Jan-Nico Zaech", "Martin Danelljan", "Luc Van Gool"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.", "url": "https://arxiv.org/abs/2310.12153"}, {"metadata": {"arXiv": "2310.11604", "Date": "Tue, 17 Oct 2023 21:57:36 ", "Title": "Language Models as Zero-Shot Trajectory Generators", "Authors": ["Teyun Kwon (1)", "Norman Di Palo (1)", "Edward Johns (1) ((1) Imperial College London)"], "Categories": "cs.RO cs.AI cs.CL cs.HC cs.LG", "Comments": ["19 pages", "21 figures"]}, "abstract": "Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as \"open the bottle cap\" and \"wipe the plate with the sponge\", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding of low-level robot control sufficient for a range of common tasks, and that they can additionally detect failures and then re-plan trajectories accordingly. Videos, code, and prompts are available at: https://www.robot-learning.uk/language-models-trajectory-generators.", "url": "https://arxiv.org/abs/2310.11604"}, {"metadata": {"arXiv": "2310.11749", "Date": "Wed, 18 Oct 2023 07:16:06 ", "Title": "Estimating Material Properties of Interacting Objects Using Sum-GP-UCB", "Authors": ["M. Yunus Seker", "Oliver Kroemer"], "Categories": "cs.RO cs.AI cs.LG"}, "abstract": "Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach was successfully evaluated on a set of scenes with a wide range of object interactions, and we showed that our method can effectively perform incremental learning without resetting the rewards of the gathered observations.", "url": "https://arxiv.org/abs/2310.11749"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
