<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2312.02168", "Date": "Mon, 30 Oct 2023 15:38:31 ", "Title": "The SVHN Dataset Is Deceptive for Probabilistic Generative Models Due to a Distribution Mismatch", "Authors": ["Tim Z. Xiao", "Johannes Zenn", "Robert Bamler"], "Categories": "cs.CV cs.LG stat.ML", "Comments": ["Accepted at NeurIPS 2023 Workshop on Distribution Shifts; 4 pages + appendix; proposed data set at https://jzenn.github.io/svhn-remix/"]}, "abstract": "The Street View House Numbers (SVHN) dataset is a popular benchmark dataset in deep learning. Originally designed for digit classification tasks, the SVHN dataset has been widely used as a benchmark for various other tasks including generative modeling. However, with this work, we aim to warn the community about an issue of the SVHN dataset as a benchmark for generative modeling tasks: we discover that the official split into training set and test set of the SVHN dataset are not drawn from the same distribution. We empirically show that this distribution mismatch has little impact on the classification task (which may explain why this issue has not been detected before), but it severely affects the evaluation of probabilistic generative models, such as Variational Autoencoders and diffusion models. As a workaround, we propose to mix and re-split the official training and test set when SVHN is used for tasks other than classification. We publish a new split and the indices we used to create it at https://jzenn.github.io/svhn-remix/ .", "url": "https://arxiv.org/abs/2312.02168"}, {"metadata": {"arXiv": "2312.02205", "Date": "Sat, 02 Dec 2023 22:45:35 ", "Title": "Disentangling the Effects of Data Augmentation and Format Transform in Self-Supervised Learning of Image Representations", "Authors": ["Neha Kalibhat", "Warren Morningstar", "Alex Bijamov", "Luyang Liu", "Karan Singhal", "Philip Mansfield"], "Categories": "cs.CV cs.LG"}, "abstract": "Self-Supervised Learning (SSL) enables training performant models using limited labeled data. One of the pillars underlying vision SSL is the use of data augmentations/perturbations of the input which do not significantly alter its semantic content. For audio and other temporal signals, augmentations are commonly used alongside format transforms such as Fourier transforms or wavelet transforms. Unlike augmentations, format transforms do not change the information contained in the data; rather, they express the same information in different coordinates. In this paper, we study the effects of format transforms and augmentations both separately and together on vision SSL. We define augmentations in frequency space called Fourier Domain Augmentations (FDA) and show that training SSL models on a combination of these and image augmentations can improve the downstream classification accuracy by up to 1.3% on ImageNet-1K. We also show improvements against SSL baselines in few-shot and transfer learning setups using FDA. Surprisingly, we also observe that format transforms can improve the quality of learned representations even without augmentations; however, the combination of the two techniques yields better quality.", "url": "https://arxiv.org/abs/2312.02205"}, {"metadata": {"arXiv": "2312.02220", "Date": "Sun, 03 Dec 2023 18:31:19 ", "Title": "QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers", "Authors": ["Amit Baras", "Alon Zolfi", "Yuval Elovici", "Asaf Shabtai"], "Categories": "cs.CV cs.CR cs.LG"}, "abstract": "In recent years, there has been a significant trend in deep neural networks (DNNs), particularly transformer-based models, of developing ever-larger and more capable models. While they demonstrate state-of-the-art performance, their growing scale requires increased computational resources (e.g., GPUs with greater memory capacity). To address this problem, quantization techniques (i.e., low-bit-precision representation and matrix multiplication) have been proposed. Most quantization techniques employ a static strategy in which the model parameters are quantized, either during training or inference, without considering the test-time sample. In contrast, dynamic quantization techniques, which have become increasingly popular, adapt during inference based on the input provided, while maintaining full-precision performance. However, their dynamic behavior and average-case performance assumption makes them vulnerable to a novel threat vector -- adversarial attacks that target the model's efficiency and availability. In this paper, we present QuantAttack, a novel attack that targets the availability of quantized models, slowing down the inference, and increasing memory usage and energy consumption. We show that carefully crafted adversarial examples, which are designed to exhaust the resources of the operating system, can trigger worst-case performance. In our experiments, we demonstrate the effectiveness of our attack on vision transformers on a wide range of tasks, both uni-modal and multi-modal. We also examine the effect of different attack variants (e.g., a universal perturbation) and the transferability between different models.", "url": "https://arxiv.org/abs/2312.02220"}, {"metadata": {"arXiv": "2312.02255", "Date": "Mon, 04 Dec 2023 18:56:08 ", "Title": "Re-Nerfing: Enforcing Geometric Constraints on Neural Radiance Fields through Novel Views Synthesis", "Authors": ["Felix Tristram", "Stefano Gasperini", "Federico Tombari", "Nassir Navab", "Benjamin Busam"], "Categories": "cs.CV cs.GR cs.LG", "Comments": ["Code will be released upon acceptance"]}, "abstract": "Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis capabilities even in large-scale, unbounded scenes, albeit requiring hundreds of views or introducing artifacts in sparser settings. Their optimization suffers from shape-radiance ambiguities wherever only a small visual overlap is available. This leads to erroneous scene geometry and artifacts. In this paper, we propose Re-Nerfing, a simple and general multi-stage approach that leverages NeRF's own view synthesis to address these limitations. With Re-Nerfing, we increase the scene's coverage and enhance the geometric consistency of novel views as follows: First, we train a NeRF with the available views. Then, we use the optimized NeRF to synthesize pseudo-views next to the original ones to simulate a stereo or trifocal setup. Finally, we train a second NeRF with both original and pseudo views while enforcing structural, epipolar constraints via the newly synthesized images. Extensive experiments on the mip-NeRF 360 dataset show the effectiveness of Re-Nerfing across denser and sparser input scenarios, bringing improvements to the state-of-the-art Zip-NeRF, even when trained with all views.", "url": "https://arxiv.org/abs/2312.02255"}, {"metadata": {"arXiv": "2312.02344", "Date": "Mon, 04 Dec 2023 21:07:13 ", "Title": "STEREOFOG -- Computational DeFogging via Image-to-Image Translation on a real-world Dataset", "Authors": ["Anton Pollak", "Rajesh Menon"], "Categories": "cs.CV cs.LG", "Comments": ["7 pages", "7 figures", "for associated dataset and Supplement file", "see https://github.com/apoll2000/stereofog"]}, "abstract": "Image-to-Image translation (I2I) is a subtype of Machine Learning (ML) that has tremendous potential in applications where two domains of images and the need for translation between the two exist, such as the removal of fog. For example, this could be useful for autonomous vehicles, which currently struggle with adverse weather conditions like fog. However, datasets for I2I tasks are not abundant and typically hard to acquire. Here, we introduce STEREOFOG, a dataset comprised of $10,067$ paired fogged and clear images, captured using a custom-built device, with the purpose of exploring I2I's potential in this domain. It is the only real-world dataset of this kind to the best of our knowledge. Furthermore, we apply and optimize the pix2pix I2I ML framework to this dataset. With the final model achieving an average Complex Wavelet-Structural Similarity (CW-SSIM) score of $0.76$, we prove the technique's suitability for the problem.", "url": "https://arxiv.org/abs/2312.02344"}, {"metadata": {"arXiv": "2312.02684", "Date": "Tue, 05 Dec 2023 11:40:41 ", "Title": "DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors", "Authors": ["Xiaze Zhang", "Ziheng Ding", "Qi Jing", "Yuejie Zhang", "Wenchao Ding", "Rui Feng"], "Categories": "cs.CV cs.LG cs.RO"}, "abstract": "Point clouds have shown significant potential in various domains, including Simultaneous Localization and Mapping (SLAM). However, existing approaches either rely on dense point clouds to achieve high localization accuracy or use generalized descriptors to reduce map size. Unfortunately, these two aspects seem to conflict with each other. To address this limitation, we propose a unified architecture, DeepPointMap, achieving excellent preference on both aspects. We utilize neural network to extract highly representative and sparse neural descriptors from point clouds, enabling memory-efficient map representation and accurate multi-scale localization tasks (e.g., odometry and loop-closure). Moreover, we showcase the versatility of our framework by extending it to more challenging multi-agent collaborative SLAM. The promising results obtained in these scenarios further emphasize the effectiveness and potential of our approach.", "url": "https://arxiv.org/abs/2312.02684"}, {"metadata": {"arXiv": "2312.02914", "Date": "Tue, 05 Dec 2023 17:39:19 ", "Title": "Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training", "Authors": ["Arun Reddy", "William Paul", "Corban Rivera", "Ketul Shah", "Celso M. de Melo", "Rama Chellappa"], "Categories": "cs.CV cs.LG"}, "abstract": "In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition. Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain. UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective. We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos. Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains. We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results.", "url": "https://arxiv.org/abs/2312.02914"}, {"metadata": {"arXiv": "2312.02916", "Date": "Tue, 05 Dec 2023 17:46:52 ", "Title": "MIND: Multi-Task Incremental Network Distillation", "Authors": ["Jacopo Bonato", "Francesco Pelosin", "Luigi Sabetta", "Alessandro Nicolosi"], "Categories": "cs.CV cs.LG"}, "abstract": "The recent surge in pervasive devices generating dynamic data streams has underscored the necessity for learning systems to adapt to data distributional shifts continually. To tackle this challenge, the research community has put forth a spectrum of methodologies, including the demanding pursuit of class-incremental learning without replay data. In this study, we present MIND, a parameter isolation method that aims to significantly enhance the performance of replay-free solutions and achieve state-of-the-art results on several widely studied datasets. Our approach introduces two main contributions: two alternative distillation procedures that significantly improve the efficiency of MIND increasing the accumulated knowledge of each sub-network, and the optimization of the BachNorm layers across tasks inside the sub-networks. Overall, MIND outperforms all the state-of-the-art methods for rehearsal-free Class-Incremental learning (with an increment in classification accuracy of approx. +6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx. +40% accuracy in Domain-Incremental scenarios. Moreover, we ablated each contribution to demonstrate its impact on performance improvement. Our results showcase the superior performance of MIND indicating its potential for addressing the challenges posed by Class-incremental and Domain-Incremental learning in resource-constrained environments.", "url": "https://arxiv.org/abs/2312.02916"}, {"metadata": {"arXiv": "2312.02974", "Date": "Tue, 05 Dec 2023 18:59:16 ", "Title": "Describing Differences in Image Sets with Natural Language", "Authors": ["Lisa Dunlap", "Yuhui Zhang", "Xiaohan Wang", "Ruiqi Zhong", "Trevor Darrell", "Jacob Steinhardt", "Joseph E. Gonzalez", "Serena Yeung-Levy"], "Categories": "cs.CV cs.CL cs.CY cs.LG"}, "abstract": "How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two $\\textbf{sets}$ of images, which we term Set Difference Captioning. This task takes in image sets $D_A$ and $D_B$, and outputs a description that is more often true on $D_A$ than $D_B$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.", "url": "https://arxiv.org/abs/2312.02974"}, {"metadata": {"arXiv": "2312.02182", "Date": "Wed, 29 Nov 2023 14:38:59 ", "Title": "Adam-like Algorithm with Smooth Clipping Attains Global Minima: Analysis Based on Ergodicity of Functional SDEs", "Authors": ["Keisuke Suzuki"], "Categories": "cs.LG math.OC math.PR"}, "abstract": "In this paper, we prove that an Adam-type algorithm with smooth clipping approaches the global minimizer of the regularized non-convex loss function. Adding smooth clipping and taking the state space as the set of all trajectories, we can apply the ergodic theory of Markov semigroups for this algorithm and investigate its asymptotic behavior. The ergodic theory we establish in this paper reduces the problem of evaluating the convergence, generalization error and discretization error of this algorithm to the problem of evaluating the difference between two functional stochastic differential equations (SDEs) with different drift coefficients. As a result of our analysis, we have shown that this algorithm minimizes the the regularized non-convex loss function with errors of the form $n^{-1/2}$, $\\eta^{1/4}$, $\\beta^{-1} \\log (\\beta + 1)$ and $e^{- c t}$. Here, $c$ is a constant and $n$, $\\eta$, $\\beta$ and $t$ denote the size of the training dataset, learning rate, inverse temperature and time, respectively.", "url": "https://arxiv.org/abs/2312.02182"}, {"metadata": {"arXiv": "2312.02195", "Date": "Sat, 02 Dec 2023 12:11:47 ", "Title": "Cancer Subtype Identification through Integrating Inter and Intra Dataset Relationships in Multi-Omics Data", "Authors": ["Mark Peelen", "Leila Bagheriye", "and Johan Kwisthout"], "Categories": "cs.LG q-bio.GN stat.AP"}, "abstract": "The integration of multi-omics data has emerged as a promising approach for gaining comprehensive insights into complex diseases such as cancer. This paper proposes a novel approach to identify cancer subtypes through the integration of multi-omics data for clustering. The proposed method, named LIDAF utilises affinity matrices based on linear relationships between and within different omics datasets (Linear Inter and Intra Dataset Affinity Fusion (LIDAF)). Canonical Correlation Analysis is in this paper employed to create distance matrices based on Euclidean distances between canonical variates. The distance matrices are converted to affinity matrices and those are fused in a three-step process. The proposed LIDAF addresses the limitations of the existing method resulting in improvement of clustering performance as measured by the Adjusted Rand Index and the Normalized Mutual Information score. Moreover, our proposed LIDAF approach demonstrates a notable enhancement in 50% of the log10 rank p-values obtained from Cox survival analysis, surpassing the performance of the best reported method, highlighting its potential of identifying distinct cancer subtypes.", "url": "https://arxiv.org/abs/2312.02195"}, {"metadata": {"arXiv": "2312.02204", "Date": "Sat, 02 Dec 2023 21:51:12 ", "Title": "Can We Learn Communication-Efficient Optimizers?", "Authors": ["Charles-\\'Etienne Joseph and Benjamin Th\\'erien and Abhinav Moudgil and Boris Knyazev and Eugene Belilovsky"], "Categories": "cs.LG"}, "abstract": "Communication-efficient variants of SGD, specifically local SGD, have received a great deal of interest in recent years. These approaches compute multiple gradient steps locally, that is on each worker, before averaging model parameters, helping relieve the critical communication bottleneck in distributed deep learning training. Although many variants of these approaches have been proposed, they can sometimes lag behind state-of-the-art adaptive optimizers for deep learning. In this work, we investigate if the recent progress in the emerging area of learned optimizers can potentially close this gap while remaining communication-efficient. Specifically, we meta-learn how to perform global updates given an update from local SGD iterations. Our results demonstrate that learned optimizers can substantially outperform local SGD and its sophisticated variants while maintaining their communication efficiency. Learned optimizers can even generalize to unseen and much larger datasets and architectures, including ImageNet and ViTs, and to unseen modalities such as language modeling. We therefore demonstrate the potential of learned optimizers for improving communication-efficient distributed learning.", "url": "https://arxiv.org/abs/2312.02204"}, {"metadata": {"arXiv": "2312.02227", "Date": "Mon, 04 Dec 2023 02:58:19 ", "Title": "Improving Multimodal Sentiment Analysis: Supervised Angular Margin-based Contrastive Learning for Enhanced Fusion Representation", "Authors": ["Cong-Duy Nguyen", "Thong Nguyen", "Duc Anh Vu", "Luu Anh Tuan"], "Categories": "cs.LG cs.CL"}, "abstract": "The effectiveness of a model is heavily reliant on the quality of the fusion representation of multiple modalities in multimodal sentiment analysis. Moreover, each modality is extracted from raw input and integrated with the rest to construct a multimodal representation. Although previous methods have proposed multimodal representations and achieved promising results, most of them focus on forming positive and negative pairs, neglecting the variation in sentiment scores within the same class. Additionally, they fail to capture the significance of unimodal representations in the fusion vector. To address these limitations, we introduce a framework called Supervised Angular-based Contrastive Learning for Multimodal Sentiment Analysis. This framework aims to enhance discrimination and generalizability of the multimodal representation and overcome biases in the fusion vector's modality. Our experimental results, along with visualizations on two widely used datasets, demonstrate the effectiveness of our approach.", "url": "https://arxiv.org/abs/2312.02227"}, {"metadata": {"arXiv": "2312.02243", "Date": "Mon, 04 Dec 2023 11:50:25 ", "Title": "FlowHON: Representing Flow Fields Using Higher-Order Networks", "Authors": ["Nan Chen", "Zhihong Li", "Jun Tao"], "Categories": "cs.LG", "Comments": ["To be submitted to TVCG"]}, "abstract": "Flow fields are often partitioned into data blocks for massively parallel computation and analysis based on blockwise relationships. However, most of the previous techniques only consider the first-order dependencies among blocks, which is insufficient in describing complex flow patterns. In this work, we present FlowHON, an approach to construct higher-order networks (HONs) from flow fields. FlowHON captures the inherent higher-order dependencies in flow fields as nodes and estimates the transitions among them as edges. We formulate the HON construction as an optimization problem with three linear transformations. The first two layers correspond to the node generation and the third one corresponds to edge estimation. Our formulation allows the node generation and edge estimation to be solved in a unified framework. With FlowHON, the rich set of traditional graph algorithms can be applied without any modification to analyze flow fields, while leveraging the higher-order information to understand the inherent structure and manage flow data for efficiency. We demonstrate the effectiveness of FlowHON using a series of downstream tasks, including estimating the density of particles during tracing, partitioning flow fields for data management, and understanding flow fields using the node-link diagram representation of networks.", "url": "https://arxiv.org/abs/2312.02243"}, {"metadata": {"arXiv": "2312.02299", "Date": "Mon, 04 Dec 2023 19:33:29 ", "Title": "Cotton Yield Prediction Using Random Forest", "Authors": ["Alakananda Mitra", "Sahila Beegum", "David Fleisher", "Vangimalla R. Reddy", "Wenguang Sun", "Chittaranjan Ray", "Dennis Timlin", "Arindam Malakar"], "Categories": "cs.LG cs.CY stat.AP", "Comments": ["6 pages", "2 figures", "3 tables"]}, "abstract": "The cotton industry in the United States is committed to sustainable production practices that minimize water, land, and energy use while improving soil health and cotton output. Climate-smart agricultural technologies are being developed to boost yields while decreasing operating expenses. Crop yield prediction, on the other hand, is difficult because of the complex and nonlinear impacts of cultivar, soil type, management, pest and disease, climate, and weather patterns on crops. To solve this issue, we employ machine learning (ML) to forecast production while considering climate change, soil diversity, cultivar, and inorganic nitrogen levels. From the 1980s to the 1990s, field data were gathered across the southern cotton belt of the United States. To capture the most current effects of climate change over the previous six years, a second data source was produced using the process-based crop model, GOSSYM. We concentrated our efforts on three distinct areas inside each of the three southern states: Texas, Mississippi, and Georgia. To simplify the amount of computations, accumulated heat units (AHU) for each set of experimental data were employed as an analogy to use time-series weather data. The Random Forest Regressor yielded a 97.75% accuracy rate, with a root mean square error of 55.05 kg/ha and an R2 of around 0.98. These findings demonstrate how an ML technique may be developed and applied as a reliable and easy-to-use model to support the cotton climate-smart initiative.", "url": "https://arxiv.org/abs/2312.02299"}, {"metadata": {"arXiv": "2312.02300", "Date": "Mon, 04 Dec 2023 19:34:08 ", "Title": "Reconsideration on evaluation of machine learning models in continuous monitoring using wearables", "Authors": ["Cheng Ding", "Zhicheng Guo", "Cynthia Rudin", "Ran Xiao", "Fadi B Nahab", "Xiao Hu"], "Categories": "cs.LG eess.SP"}, "abstract": "This paper explores the challenges in evaluating machine learning (ML) models for continuous health monitoring using wearable devices beyond conventional metrics. We state the complexities posed by real-world variability, disease dynamics, user-specific characteristics, and the prevalence of false notifications, necessitating novel evaluation strategies. Drawing insights from large-scale heart studies, the paper offers a comprehensive guideline for robust ML model evaluation on continuous health monitoring.", "url": "https://arxiv.org/abs/2312.02300"}, {"metadata": {"arXiv": "2312.02327", "Date": "Mon, 04 Dec 2023 20:24:09 ", "Title": "FLea: Improving federated learning on scarce and label-skewed data via privacy-preserving feature augmentation", "Authors": ["Tong Xia and Abhirup Ghosh and Cecilia Mascolo"], "Categories": "cs.LG cs.CR cs.DC"}, "abstract": "Learning a global model by abstracting the knowledge, distributed across multiple clients, without aggregating the raw data is the primary goal of Federated Learning (FL). Typically, this works in rounds alternating between parallel local training at several clients, followed by model aggregation at a server. We found that existing FL methods under-perform when local datasets are small and present severe label skew as these lead to over-fitting and local model bias. This is a realistic setting in many real-world applications. To address the problem, we propose \\textit{FLea}, a unified framework that tackles over-fitting and local bias by encouraging clients to exchange privacy-protected features to aid local training. The features refer to activations from an intermediate layer of the model, which are obfuscated before being shared with other clients to protect sensitive information in the data. \\textit{FLea} leverages a novel way of combining local and shared features as augmentations to enhance local model learning. Our extensive experiments demonstrate that \\textit{FLea} outperforms the start-of-the-art FL methods, sharing only model parameters, by up to $17.6\\%$, and FL methods that share data augmentations by up to $6.3\\%$, while reducing the privacy vulnerability associated with shared data augmentations.", "url": "https://arxiv.org/abs/2312.02327"}, {"metadata": {"arXiv": "2312.02380", "Date": "Mon, 04 Dec 2023 22:51:02 ", "Title": "FaultFormer: Transformer-based Prediction of Bearing Faults", "Authors": ["Anthony Zhou and Amir Barati Farimani"], "Categories": "cs.LG eess.SP", "Comments": ["21 pages", "5 figures"]}, "abstract": "The growth of deep learning in the past decade has motivated important applications to smart manufacturing and machine health monitoring. In particular, vibration data offers a rich and reliable source to provide meaningful insights into machine health and predictive maintenance. In this work, we present a Transformer based framework for analyzing vibration signals to predict different types of bearing faults (FaultFormer). In particular, we process signal data using data augmentations and extract their Fourier modes to train a transformer encoder to achieve state of the art accuracies. The attention mechanism as well as model outputs were analyzed to confirm the transformer's ability to automatically extract features within signals and learn both global and local relationships to make classifications. Lastly, two pretraining strategies were proposed to pave the way for large, generalizable transformers that could adapt to new data, situations, or machinery on the production floor.", "url": "https://arxiv.org/abs/2312.02380"}, {"metadata": {"arXiv": "2312.02400", "Date": "Tue, 05 Dec 2023 00:09:57 ", "Title": "Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation", "Authors": ["Sai Venkatesh Chilukoti", "Md Imran Hossen", "Liqun Shan", "Vijay Srinivas Tida", "and Xiai Hei"], "Categories": "cs.LG cs.CR", "Comments": ["25 pages single column", "2 figures"], "MSC-class": "26, 40"}, "abstract": "DP-SGD has emerged as a popular method to protect personally identifiable information in deep learning applications. Unfortunately, DP-SGD's per-sample gradient clipping and uniform noise addition during training can significantly degrade model utility. To enhance the model's utility, researchers proposed various adaptive DP-SGD methods. However, we examine and discover that these techniques result in greater privacy leakage or lower accuracy than the traditional DP-SGD method, or a lack of evaluation on a complex data set such as CIFAR100. To address these limitations, we propose an Auto DP-SGD. Our method automates clipping threshold estimation based on the DL model's gradient norm and scales the gradients of each training sample without losing gradient information. This helps to improve the algorithm's utility while using a less privacy budget. To further improve accuracy, we introduce automatic noise multiplier decay mechanisms to decrease the noise multiplier after every epoch. Finally, we develop closed-form mathematical expressions using tCDP accountant for automatic noise multiplier and automatic clipping threshold estimation. Through extensive experimentation, we demonstrate that Auto DP-SGD outperforms existing SOTA DP-SGD methods in privacy and accuracy on various benchmark datasets. We also show that privacy can be improved by lowering the scale factor and using learning rate schedulers without significantly reducing accuracy. Specifically, Auto DP-SGD, when used with a step noise multiplier, improves accuracy by 3.20, 1.57, 6.73, and 1.42 for the MNIST, CIFAR10, CIFAR100, and AG News Corpus datasets, respectively. Furthermore, it obtains a substantial reduction in the privacy budget of 94.9, 79.16, 67.36, and 53.37 for the corresponding data sets.", "url": "https://arxiv.org/abs/2312.02400"}, {"metadata": {"arXiv": "2312.02407", "Date": "Tue, 05 Dec 2023 00:46:29 ", "Title": "Robust Clustering using Hyperdimensional Computing", "Authors": ["Lulu Ge", "Keshab K. Parhi"], "Categories": "cs.LG cs.DB cs.SC"}, "abstract": "This paper addresses the clustering of data in the hyperdimensional computing (HDC) domain. In prior work, an HDC-based clustering framework, referred to as HDCluster, has been proposed. However, the performance of the existing HDCluster is not robust. The performance of HDCluster is degraded as the hypervectors for the clusters are chosen at random during the initialization step. To overcome this bottleneck, we assign the initial cluster hypervectors by exploring the similarity of the encoded data, referred to as \\textit{query} hypervectors. Intra-cluster hypervectors have a higher similarity than inter-cluster hypervectors. Harnessing the similarity results among query hypervectors, this paper proposes four HDC-based clustering algorithms: similarity-based k-means, equal bin-width histogram, equal bin-height histogram, and similarity-based affinity propagation. Experimental results illustrate that: (i) Compared to the existing HDCluster, our proposed HDC-based clustering algorithms can achieve better accuracy, more robust performance, fewer iterations, and less execution time. Similarity-based affinity propagation outperforms the other three HDC-based clustering algorithms on eight datasets by 2~38% in clustering accuracy. (ii) Even for one-pass clustering, i.e., without any iterative update of the cluster hypervectors, our proposed algorithms can provide more robust clustering accuracy than HDCluster. (iii) Over eight datasets, five out of eight can achieve higher or comparable accuracy when projected onto the hyperdimensional space. Traditional clustering is more desirable than HDC when the number of clusters, $k$, is large.", "url": "https://arxiv.org/abs/2312.02407"}, {"metadata": {"arXiv": "2312.02438", "Date": "Tue, 05 Dec 2023 02:38:04 ", "Title": "Adaptive Instrument Design for Indirect Experiments", "Authors": ["Yash Chandak", "Shiv Shankar", "Vasilis Syrgkanis", "Emma Brunskill"], "Categories": "cs.LG"}, "abstract": "Indirect experiments provide a valuable framework for estimating treatment effects in situations where conducting randomized control trials (RCTs) is impractical or unethical. Unlike RCTs, indirect experiments estimate treatment effects by leveraging (conditional) instrumental variables, enabling estimation through encouragement and recommendation rather than strict treatment assignment. However, the sample efficiency of such estimators depends not only on the inherent variability in outcomes but also on the varying compliance levels of users with the instrumental variables and the choice of estimator being used, especially when dealing with numerous instrumental variables. While adaptive experiment design has a rich literature for direct experiments, in this paper we take the initial steps towards enhancing sample efficiency for indirect experiments by adaptively designing a data collection policy over instrumental variables. Our main contribution is a practical computational procedure that utilizes influence functions to search for an optimal data collection policy, minimizing the mean-squared error of the desired (non-linear) estimator. Through experiments conducted in various domains inspired by real-world applications, we showcase how our method can significantly improve the sample efficiency of indirect experiments.", "url": "https://arxiv.org/abs/2312.02438"}, {"metadata": {"arXiv": "2312.02462", "Date": "Tue, 05 Dec 2023 03:25:45 ", "Title": "Dimensionality Reduction and Dynamical Mode Recognition of Circular Arrays of Flame Oscillators Using Deep Neural Network", "Authors": ["Weiming Xu", "Tao Yang", "Peng Zhang"], "Categories": "cs.LG physics.flu-dyn", "Comments": ["17 pages", "10 figures", "research paper"]}, "abstract": "Oscillatory combustion in aero engines and modern gas turbines often has significant adverse effects on their operation, and accurately recognizing various oscillation modes is the prerequisite for understanding and controlling combustion instability. However, the high-dimensional spatial-temporal data of a complex combustion system typically poses considerable challenges to the dynamical mode recognition. Based on a two-layer bidirectional long short-term memory variational autoencoder (Bi-LSTM-VAE) dimensionality reduction model and a two-dimensional Wasserstein distance-based classifier (WDC), this study proposes a promising method (Bi-LSTM-VAE-WDC) for recognizing dynamical modes in oscillatory combustion systems. Specifically, the Bi-LSTM-VAE dimension reduction model was introduced to reduce the high-dimensional spatial-temporal data of the combustion system to a low-dimensional phase space; Gaussian kernel density estimates (GKDE) were computed based on the distribution of phase points in a grid; two-dimensional WD values were calculated from the GKDE maps to recognize the oscillation modes. The time-series data used in this study were obtained from numerical simulations of circular arrays of laminar flame oscillators. The results show that the novel Bi-LSTM-VAE method can produce a non-overlapping distribution of phase points, indicating an effective unsupervised mode recognition and classification. Furthermore, the present method exhibits a more prominent performance than VAE and PCA (principal component analysis) for distinguishing dynamical modes in complex flame systems, implying its potential in studying turbulent combustion.", "url": "https://arxiv.org/abs/2312.02462"}, {"metadata": {"arXiv": "2312.02469", "Date": "Tue, 05 Dec 2023 03:39:54 ", "Title": "Learning Energy-based Model via Dual-MCMC Teaching", "Authors": ["Jiali Cui", "Tian Han"], "Categories": "cs.LG cs.CV stat.CO"}, "abstract": "This paper studies the fundamental learning problem of the energy-based model (EBM). Learning the EBM can be achieved using the maximum likelihood estimation (MLE), which typically involves the Markov Chain Monte Carlo (MCMC) sampling, such as the Langevin dynamics. However, the noise-initialized Langevin dynamics can be challenging in practice and hard to mix. This motivates the exploration of joint training with the generator model where the generator model serves as a complementary model to bypass MCMC sampling. However, such a method can be less accurate than the MCMC and result in biased EBM learning. While the generator can also serve as an initializer model for better MCMC sampling, its learning can be biased since it only matches the EBM and has no access to empirical training examples. Such biased generator learning may limit the potential of learning the EBM. To address this issue, we present a joint learning framework that interweaves the maximum likelihood learning algorithm for both the EBM and the complementary generator model. In particular, the generator model is learned by MLE to match both the EBM and the empirical data distribution, making it a more informative initializer for MCMC sampling of EBM. Learning generator with observed examples typically requires inference of the generator posterior. To ensure accurate and efficient inference, we adopt the MCMC posterior sampling and introduce a complementary inference model to initialize such latent MCMC sampling. We show that three separate models can be seamlessly integrated into our joint framework through two (dual-) MCMC teaching, enabling effective and efficient EBM learning.", "url": "https://arxiv.org/abs/2312.02469"}, {"metadata": {"arXiv": "2312.02470", "Date": "Tue, 05 Dec 2023 03:41:17 ", "Title": "Generator Born from Classifier", "Authors": ["Runpeng Yu", "Xinchao Wang"], "Categories": "cs.LG cs.CV"}, "abstract": "In this paper, we make a bold attempt toward an ambitious task: given a pre-trained classifier, we aim to reconstruct an image generator, without relying on any data samples. From a black-box perspective, this challenge seems intractable, since it inevitably involves identifying the inverse function for a classifier, which is, by nature, an information extraction process. As such, we resort to leveraging the knowledge encapsulated within the parameters of the neural network. Grounded on the theory of Maximum-Margin Bias of gradient descent, we propose a novel learning paradigm, in which the generator is trained to ensure that the convergence conditions of the network parameters are satisfied over the generated distribution of the samples. Empirical validation from various image generation tasks substantiates the efficacy of our strategy.", "url": "https://arxiv.org/abs/2312.02470"}, {"metadata": {"arXiv": "2312.02473", "Date": "Tue, 05 Dec 2023 03:58:05 ", "Title": "NeutronStream: A Dynamic GNN Training Framework with Sliding Window for Graph Streams", "Authors": ["Chaoyi Chen", "Dechao Gao", "Yanfeng Zhang", "Qiange Wang", "Zhenbo Fu", "Xuecang Zhang", "Junhua Zhu", "Yu Gu", "Ge Yu"], "Categories": "cs.LG cs.DC", "Comments": ["12 pages", "15 figures"]}, "abstract": "Existing Graph Neural Network (GNN) training frameworks have been designed to help developers easily create performant GNN implementations. However, most existing GNN frameworks assume that the input graphs are static, but ignore that most real-world graphs are constantly evolving. Though many dynamic GNN models have emerged to learn from evolving graphs, the training process of these dynamic GNNs is dramatically different from traditional GNNs in that it captures both the spatial and temporal dependencies of graph updates. This poses new challenges for designing dynamic GNN training frameworks. First, the traditional batched training method fails to capture real-time structural evolution information. Second, the time-dependent nature makes parallel training hard to design. Third, it lacks system supports for users to efficiently implement dynamic GNNs. In this paper, we present NeutronStream, a framework for training dynamic GNN models. NeutronStream abstracts the input dynamic graph into a chronologically updated stream of events and processes the stream with an optimized sliding window to incrementally capture the spatial-temporal dependencies of events. Furthermore, NeutronStream provides a parallel execution engine to tackle the sequential event processing challenge to achieve high performance. NeutronStream also integrates a built-in graph storage structure that supports dynamic updates and provides a set of easy-to-use APIs that allow users to express their dynamic GNNs. Our experimental results demonstrate that, compared to state-of-the-art dynamic GNN implementations, NeutronStream achieves speedups ranging from 1.48X to 5.87X and an average accuracy improvement of 3.97%.", "url": "https://arxiv.org/abs/2312.02473"}, {"metadata": {"arXiv": "2312.02490", "Date": "Tue, 05 Dec 2023 04:42:04 ", "Title": "Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT Systems", "Authors": ["Phai Vu Dinh", "Quang Uy Nguyen", "Dinh Thai Hoang", "Diep N. Nguyen", "Son Pham Bao", "and Eryk Dutkiewicz"], "Categories": "cs.LG cs.CR"}, "abstract": "Intrusion detection systems (IDSs) play a critical role in protecting billions of IoT devices from malicious attacks. However, the IDSs for IoT devices face inherent challenges of IoT systems, including the heterogeneity of IoT data/devices, the high dimensionality of training data, and the imbalanced data. Moreover, the deployment of IDSs on IoT systems is challenging, and sometimes impossible, due to the limited resources such as memory/storage and computing capability of typical IoT devices. To tackle these challenges, this article proposes a novel deep neural network/architecture called Constrained Twin Variational Auto-Encoder (CTVAE) that can feed classifiers of IDSs with more separable/distinguishable and lower-dimensional representation data. Additionally, in comparison to the state-of-the-art neural networks used in IDSs, CTVAE requires less memory/storage and computing power, hence making it more suitable for IoT IDS systems. Extensive experiments with the 11 most popular IoT botnet datasets show that CTVAE can boost around 1% in terms of accuracy and Fscore in detection attack compared to the state-of-the-art machine learning and representation learning methods, whilst the running time for attack detection is lower than 2E-6 seconds and the model size is lower than 1 MB. We also further investigate various characteristics of CTVAE in the latent space and in the reconstruction representation to demonstrate its efficacy compared with current well-known methods.", "url": "https://arxiv.org/abs/2312.02490"}, {"metadata": {"arXiv": "2312.02491", "Date": "Tue, 05 Dec 2023 04:43:23 ", "Title": "Pseudo Replay-based Class Continual Learning for Online New Category Anomaly Detection in Additive Manufacturing", "Authors": ["Zhangyue Shi", "Tianxin Xie", "Chenang Liu", "Yuxuan Li"], "Categories": "cs.LG stat.ML"}, "abstract": "The incorporation of advanced sensors and machine learning techniques has enabled modern manufacturing enterprises to perform data-driven in-situ quality monitoring based on the sensor data collected in manufacturing processes. However, one critical challenge is that newly presented defect category may manifest as the manufacturing process continues, resulting in monitoring performance deterioration of previously trained machine learning models. Hence, there is an increasing need for empowering machine learning model to learn continually. Among all continual learning methods, memory-based continual learning has the best performance but faces the constraints of data storage capacity. To address this issue, this paper develops a novel pseudo replay-based continual learning by integrating class incremental learning and oversampling-based data generation. Without storing all the data, the developed framework could generate high-quality data representing previous classes to train machine learning model incrementally when new category anomaly occurs. In addition, it could even enhance the monitoring performance since it also effectively improves the data quality. The effectiveness of the proposed framework is validated in an additive manufacturing process, which leverages supervised classification problem for anomaly detection. The experimental results show that the developed method is very promising in detecting novel anomaly while maintaining a good performance on the previous task and brings up more flexibility in model architecture.", "url": "https://arxiv.org/abs/2312.02491"}, {"metadata": {"arXiv": "2312.02554", "Date": "Tue, 05 Dec 2023 07:52:12 ", "Title": "ULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference", "Authors": ["Tianchi Cai", "Xierui Song", "Jiyan Jiang", "Fei Teng", "Jinjie Gu", "Guannan Zhang"], "Categories": "cs.LG cs.CL"}, "abstract": "Language model alignment is a cutting-edge technique in large language model training to align the model output to user's intent, e.g., being helpful and harmless. Recent alignment framework consists of two steps: supervised fine-tuning with demonstration data and preference learning with human preference data. Previous preference learning methods, such as RLHF and DPO, mainly focus on pair-wise preference data. However, in many real-world scenarios where human feedbacks are intrinsically point-wise, these methods will suffer from information loss or even fail. To fill this gap, in this paper, we first develop a preference learning method called point-wise DPO to tackle point-wise preference data. Further revelation on the connection between supervised fine-tuning and point-wise preference learning enables us to develop a unified framework for both human demonstration and point-wise preference data, which sheds new light on the construction of preference dataset. Extensive experiments on point-wise datasets with binary or continuous labels demonstrate the superior performance and efficiency of our proposed methods. A new dataset with high-quality demonstration samples on harmlessness is constructed and made publicly available.", "url": "https://arxiv.org/abs/2312.02554"}, {"metadata": {"arXiv": "2312.02592", "Date": "Tue, 05 Dec 2023 09:09:21 ", "Title": "FRAPP\\'E: A Post-Processing Framework for Group Fairness Regularization", "Authors": ["Alexandru \\c{T}ifrea", "Preethi Lahoti", "Ben Packer", "Yoni Halpern", "Ahmad Beirami and Flavien Prost"], "Categories": "cs.LG cs.CY", "Comments": ["Presubmission"]}, "abstract": "Post-processing mitigation techniques for group fairness generally adjust the decision threshold of a base model in order to improve fairness. Methods in this family exhibit several advantages that make them appealing in practice: post-processing requires no access to the model training pipeline, is agnostic to the base model architecture, and offers a reduced computation cost compared to in-processing. Despite these benefits, existing methods face other challenges that limit their applicability: they require knowledge of the sensitive attributes at inference time and are oftentimes outperformed by in-processing. In this paper, we propose a general framework to transform any in-processing method with a penalized objective into a post-processing procedure. The resulting method is specifically designed to overcome the aforementioned shortcomings of prior post-processing approaches. Furthermore, we show theoretically and through extensive experiments on real-world data that the resulting post-processing method matches or even surpasses the fairness-error trade-off offered by the in-processing counterpart.", "url": "https://arxiv.org/abs/2312.02592"}, {"metadata": {"arXiv": "2312.02596", "Date": "Tue, 05 Dec 2023 09:15:10 ", "Title": "TSVR+: Twin support vector regression with privileged information", "Authors": ["Anuradha Kumari", "M. Tanveer"], "Categories": "cs.LG"}, "abstract": "In the realm of machine learning, the data may contain additional attributes, known as privileged information (PI). The main purpose of PI is to assist in the training of the model and then utilize the acquired knowledge to make predictions for unseen samples. Support vector regression (SVR) is an effective regression model, however, it has a low learning speed due to solving a convex quadratic problem (QP) subject to a pair of constraints. In contrast, twin support vector regression (TSVR) is more efficient than SVR as it solves two QPs each subject to one set of constraints. However, TSVR and its variants are trained only on regular features and do not use privileged features for training. To fill this gap, we introduce a fusion of TSVR with learning using privileged information (LUPI) and propose a novel approach called twin support vector regression with privileged information (TSVR+). The regularization terms in the proposed TSVR+ capture the essence of statistical learning theory and implement the structural risk minimization principle. We use the successive overrelaxation (SOR) technique to solve the optimization problem of the proposed TSVR+, which enhances the training efficiency. As far as our knowledge extends, the integration of the LUPI concept into twin variants of regression models is a novel advancement. The numerical experiments conducted on UCI, stock and time series data collectively demonstrate the superiority of the proposed model.", "url": "https://arxiv.org/abs/2312.02596"}, {"metadata": {"arXiv": "2312.02611", "Date": "Tue, 05 Dec 2023 09:39:04 ", "Title": "Privacy-Aware Data Acquisition under Data Similarity in Regression Markets", "Authors": ["Shashi Raj Pandey", "Pierre Pinson", "and Petar Popovski"], "Categories": "cs.LG cs.CR cs.GT", "Comments": ["Submitted to IEEE Transactions on Neural Networks and Learning Systems (submission version)"]}, "abstract": "Data markets facilitate decentralized data exchange for applications such as prediction, learning, or inference. The design of these markets is challenged by varying privacy preferences as well as data similarity among data owners. Related works have often overlooked how data similarity impacts pricing and data value through statistical information leakage. We demonstrate that data similarity and privacy preferences are integral to market design and propose a query-response protocol using local differential privacy for a two-party data acquisition mechanism. In our regression data market model, we analyze strategic interactions between privacy-aware owners and the learner as a Stackelberg game over the asked price and privacy factor. Finally, we numerically evaluate how data similarity affects market participation and traded data value.", "url": "https://arxiv.org/abs/2312.02611"}, {"metadata": {"arXiv": "2312.02614", "Date": "Tue, 05 Dec 2023 09:44:45 ", "Title": "Prompt Optimization via Adversarial In-Context Learning", "Authors": ["Xuan Long Do", "Yiran Zhao", "Hannah Brown", "Yuxi Xie", "James Xu Zhao", "Nancy F. Chen", "Kenji Kawaguchi", "Michael Qizhe Xie", "Junxian He"], "Categories": "cs.LG cs.CL", "Comments": ["21 pages"]}, "abstract": "We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 11 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, because our method uses pre-trained models and updates only prompts rather than model parameters, it is computationally efficient, easy to extend to any LLM and task, and effective in low-resource settings.", "url": "https://arxiv.org/abs/2312.02614"}, {"metadata": {"arXiv": "2312.02615", "Date": "Tue, 05 Dec 2023 09:44:47 ", "Title": "Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models", "Authors": ["Sungik Choi", "Hankook Lee", "Honglak Lee", "Moontae Lee"], "Categories": "cs.LG cs.CV", "Comments": ["NeurIPS 2023"]}, "abstract": "Novelty detection is a fundamental task of machine learning which aims to detect abnormal ($\\textit{i.e.}$ out-of-distribution (OOD)) samples. Since diffusion models have recently emerged as the de facto standard generative framework with surprising generation results, novelty detection via diffusion models has also gained much attention. Recent methods have mainly utilized the reconstruction property of in-distribution samples. However, they often suffer from detecting OOD samples that share similar background information to the in-distribution data. Based on our observation that diffusion models can \\emph{project} any sample to an in-distribution sample with similar background information, we propose \\emph{Projection Regret (PR)}, an efficient novelty detection method that mitigates the bias of non-semantic information. To be specific, PR computes the perceptual distance between the test image and its diffusion-based projection to detect abnormality. Since the perceptual distance often fails to capture semantic changes when the background information is dominant, we cancel out the background bias by comparing it against recursive projections. Extensive experiments demonstrate that PR outperforms the prior art of generative-model-based novelty detection methods by a significant margin.", "url": "https://arxiv.org/abs/2312.02615"}, {"metadata": {"arXiv": "2312.02619", "Date": "Tue, 05 Dec 2023 09:49:50 ", "Title": "Rethinking and Simplifying Bootstrapped Graph Latents", "Authors": ["Wangbin Sun", "Jintang Li", "Liang Chen", "Bingzhe Wu", "Yatao Bian", "Zibin Zheng"], "Categories": "cs.LG", "Comments": ["Accepted by WSDM 2024"]}, "abstract": "Graph contrastive learning (GCL) has emerged as a representative paradigm in graph self-supervised learning, where negative samples are commonly regarded as the key to preventing model collapse and producing distinguishable representations. Recent studies have shown that GCL without negative samples can achieve state-of-the-art performance as well as scalability improvement, with bootstrapped graph latent (BGRL) as a prominent step forward. However, BGRL relies on a complex architecture to maintain the ability to scatter representations, and the underlying mechanisms enabling the success remain largely unexplored. In this paper, we introduce an instance-level decorrelation perspective to tackle the aforementioned issue and leverage it as a springboard to reveal the potential unnecessary model complexity within BGRL. Based on our findings, we present SGCL, a simple yet effective GCL framework that utilizes the outputs from two consecutive iterations as positive pairs, eliminating the negative samples. SGCL only requires a single graph augmentation and a single graph encoder without additional parameters. Extensive experiments conducted on various graph benchmarks demonstrate that SGCL can achieve competitive performance with fewer parameters, lower time and space costs, and significant convergence speedup.", "url": "https://arxiv.org/abs/2312.02619"}, {"metadata": {"arXiv": "2312.02658", "Date": "Tue, 05 Dec 2023 10:52:33 ", "Title": "Do AI models produce better weather forecasts than physics-based models? A quantitative evaluation case study of Storm Ciar\\'an", "Authors": ["Andrew J. Charlton-Perez", "Helen F. Dacre", "Simon Driscoll", "Suzanne L. Gray", "Ben Harvey", "Natalie J. Harvey", "Kieran M. R. Hunt", "Robert W. Lee", "Ranjini Swaminathan", "Remy Vandaele", "Ambrogio Volont\\'e"], "Categories": "cs.LG physics.ao-ph"}, "abstract": "There has been huge recent interest in the potential of making operational weather forecasts using machine learning techniques. As they become a part of the weather forecasting toolbox, there is a pressing need to understand how well current machine learning models can simulate high-impactweather events. We compare forecasts of Storm Ciar\\'an, a European windstorm that caused sixteen deaths and extensive damage in Northern Europe, made by machine learning and numericalweather prediction models. The four machine learning models considered (FourCastNet, Pangu-Weather, GraphCast and FourCastNet-v2) produce forecasts that accurately capture the synoptic-scale structure of the cyclone including the position of the cloud head, shape of the warm sector and location of warm conveyor belt jet, and the large-scale dynamical drivers important for the rapid storm development such as the position of the storm relative to the upper-level jet exit. However, their ability to resolve the more detailed structures important for issuing weather warnings is more mixed. All of the machine learning models underestimate the peak amplitude of winds associated with the storm, only some machine learning models resolve the warm core seclusion and none of the machine learning models capture the sharp bent-back warm frontal gradient. Our study shows there is a great deal about the performance and properties of machine learning weather forecasts that can be derived from case studies of high-impact weather events such as Storm Ciar\\'an.", "url": "https://arxiv.org/abs/2312.02658"}, {"metadata": {"arXiv": "2312.02661", "Date": "Tue, 05 Dec 2023 10:56:25 ", "Title": "A Self-Commissioning Edge Computing Method for Data-Driven Anomaly Detection in Power Electronic Systems", "Authors": ["Pere Izquierdo Gomez", "Miguel E. Lopez Gajardo", "Nenad Mijatovic", "Tomislav Dragicevic"], "Categories": "cs.LG eess.SP"}, "abstract": "Ensuring the reliability of power electronic converters is a matter of great importance, and data-driven condition monitoring techniques are cementing themselves as an important tool for this purpose. However, translating methods that work well in controlled lab environments to field applications presents significant challenges, notably because of the limited diversity and accuracy of the lab training data. By enabling the use of field data, online machine learning can be a powerful tool to overcome this problem, but it introduces additional challenges in ensuring the stability and predictability of the training processes. This work presents an edge computing method that mitigates these shortcomings with minimal additional memory usage, by employing an autonomous algorithm that prioritizes the storage of training samples with larger prediction errors. The method is demonstrated on the use case of a self-commissioning condition monitoring system, in the form of a thermal anomaly detection scheme for a variable frequency motor drive, where the algorithm self-learned to distinguish normal and anomalous operation with minimal prior knowledge. The obtained results, based on experimental data, show a significant improvement in prediction accuracy and training speed, when compared to equivalent models trained online without the proposed data selection process.", "url": "https://arxiv.org/abs/2312.02661"}, {"metadata": {"arXiv": "2312.02708", "Date": "Tue, 05 Dec 2023 12:09:45 ", "Title": "(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More", "Authors": ["Jan Schuchardt", "Yan Scholten", "Stephan G\\\"unnemann"], "Categories": "cs.LG cs.CR stat.ML", "Comments": ["Accepted at NeurIPS 2023"]}, "abstract": "A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.", "url": "https://arxiv.org/abs/2312.02708"}, {"metadata": {"arXiv": "2312.02730", "Date": "Tue, 05 Dec 2023 12:48:04 ", "Title": "Towards Measuring Representational Similarity of Large Language Models", "Authors": ["Max Klabunde", "Mehdi Ben Amor", "Michael Granitzer", "Florian Lemmerich"], "Categories": "cs.LG cs.CL", "Comments": ["Extended abstract in UniReps Workshop @ NeurIPS 2023"]}, "abstract": "Understanding the similarity of the numerous released large language models (LLMs) has many uses, e.g., simplifying model selection, detecting illegal model reuse, and advancing our understanding of what makes LLMs perform well. In this work, we measure the similarity of representations of a set of LLMs with 7B parameters. Our results suggest that some LLMs are substantially different from others. We identify challenges of using representational similarity measures that suggest the need of careful study of similarity scores to avoid false conclusions.", "url": "https://arxiv.org/abs/2312.02730"}, {"metadata": {"arXiv": "2312.02739", "Date": "Tue, 05 Dec 2023 13:06:25 ", "Title": "LExCI: A Framework for Reinforcement Learning with Embedded Systems", "Authors": ["Kevin Badalian", "Lucas Koch", "Tobias Brinkmann", "Mario Picerno", "Marius Wegener", "Sung-Yong Lee", "Jakob Andert"], "Categories": "cs.LG", "Comments": ["The code", "models", "and data used for this work are available in a separate branch of LExCI's GitHub repository (https://github.com/mechatronics-RWTH/lexci-2/tree/lexci_paper). This paper has been submitted to Applied Intelligence (https://link.springer.com/journal/10489)"]}, "abstract": "Advances in artificial intelligence (AI) have led to its application in many areas of everyday life. In the context of control engineering, reinforcement learning (RL) represents a particularly promising approach as it is centred around the idea of allowing an agent to freely interact with its environment to find an optimal strategy. One of the challenges professionals face when training and deploying RL agents is that the latter often have to run on dedicated embedded devices. This could be to integrate them into an existing toolchain or to satisfy certain performance criteria like real-time constraints. Conventional RL libraries, however, cannot be easily utilised in conjunction with that kind of hardware. In this paper, we present a framework named LExCI, the Learning and Experiencing Cycle Interface, which bridges this gap and provides end-users with a free and open-source tool for training agents on embedded systems using the open-source library RLlib. Its operability is demonstrated with two state-of-the-art RL-algorithms and a rapid control prototyping system.", "url": "https://arxiv.org/abs/2312.02739"}, {"metadata": {"arXiv": "2312.02770", "Date": "Tue, 05 Dec 2023 14:00:32 ", "Title": "Learning \"Look-Ahead\" Nonlocal Traffic Dynamics in a Ring Road", "Authors": ["Chenguang Zhao", "Huan Yu"], "Categories": "cs.LG cs.SY eess.SY"}, "abstract": "The macroscopic traffic flow model is widely used for traffic control and management. To incorporate drivers' anticipative behaviors and to remove impractical speed discontinuity inherent in the classic Lighthill-Whitham-Richards (LWR) traffic model, nonlocal partial differential equation (PDE) models with ``look-ahead\" dynamics have been proposed, which assume that the speed is a function of weighted downstream traffic density. However, it lacks data validation on two important questions: whether there exist nonlocal dynamics, and how the length and weight of the ``look-ahead\" window affect the spatial temporal propagation of traffic densities. In this paper, we adopt traffic trajectory data from a ring-road experiment and design a physics-informed neural network to learn the fundamental diagram and look-ahead kernel that best fit the data, and reinvent a data-enhanced nonlocal LWR model via minimizing the loss function combining the data discrepancy and the nonlocal model discrepancy. Results show that the learned nonlocal LWR yields a more accurate prediction of traffic wave propagation in three different scenarios: stop-and-go oscillations, congested, and free traffic. We first demonstrate the existence of ``look-ahead\" effect with real traffic data. The optimal nonlocal kernel is found out to take a length of around 35 to 50 meters, and the kernel weight within 5 meters accounts for the majority of the nonlocal effect. Our results also underscore the importance of choosing a priori physics in machine learning models.", "url": "https://arxiv.org/abs/2312.02770"}, {"metadata": {"arXiv": "2312.02780", "Date": "Tue, 05 Dec 2023 14:12:15 ", "Title": "Scaling Laws for Adversarial Attacks on Language Model Activations", "Authors": ["Stanislav Fort"], "Categories": "cs.LG cs.CL cs.CR", "Comments": ["15 pages", "9 figures"]}, "abstract": "We explore a class of adversarial attacks targeting the activations of language models. By manipulating a relatively small subset of model activations, $a$, we demonstrate the ability to control the exact prediction of a significant number (in some cases up to 1000) of subsequent tokens $t$. We empirically verify a scaling law where the maximum number of target tokens $t_\\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose activations the attacker controls as $t_\\mathrm{max} = \\kappa a$. We find that the number of bits of control in the input space needed to control a single bit in the output space (what we call attack resistance $\\chi$) is remarkably constant between $\\approx 16$ and $\\approx 25$ over 2 orders of magnitude of model sizes for different language models. Compared to attacks on tokens, attacks on activations are predictably much stronger, however, we identify a surprising regularity where one bit of input steered either via activations or via tokens is able to exert control over a similar amount of output bits. This gives support for the hypothesis that adversarial attacks are a consequence of dimensionality mismatch between the input and output spaces. A practical implication of the ease of attacking language model activations instead of tokens is for multi-modal and selected retrieval models, where additional data sources are added as activations directly, sidestepping the tokenized input. This opens up a new, broad attack surface. By using language models as a controllable test-bed to study adversarial attacks, we were able to experiment with input-output dimensions that are inaccessible in computer vision, especially where the output dimension dominates.", "url": "https://arxiv.org/abs/2312.02780"}, {"metadata": {"arXiv": "2312.02798", "Date": "Tue, 05 Dec 2023 14:35:11 ", "Title": "Weakly Supervised Detection of Hallucinations in LLM Activations", "Authors": ["Miriam Rateike", "Celia Cintas", "John Wamburu", "Tanya Akumu", "Skyler Speakman"], "Categories": "cs.LG cs.CL"}, "abstract": "We propose an auditing method to identify whether a large language model (LLM) encodes patterns such as hallucinations in its internal states, which may propagate to downstream tasks. We introduce a weakly supervised auditing technique using a subset scanning approach to detect anomalous patterns in LLM activations from pre-trained models. Importantly, our method does not need knowledge of the type of patterns a-priori. Instead, it relies on a reference dataset devoid of anomalies during testing. Further, our approach enables the identification of pivotal nodes responsible for encoding these patterns, which may offer crucial insights for fine-tuning specific sub-networks for bias mitigation. We introduce two new scanning methods to handle LLM activations for anomalous sentences that may deviate from the expected distribution in either direction. Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally. Importantly, our scanning approach, without prior exposure to false statements, performs comparably to a fully supervised out-of-distribution classifier.", "url": "https://arxiv.org/abs/2312.02798"}, {"metadata": {"arXiv": "2312.02804", "Date": "Tue, 05 Dec 2023 14:44:58 ", "Title": "Score-Aware Policy-Gradient Methods and Performance Guarantees using Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks and Queueing Systems", "Authors": ["C\\'eline Comte", "Matthieu Jonckheere", "Jaron Sanders and Albert Senen-Cerda"], "Categories": "cs.LG cs.PF math.OC math.PR", "Comments": ["45 pages", "5 figures"]}, "abstract": "Stochastic networks and queueing systems often lead to Markov decision processes (MDPs) with large state and action spaces as well as nonconvex objective functions, which hinders the convergence of many reinforcement learning (RL) algorithms. Policy-gradient methods perform well on MDPs with large state and action spaces, but they sometimes experience slow convergence due to the high variance of the gradient estimator. In this paper, we show that some of these difficulties can be circumvented by exploiting the structure of the underlying MDP. We first introduce a new family of gradient estimators called score-aware gradient estimators (SAGEs). When the stationary distribution of the MDP belongs to an exponential family parametrized by the policy parameters, SAGEs allow us to estimate the policy gradient without relying on value-function estimation, contrary to classical policy-gradient methods like actor-critic. To demonstrate their applicability, we examine two common control problems arising in stochastic networks and queueing systems whose stationary distributions have a product-form, a special case of exponential families. As a second contribution, we show that, under appropriate assumptions, the policy under a SAGE-based policy-gradient method has a large probability of converging to an optimal policy, provided that it starts sufficiently close to it, even with a nonconvex objective function and multiple maximizers. Our key assumptions are that, locally around a maximizer, a nondegeneracy property of the Hessian of the objective function holds and a Lyapunov function exists. Finally, we conduct a numerical comparison between a SAGE-based policy-gradient method and an actor-critic algorithm. The results demonstrate that the SAGE-based method finds close-to-optimal policies more rapidly, highlighting its superior performance over the traditional actor-critic method.", "url": "https://arxiv.org/abs/2312.02804"}, {"metadata": {"arXiv": "2312.02852", "Date": "Tue, 05 Dec 2023 16:09:31 ", "Title": "Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems", "Authors": ["Tom Savage", "Ehecatl Antonio del Rio Chanona"], "Categories": "cs.LG cs.HC math.OC", "Comments": ["NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World. Main text: 6 pages"]}, "abstract": "Domain experts often possess valuable physical insights that are overlooked in fully automated decision-making processes such as Bayesian optimisation. In this article we apply high-throughput (batch) Bayesian optimisation alongside anthropological decision theory to enable domain experts to influence the selection of optimal experiments. Our methodology exploits the hypothesis that humans are better at making discrete choices than continuous ones and enables experts to influence critical early decisions. At each iteration we solve an augmented multi-objective optimisation problem across a number of alternate solutions, maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability. By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct, from which the expert selects one for evaluation. We demonstrate that even in the case of an uninformed practitioner, our algorithm recovers the regret of standard Bayesian optimisation.", "url": "https://arxiv.org/abs/2312.02852"}, {"metadata": {"arXiv": "2312.02859", "Date": "Tue, 05 Dec 2023 16:13:50 ", "Title": "Lessons from Usable ML Deployments and Application to Wind Turbine Monitoring", "Authors": ["Alexandra Zytek", "Wei-En Wang", "Sofia Koukoura", "and Kalyan Veeramachaneni"], "Categories": "cs.LG", "Comments": ["Presented in XAI in Action: Past", "Present", "and Future Applications @ NeurIPS 2023. 8 pages", "3 figures"]}, "abstract": "Through past experiences deploying what we call usable ML (one step beyond explainable ML, including both explanations and other augmenting information) to real-world domains, we have learned three key lessons. First, many organizations are beginning to hire people who we call ``bridges'' because they bridge the gap between ML developers and domain experts, and these people fill a valuable role in developing usable ML applications. Second, a configurable system that enables easily iterating on usable ML interfaces during collaborations with bridges is key. Finally, there is a need for continuous, in-deployment evaluations to quantify the real-world impact of usable ML. Throughout this paper, we apply these lessons to the task of wind turbine monitoring, an essential task in the renewable energy domain. Turbine engineers and data analysts must decide whether to perform costly in-person investigations on turbines to prevent potential cases of brakepad failure, and well-tuned usable ML interfaces can aid with this decision-making process. Through the applications of our lessons to this task, we hope to demonstrate the potential real-world impact of usable ML in the renewable energy domain.", "url": "https://arxiv.org/abs/2312.02859"}, {"metadata": {"arXiv": "2312.02867", "Date": "Tue, 05 Dec 2023 16:27:51 ", "Title": "Semi-Supervised Health Index Monitoring with Feature Generation and Fusion", "Authors": ["Ga\\\"etan Frusque", "Ismail Nejjar", "Majid Nabavi", "Olga Fink"], "Categories": "cs.LG stat.ME", "Comments": ["13 pages", "8 figures"]}, "abstract": "The Health Index (HI) is crucial for evaluating system health, aiding tasks like anomaly detection and predicting remaining useful life for systems demanding high safety and reliability. Tight monitoring is crucial for achieving high precision at a lower cost, with applications such as spray coating. Obtaining HI labels in real-world applications is often cost-prohibitive, requiring continuous, precise health measurements. Therefore, it is more convenient to leverage run-to failure datasets that may provide potential indications of machine wear condition, making it necessary to apply semi-supervised tools for HI construction. In this study, we adapt the Deep Semi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use the DeepSAD embedding as a condition indicators to address interpretability challenges and sensitivity to system-specific factors. Then, we introduce a diversity loss to enrich condition indicators. We employ an alternating projection algorithm with isotonic constraints to transform the DeepSAD embedding into a normalized HI with an increasing trend. Validation on the PHME 2010 milling dataset, a recognized benchmark with ground truth HIs demonstrates meaningful HIs estimations. Our methodology is then applied to monitor wear states of thermal spray coatings using high-frequency voltage. Our contributions create opportunities for more accessible and reliable HI estimation, particularly in cases where obtaining ground truth HI labels is unfeasible.", "url": "https://arxiv.org/abs/2312.02867"}, {"metadata": {"arXiv": "2312.02871", "Date": "Tue, 05 Dec 2023 16:39:24 ", "Title": "Attention-enhanced neural differential equations for physics-informed deep learning of ion transport", "Authors": ["Danyal Rehman and John H. Lienhard"], "Categories": "cs.LG math-ph math.MP physics.comp-ph", "Comments": ["8 pages", "2 figures. Accepted in the NeurIPS Machine Learning and the Physical Sciences Workshop"]}, "abstract": "Species transport models typically combine partial differential equations (PDEs) with relations from hindered transport theory to quantify electromigrative, convective, and diffusive transport through complex nanoporous systems; however, these formulations are frequently substantial simplifications of the governing dynamics, leading to the poor generalization performance of PDE-based models. Given the growing interest in deep learning methods for the physical sciences, we develop a machine learning-based approach to characterize ion transport across nanoporous membranes. Our proposed framework centers around attention-enhanced neural differential equations that incorporate electroneutrality-based inductive biases to improve generalization performance relative to conventional PDE-based methods. In addition, we study the role of the attention mechanism in illuminating physically-meaningful ion-pairing relationships across diverse mixture compositions. Further, we investigate the importance of pre-training on simulated data from PDE-based models, as well as the performance benefits from hard vs. soft inductive biases. Our results indicate that physics-informed deep learning solutions can outperform their classical PDE-based counterparts and provide promising avenues for modelling complex transport phenomena across diverse applications.", "url": "https://arxiv.org/abs/2312.02871"}, {"metadata": {"arXiv": "2312.02901", "Date": "Tue, 05 Dec 2023 17:15:16 ", "Title": "Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive Review", "Authors": ["Cristiano Mesquita Garcia and Ramon Simoes Abilio and Alessandro Lameiras Koerich and Alceu de Souza Britto Jr. and Jean Paul Barddal"], "Categories": "cs.LG cs.CL cs.IR", "Comments": ["49 pages"]}, "abstract": "Due to the advent and increase in the popularity of the Internet, people have been producing and disseminating textual data in several ways, such as reviews, social media posts, and news articles. As a result, numerous researchers have been working on discovering patterns in textual data, especially because social media posts function as social sensors, indicating peoples' opinions, interests, etc. However, most tasks regarding natural language processing are addressed using traditional machine learning methods and static datasets. This setting can lead to several problems, such as an outdated dataset, which may not correspond to reality, and an outdated model, which has its performance degrading over time. Concept drift is another aspect that emphasizes these issues, which corresponds to data distribution and pattern changes. In a text stream scenario, it is even more challenging due to its characteristics, such as the high speed and data arriving sequentially. In addition, models for this type of scenario must adhere to the constraints mentioned above while learning from the stream by storing texts for a limited time and consuming low memory. In this study, we performed a systematic literature review regarding concept drift adaptation in text stream scenarios. Considering well-defined criteria, we selected 40 papers to unravel aspects such as text drift categories, types of text drift detection, model update mechanism, the addressed stream mining tasks, types of text representations, and text representation update mechanism. In addition, we discussed drift visualization and simulation and listed real-world datasets used in the selected papers. Therefore, this paper comprehensively reviews the concept drift adaptation in text stream mining scenarios.", "url": "https://arxiv.org/abs/2312.02901"}, {"metadata": {"arXiv": "2312.02396", "Date": "Mon, 04 Dec 2023 23:26:12 ", "Title": "Unsupervised Change Detection for Space Habitats Using 3D Point Clouds", "Authors": ["Jamie Santos", "Holly Dinkel", "Julia Di", "Paulo V.K. Borges", "Marina Moreira", "Oleg Alexandrov", "Brian Coltin", "and Trey Smith"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["15 pages", "7 figures", "Manuscript will be presented at the AIAA SciTech Forum in Orlando", "FL", "USA", "8 - 12 January 2024"]}, "abstract": "This work presents an algorithm for scene change detection from point clouds to enable autonomous robotic caretaking in future space habitats. Autonomous robotic systems will help maintain future deep-space habitats, such as the Gateway space station, which will be uncrewed for extended periods. Existing scene analysis software used on the International Space Station (ISS) relies on manually-labeled images for detecting changes. In contrast, the algorithm presented in this work uses raw, unlabeled point clouds as inputs. The algorithm first applies modified Expectation-Maximization Gaussian Mixture Model (GMM) clustering to two input point clouds. It then performs change detection by comparing the GMMs using the Earth Mover's Distance. The algorithm is validated quantitatively and qualitatively using a test dataset collected by an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth images taken directly by Astrobee and full-scene reconstructed maps built with RGB-D and pose data from Astrobee. The runtimes of the approach are also analyzed in depth. The source code is publicly released to promote further development.", "url": "https://arxiv.org/abs/2312.02396"}, {"metadata": {"arXiv": "2312.02649", "Date": "Tue, 05 Dec 2023 10:40:48 ", "Title": "A Q-learning approach to the continuous control problem of robot inverted pendulum balancing", "Authors": ["Mohammad Safeea", "Pedro Neto"], "Categories": "cs.RO cs.LG", "DOI": "10.1016/j.iswa.2023.200313"}, "abstract": "This study evaluates the application of a discrete action space reinforcement learning method (Q-learning) to the continuous control problem of robot inverted pendulum balancing. To speed up the learning process and to overcome technical difficulties related to the direct learning on the real robotic system, the learning phase is performed in simulation environment. A mathematical model of the system dynamics is implemented, deduced by curve fitting on data acquired from the real system. The proposed approach demonstrated feasible, featuring its application on a real world robot that learned to balance an inverted pendulum. This study also reinforces and demonstrates the importance of an accurate representation of the physical world in simulation to achieve a more efficient implementation of reinforcement learning algorithms in real world, even when using a discrete action space algorithm to control a continuous action.", "url": "https://arxiv.org/abs/2312.02649"}, {"metadata": {"arXiv": "2312.02478", "Date": "Tue, 05 Dec 2023 04:06:09 ", "Title": "RL-Based Cargo-UAV Trajectory Planning and Cell Association for Minimum Handoffs, Disconnectivity, and Energy Consumption", "Authors": ["Nesrine Cherif", "Wael Jaafar", "Halim Yanikomeroglu", "Abbas Yongacoglu"], "Categories": "eess.SY cs.LG cs.RO cs.SY"}, "abstract": "Unmanned aerial vehicle (UAV) is a promising technology for last-mile cargo delivery. However, the limited on-board battery capacity, cellular unreliability, and frequent handoffs in the airspace are the main obstacles to unleash its full potential. Given that existing cellular networks were primarily designed to service ground users, re-utilizing the same architecture for highly mobile aerial users, e.g., cargo-UAVs, is deemed challenging. Indeed, to ensure a safe delivery using cargo-UAVs, it is crucial to utilize the available energy efficiently, while guaranteeing reliable connectivity for command-and-control and avoiding frequent handoff. To achieve this goal, we propose a novel approach for joint cargo-UAV trajectory planning and cell association. Specifically, we formulate the cargo-UAV mission as a multi-objective problem aiming to 1) minimize energy consumption, 2) reduce handoff events, and 3) guarantee cellular reliability along the trajectory. We leverage reinforcement learning (RL) to jointly optimize the cargo-UAV's trajectory and cell association. Simulation results demonstrate a performance improvement of our proposed method, in terms of handoffs, disconnectivity, and energy consumption, compared to benchmarks.", "url": "https://arxiv.org/abs/2312.02478"}, {"metadata": {"arXiv": "2312.02206", "Date": "Sat, 02 Dec 2023 23:11:41 ", "Title": "Axiomatic Preference Modeling for Longform Question Answering", "Authors": ["Corby Rosset", "Guoqing Zheng", "Victor Dibia", "Ahmed Awadallah", "Paul Bennett"], "Categories": "cs.AI cs.CL", "Comments": ["Accepted to EMNLP 2023"]}, "abstract": "The remarkable abilities of large language models (LLMs) like GPT-4 partially stem from post-training processes like Reinforcement Learning from Human Feedback (RLHF) involving human preferences encoded in a reward model. However, these reward models (RMs) often lack direct knowledge of why, or under what principles, the preferences annotations were made. In this study, we identify principles that guide RMs to better align with human preferences, and then develop an axiomatic framework to generate a rich variety of preference signals to uphold them. We use these axiomatic signals to train a model for scoring answers to longform questions. Our approach yields a Preference Model with only about 220M parameters that agrees with gold human-annotated preference labels more often than GPT-4. The contributions of this work include: training a standalone preference model that can score human- and LLM-generated answers on the same scale; developing an axiomatic framework for generating training data pairs tailored to certain principles; and showing that a small amount of axiomatic signals can help small models outperform GPT-4 in preference scoring. We release our model on huggingface: https://huggingface.co/corbyrosset/axiomatic_preference_model", "url": "https://arxiv.org/abs/2312.02206"}, {"metadata": {"arXiv": "2312.02231", "Date": "Mon, 04 Dec 2023 05:16:53 ", "Title": "Quality Diversity in the Amorphous Fortress (QD-AF): Evolving for Complexity in 0-Player Games", "Authors": ["Sam Earle", "M Charity", "Dipika Rajesh", "Mayu Wilson", "Julian Togelius"], "Categories": "cs.AI cs.MA", "Comments": ["18 pages", "7 figures", "ALOE workship at NeurIPS 2023"]}, "abstract": "We explore the generation of diverse environments using the Amorphous Fortress (AF) simulation framework. AF defines a set of Finite State Machine (FSM) nodes and edges that can be recombined to control the behavior of agents in the `fortress' grid-world. The behaviors and conditions of the agents within the framework are designed to capture the common building blocks of multi-agent artificial life and reinforcement learning environments. Using quality diversity evolutionary search, we generate diverse sets of environments. These environments exhibit certain types of complexity according to measures of agents' FSM architectures and activations, and collective behaviors. Our approach, Quality Diversity in Amorphous Fortress (QD-AF) generates families of 0-player games akin to simplistic ecological models, and we identify the emergence of both competitive and co-operative multi-agent and multi-species survival dynamics. We argue that these generated worlds can collectively serve as training and testing grounds for learning algorithms.", "url": "https://arxiv.org/abs/2312.02231"}, {"metadata": {"arXiv": "2312.02405", "Date": "Tue, 05 Dec 2023 00:29:44 ", "Title": "BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks", "Authors": ["Stephanie Milani", "Anssi Kanervisto", "Karolis Ramanauskas", "Sander Schulhoff", "Brandon Houghton", "Rohin Shah"], "Categories": "cs.AI", "Comments": ["NeurIPS 2023 Datasets and Benchmarks Oral. Dataset links are available on Github: https://github.com/minerllabs/basalt-benchmark"]}, "abstract": "The MineRL BASALT competition has served to catalyze advances in learning from human feedback through four hard-to-specify tasks in Minecraft, such as create and photograph a waterfall. Given the completion of two years of BASALT competitions, we offer to the community a formalized benchmark through the BASALT Evaluation and Demonstrations Dataset (BEDD), which serves as a resource for algorithm development and performance assessment. BEDD consists of a collection of 26 million image-action pairs from nearly 14,000 videos of human players completing the BASALT tasks in Minecraft. It also includes over 3,000 dense pairwise human evaluations of human and algorithmic agents. These comparisons serve as a fixed, preliminary leaderboard for evaluating newly-developed algorithms. To enable this comparison, we present a streamlined codebase for benchmarking new algorithms against the leaderboard. In addition to presenting these datasets, we conduct a detailed analysis of the data from both datasets to guide algorithm development and evaluation. The released code and data are available at https://github.com/minerllabs/basalt-benchmark .", "url": "https://arxiv.org/abs/2312.02405"}, {"metadata": {"arXiv": "2312.02439", "Date": "Tue, 05 Dec 2023 02:41:57 ", "Title": "Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation", "Authors": ["Shanshan Zhong", "Zhongzhan Huang", "Shanghua Gao", "Wushao Wen", "Liang Lin", "Marinka Zitnik", "Pan Zhou"], "Categories": "cs.AI cs.CL cs.CV", "Comments": ["Technical report"]}, "abstract": "Chain-of-Thought (CoT) guides large language models (LLMs) to reason step-by-step, and can motivate their logical reasoning ability. While effective for logical tasks, CoT is not conducive to creative problem-solving which often requires out-of-box thoughts and is crucial for innovation advancements. In this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a non-sequential, creative paradigm involving strong associations and knowledge leaps. To this end, we study LLMs on the popular Oogiri game which needs participants to have good creativity and strong associative thinking for responding unexpectedly and humorously to the given image, text, or both, and thus is suitable for LoT study. Then to investigate LLMs' LoT ability in the Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset which contains over 130,000 samples from the Oogiri game, and observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game. Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve LLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into LoT-oriented instruction tuning data to train pretrained LLM for achieving certain LoT humor generation and discrimination abilities. Then CLoT designs an explorative self-refinement that encourages the LLM to generate more creative LoT data via exploring parallels between seemingly unrelated concepts and selects high-quality data to train itself for self-refinement. CLoT not only excels in humor generation in the Oogiri game but also boosts creative abilities in various tasks like cloud guessing game and divergent association task. These findings advance our understanding and offer a pathway to improve LLMs' creative capacities for innovative applications across domains. The dataset, code, and models will be released online. https://github.com/sail-sg/CLoT.", "url": "https://arxiv.org/abs/2312.02439"}, {"metadata": {"arXiv": "2312.02561", "Date": "Tue, 05 Dec 2023 08:07:32 ", "Title": "DanZero+: Dominating the GuanDan Game through Reinforcement Learning", "Authors": ["Youpeng Zhao and Yudong Lu and Jian Zhao and Wengang Zhou and Houqiang Li"], "Categories": "cs.AI", "Comments": ["arXiv admin note: text overlap with arXiv:2210.17087"]}, "abstract": "The utilization of artificial intelligence (AI) in card games has been a well-explored subject within AI research for an extensive period. Recent advancements have propelled AI programs to showcase expertise in intricate card games such as Mahjong, DouDizhu, and Texas Hold'em. In this work, we aim to develop an AI program for an exceptionally complex and popular card game called GuanDan. This game involves four players engaging in both competitive and cooperative play throughout a long process to upgrade their level, posing great challenges for AI due to its expansive state and action space, long episode length, and complex rules. Employing reinforcement learning techniques, specifically Deep Monte Carlo (DMC), and a distributed training framework, we first put forward an AI program named DanZero for this game. Evaluation against baseline AI programs based on heuristic rules highlights the outstanding performance of our bot. Besides, in order to further enhance the AI's capabilities, we apply policy-based reinforcement learning algorithm to GuanDan. To address the challenges arising from the huge action space, which will significantly impact the performance of policy-based algorithms, we adopt the pre-trained model to facilitate the training process and the achieved AI program manages to achieve a superior performance.", "url": "https://arxiv.org/abs/2312.02561"}, {"metadata": {"arXiv": "2312.02706", "Date": "Tue, 05 Dec 2023 12:07:30 ", "Title": "Large Knowledge Model: Perspectives and Challenges", "Authors": ["Huajun Chen"], "Categories": "cs.AI cs.CL", "Comments": ["This is an early draft subject to revision in a near future"]}, "abstract": "Humankind's understanding of the world is fundamentally linked to our perception and cognition, with \\emph{human languages} serving as one of the major carriers of \\emph{world knowledge}. In this vein, \\emph{Large Language Models} (LLMs) like ChatGPT epitomize the pre-training of extensive, sequence-based world knowledge into neural networks, facilitating the processing and manipulation of this knowledge in a parametric space. This article explores large models through the lens of ``knowledge''. We initially investigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in enhancing LLMs, covering aspects like knowledge-augmented language model, structure-inducing pre-training, knowledgeable prompts, structured CoT, knowledge editing, semantic tools for LLM and knowledgeable AI agents. Subsequently, we examine how LLMs can amplify traditional symbolic knowledge bases, encompassing aspects like using LLM as KG builder and controller, structured knowledge pretraining, LLM-enhanced symbolic reasoning, and the amalgamation of perception with cognition. Considering the intricate nature of human knowledge, we advocate for the creation of \\emph{Large Knowledge Models} (LKM), specifically engineered to manage diversified spectrum of knowledge structures. This ambitious undertaking could entail several key challenges, such as disentangling knowledge representation from language models, restructuring pre-training with structured knowledge, and building large commonsense models, among others. We finally propose a five-``A'' principle to distinguish the concept of LKM.", "url": "https://arxiv.org/abs/2312.02706"}, {"metadata": {"arXiv": "2312.02188", "Date": "Fri, 01 Dec 2023 23:56:00 ", "Title": "Video Summarization: Towards Entity-Aware Captions", "Authors": ["Hammad A. Ayyubi", "Tianqi Liu", "Arsha Nagrani", "Xudong Lin", "Mingda Zhang", "Anurag Arnab", "Feng Han", "Yukun Zhu", "Jialu Liu", "Shih-Fu Chang"], "Categories": "cs.CV cs.AI cs.CL cs.MM"}, "abstract": "Existing popular video captioning benchmarks and models deal with generic captions devoid of specific person, place or organization named entities. In contrast, news videos present a challenging setting where the caption requires such named entities for meaningful summarization. As such, we propose the task of summarizing news video directly to entity-aware captions. We also release a large-scale dataset, VIEWS (VIdeo NEWS), to support research on this task. Further, we propose a method that augments visual information from videos with context retrieved from external world knowledge to generate entity-aware captions. We demonstrate the effectiveness of our approach on three video captioning models. We also show that our approach generalizes to existing news image captions dataset. With all the extensive experiments and insights, we believe we establish a solid basis for future research on this challenging task.", "url": "https://arxiv.org/abs/2312.02188"}, {"metadata": {"arXiv": "2312.02189", "Date": "Sat, 02 Dec 2023 02:27:58 ", "Title": "StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D", "Authors": ["Pengsheng Guo", "Hans Hao", "Adam Caccavale", "Zhongzheng Ren", "Edward Zhang", "Qi Shan", "Aditya Sankar", "Alexander G. Schwing", "Alex Colburn", "Fangchang Ma"], "Categories": "cs.CV cs.AI"}, "abstract": "In the realm of text-to-3D generation, utilizing 2D diffusion models through score distillation sampling (SDS) frequently leads to issues such as blurred appearances and multi-faced geometry, primarily due to the intrinsically noisy nature of the SDS loss. Our analysis identifies the core of these challenges as the interaction among noise levels in the 2D diffusion process, the architecture of the diffusion network, and the 3D model representation. To overcome these limitations, we present StableDreamer, a methodology incorporating three advances. First, inspired by InstructNeRF2NeRF, we formalize the equivalence of the SDS generative prior and a simple supervised L2 reconstruction loss. This finding provides a novel tool to debug SDS, which we use to show the impact of time-annealing noise levels on reducing multi-faced geometries. Second, our analysis shows that while image-space diffusion contributes to geometric precision, latent-space diffusion is crucial for vivid color rendition. Based on this observation, StableDreamer introduces a two-stage training strategy that effectively combines these aspects, resulting in high-fidelity 3D models. Third, we adopt an anisotropic 3D Gaussians representation, replacing Neural Radiance Fields (NeRFs), to enhance the overall quality, reduce memory usage during training, and accelerate rendering speeds, and better capture semi-transparent objects. StableDreamer reduces multi-face geometries, generates fine details, and converges stably.", "url": "https://arxiv.org/abs/2312.02189"}, {"metadata": {"arXiv": "2312.02191", "Date": "Sat, 02 Dec 2023 07:32:24 ", "Title": "Prompt Tuning for Zero-shot Compositional Learning", "Authors": ["Lingyu Zhang", "Ting Hua", "Yilin Shen", "Hongxia Jin"], "Categories": "cs.CV cs.AI"}, "abstract": "Open World Compositional Zero-Shot Learning (OW-CZSL) is known to be an extremely challenging task, which aims to recognize unseen compositions formed from seen attributes and objects without any prior assumption of the output space. In order to achieve this goal, a model has to be \"smart\" and \"knowledgeable\". To be smart, a model should be good at reasoning the interactions between attributes and objects from the seen compositions. While \"knowledgeable\" means the model owns \"common sense\" to the open world that can \"foresee\" some features of the unseen compositions. Most previous work focuses on the \"smart\" part, while few of them provided an effective solution to achieve the \"knowledgeable\" goal. In this paper, we proposed a framework named Multi-Modal Prompt Tuning (MMPT) to inherit the \"knowledgeable\" property from the large pre-trained vision-language model. Extensive experiments show that our proposed MMPT obtains new state-of-the-art results in OW-CZSL task. On the UT-Zappos dataset, MMPT pushes the AUC score to $29.8$, while the previous best score is $26.5$. On the more challenging MIT-States dataset, the AUC score of MMPT is 1.5 times better than the current state-of-the-art.", "url": "https://arxiv.org/abs/2312.02191"}, {"metadata": {"arXiv": "2312.02200", "Date": "Sat, 02 Dec 2023 19:33:42 ", "Title": "An Empirical Study of Automated Mislabel Detection in Real World Vision Datasets", "Authors": ["Maya Srikanth", "Jeremy Irvin", "Brian Wesley Hill", "Felipe Godoy", "Ishan Sabane", "Andrew Y. Ng"], "Categories": "cs.CV cs.AI stat.AP"}, "abstract": "Major advancements in computer vision can primarily be attributed to the use of labeled datasets. However, acquiring labels for datasets often results in errors which can harm model performance. Recent works have proposed methods to automatically identify mislabeled images, but developing strategies to effectively implement them in real world datasets has been sparsely explored. Towards improved data-centric methods for cleaning real world vision datasets, we first conduct more than 200 experiments carefully benchmarking recently developed automated mislabel detection methods on multiple datasets under a variety of synthetic and real noise settings with varying noise levels. We compare these methods to a Simple and Efficient Mislabel Detector (SEMD) that we craft, and find that SEMD performs similarly to or outperforms prior mislabel detection approaches. We then apply SEMD to multiple real world computer vision datasets and test how dataset size, mislabel removal strategy, and mislabel removal amount further affect model performance after retraining on the cleaned data. With careful design of the approach, we find that mislabel removal leads per-class performance improvements of up to 8% of a retrained classifier in smaller data regimes.", "url": "https://arxiv.org/abs/2312.02200"}, {"metadata": {"arXiv": "2312.02238", "Date": "Mon, 04 Dec 2023 09:19:38 ", "Title": "X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model", "Authors": ["Lingmin Ran", "Xiaodong Cun", "JiaWei Liu", "Rui Zhao", "Song Zijie", "Xintao Wang", "Jussi Keppo", "Mike Zheng Shou"], "Categories": "cs.CV cs.AI cs.MM", "Comments": ["Project page: https://showlab.github.io/X-Adapter/"]}, "abstract": "We introduce X-Adapter, a universal upgrader to enable the pretrained plug-and-play modules (e.g., ControlNet, LoRA) to work directly with the upgraded text-to-image diffusion model (e.g., SDXL) without further retraining. We achieve this goal by training an additional network to control the frozen upgraded model with the new text-image data pairs. In detail, X-Adapter keeps a frozen copy of the old model to preserve the connectors of different plugins. Additionally, X-Adapter adds trainable mapping layers that bridge the decoders from models of different versions for feature remapping. The remapped features will be used as guidance for the upgraded model. To enhance the guidance ability of X-Adapter, we employ a null-text training strategy for the upgraded model. After training, we also introduce a two-stage denoising strategy to align the initial latents of X-Adapter and the upgraded model. Thanks to our strategies, X-Adapter demonstrates universal compatibility with various plugins and also enables plugins of different versions to work together, thereby expanding the functionalities of diffusion community. To verify the effectiveness of the proposed method, we conduct extensive experiments and the results show that X-Adapter may facilitate wider application in the upgraded foundational diffusion model.", "url": "https://arxiv.org/abs/2312.02238"}, {"metadata": {"arXiv": "2312.02240", "Date": "Mon, 04 Dec 2023 10:27:09 ", "Title": "Contrastive Learning-Based Spectral Knowledge Distillation for Multi-Modality and Missing Modality Scenarios in Semantic Segmentation", "Authors": ["Aniruddh Sikdar", "Jayant Teotia", "Suresh Sundaram"], "Categories": "cs.CV cs.AI", "Comments": ["10 pages", "6 figures"]}, "abstract": "Improving the performance of semantic segmentation models using multispectral information is crucial, especially for environments with low-light and adverse conditions. Multi-modal fusion techniques pursue either the learning of cross-modality features to generate a fused image or engage in knowledge distillation but address multimodal and missing modality scenarios as distinct issues, which is not an optimal approach for multi-sensor models. To address this, a novel multi-modal fusion approach called CSK-Net is proposed, which uses a contrastive learning-based spectral knowledge distillation technique along with an automatic mixed feature exchange mechanism for semantic segmentation in optical (EO) and infrared (IR) images. The distillation scheme extracts detailed textures from the optical images and distills them into the optical branch of CSK-Net. The model encoder consists of shared convolution weights with separate batch norm (BN) layers for both modalities, to capture the multi-spectral information from different modalities of the same objects. A Novel Gated Spectral Unit (GSU) and mixed feature exchange strategy are proposed to increase the correlation of modality-shared information and decrease the modality-specific information during the distillation process. Comprehensive experiments show that CSK-Net surpasses state-of-the-art models in multi-modal tasks and for missing modalities when exclusively utilizing IR data for inference across three public benchmarking datasets. For missing modality scenarios, the performance increase is achieved without additional computational costs compared to the baseline segmentation models.", "url": "https://arxiv.org/abs/2312.02240"}, {"metadata": {"arXiv": "2312.02256", "Date": "Mon, 04 Dec 2023 18:58:38 ", "Title": "EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Motion Generation", "Authors": ["Wenyang Zhou", "Zhiyang Dou", "Zeyu Cao", "Zhouyingcheng Liao", "Jingbo Wang", "Wenjia Wang", "Yuan Liu", "Taku Komura", "Wenping Wang", "Lingjie Liu"], "Categories": "cs.CV cs.AI cs.GR", "Comments": ["Project Page: https://frank-zy-dou.github.io/projects/EMDM/index.html"]}, "abstract": "We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Although previous motion diffusion models have shown impressive results, they struggle to achieve fast generation while maintaining high-quality human motions. Motion latent diffusion has been proposed for efficient motion generation. However, effectively learning a latent space can be non-trivial in such a two-stage manner. Meanwhile, accelerating motion sampling by increasing the step size, e.g., DDIM, typically leads to a decline in motion quality due to the inapproximation of complex data distributions when naively increasing the step size. In this paper, we propose EMDM that allows for much fewer sample steps for fast motion generation by modeling the complex denoising distribution during multiple sampling steps. Specifically, we develop a Conditional Denoising Diffusion GAN to capture multimodal data distributions conditioned on both control signals, i.e., textual description and denoising time step. By modeling the complex data distribution, a larger sampling step size and fewer steps are achieved during motion synthesis, significantly accelerating the generation process. To effectively capture the human dynamics and reduce undesired artifacts, we employ motion geometric loss during network training, which improves the motion quality and training efficiency. As a result, EMDM achieves a remarkable speed-up at the generation stage while maintaining high-quality motion generation in terms of fidelity and diversity.", "url": "https://arxiv.org/abs/2312.02256"}, {"metadata": {"arXiv": "2312.02338", "Date": "Mon, 04 Dec 2023 20:47:48 ", "Title": "A Contrastive Compositional Benchmark for Text-to-Image Synthesis: A Study with Unified Text-to-Image Fidelity Metrics", "Authors": ["Xiangru Zhu", "Penglei Sun", "Chengyu Wang", "Jingping Liu", "Zhixu Li", "Yanghua Xiao", "Jun Huang"], "Categories": "cs.CV cs.AI cs.MM", "Comments": ["17 pages", "14 figures", "11 tables"]}, "abstract": "Text-to-image (T2I) synthesis has recently achieved significant advancements. However, challenges remain in the model's compositionality, which is the ability to create new combinations from known components. We introduce Winoground-T2I, a benchmark designed to evaluate the compositionality of T2I models. This benchmark includes 11K complex, high-quality contrastive sentence pairs spanning 20 categories. These contrastive sentence pairs with subtle differences enable fine-grained evaluations of T2I synthesis models. Additionally, to address the inconsistency across different metrics, we propose a strategy that evaluates the reliability of various metrics by using comparative sentence pairs. We use Winoground-T2I with a dual objective: to evaluate the performance of T2I models and the metrics used for their evaluation. Finally, we provide insights into the strengths and weaknesses of these metrics and the capabilities of current T2I models in tackling challenges across a range of complex compositional categories. Our benchmark is publicly available at https://github.com/zhuxiangru/Winoground-T2I .", "url": "https://arxiv.org/abs/2312.02338"}, {"metadata": {"arXiv": "2312.02366", "Date": "Mon, 04 Dec 2023 21:47:10 ", "Title": "Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks", "Authors": ["Mohammed Baharoon", "Waseem Qureshi", "Jiahong Ouyang", "Yanwu Xu", "Kilian Phol", "Abdulrhman Aljouie", "Wei Peng"], "Categories": "cs.CV cs.AI"}, "abstract": "The integration of deep learning systems into the medical domain has been hindered by the resource-intensive process of data annotation and the inability of these systems to generalize to different data distributions. Foundation models, which are models pre-trained on large datasets, have emerged as a solution to reduce reliance on annotated data and enhance model generalizability and robustness. DINOv2, an open-source foundation model pre-trained with self-supervised learning on 142 million curated natural images, excels in extracting general-purpose visual representations, exhibiting promising capabilities across various vision tasks. Nevertheless, a critical question remains unanswered regarding DINOv2's adaptability to radiological imaging, and the clarity on whether its features are sufficiently general to benefit radiology image analysis is yet to be established. Therefore, this study comprehensively evaluates DINOv2 for radiology, conducting over 100 experiments across diverse modalities (X-ray, CT, and MRI). Tasks include disease classification and organ segmentation on both 2D and 3D images, evaluated under different settings like kNN, few-shot learning, linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning, to measure the effectiveness and generalizability of the DINOv2 feature embeddings. Comparative analyses with established medical image analysis models, U-Net and TransUnet for segmentation, and CNN and ViT models pre-trained via supervised, weakly supervised, and self-supervised learning for classification, reveal DINOv2's superior performance in segmentation tasks and competitive results in disease classification. The findings contribute insights to potential avenues for optimizing pre-training strategies for medical imaging and enhancing the broader understanding of DINOv2's role in bridging the gap between natural and radiological image analysis.", "url": "https://arxiv.org/abs/2312.02366"}, {"metadata": {"arXiv": "2312.02481", "Date": "Tue, 05 Dec 2023 04:15:22 ", "Title": "Learning to Holistically Detect Bridges from Large-Size VHR Remote Sensing Imagery", "Authors": ["Yansheng Li", "Junwei Luo", "Yongjun Zhang", "Yihua Tan", "Jin-Gang Yu", "Song Bai"], "Categories": "cs.CV cs.AI", "Comments": ["16 pages", "11 figures", "6 tables; due to the limitation \"The abstract field cannot be longer than 1,920 characters\"", "the abstract appearing here is slightly shorter than that in the PDF file"]}, "abstract": "Bridge detection in remote sensing images (RSIs) plays a crucial role in various applications, but it poses unique challenges compared to the detection of other objects. In RSIs, bridges exhibit considerable variations in terms of their spatial scales and aspect ratios. Therefore, to ensure the visibility and integrity of bridges, it is essential to perform holistic bridge detection in large-size very-high-resolution (VHR) RSIs. However, the lack of datasets with large-size VHR RSIs limits the deep learning algorithms' performance on bridge detection. Due to the limitation of GPU memory in tackling large-size images, deep learning-based object detection methods commonly adopt the cropping strategy, which inevitably results in label fragmentation and discontinuous prediction. To ameliorate the scarcity of datasets, this paper proposes a large-scale dataset named GLH-Bridge comprising 6,000 VHR RSIs sampled from diverse geographic locations across the globe. These images encompass a wide range of sizes, varying from 2,048*2,048 to 16,38*16,384 pixels, and collectively feature 59,737 bridges. Furthermore, we present an efficient network for holistic bridge detection (HBD-Net) in large-size RSIs. The HBD-Net presents a separate detector-based feature fusion (SDFF) architecture and is optimized via a shape-sensitive sample re-weighting (SSRW) strategy. Based on the proposed GLH-Bridge dataset, we establish a bridge detection benchmark including the OBB and HBB tasks, and validate the effectiveness of the proposed HBD-Net. Additionally, cross-dataset generalization experiments on two publicly available datasets illustrate the strong generalization capability of the GLH-Bridge dataset.", "url": "https://arxiv.org/abs/2312.02481"}, {"metadata": {"arXiv": "2312.02501", "Date": "Tue, 05 Dec 2023 05:08:08 ", "Title": "Inspecting Model Fairness in Ultrasound Segmentation Tasks", "Authors": ["Zikang Xu", "Fenghe Tang", "Quan Quan", "Jianrui Ding", "Chunping Ning", "S. Kevin Zhou"], "Categories": "cs.CV cs.AI", "Comments": ["Submitted to ISBI 2024"]}, "abstract": "With the rapid expansion of machine learning and deep learning (DL), researchers are increasingly employing learning-based algorithms to alleviate diagnostic challenges across diverse medical tasks and applications. While advancements in diagnostic precision are notable, some researchers have identified a concerning trend: their models exhibit biased performance across subgroups characterized by different sensitive attributes. This bias not only infringes upon the rights of patients but also has the potential to lead to life-altering consequences. In this paper, we inspect a series of DL segmentation models using two ultrasound datasets, aiming to assess the presence of model unfairness in these specific tasks. Our findings reveal that even state-of-the-art DL algorithms demonstrate unfair behavior in ultrasound segmentation tasks. These results serve as a crucial warning, underscoring the necessity for careful model evaluation before their deployment in real-world scenarios. Such assessments are imperative to ensure ethical considerations and mitigate the risk of adverse impacts on patient outcomes.", "url": "https://arxiv.org/abs/2312.02501"}, {"metadata": {"arXiv": "2312.02512", "Date": "Tue, 05 Dec 2023 05:36:44 ", "Title": "AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation", "Authors": ["Jeongsoo Choi", "Se Jin Park", "Minsu Kim", "Yong Man Ro"], "Categories": "cs.CV cs.AI cs.MM eess.AS"}, "abstract": "This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework, where the input and output of the system are multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key advantages can be brought: 1) We can perform real-like conversations with individuals worldwide in a virtual meeting by utilizing our own primary languages. In contrast to Speech-to-Speech Translation (A2A), which solely translates between audio modalities, the proposed AV2AV directly translates between audio-visual speech. This capability enhances the dialogue experience by presenting synchronized lip movements along with the translated speech. 2) We can improve the robustness of the spoken language translation system. By employing the complementary information of audio-visual speech, the system can effectively translate spoken language even in the presence of acoustic noise, showcasing robust performance. To mitigate the problem of the absence of a parallel AV2AV translation dataset, we propose to train our spoken language translation system with the audio-only dataset of A2A. This is done by learning unified audio-visual speech representations through self-supervised learning in advance to train the translation system. Moreover, we propose an AV-Renderer that can generate raw audio and video in parallel. It is designed with zero-shot speaker modeling, thus the speaker in source audio-visual speech can be maintained at the target translated audio-visual speech. The effectiveness of AV2AV is evaluated with extensive experiments in a many-to-many language translation setting. The demo page is available on https://choijeongsoo.github.io/av2av.", "url": "https://arxiv.org/abs/2312.02512"}, {"metadata": {"arXiv": "2312.02521", "Date": "Tue, 05 Dec 2023 06:04:16 ", "Title": "Retrieving Conditions from Reference Images for Diffusion Models", "Authors": ["Haoran Tang", "Xin Zhou", "Jieren Deng", "Zhihong Pan", "Hao Tian", "Pratik Chaudhari"], "Categories": "cs.CV cs.AI"}, "abstract": "Recent diffusion-based subject driven generative methods have enabled image generations with good fidelity for specific objects or human portraits. However, to achieve better versatility for applications, we argue that not only improved datasets and evaluations are desired, but also more careful methods to retrieve only relevant information from conditional images are anticipated. To this end, we propose an anime figures dataset RetriBooru-V1, with enhanced identity and clothing labels. We state new tasks enabled by this dataset, and introduce a new diversity metric to measure success in completing these tasks, quantifying the flexibility of image generations. We establish an RAG-inspired baseline method, designed to retrieve precise conditional information from reference images. Then, we compare with current methods on existing task to demonstrate the capability of the proposed method. Finally, we provide baseline experiment results on new tasks, and conduct ablation studies on the possible structural choices.", "url": "https://arxiv.org/abs/2312.02521"}, {"metadata": {"arXiv": "2312.02545", "Date": "Tue, 05 Dec 2023 07:23:22 ", "Title": "Graph Information Bottleneck for Remote Sensing Segmentation", "Authors": ["Yuntao Shou", "Wei Ai", "Tao Meng"], "Categories": "cs.CV cs.AI", "Comments": ["13 pages", "6 figures"]}, "abstract": "Remote sensing segmentation has a wide range of applications in environmental protection, and urban change detection, etc. Despite the success of deep learning-based remote sensing segmentation methods (e.g., CNN and Transformer), they are not flexible enough to model irregular objects. In addition, existing graph contrastive learning methods usually adopt the way of maximizing mutual information to keep the node representations consistent between different graph views, which may cause the model to learn task-independent redundant information. To tackle the above problems, this paper treats images as graph structures and introduces a simple contrastive vision GNN (SC-ViG) architecture for remote sensing segmentation. Specifically, we construct a node-masked and edge-masked graph view to obtain an optimal graph structure representation, which can adaptively learn whether to mask nodes and edges. Furthermore, this paper innovatively introduces information bottleneck theory into graph contrastive learning to maximize task-related information while minimizing task-independent redundant information. Finally, we replace the convolutional module in UNet with the SC-ViG module to complete the segmentation and classification tasks of remote sensing images. Extensive experiments on publicly available real datasets demonstrate that our method outperforms state-of-the-art remote sensing image segmentation methods.", "url": "https://arxiv.org/abs/2312.02545"}, {"metadata": {"arXiv": "2312.02663", "Date": "Tue, 05 Dec 2023 11:02:45 ", "Title": "FaceStudio: Put Your Face Everywhere in Seconds", "Authors": ["Yuxuan Yan", "Chi Zhang", "Rui Wang", "Pei Cheng", "Gang Yu", "Bin Fu"], "Categories": "cs.CV cs.AI", "Comments": ["Project homepage: https://icoz69.github.io/facestudio/"]}, "abstract": "This study investigates identity-preserving image synthesis, an intriguing task in image generation that seeks to maintain a subject's identity while adding a personalized, stylistic touch. Traditional methods, such as Textual Inversion and DreamBooth, have made strides in custom image creation, but they come with significant drawbacks. These include the need for extensive resources and time for fine-tuning, as well as the requirement for multiple reference images. To overcome these challenges, our research introduces a novel approach to identity-preserving synthesis, with a particular focus on human images. Our model leverages a direct feed-forward mechanism, circumventing the need for intensive fine-tuning, thereby facilitating quick and efficient image generation. Central to our innovation is a hybrid guidance framework, which combines stylized images, facial images, and textual prompts to guide the image generation process. This unique combination enables our model to produce a variety of applications, such as artistic portraits and identity-blended images. Our experimental results, including both qualitative and quantitative evaluations, demonstrate the superiority of our method over existing baseline models and previous works, particularly in its remarkable efficiency and ability to preserve the subject's identity with high fidelity.", "url": "https://arxiv.org/abs/2312.02663"}, {"metadata": {"arXiv": "2312.02699", "Date": "Tue, 05 Dec 2023 12:02:53 ", "Title": "Enhancing Vehicle Entrance and Parking Management: Deep Learning Solutions for Efficiency and Security", "Authors": ["Muhammad Umer Ramzan", "Usman Ali", "Syed Haider Abbas Naqvi", "Zeeshan Aslam", "Tehseen", "Husnain Ali", "Muhammad Faheem"], "Categories": "cs.CV cs.AI", "Comments": ["Accepted for publication in the 25th International Multitopic Conference (INMIC) IEEE 2023", "6 Pages", "3 figures"]}, "abstract": "The auto-management of vehicle entrance and parking in any organization is a complex challenge encompassing record-keeping, efficiency, and security concerns. Manual methods for tracking vehicles and finding parking spaces are slow and a waste of time. To solve the problem of auto management of vehicle entrance and parking, we have utilized state-of-the-art deep learning models and automated the process of vehicle entrance and parking into any organization. To ensure security, our system integrated vehicle detection, license number plate verification, and face detection and recognition models to ensure that the person and vehicle are registered with the organization. We have trained multiple deep-learning models for vehicle detection, license number plate detection, face detection, and recognition, however, the YOLOv8n model outperformed all the other models. Furthermore, License plate recognition is facilitated by Google's Tesseract-OCR Engine. By integrating these technologies, the system offers efficient vehicle detection, precise identification, streamlined record keeping, and optimized parking slot allocation in buildings, thereby enhancing convenience, accuracy, and security. Future research opportunities lie in fine-tuning system performance for a wide range of real-world applications.", "url": "https://arxiv.org/abs/2312.02699"}, {"metadata": {"arXiv": "2312.02705", "Date": "Tue, 05 Dec 2023 12:07:27 ", "Title": "Unified learning-based lossy and lossless JPEG recompression", "Authors": ["Jianghui Zhang", "Yuanyuan Wang", "Lina Guo", "Jixiang Luo", "Tongda Xu", "Yan Wang", "Zhi Wang", "Hongwei Qin"], "Categories": "cs.CV cs.AI", "DOI": "10.1109/ICIP49359.2023.10222354"}, "abstract": "JPEG is still the most widely used image compression algorithm. Most image compression algorithms only consider uncompressed original image, while ignoring a large number of already existing JPEG images. Recently, JPEG recompression approaches have been proposed to further reduce the size of JPEG files. However, those methods only consider JPEG lossless recompression, which is just a special case of the rate-distortion theorem. In this paper, we propose a unified lossly and lossless JPEG recompression framework, which consists of learned quantization table and Markovian hierarchical variational autoencoders. Experiments show that our method can achieve arbitrarily low distortion when the bitrate is close to the upper bound, namely the bitrate of the lossless compression model. To the best of our knowledge, this is the first learned method that bridges the gap between lossy and lossless recompression of JPEG images.", "url": "https://arxiv.org/abs/2312.02705"}, {"metadata": {"arXiv": "2312.02781", "Date": "Tue, 05 Dec 2023 14:12:38 ", "Title": "PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features", "Authors": ["Tianshun Han", "Shengnan Gui", "Yiqing Huang", "Baihui Li", "Lijian Liu", "Benjia Zhou", "Ning Jiang", "Quan Lu", "Ruicong Zhi", "Yanyan Liang", "Du Zhang", "Jun Wan"], "Categories": "cs.CV cs.AI"}, "abstract": "Speech-driven 3D facial animation has improved a lot recently while most related works only utilize acoustic modality and neglect the influence of visual and textual cues, leading to unsatisfactory results in terms of precision and coherence. We argue that visual and textual cues are not trivial information. Therefore, we present a novel framework, namely PMMTalk, using complementary Pseudo Multi-Modal features for improving the accuracy of facial animation. The framework entails three modules: PMMTalk encoder, cross-modal alignment module, and PMMTalk decoder. Specifically, the PMMTalk encoder employs the off-the-shelf talking head generation architecture and speech recognition technology to extract visual and textual information from speech, respectively. Subsequently, the cross-modal alignment module aligns the audio-image-text features at temporal and semantic levels. Then PMMTalk decoder is employed to predict lip-syncing facial blendshape coefficients. Contrary to prior methods, PMMTalk only requires an additional random reference face image but yields more accurate results. Additionally, it is artist-friendly as it seamlessly integrates into standard animation production workflows by introducing facial blendshape coefficients. Finally, given the scarcity of 3D talking face datasets, we introduce a large-scale 3D Chinese Audio-Visual Facial Animation (3D-CAVFA) dataset. Extensive experiments and user studies show that our approach outperforms the state of the art. We recommend watching the supplementary video.", "url": "https://arxiv.org/abs/2312.02781"}, {"metadata": {"arXiv": "2312.02813", "Date": "Tue, 05 Dec 2023 14:56:55 ", "Title": "BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models", "Authors": ["Fengyuan Shi", "Jiaxi Gu", "Hang Xu", "Songcen Xu", "Wei Zhang", "Limin Wang"], "Categories": "cs.CV cs.AI"}, "abstract": "Diffusion models have made tremendous progress in text-driven image and video generation. Now text-to-image foundation models are widely applied to various downstream image synthesis tasks, such as controllable image generation and image editing, while downstream video synthesis tasks are less explored for several reasons. First, it requires huge memory and compute overhead to train a video generation foundation model. Even with video foundation models, additional costly training is still required for downstream video synthesis tasks. Second, although some works extend image diffusion models into videos in a training-free manner, temporal consistency cannot be well kept. Finally, these adaption methods are specifically designed for one task and fail to generalize to different downstream video synthesis tasks. To mitigate these issues, we propose a training-free general-purpose video synthesis framework, coined as BIVDiff, via bridging specific image diffusion models and general text-to-video foundation diffusion models. Specifically, we first use an image diffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video generation, then perform Mixed Inversion on the generated video, and finally input the inverted latents into the video diffusion model for temporal smoothing. Decoupling image and video models enables flexible image model selection for different purposes, which endows the framework with strong task generalization and high efficiency. To validate the effectiveness and general use of BIVDiff, we perform a wide range of video generation tasks, including controllable video generation video editing, video inpainting and outpainting. Our project page is available at https://bivdiff.github.io.", "url": "https://arxiv.org/abs/2312.02813"}, {"metadata": {"arXiv": "2312.02970", "Date": "Tue, 05 Dec 2023 18:58:26 ", "Title": "Alchemist: Parametric Control of Material Properties with Diffusion Models", "Authors": ["Prafull Sharma", "Varun Jampani", "Yuanzhen Li", "Xuhui Jia", "Dmitry Lagun", "Fredo Durand", "William T. Freeman", "Mark Matthews"], "Categories": "cs.CV cs.AI cs.GR"}, "abstract": "We propose a method to control material attributes of objects like roughness, metallic, albedo, and transparency in real images. Our method capitalizes on the generative prior of text-to-image models known for photorealism, employing a scalar value and instructions to alter low-level material properties. Addressing the lack of datasets with controlled material attributes, we generated an object-centric synthetic dataset with physically-based materials. Fine-tuning a modified pre-trained text-to-image model on this synthetic dataset enables us to edit material properties in real-world images while preserving all other attributes. We show the potential application of our model to material edited NeRFs.", "url": "https://arxiv.org/abs/2312.02970"}, {"metadata": {"arXiv": "2312.02493", "Date": "Tue, 05 Dec 2023 04:51:19 ", "Title": "Flexible Communication for Optimal Distributed Learning over Unpredictable Networks", "Authors": ["Sahil Tyagi", "Martin Swany"], "Categories": "cs.DC cs.AI cs.CV", "Comments": ["2023 IEEE International Conference on Big Data (BigData)"]}, "abstract": "Gradient compression alleviates expensive communication in distributed deep learning by sending fewer values and its corresponding indices, typically via Allgather (AG). Training with high compression ratio (CR) achieves high accuracy like DenseSGD, but has lower parallel scaling due to high communication cost (i.e., parallel efficiency). Using lower CRs improves parallel efficiency by lowering synchronization cost, but degrades model accuracy as well (statistical efficiency). Further, speedup attained with different models and CRs also varies with network latency, effective bandwidth and collective op used for aggregation. In many cases, collectives like Allreduce (AR) have lower cost than AG to exchange the same amount of data. In this paper, we propose an AR-compatible Topk compressor that is bandwidth-optimal and thus performs better than AG in certain network configurations. We develop a flexible communication strategy that switches between AG and AR based on which collective is optimal in the current settings, and model the pareto-relationship between parallel and statistical efficiency as a multi-objective optimization (MOO) problem to dynamically adjust CR and accelerate training while still converging to high accuracy.", "url": "https://arxiv.org/abs/2312.02493"}, {"metadata": {"arXiv": "2312.02531", "Date": "Tue, 05 Dec 2023 06:28:33 ", "Title": "PolyFit: A Peg-in-hole Assembly Framework for Unseen Polygon Shapes via Sim-to-real Adaptation", "Authors": ["Geonhyup Lee", "Joosoon Lee", "Sangjun Noh", "Minhwan Ko", "Kangmin Kim and Kyoobin Lee"], "Categories": "cs.RO cs.AI", "Comments": ["8 pages", "8 figures", "3 tables"]}, "abstract": "The study addresses the foundational and challenging task of peg-in-hole assembly in robotics, where misalignments caused by sensor inaccuracies and mechanical errors often result in insertion failures or jamming. This research introduces PolyFit, representing a paradigm shift by transitioning from a reinforcement learning approach to a supervised learning methodology. PolyFit is a Force/Torque (F/T)-based supervised learning framework designed for 5-DoF peg-in-hole assembly. It utilizes F/T data for accurate extrinsic pose estimation and adjusts the peg pose to rectify misalignments. Extensive training in a simulated environment involves a dataset encompassing a diverse range of peg-hole shapes, extrinsic poses, and their corresponding contact F/T readings. To enhance extrinsic pose estimation, a multi-point contact strategy is integrated into the model input, recognizing that identical F/T readings can indicate different poses. The study proposes a sim-to-real adaptation method for real-world application, using a sim-real paired dataset to enable effective generalization to complex and unseen polygon shapes. PolyFit achieves impressive peg-in-hole success rates of 97.3% and 96.3% for seen and unseen shapes in simulations, respectively. Real-world evaluations further demonstrate substantial success rates of 86.7% and 85.0%, highlighting the robustness and adaptability of the proposed method.", "url": "https://arxiv.org/abs/2312.02531"}, {"metadata": {"arXiv": "2312.02677", "Date": "Tue, 05 Dec 2023 11:32:25 ", "Title": "Contact Energy Based Hindsight Experience Prioritization", "Authors": ["Erdi Sayar", "Zhenshan Bing", "Carlo D'Eramo", "Ozgur S. Oguz", "Alois Knoll"], "Categories": "cs.RO cs.AI"}, "abstract": "Multi-goal robot manipulation tasks with sparse rewards are difficult for reinforcement learning (RL) algorithms due to the inefficiency in collecting successful experiences. Recent algorithms such as Hindsight Experience Replay (HER) expedite learning by taking advantage of failed trajectories and replacing the desired goal with one of the achieved states so that any failed trajectory can be utilized as a contribution to learning. However, HER uniformly chooses failed trajectories, without taking into account which ones might be the most valuable for learning. In this paper, we address this problem and propose a novel approach Contact Energy Based Prioritization~(CEBP) to select the samples from the replay buffer based on rich information due to contact, leveraging the touch sensors in the gripper of the robot and object displacement. Our prioritization scheme favors sampling of contact-rich experiences, which are arguably the ones providing the largest amount of information. We evaluate our proposed approach on various sparse reward robotic tasks and compare them with the state-of-the-art methods. We show that our method surpasses or performs on par with those methods on robot manipulation tasks. Finally, we deploy the trained policy from our method to a real Franka robot for a pick-and-place task. We observe that the robot can solve the task successfully. The videos and code are publicly available at: https://erdiphd.github.io/HER_force", "url": "https://arxiv.org/abs/2312.02677"}, {"metadata": {"arXiv": "2312.02976", "Date": "Tue, 05 Dec 2023 18:59:45 ", "Title": "Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World", "Authors": ["Kiana Ehsani", "Tanmay Gupta", "Rose Hendrix", "Jordi Salvador", "Luca Weihs", "Kuo-Hao Zeng", "Kunal Pratap Singh", "Yejin Kim", "Winson Han", "Alvaro Herrasti", "Ranjay Krishna", "Dustin Schwenk", "Eli VanderBilt", "Aniruddha Kembhavi"], "Categories": "cs.RO cs.AI cs.CV", "Comments": ["First six authors contributed equally. Project page: https://spoc-robot.github.io/"]}, "abstract": "Reinforcement learning (RL) with dense rewards and imitation learning (IL) with human-generated trajectories are the most widely used approaches for training modern embodied agents. RL requires extensive reward shaping and auxiliary losses and is often too slow and ineffective for long-horizon tasks. While IL with human supervision is effective, collecting human trajectories at scale is extremely expensive. In this work, we show that imitating shortest-path planners in simulation produces agents that, given a language instruction, can proficiently navigate, explore, and manipulate objects in both simulation and in the real world using only RGB sensors (no depth map or GPS coordinates). This surprising result is enabled by our end-to-end, transformer-based, SPOC architecture, powerful visual encoders paired with extensive image augmentation, and the dramatic scale and diversity of our training data: millions of frames of shortest-path-expert trajectories collected inside approximately 200,000 procedurally generated houses containing 40,000 unique 3D assets. Our models, data, training code, and newly proposed 10-task benchmarking suite CHORES will be open-sourced.", "url": "https://arxiv.org/abs/2312.02976"}, {"metadata": {"arXiv": "2312.02309", "Date": "Mon, 04 Dec 2023 19:45:06 ", "Title": "Training Reinforcement Learning Agents and Humans With Difficulty-Conditioned Generators", "Authors": ["Sidney Tio", "Jimmy Ho", "Pradeep Varakantham"], "Categories": "cs.AI cs.HC cs.LG"}, "abstract": "We adapt Parameterized Environment Response Model (PERM), a method for training both Reinforcement Learning (RL) Agents and human learners in parameterized environments by directly modeling difficulty and ability. Inspired by Item Response Theory (IRT), PERM aligns environment difficulty with individual ability, creating a Zone of Proximal Development-based curriculum. Remarkably, PERM operates without real-time RL updates and allows for offline training, ensuring its adaptability across diverse students. We present a two-stage training process that capitalizes on PERM's adaptability, and demonstrate its effectiveness in training RL agents and humans in an empirical study.", "url": "https://arxiv.org/abs/2312.02309"}, {"metadata": {"arXiv": "2312.02519", "Date": "Tue, 05 Dec 2023 06:00:52 ", "Title": "Creative Agents: Empowering Agents with Imagination for Creative Tasks", "Authors": ["Chi Zhang", "Penglin Cai", "Yuhui Fu", "Haoqi Yuan", "Zongqing Lu"], "Categories": "cs.AI cs.LG", "Comments": ["The first two authors contribute equally"]}, "abstract": "We study building embodied agents for open-ended creative tasks. While existing methods build instruction-following agents that can perform diverse open-ended tasks, none of them demonstrates creativity -- the ability to give novel and diverse task solutions implicit in the language instructions. This limitation comes from their inability to convert abstract language instructions into concrete task goals in the environment and perform long-horizon planning for such complicated goals. Given the observation that humans perform creative tasks with the help of imagination, we propose a class of solutions for creative agents, where the controller is enhanced with an imaginator that generates detailed imaginations of task outcomes conditioned on language instructions. We introduce several approaches to implementing the components of creative agents. We implement the imaginator with either a large language model for textual imagination or a diffusion model for visual imagination. The controller can either be a behavior-cloning policy learned from data or a pre-trained foundation model generating executable codes in the environment. We benchmark creative tasks with the challenging open-world game Minecraft, where the agents are asked to create diverse buildings given free-form language instructions. In addition, we propose novel evaluation metrics for open-ended creative tasks utilizing GPT-4V, which holds many advantages over existing metrics. We perform a detailed experimental analysis of creative agents, showing that creative agents are the first AI agents accomplishing diverse building creation in the survival mode of Minecraft. Our benchmark and models are open-source for future research on creative agents (https://github.com/PKU-RL/Creative-Agents).", "url": "https://arxiv.org/abs/2312.02519"}, {"metadata": {"arXiv": "2312.02665", "Date": "Tue, 05 Dec 2023 11:10:05 ", "Title": "Lights out: training RL agents robust to temporary blindness", "Authors": ["N. Ordonez", "M. Tromp", "P. M. Julbe", "and W. B\\\"ohmer"], "Categories": "cs.AI cs.LG"}, "abstract": "Agents trained with DQN rely on an observation at each timestep to decide what action to take next. However, in real world applications observations can change or be missing entirely. Examples of this could be a light bulb breaking down, or the wallpaper in a certain room changing. While these situations change the actual observation, the underlying optimal policy does not change. Because of this we want our agent to continue taking actions until it receives a (recognized) observation again. To achieve this we introduce a combination of a neural network architecture that uses hidden representations of the observations and a novel n-step loss function. Our implementation is able to withstand location based blindness stretches longer than the ones it was trained on, and therefore shows robustness to temporary blindness. For access to our implementation, please email Nathan, Marije, or Pau.", "url": "https://arxiv.org/abs/2312.02665"}, {"metadata": {"arXiv": "2312.02186", "Date": "Fri, 01 Dec 2023 20:16:02 ", "Title": "Identifying Spurious Correlations using Counterfactual Alignment", "Authors": ["Joseph Paul Cohen and Louis Blankemeier and Akshay Chaudhari"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Models driven by spurious correlations often yield poor generalization performance. We propose the counterfactual alignment method to detect and explore spurious correlations of black box classifiers. Counterfactual images generated with respect to one classifier can be input into other classifiers to see if they also induce changes in the outputs of these classifiers. The relationship between these responses can be quantified and used to identify specific instances where a spurious correlation exists as well as compute aggregate statistics over a dataset. Our work demonstrates the ability to detect spurious correlations in face attribute classifiers. This is validated by observing intuitive trends in a face attribute classifier as well as fabricating spurious correlations and detecting their presence, both visually and quantitatively. Further, utilizing the CF alignment method, we demonstrate that we can rectify spurious correlations identified in classifiers.", "url": "https://arxiv.org/abs/2312.02186"}, {"metadata": {"arXiv": "2312.02199", "Date": "Sat, 02 Dec 2023 19:17:04 ", "Title": "USat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite Imagery", "Authors": ["Jeremy Irvin", "Lucas Tao", "Joanne Zhou", "Yuntao Ma", "Langston Nashold", "Benjamin Liu", "Andrew Y. Ng"], "Categories": "cs.CV cs.AI cs.LG eess.IV stat.AP"}, "abstract": "Large, self-supervised vision models have led to substantial advancements for automatically interpreting natural images. Recent works have begun tailoring these methods to remote sensing data which has rich structure with multi-sensor, multi-spectral, and temporal information providing massive amounts of self-labeled data that can be used for self-supervised pre-training. In this work, we develop a new encoder architecture called USat that can input multi-spectral data from multiple sensors for self-supervised pre-training. USat is a vision transformer with modified patch projection layers and positional encodings to model spectral bands with varying spatial scales from multiple sensors. We integrate USat into a Masked Autoencoder (MAE) self-supervised pre-training procedure and find that a pre-trained USat outperforms state-of-the-art self-supervised MAE models trained on remote sensing data on multiple remote sensing benchmark datasets (up to 8%) and leads to improvements in low data regimes (up to 7%). Code and pre-trained weights are available at https://github.com/stanfordmlgroup/USat .", "url": "https://arxiv.org/abs/2312.02199"}, {"metadata": {"arXiv": "2312.02246", "Date": "Mon, 04 Dec 2023 14:45:56 ", "Title": "Conditional Variational Diffusion Models", "Authors": ["Gabriel della Maggiora", "Luis Alberto Croquevielle", "Nikita Desphande", "Harry Horsley", "Thomas Heinis", "Artur Yakimovich"], "Categories": "cs.CV cs.AI cs.LG stat.ML", "Comments": ["Denoising Diffusion Probabilistic Models", "Inverse Problems", "Generative Models", "Super Resolution", "Phase Quantification", "Variational Methods"], "ACM-class": "I.2.6"}, "abstract": "Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior results to previous methods and fine-tuned diffusion models. We conclude that fine-tuning the schedule by experimentation should be avoided because it can be learned during training in a stable way that yields better results.", "url": "https://arxiv.org/abs/2312.02246"}, {"metadata": {"arXiv": "2312.02253", "Date": "Mon, 04 Dec 2023 18:35:27 ", "Title": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images", "Authors": ["Zhuoran Yu", "Chenchen Zhu", "Sean Culatana", "Raghuraman Krishnamoorthi", "Fanyi Xiao and Yong Jae Lee"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Recent advances in generative deep learning have enabled the creation of high-quality synthetic images in text-to-image generation. Prior work shows that fine-tuning a pretrained diffusion model on ImageNet and generating synthetic training images from the finetuned model can enhance an ImageNet classifier's performance. However, performance degrades as synthetic images outnumber real ones. In this paper, we explore whether generative fine-tuning is essential for this improvement and whether it is possible to further scale up training using more synthetic data. We present a new framework leveraging off-the-shelf generative models to generate synthetic training images, addressing multiple challenges: class name ambiguity, lack of diversity in naive prompts, and domain shifts. Specifically, we leverage large language models (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we propose contextualized diversification (CD) and stylized diversification (SD) methods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage domain adaptation techniques with auxiliary batch normalization for synthetic images. Our framework consistently enhances recognition model performance with more synthetic data, up to 6x of original ImageNet size showcasing the potential of synthetic data for improved recognition models and strong out-of-domain generalization.", "url": "https://arxiv.org/abs/2312.02253"}, {"metadata": {"arXiv": "2312.02310", "Date": "Mon, 04 Dec 2023 19:48:02 ", "Title": "VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding", "Authors": ["Yizhou Wang", "Ruiyi Zhang", "Haoliang Wang", "Uttaran Bhattacharya", "Yun Fu and Gang Wu"], "Categories": "cs.CV cs.AI cs.CL cs.LG"}, "abstract": "Recent advancements in language-model-based video understanding have been progressing at a remarkable pace, spurred by the introduction of Large Language Models (LLMs). However, the focus of prior research has been predominantly on devising a projection layer that maps video features to tokens, an approach that is both rudimentary and inefficient. In our study, we introduce a cutting-edge framework, VaQuitA, designed to refine the synergy between video and textual information. At the data level, instead of sampling frames uniformly, we implement a sampling method guided by CLIP-score rankings, which enables a more aligned selection of frames with the given question. At the feature level, we integrate a trainable Video Perceiver alongside a Visual-Query Transformer (abbreviated as VQ-Former), which bolsters the interplay between the input question and the video features. We also discover that incorporating a simple prompt, \"Please be critical\", into the LLM input can substantially enhance its video comprehension capabilities. Our experimental results indicate that VaQuitA consistently sets a new benchmark for zero-shot video question-answering tasks and is adept at producing high-quality, multi-turn video dialogues with users.", "url": "https://arxiv.org/abs/2312.02310"}, {"metadata": {"arXiv": "2312.02364", "Date": "Mon, 04 Dec 2023 21:46:21 ", "Title": "Class-Discriminative Attention Maps for Vision Transformers", "Authors": ["Lennart Brocki and Neo Christopher Chung"], "Categories": "cs.CV cs.AI cs.LG stat.ML"}, "abstract": "Interpretability methods are critical components for examining and exploring deep neural networks (DNN), as well as increasing our understanding of and trust in them. Vision transformers (ViT), which can be trained to state-of-the-art performance with a self-supervised learning (SSL) training method, provide built-in attention maps (AM). While AMs can provide high-quality semantic segmentation of input images, they do not account for any signal coming from a downstream classifier. We introduce class-discriminative attention maps (CDAM), a novel post-hoc explanation method that is highly sensitive to the target class. Our method essentially scales attention scores by how relevant the corresponding tokens are for the predictions of a classifier head. Alternative to classifier outputs, CDAM can also explain a user-defined concept by targeting similarity measures in the latent space of the ViT. This allows for explanations of arbitrary concepts, defined by the user through a few sample images. We investigate the operating characteristics of CDAM in comparison with relevance propagation (RP) and token ablation maps (TAM), an alternative to pixel occlusion methods. CDAM is highly class-discriminative and semantically relevant, while providing implicit regularization of relevance scores. PyTorch implementation: \\url{https://github.com/lenbrocki/CDAM} Web live demo: \\url{https://cdam.informatism.com/}", "url": "https://arxiv.org/abs/2312.02364"}, {"metadata": {"arXiv": "2312.02608", "Date": "Tue, 05 Dec 2023 09:34:56 ", "Title": "Panoptica -- instance-wise evaluation of 3D semantic and instance segmentation maps", "Authors": ["Florian Kofler", "Hendrik M\\\"oller", "Josef A. Buchner", "Ezequiel de la Rosa", "Ivan Ezhov", "Marcel Rosier", "Isra Mekki", "Suprosanna Shit", "Moritz Negwer", "Rami Al-Maskari", "Ali Ert\\\"urk", "Shankeeth Vinayahalingam", "Fabian Isensee", "Sarthak Pati", "Daniel Rueckert", "Jan S. Kirschke", "Stefan K. Ehrlich", "Annika Reinke", "Bjoern Menze", "Benedikt Wiestler", "Marie Piraud"], "Categories": "cs.CV cs.AI cs.LG eess.IV", "Comments": ["15 pages", "6 figures", "3 tables"]}, "abstract": "This paper introduces panoptica, a versatile and performance-optimized package designed for computing instance-wise segmentation quality metrics from 2D and 3D segmentation maps. panoptica addresses the limitations of existing metrics and provides a modular framework that complements the original intersection over union-based panoptic quality with other metrics, such as the distance metric Average Symmetric Surface Distance. The package is open-source, implemented in Python, and accompanied by comprehensive documentation and tutorials. panoptica employs a three-step metrics computation process to cover diverse use cases. The efficacy of panoptica is demonstrated on various real-world biomedical datasets, where an instance-wise evaluation is instrumental for an accurate representation of the underlying clinical task. Overall, we envision panoptica as a valuable tool facilitating in-depth evaluation of segmentation methods.", "url": "https://arxiv.org/abs/2312.02608"}, {"metadata": {"arXiv": "2312.02696", "Date": "Tue, 05 Dec 2023 11:55:47 ", "Title": "Analyzing and Improving the Training Dynamics of Diffusion Models", "Authors": ["Tero Karras", "Miika Aittala", "Jaakko Lehtinen", "Janne Hellsten", "Timo Aila", "Samuli Laine"], "Categories": "cs.CV cs.AI cs.LG cs.NE stat.ML"}, "abstract": "Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high-level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on expectation. We find that systematic application of this philosophy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational complexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling. As an independent contribution, we present a method for setting the exponential moving average (EMA) parameters post-hoc, i.e., after completing the training run. This allows precise tuning of EMA length without the cost of performing several training runs, and reveals its surprising interactions with network architecture, training time, and guidance.", "url": "https://arxiv.org/abs/2312.02696"}, {"metadata": {"arXiv": "2312.02843", "Date": "Tue, 05 Dec 2023 15:53:24 ", "Title": "Are Vision Transformers More Data Hungry Than Newborn Visual Systems?", "Authors": ["Lalit Pandey", "Samantha M. W. Wood", "Justin N. Wood"], "Categories": "cs.CV cs.AI cs.LG cs.NE", "Comments": ["Accepted in Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)"]}, "abstract": "Vision transformers (ViTs) are top performing models on many computer vision benchmarks and can accurately predict human behavior on object recognition tasks. However, researchers question the value of using ViTs as models of biological learning because ViTs are thought to be more data hungry than brains, with ViTs requiring more training data to reach similar levels of performance. To test this assumption, we directly compared the learning abilities of ViTs and animals, by performing parallel controlled rearing experiments on ViTs and newborn chicks. We first raised chicks in impoverished visual environments containing a single object, then simulated the training data available in those environments by building virtual animal chambers in a video game engine. We recorded the first-person images acquired by agents moving through the virtual chambers and used those images to train self supervised ViTs that leverage time as a teaching signal, akin to biological visual systems. When ViTs were trained through the eyes of newborn chicks, the ViTs solved the same view invariant object recognition tasks as the chicks. Thus, ViTs were not more data hungry than newborn visual systems: both learned view invariant object representations in impoverished visual environments. The flexible and generic attention based learning mechanism in ViTs combined with the embodied data streams available to newborn animals appears sufficient to drive the development of animal-like object recognition.", "url": "https://arxiv.org/abs/2312.02843"}, {"metadata": {"arXiv": "2312.02957", "Date": "Tue, 05 Dec 2023 18:41:03 ", "Title": "Classification for everyone : Building geography agnostic models for fairer recognition", "Authors": ["Akshat Jindal", "Shreya Singh", "Soham Gadgil"], "Categories": "cs.CV cs.AI cs.CY cs.LG"}, "abstract": "In this paper, we analyze different methods to mitigate inherent geographical biases present in state of the art image classification models. We first quantitatively present this bias in two datasets - The Dollar Street Dataset and ImageNet, using images with location information. We then present different methods which can be employed to reduce this bias. Finally, we analyze the effectiveness of the different techniques on making these models more robust to geographical locations of the images.", "url": "https://arxiv.org/abs/2312.02957"}, {"metadata": {"arXiv": "2312.02179", "Date": "Tue, 28 Nov 2023 17:47:32 ", "Title": "Training Chain-of-Thought via Latent-Variable Inference", "Authors": ["Du Phan", "Matthew D. Hoffman", "David Dohan", "Sholto Douglas", "Tuan Anh Le", "Aaron Parisi", "Pavel Sountsov", "Charles Sutton", "Sharad Vikram", "Rif A. Saurous"], "Categories": "cs.LG cs.AI cs.CL", "Comments": ["23 pages", "37th Conference on Neural Information Processing Systems (NeurIPS 2023)"]}, "abstract": "Large language models (LLMs) solve problems more accurately and interpretably when instructed to work out the answer step by step using a ``chain-of-thought'' (CoT) prompt. One can also improve LLMs' performance on a specific task by supervised fine-tuning, i.e., by using gradient ascent on some tunable parameters to maximize the average log-likelihood of correct answers from a labeled training set. Naively combining CoT with supervised tuning requires supervision not just of the correct answers, but also of detailed rationales that lead to those answers; these rationales are expensive to produce by hand. Instead, we propose a fine-tuning strategy that tries to maximize the \\emph{marginal} log-likelihood of generating a correct answer using CoT prompting, approximately averaging over all possible rationales. The core challenge is sampling from the posterior over rationales conditioned on the correct answer; we address it using a simple Markov-chain Monte Carlo (MCMC) expectation-maximization (EM) algorithm inspired by the self-taught reasoner (STaR), memoized wake-sleep, Markovian score climbing, and persistent contrastive divergence. This algorithm also admits a novel control-variate technique that drives the variance of our gradient estimates to zero as the model improves. Applying our technique to GSM8K and the tasks in BIG-Bench Hard, we find that this MCMC-EM fine-tuning technique typically improves the model's accuracy on held-out examples more than STaR or prompt-tuning with or without CoT.", "url": "https://arxiv.org/abs/2312.02179"}, {"metadata": {"arXiv": "2312.02185", "Date": "Fri, 01 Dec 2023 17:03:27 ", "Title": "Virtual Fusion with Contrastive Learning for Single Sensor-based Activity Recognition", "Authors": ["Duc-Anh Nguyen", "Cuong Pham", "Nhien-An Le-Khac"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Various types of sensors can be used for Human Activity Recognition (HAR), and each of them has different strengths and weaknesses. Sometimes a single sensor cannot fully observe the user's motions from its perspective, which causes wrong predictions. While sensor fusion provides more information for HAR, it comes with many inherent drawbacks like user privacy and acceptance, costly set-up, operation, and maintenance. To deal with this problem, we propose Virtual Fusion - a new method that takes advantage of unlabeled data from multiple time-synchronized sensors during training, but only needs one sensor for inference. Contrastive learning is adopted to exploit the correlation among sensors. Virtual Fusion gives significantly better accuracy than training with the same single sensor, and in some cases, it even surpasses actual fusion using multiple sensors at test time. We also extend this method to a more general version called Actual Fusion within Virtual Fusion (AFVF), which uses a subset of training sensors during inference. Our method achieves state-of-the-art accuracy and F1-score on UCI-HAR and PAMAP2 benchmark datasets. Implementation is available upon request.", "url": "https://arxiv.org/abs/2312.02185"}, {"metadata": {"arXiv": "2312.02210", "Date": "Sun, 03 Dec 2023 04:22:29 ", "Title": "Low-Precision Mixed-Computation Models for Inference on Edge", "Authors": ["Seyedarmin Azizi", "Mahdi Nazemi", "Mehdi Kamal", "Massoud Pedram"], "Categories": "cs.LG cs.AI"}, "abstract": "This paper presents a mixed-computation neural network processing approach for edge applications that incorporates low-precision (low-width) Posit and low-precision fixed point (FixP) number systems. This mixed-computation approach employs 4-bit Posit (Posit4), which has higher precision around zero, for representing weights with high sensitivity, while it uses 4-bit FixP (FixP4) for representing other weights. A heuristic for analyzing the importance and the quantization error of the weights is presented to assign the proper number system to different weights. Additionally, a gradient approximation for Posit representation is introduced to improve the quality of weight updates in the backpropagation process. Due to the high energy consumption of the fully Posit-based computations, neural network operations are carried out in FixP or Posit/FixP. An efficient hardware implementation of a MAC operation with a first Posit operand and FixP for a second operand and accumulator is presented. The efficacy of the proposed low-precision mixed-computation approach is extensively assessed on vision and language models. The results show that, on average, the accuracy of the mixed-computation is about 1.5% higher than that of FixP with a cost of 0.19% energy overhead.", "url": "https://arxiv.org/abs/2312.02210"}, {"metadata": {"arXiv": "2312.02213", "Date": "Sun, 03 Dec 2023 07:03:04 ", "Title": "JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization", "Authors": ["Shang-Ching Liu", "ShengKun Wang", "Wenqi Lin", "Chung-Wei Hsiung", "Yi-Chen Hsieh", "Yu-Ping Cheng", "Sian-Hong Luo", "Tsungyao Chang", "Jianwei Zhang"], "Categories": "cs.LG cs.AI cs.DB stat.AP"}, "abstract": "In this study, we introduce JarviX, a sophisticated data analytics framework. JarviX is designed to employ Large Language Models (LLMs) to facilitate an automated guide and execute high-precision data analyzes on tabular datasets. This framework emphasizes the significance of varying column types, capitalizing on state-of-the-art LLMs to generate concise data insight summaries, propose relevant analysis inquiries, visualize data effectively, and provide comprehensive explanations for results drawn from an extensive data analysis pipeline. Moreover, JarviX incorporates an automated machine learning (AutoML) pipeline for predictive modeling. This integration forms a comprehensive and automated optimization cycle, which proves particularly advantageous for optimizing machine configuration. The efficacy and adaptability of JarviX are substantiated through a series of practical use case studies.", "url": "https://arxiv.org/abs/2312.02213"}, {"metadata": {"arXiv": "2312.02230", "Date": "Mon, 04 Dec 2023 03:43:26 ", "Title": "A Simple and Scalable Representation for Graph Generation", "Authors": ["Yunhui Jang", "Seul Lee", "Sungsoo Ahn"], "Categories": "cs.LG cs.AI", "Comments": ["26 pages"]}, "abstract": "Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar. Our findings reveal that the adoption of this compact representation not only enhances scalability but also bolsters performance by simplifying the graph generation process. We conduct a comprehensive evaluation across ten non-attributed and two molecular graph generation tasks, demonstrating the effectiveness of GEEL.", "url": "https://arxiv.org/abs/2312.02230"}, {"metadata": {"arXiv": "2312.02236", "Date": "Mon, 04 Dec 2023 08:06:59 ", "Title": "Rethinking Adversarial Training with Neural Tangent Kernel", "Authors": ["Guanlin Li", "Han Qiu", "Shangwei Guo", "Jiwei Li", "Tianwei Zhang"], "Categories": "cs.LG cs.AI"}, "abstract": "Adversarial training (AT) is an important and attractive topic in deep learning security, exhibiting mysteries and odd properties. Recent studies of neural network training dynamics based on Neural Tangent Kernel (NTK) make it possible to reacquaint AT and deeply analyze its properties. In this paper, we perform an in-depth investigation of AT process and properties with NTK, such as NTK evolution. We uncover three new findings that are missed in previous works. First, we disclose the impact of data normalization on AT and the importance of unbiased estimators in batch normalization layers. Second, we experimentally explore the kernel dynamics and propose more time-saving AT methods. Third, we study the spectrum feature inside the kernel to address the catastrophic overfitting problem. To the best of our knowledge, it is the first work leveraging the observations of kernel dynamics to improve existing AT methods.", "url": "https://arxiv.org/abs/2312.02236"}, {"metadata": {"arXiv": "2312.02247", "Date": "Mon, 04 Dec 2023 14:50:23 ", "Title": "Federated Active Learning for Target Domain Generalisation", "Authors": ["Razvan Caramalau", "Binod Bhattarai", "Danail Stoyanov"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "In this paper, we introduce Active Learning framework in Federated Learning for Target Domain Generalisation, harnessing the strength from both learning paradigms. Our framework, FEDALV, composed of Active Learning (AL) and Federated Domain Generalisation (FDG), enables generalisation of an image classification model trained from limited source domain client's data without sharing images to an unseen target domain. To this end, our FDG, FEDA, consists of two optimisation updates during training, one at the client and another at the server level. For the client, the introduced losses aim to reduce feature complexity and condition alignment, while in the server, the regularisation limits free energy biases between source and target obtained by the global model. The remaining component of FEDAL is AL with variable budgets, which queries the server to retrieve and sample the most informative local data for the targeted client. We performed multiple experiments on FDG w/ and w/o AL and compared with both conventional FDG baselines and Federated Active Learning baselines. Our extensive quantitative experiments demonstrate the superiority of our method in accuracy and efficiency compared to the multiple contemporary methods. FEDALV manages to obtain the performance of the full training target accuracy while sampling as little as 5% of the source client's data.", "url": "https://arxiv.org/abs/2312.02247"}, {"metadata": {"arXiv": "2312.02254", "Date": "Mon, 04 Dec 2023 18:45:28 ", "Title": "Innovations in Agricultural Forecasting: A Multivariate Regression Study on Global Crop Yield Prediction", "Authors": ["Ishaan Gupta", "Samyutha Ayalasomayajula", "Yashas Shashidhara", "Anish Kataria", "Shreyas Shashidhara", "Krishita Kataria", "Aditya Undurti"], "Categories": "cs.LG cs.AI", "Comments": ["12 pages", "8 figures", "1 table", "Guided by Dr. Aditya Undurti"], "MSC-class": "68W03"}, "abstract": "The prediction of crop yields internationally is a crucial objective in agricultural research. Thus, this study implements 6 regression models (Linear, Tree, Gradient Descent, Gradient Boosting, K- Nearest Neighbors, and Random Forest) to predict crop yields in 196 countries. Given 4 key training parameters, pesticides (tonnes), rainfall (mm), temperature (Celsius), and yield (hg/ha), it was found that our Random Forest Regression model achieved a determination coefficient (r^2) of 0.94, with a margin of error (ME) of .03. The models were trained and tested using the Food and Agricultural Organization of the United Nations data, along with the World Bank Climate Change Data Catalog. Furthermore, each parameter was analyzed to understand how varying factors could impact overall yield. We used unconventional models, contrary to generally used Deep Learning (DL) and Machine Learning (ML) models, combined with recently collected data to implement a unique approach in our research. Existing scholarship would benefit from understanding the most optimal model for agricultural research, specifically using the United Nations data.", "url": "https://arxiv.org/abs/2312.02254"}, {"metadata": {"arXiv": "2312.02308", "Date": "Mon, 04 Dec 2023 19:44:04 ", "Title": "AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse Catalysts Design", "Authors": ["Romain Lacombe", "Lucas Hendren", "Khalid El-Awady"], "Categories": "cs.LG cs.AI physics.chem-ph", "Comments": ["37th Conference on Neural Information Processing Systems (NeurIPS 2023)", "AI for Accelerated Materials Design Workshop"]}, "abstract": "A central challenge of the clean energy transition is the development of catalysts for low-emissions technologies. Recent advances in Machine Learning for quantum chemistry drastically accelerate the computation of catalytic activity descriptors such as adsorption energies. Here we introduce AdsorbRL, a Deep Reinforcement Learning agent aiming to identify potential catalysts given a multi-objective binding energy target, trained using offline learning on the Open Catalyst 2020 and Materials Project data sets. We experiment with Deep Q-Network agents to traverse the space of all ~160,000 possible unary, binary and ternary compounds of 55 chemical elements, with very sparse rewards based on adsorption energy known for only between 2,000 and 3,000 catalysts per adsorbate. To constrain the actions space, we introduce Random Edge Traversal and train a single-objective DQN agent on the known states subgraph, which we find strengthens target binding energy by an average of 4.1 eV. We extend this approach to multi-objective, goal-conditioned learning, and train a DQN agent to identify materials with the highest (respectively lowest) adsorption energies for multiple simultaneous target adsorbates. We experiment with Objective Sub-Sampling, a novel training scheme aimed at encouraging exploration in the multi-objective setup, and demonstrate simultaneous adsorption energy improvement across all target adsorbates, by an average of 0.8 eV. Overall, our results suggest strong potential for Deep Reinforcement Learning applied to the inverse catalysts design problem.", "url": "https://arxiv.org/abs/2312.02308"}, {"metadata": {"arXiv": "2312.02312", "Date": "Mon, 04 Dec 2023 19:52:12 ", "Title": "Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games", "Authors": ["Lukas Sch\\\"afer", "Logan Jones", "Anssi Kanervisto", "Yuhan Cao", "Tabish Rashid", "Raluca Georgescu", "Dave Bignell", "Siddhartha Sen", "Andrea Trevi\\~no Gavito", "Sam Devlin"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Preprint"]}, "abstract": "Video games have served as useful benchmarks for the decision making community, but going beyond Atari games towards training agents in modern games has been prohibitively expensive for the vast majority of the research community. Recent progress in the research, development and open release of large vision models has the potential to amortize some of these costs across the community. However, it is currently unclear which of these models have learnt representations that retain information critical for sequential decision making. Towards enabling wider participation in the research of gameplaying agents in modern games, we present a systematic study of imitation learning with publicly available visual encoders compared to the typical, task-specific, end-to-end training approach in Minecraft, Minecraft Dungeons and Counter-Strike: Global Offensive.", "url": "https://arxiv.org/abs/2312.02312"}, {"metadata": {"arXiv": "2312.02339", "Date": "Mon, 04 Dec 2023 20:48:18 ", "Title": "Expressive Sign Equivariant Networks for Spectral Geometric Learning", "Authors": ["Derek Lim and Joshua Robinson and Stefanie Jegelka and Haggai Maron"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["NeurIPS 2023 Spotlight"]}, "abstract": "Recent work has shown the utility of developing machine learning models that respect the structure and symmetries of eigenvectors. These works promote sign invariance, since for any eigenvector v the negation -v is also an eigenvector. However, we show that sign invariance is theoretically limited for tasks such as building orthogonally equivariant models and learning node positional encodings for link prediction in graphs. In this work, we demonstrate the benefits of sign equivariance for these tasks. To obtain these benefits, we develop novel sign equivariant neural network architectures. Our models are based on a new analytic characterization of sign equivariant polynomials and thus inherit provable expressiveness properties. Controlled synthetic experiments show that our networks can achieve the theoretically predicted benefits of sign equivariant models. Code is available at https://github.com/cptq/Sign-Equivariant-Nets.", "url": "https://arxiv.org/abs/2312.02339"}, {"metadata": {"arXiv": "2312.02355", "Date": "Mon, 04 Dec 2023 21:35:13 ", "Title": "When is Offline Policy Selection Sample Efficient for Reinforcement Learning?", "Authors": ["Vincent Liu", "Prabhat Nagarajan", "Andrew Patterson", "Martha White"], "Categories": "cs.LG cs.AI"}, "abstract": "Offline reinforcement learning algorithms often require careful hyperparameter tuning. Consequently, before deployment, we need to select amongst a set of candidate policies. As yet, however, there is little understanding about the fundamental limits of this offline policy selection (OPS) problem. In this work we aim to provide clarity on when sample efficient OPS is possible, primarily by connecting OPS to off-policy policy evaluation (OPE) and Bellman error (BE) estimation. We first show a hardness result, that in the worst case, OPS is just as hard as OPE, by proving a reduction of OPE to OPS. As a result, no OPS method can be more sample efficient than OPE in the worst case. We then propose a BE method for OPS, called Identifiable BE Selection (IBES), that has a straightforward method for selecting its own hyperparameters. We highlight that using IBES for OPS generally has more requirements than OPE methods, but if satisfied, can be more sample efficient. We conclude with an empirical study comparing OPE and IBES, and by showing the difficulty of OPS on an offline Atari benchmark dataset.", "url": "https://arxiv.org/abs/2312.02355"}, {"metadata": {"arXiv": "2312.02416", "Date": "Tue, 05 Dec 2023 01:12:56 ", "Title": "Towards Fast and Stable Federated Learning: Confronting Heterogeneity via Knowledge Anchor", "Authors": ["Jinqian Chen", "Jihua Zhu", "Qinghai Zheng"], "Categories": "cs.LG cs.AI", "Comments": ["Published in ACM MM23"], "MSC-class": "68T99", "DOI": "10.1145/3581783.3612597"}, "abstract": "Federated learning encounters a critical challenge of data heterogeneity, adversely affecting the performance and convergence of the federated model. Various approaches have been proposed to address this issue, yet their effectiveness is still limited. Recent studies have revealed that the federated model suffers severe forgetting in local training, leading to global forgetting and performance degradation. Although the analysis provides valuable insights, a comprehensive understanding of the vulnerable classes and their impact factors is yet to be established. In this paper, we aim to bridge this gap by systematically analyzing the forgetting degree of each class during local training across different communication rounds. Our observations are: (1) Both missing and non-dominant classes suffer similar severe forgetting during local training, while dominant classes show improvement in performance. (2) When dynamically reducing the sample size of a dominant class, catastrophic forgetting occurs abruptly when the proportion of its samples is below a certain threshold, indicating that the local model struggles to leverage a few samples of a specific class effectively to prevent forgetting. Motivated by these findings, we propose a novel and straightforward algorithm called Federated Knowledge Anchor (FedKA). Assuming that all clients have a single shared sample for each class, the knowledge anchor is constructed before each local training stage by extracting shared samples for missing classes and randomly selecting one sample per class for non-dominant classes. The knowledge anchor is then utilized to correct the gradient of each mini-batch towards the direction of preserving the knowledge of the missing and non-dominant classes. Extensive experimental results demonstrate that our proposed FedKA achieves fast and stable convergence, significantly improving accuracy on popular benchmarks.", "url": "https://arxiv.org/abs/2312.02416"}, {"metadata": {"arXiv": "2312.02515", "Date": "Tue, 05 Dec 2023 05:38:38 ", "Title": "ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU", "Authors": ["Zhengmao Ye and Dengchun Li and Jingqi Tian and Tingfeng Lan and Jie Zuo and Lei Duan and Hui Lu and Yexi Jiang and Jian Sha and Ke Zhang and Mingjie Tang"], "Categories": "cs.LG cs.AI", "Comments": ["14 pages", "14 figures"]}, "abstract": "Transformer-based large language models (LLMs) have demonstrated outstanding performance across diverse domains, particularly when fine-turned for specific domains. Recent studies suggest that the resources required for fine-tuning LLMs can be economized through parameter-efficient methods such as Low-Rank Adaptation (LoRA). While LoRA effectively reduces computational burdens and resource demands, it currently supports only a single-job fine-tuning setup. In this paper, we present ASPEN, a high-throughput framework for fine-tuning LLMs. ASPEN efficiently trains multiple jobs on a single GPU using the LoRA method, leveraging shared pre-trained model and adaptive scheduling. ASPEN is compatible with transformer-based language models like LLaMA and ChatGLM, etc. Experiments show that ASPEN saves 53% of GPU memory when training multiple LLaMA-7B models on NVIDIA A100 80GB GPU and boosts training throughput by about 17% compared to existing methods when training with various pre-trained models on different GPUs. The adaptive scheduling algorithm reduces turnaround time by 24%, end-to-end training latency by 12%, prioritizing jobs and preventing out-of-memory issues.", "url": "https://arxiv.org/abs/2312.02515"}, {"metadata": {"arXiv": "2312.02517", "Date": "Tue, 05 Dec 2023 05:52:44 ", "Title": "Simplifying Neural Network Training Under Class Imbalance", "Authors": ["Ravid Shwartz-Ziv and Micah Goldblum and Yucen Lily Li and C. Bayan Bruss and Andrew Gordon Wilson"], "Categories": "cs.LG cs.AI", "Comments": ["NeurIPS 2023. Code available at https://github.com/ravidziv/SimplifyingImbalancedTraining"]}, "abstract": "Real-world datasets are often highly class-imbalanced, which can adversely impact the performance of deep learning models. The majority of research on training neural networks under class imbalance has focused on specialized loss functions, sampling techniques, or two-stage training procedures. Notably, we demonstrate that simply tuning existing components of standard deep learning pipelines, such as the batch size, data augmentation, optimizer, and label smoothing, can achieve state-of-the-art performance without any such specialized class imbalance methods. We also provide key prescriptions and considerations for training under class imbalance, and an understanding of why imbalance methods succeed or fail.", "url": "https://arxiv.org/abs/2312.02517"}, {"metadata": {"arXiv": "2312.02522", "Date": "Tue, 05 Dec 2023 06:05:04 ", "Title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation", "Authors": ["Xinyi Yang", "Xinting Yang", "Chao Yu", "Jiayu Chen", "Huazhong Yang and Yu Wang"], "Categories": "cs.LG cs.AI cs.RO", "Comments": ["Submitted to IEEE RA-L"]}, "abstract": "We investigate the problem of decentralized multi-agent navigation tasks, where multiple agents need to reach initially unassigned targets in a limited time. Classical planning-based methods suffer from expensive computation overhead at each step and offer limited expressiveness for complex cooperation strategies. In contrast, reinforcement learning (RL) has recently become a popular paradigm for addressing this issue. However, RL struggles with low data efficiency and cooperation when directly exploring (nearly) optimal policies in the large search space, especially with an increased agent number (e.g., 10+ agents) or in complex environments (e.g., 3D simulators). In this paper, we propose Multi-Agent Scalable GNN-based P lanner (MASP), a goal-conditioned hierarchical planner for navigation tasks with a substantial number of agents. MASP adopts a hierarchical framework to divide a large search space into multiple smaller spaces, thereby reducing the space complexity and accelerating training convergence. We also leverage graph neural networks (GNN) to model the interaction between agents and goals, improving goal achievement. Besides, to enhance generalization capabilities in scenarios with unseen team sizes, we divide agents into multiple groups, each with a previously trained number of agents. The results demonstrate that MASP outperforms classical planning-based competitors and RL baselines, achieving a nearly 100% success rate with minimal training data in both multi-agent particle environments (MPE) with 50 agents and a quadrotor 3-dimensional environment (OmniDrones) with 20 agents. Furthermore, the learned policy showcases zero-shot generalization across unseen team sizes.", "url": "https://arxiv.org/abs/2312.02522"}, {"metadata": {"arXiv": "2312.02530", "Date": "Tue, 05 Dec 2023 06:28:19 ", "Title": "MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection", "Authors": ["Junho Song", "Keonwoo Kim", "Jeonglyul Oh", "Sungzoon Cho"], "Categories": "cs.LG cs.AI"}, "abstract": "Detecting anomalies in real-world multivariate time series data is challenging due to complex temporal dependencies and inter-variable correlations. Recently, reconstruction-based deep models have been widely used to solve the problem. However, these methods still suffer from an over-generalization issue and fail to deliver consistently high performance. To address this issue, we propose the MEMTO, a memory-guided Transformer using a reconstruction-based approach. It is designed to incorporate a novel memory module that can learn the degree to which each memory item should be updated in response to the input data. To stabilize the training procedure, we use a two-phase training paradigm which involves using K-means clustering for initializing memory items. Additionally, we introduce a bi-dimensional deviation-based detection criterion that calculates anomaly scores considering both input space and latent space. We evaluate our proposed method on five real-world datasets from diverse domains, and it achieves an average anomaly detection F1-score of 95.74%, significantly outperforming the previous state-of-the-art methods. We also conduct extensive experiments to empirically validate the effectiveness of our proposed model's key components.", "url": "https://arxiv.org/abs/2312.02530"}, {"metadata": {"arXiv": "2312.02566", "Date": "Tue, 05 Dec 2023 08:24:26 ", "Title": "Structured World Representations in Maze-Solving Transformers", "Authors": ["Michael Igorevich Ivanitskiy", "Alex F. Spies", "Tilman R\\\"auker", "Guillaume Corlouer", "Chris Mathwin", "Lucia Quirke", "Can Rager", "Rusheb Shah", "Dan Valentine", "Cecilia Diniz Behn", "Katsumi Inoue", "Samy Wu Fung"], "Categories": "cs.LG cs.AI", "Comments": ["15 pages", "18 figures", "15 tables. Corresponding author: Michael Ivanitskiy (mivanits@mines.edu). Code available at https://github.com/understanding-search/structured-representations-maze-transformers"]}, "abstract": "Transformer models underpin many recent advances in practical machine learning applications, yet understanding their internal behavior continues to elude researchers. Given the size and complexity of these models, forming a comprehensive picture of their inner workings remains a significant challenge. To this end, we set out to understand small transformer models in a more tractable setting: that of solving mazes. In this work, we focus on the abstractions formed by these models and find evidence for the consistent emergence of structured internal representations of maze topology and valid paths. We demonstrate this by showing that the residual stream of only a single token can be linearly decoded to faithfully reconstruct the entire maze. We also find that the learned embeddings of individual tokens have spatial structure. Furthermore, we take steps towards deciphering the circuity of path-following by identifying attention heads (dubbed $\\textit{adjacency heads}$), which are implicated in finding valid subsequent tokens.", "url": "https://arxiv.org/abs/2312.02566"}, {"metadata": {"arXiv": "2312.02573", "Date": "Tue, 05 Dec 2023 08:41:23 ", "Title": "UTBoost: A Tree-boosting based System for Uplift Modeling", "Authors": ["Junjie Gao", "Xiangyu Zheng", "DongDong Wang", "Zhixiang Huang", "Bangqi Zheng", "Kai Yang"], "Categories": "cs.LG cs.AI", "Comments": ["11 pages", "3 figures"]}, "abstract": "Uplift modeling refers to the set of machine learning techniques that a manager may use to estimate customer uplift, that is, the net effect of an action on some customer outcome. By identifying the subset of customers for whom a treatment will have the greatest effect, uplift models assist decision-makers in optimizing resource allocations and maximizing overall returns. Accurately estimating customer uplift poses practical challenges, as it requires assessing the difference between two mutually exclusive outcomes for each individual. In this paper, we propose two innovative adaptations of the well-established Gradient Boosting Decision Trees (GBDT) algorithm, which learn the causal effect in a sequential way and overcome the counter-factual nature. Both approaches innovate existing techniques in terms of ensemble learning method and learning objectives, respectively. Experiments on large-scale datasets demonstrate the usefulness of the proposed methods, which often yielding remarkable improvements over base models. To facilitate the application, we develop the UTBoost, an end-to-end tree boosting system specifically designed for uplift modeling. The package is open source and has been optimized for training speed to meet the needs of real industrial applications.", "url": "https://arxiv.org/abs/2312.02573"}, {"metadata": {"arXiv": "2312.02622", "Date": "Tue, 05 Dec 2023 09:55:49 ", "Title": "On the Initialization of Graph Neural Networks", "Authors": ["Jiahang Li", "Yakun Song", "Xiang Song", "David Paul Wipf"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted by ICML 2023"]}, "abstract": "Graph Neural Networks (GNNs) have displayed considerable promise in graph representation learning across various applications. The core learning process requires the initialization of model weight matrices within each GNN layer, which is typically accomplished via classic initialization methods such as Xavier initialization. However, these methods were originally motivated to stabilize the variance of hidden embeddings and gradients across layers of Feedforward Neural Networks (FNNs) and Convolutional Neural Networks (CNNs) to avoid vanishing gradients and maintain steady information flow. In contrast, within the GNN context classical initializations disregard the impact of the input graph structure and message passing on variance. In this paper, we analyze the variance of forward and backward propagation across GNN layers and show that the variance instability of GNN initializations comes from the combined effect of the activation function, hidden dimension, graph structure and message passing. To better account for these influence factors, we propose a new initialization method for Variance Instability Reduction within GNN Optimization (Virgo), which naturally tends to equate forward and backward variances across successive layers. We conduct comprehensive experiments on 15 datasets to show that Virgo can lead to superior model performance and more stable variance at initialization on node classification, link prediction and graph classification tasks. Codes are in https://github.com/LspongebobJH/virgo_icml2023.", "url": "https://arxiv.org/abs/2312.02622"}, {"metadata": {"arXiv": "2312.02646", "Date": "Tue, 05 Dec 2023 10:37:54 ", "Title": "SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal Forecasting", "Authors": ["Xiaobei Zou", "Luolin Xiong", "Yang Tang", "Jurgen Kurths"], "Categories": "cs.LG cs.AI", "Comments": ["13 pages", "7figures"]}, "abstract": "Spatio-temporal forecasting in various domains, like traffic prediction and weather forecasting, is a challenging endeavor, primarily due to the difficulties in modeling propagation dynamics and capturing high-dimensional interactions among nodes. Despite the significant strides made by graph-based networks in spatio-temporal forecasting, there remain two pivotal factors closely related to forecasting performance that need further consideration: time delays in propagation dynamics and multi-scale high-dimensional interactions. In this work, we present a Series-Aligned Multi-Scale Graph Learning (SAMSGL) framework, aiming to enhance forecasting performance. In order to handle time delays in spatial interactions, we propose a series-aligned graph convolution layer to facilitate the aggregation of non-delayed graph signals, thereby mitigating the influence of time delays for the improvement in accuracy. To understand global and local spatio-temporal interactions, we develop a spatio-temporal architecture via multi-scale graph learning, which encompasses two essential components: multi-scale graph structure learning and graph-fully connected (Graph-FC) blocks. The multi-scale graph structure learning includes a global graph structure to learn both delayed and non-delayed node embeddings, as well as a local one to learn node variations influenced by neighboring factors. The Graph-FC blocks synergistically fuse spatial and temporal information to boost prediction accuracy. To evaluate the performance of SAMSGL, we conduct experiments on meteorological and traffic forecasting datasets, which demonstrate its effectiveness and superiority.", "url": "https://arxiv.org/abs/2312.02646"}, {"metadata": {"arXiv": "2312.02674", "Date": "Tue, 05 Dec 2023 11:29:54 ", "Title": "Amortized Bayesian Decision Making for simulation-based models", "Authors": ["Mila Gorecki", "Jakob H. Macke", "Michael Deistler"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Simulation-based inference (SBI) provides a powerful framework for inferring posterior distributions of stochastic simulators in a wide range of domains. In many settings, however, the posterior distribution is not the end goal itself -- rather, the derived parameter values and their uncertainties are used as a basis for deciding what actions to take. Unfortunately, because posterior distributions provided by SBI are (potentially crude) approximations of the true posterior, the resulting decisions can be suboptimal. Here, we address the question of how to perform Bayesian decision making on stochastic simulators, and how one can circumvent the need to compute an explicit approximation to the posterior. Our method trains a neural network on simulated data and can predict the expected cost given any data and action, and can, thus, be directly used to infer the action with lowest cost. We apply our method to several benchmark problems and demonstrate that it induces similar cost as the true posterior distribution. We then apply the method to infer optimal actions in a real-world simulator in the medical neurosciences, the Bayesian Virtual Epileptic Patient, and demonstrate that it allows to infer actions associated with low cost after few simulations.", "url": "https://arxiv.org/abs/2312.02674"}, {"metadata": {"arXiv": "2312.02682", "Date": "Tue, 05 Dec 2023 11:40:24 ", "Title": "H-GAP: Humanoid Control with a Generalist Planner", "Authors": ["Zhengyao Jiang", "Yingchen Xu", "Nolan Wagener", "Yicheng Luo", "Michael Janner", "Edward Grefenstette", "Tim Rockt\\\"aschel", "Yuandong Tian"], "Categories": "cs.LG cs.AI cs.RO", "Comments": ["18 pages including appendix", "4 figures"]}, "abstract": "Humanoid control is an important research challenge offering avenues for integration into human-centric infrastructures and enabling physics-driven humanoid animations. The daunting challenges in this field stem from the difficulty of optimizing in high-dimensional action spaces and the instability introduced by the bipedal morphology of humanoids. However, the extensive collection of human motion-captured data and the derived datasets of humanoid trajectories, such as MoCapAct, paves the way to tackle these challenges. In this context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a state-action trajectory generative model trained on humanoid trajectories derived from human motion-captured data, capable of adeptly handling downstream control tasks with Model Predictive Control (MPC). For 56 degrees of freedom humanoid, we empirically demonstrate that H-GAP learns to represent and generate a wide range of motor behaviours. Further, without any learning from online interactions, it can also flexibly transfer these behaviors to solve novel downstream control tasks via planning. Notably, H-GAP excels established MPC baselines that have access to the ground truth dynamics model, and is superior or comparable to offline RL methods trained for individual tasks. Finally, we do a series of empirical studies on the scaling properties of H-GAP, showing the potential for performance gains via additional data but not computing. Code and videos are available at https://ycxuyingchen.github.io/hgap/.", "url": "https://arxiv.org/abs/2312.02682"}, {"metadata": {"arXiv": "2312.02720", "Date": "Tue, 05 Dec 2023 12:34:51 ", "Title": "Towards the Inferrence of Structural Similarity of Combinatorial Landscapes", "Authors": ["Mingyu Huang", "Ke Li"], "Categories": "cs.LG cs.AI"}, "abstract": "One of the most common problem-solving heuristics is by analogy. For a given problem, a solver can be viewed as a strategic walk on its fitness landscape. Thus if a solver works for one problem instance, we expect it will also be effective for other instances whose fitness landscapes essentially share structural similarities with each other. However, due to the black-box nature of combinatorial optimization, it is far from trivial to infer such similarity in real-world scenarios. To bridge this gap, by using local optima network as a proxy of fitness landscapes, this paper proposed to leverage graph data mining techniques to conduct qualitative and quantitative analyses to explore the latent topological structural information embedded in those landscapes. By conducting large-scale empirical experiments on three classic combinatorial optimization problems, we gain concrete evidence to support the existence of structural similarity between landscapes of the same classes within neighboring dimensions. We also interrogated the relationship between landscapes of different problem classes.", "url": "https://arxiv.org/abs/2312.02720"}, {"metadata": {"arXiv": "2312.02826", "Date": "Tue, 05 Dec 2023 15:19:29 ", "Title": "Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault Diagnosis", "Authors": ["Florent Forest", "Olga Fink"], "Categories": "cs.LG cs.AI eess.SP stat.ML", "Comments": ["23 pages. Under review"], "MSC-class": "68T07, 62H30", "ACM-class": "I.2.6; J.2"}, "abstract": "Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be an effective and flexible solution, attracting extensive research. Deep neural networks can learn rich representations from vast amounts of representative labeled data for various applications. In IFD, they achieve high classification performance from signals in an end-to-end manner, without requiring extensive domain knowledge. However, deep learning models usually only perform well on the data distribution they have been trained on. When applied to a different distribution, they may experience performance drops. This is also observed in IFD, where assets are often operated in working conditions different from those in which labeled data have been collected. Unsupervised domain adaptation (UDA) deals with the scenario where labeled data are available in a source domain, and only unlabeled data are available in a target domain, where domains may correspond to operating conditions. Recent methods rely on training with confident pseudo-labels for target samples. However, the confidence-based selection of pseudo-labels is hindered by poorly calibrated confidence estimates in the target domain, primarily due to over-confident predictions, which limits the quality of pseudo-labels and leads to error accumulation. In this paper, we propose a novel UDA method called Calibrated Adaptive Teacher (CAT), where we propose to calibrate the predictions of the teacher network throughout the self-training process, leveraging post-hoc calibration techniques. We evaluate CAT on domain-adaptive IFD and perform extensive experiments on the Paderborn benchmark for bearing fault diagnosis under varying operating conditions. Our proposed method achieves state-of-the-art performance on most transfer tasks.", "url": "https://arxiv.org/abs/2312.02826"}, {"metadata": {"arXiv": "2312.02829", "Date": "Tue, 05 Dec 2023 15:25:45 ", "Title": "MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition", "Authors": ["Nicolas Menet (1 and 2)", "Michael Hersche (1 and 2)", "Geethan Karunaratne (1)", "Luca Benini (2)", "Abu Sebastian (1)", "Abbas Rahimi (1) ((1) IBM Research - Zurich", "(2) ETH Zurich)"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["accepted in NeurIPS 2023"]}, "abstract": "With the advent of deep learning, progressively larger neural networks have been designed to solve complex tasks. We take advantage of these capacity-rich models to lower the cost of inference by exploiting computation in superposition. To reduce the computational burden per input, we propose Multiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling many inputs at once. MIMONets augment various deep neural network architectures with variable binding mechanisms to represent an arbitrary number of inputs in a compositional data structure via fixed-width distributed representations. Accordingly, MIMONets adapt nonlinear neural transformations to process the data structure holistically, leading to a speedup nearly proportional to the number of superposed input items in the data structure. After processing in superposition, an unbinding mechanism recovers each transformed input of interest. MIMONets also provide a dynamic trade-off between accuracy and throughput by an instantaneous on-demand switching between a set of accuracy-throughput operating points, yet within a single set of fixed parameters. We apply the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMOFormer, respectively. Empirical evaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy delta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and CIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining a high average accuracy within a [-1.07, -3.43]% delta on the long range arena benchmark. Finally, we provide mathematical bounds on the interference between superposition channels in MIMOFormer. Our code is available at https://github.com/IBM/multiple-input-multiple-output-nets.", "url": "https://arxiv.org/abs/2312.02829"}, {"metadata": {"arXiv": "2312.02858", "Date": "Tue, 05 Dec 2023 16:13:34 ", "Title": "Towards Causal Representations of Climate Model Data", "Authors": ["Julien Boussard", "Chandni Nagda", "Julia Kaltenborn", "Charlotte Emilie Elektra Lange", "Philippe Brouillard", "Yaniv Gurwicz", "Peer Nowack", "David Rolnick"], "Categories": "cs.LG cs.AI physics.ao-ph stat.ME"}, "abstract": "Climate models, such as Earth system models (ESMs), are crucial for simulating future climate change based on projected Shared Socioeconomic Pathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated and invaluable, machine learning-based emulators trained on existing simulation data can project additional climate scenarios much faster and are computationally efficient. However, they often lack generalizability and interpretability. This work delves into the potential of causal representation learning, specifically the \\emph{Causal Discovery with Single-parent Decoding} (CDSD) method, which could render climate model emulation efficient \\textit{and} interpretable. We evaluate CDSD on multiple climate datasets, focusing on emissions, temperature, and precipitation. Our findings shed light on the challenges, limitations, and promise of using CDSD as a stepping stone towards more interpretable and robust climate model emulation.", "url": "https://arxiv.org/abs/2312.02858"}, {"metadata": {"arXiv": "2312.02872", "Date": "Tue, 05 Dec 2023 16:39:32 ", "Title": "Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction", "Authors": ["Angie Nataly Melo", "Carlota Salinas and Miguel Angel Sotelo"], "Categories": "cs.LG cs.AI cs.NE cs.SY eess.SY"}, "abstract": "In the context of autonomous driving, pedestrian crossing prediction is a key component for improving road safety. Presently, the focus of these predictions extends beyond achieving trustworthy results; it is shifting towards the explainability and interpretability of these predictions. This research introduces a novel neuro-symbolic approach that combines deep learning and fuzzy logic for an explainable and interpretable pedestrian crossing prediction. We have developed an explainable predictor (ExPedCross), which utilizes a set of explainable features and employs a fuzzy inference system to predict whether the pedestrian will cross or not. Our approach was evaluated on both the PIE and JAAD datasets. The results offer experimental insights into achieving explainability and interpretability in the pedestrian crossing prediction task. Furthermore, the testing results yield a set of guidelines and recommendations regarding the process of dataset selection, feature selection, and explainability.", "url": "https://arxiv.org/abs/2312.02872"}, {"metadata": {"arXiv": "2312.02873", "Date": "Tue, 05 Dec 2023 16:39:41 ", "Title": "Toward autocorrection of chemical process flowsheets using large language models", "Authors": ["Lukas Schulze Balhorn and Marc Caballero and Artur M. Schweidtmann"], "Categories": "cs.LG cs.AI"}, "abstract": "The process engineering domain widely uses Process Flow Diagrams (PFDs) and Process and Instrumentation Diagrams (P&IDs) to represent process flows and equipment configurations. However, the P&IDs and PFDs, hereafter called flowsheets, can contain errors causing safety hazards, inefficient operation, and unnecessary expenses. Correcting and verifying flowsheets is a tedious, manual process. We propose a novel generative AI methodology for automatically identifying errors in flowsheets and suggesting corrections to the user, i.e., autocorrecting flowsheets. Inspired by the breakthrough of Large Language Models (LLMs) for grammatical autocorrection of human language, we investigate LLMs for the autocorrection of flowsheets. The input to the model is a potentially erroneous flowsheet and the output of the model are suggestions for a corrected flowsheet. We train our autocorrection model on a synthetic dataset in a supervised manner. The model achieves a top-1 accuracy of 80% and a top-5 accuracy of 84% on an independent test dataset of synthetically generated flowsheets. The results suggest that the model can learn to autocorrect the synthetic flowsheets. We envision that flowsheet autocorrection will become a useful tool for chemical engineers.", "url": "https://arxiv.org/abs/2312.02873"}, {"metadata": {"arXiv": "2312.02352", "Date": "Mon, 04 Dec 2023 21:32:00 ", "Title": "Working Backwards: Learning to Place by Picking", "Authors": ["Oliver Limoyo", "Abhisek Konar", "Trevor Ablett", "Jonathan Kelly", "Francois R. Hogan", "Gregory Dudek"], "Categories": "cs.RO cs.AI cs.LG"}, "abstract": "We present Learning to Place by Picking (LPP), a method capable of autonomously collecting demonstrations for a family of placing tasks in which objects must be manipulated to specific locations. With LPP, we approach the learning of robotic object placement policies by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects that are initially located at their target placement locations. Our system is capable of collecting hundreds of demonstrations without human intervention by using a combination of tactile sensing and compliant control for grasps. We train a policy directly from visual observations through behaviour cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the training environment without privileged information (e.g., placing a plate picked up from a table and not at the original placement location). We validate our approach on home robotic scenarios that include dishwasher loading and table setting. Our approach yields robotic placing policies that outperform policies trained with kinesthetic teaching, both in terms of performance and data efficiency, while requiring no human supervision.", "url": "https://arxiv.org/abs/2312.02352"}, {"metadata": {"arXiv": "2312.02975", "Date": "Tue, 05 Dec 2023 18:59:23 ", "Title": "Dexterous Functional Grasping", "Authors": ["Ananye Agarwal", "Shagun Uppal", "Kenneth Shaw", "Deepak Pathak"], "Categories": "cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY", "Comments": ["In CoRL 2023. Website at https://dexfunc.github.io/"]}, "abstract": "While there have been significant strides in dexterous manipulation, most of it is limited to benchmark tasks like in-hand reorientation which are of limited utility in the real world. The main benefit of dexterous hands over two-fingered ones is their ability to pickup tools and other objects (including thin ones) and grasp them firmly to apply force. However, this task requires both a complex understanding of functional affordances as well as precise low-level control. While prior work obtains affordances from human data this approach doesn't scale to low-level control. Similarly, simulation training cannot give the robot an understanding of real-world semantics. In this paper, we aim to combine the best of both worlds to accomplish functional grasping for in-the-wild objects. We use a modular approach. First, affordances are obtained by matching corresponding regions of different objects and then a low-level policy trained in sim is run to grasp it. We propose a novel application of eigengrasps to reduce the search space of RL using a small amount of human data and find that it leads to more stable and physically realistic motion. We find that eigengrasp action space beats baselines in simulation and outperforms hardcoded grasping in real and matches or outperforms a trained human teleoperator. Results visualizations and videos at https://dexfunc.github.io/", "url": "https://arxiv.org/abs/2312.02975"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
