<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2310.05943", "Date": "Tue, 29 Aug 2023 07:05:56 ", "Title": "Analysis of Learned Features and Framework for Potato Disease Detection", "Authors": ["Shikha Gupta", "Soma Chakraborty", "Renu Rameshan"], "Categories": "cs.CV cs.LG", "Comments": ["15 pages", "8 figures"]}, "abstract": "For applications like plant disease detection, usually, a model is trained on publicly available data and tested on field data. This means that the test data distribution is not the same as the training data distribution, which affects the classifier performance adversely. We handle this dataset shift by ensuring that the features are learned from disease spots in the leaf or healthy regions, as applicable. This is achieved using a faster Region-based convolutional neural network (RCNN) as one of the solutions and an attention-based network as the other. The average classification accuracies of these classifiers are approximately 95% while evaluated on the test set corresponding to their training dataset. These classifiers also performed equivalently, with an average score of 84% on a dataset not seen during the training phase.", "url": "https://arxiv.org/abs/2310.05943"}, {"metadata": {"arXiv": "2310.05947", "Date": "Sun, 03 Sep 2023 14:20:58 ", "Title": "Robust and Efficient Interference Neural Networks for Defending Against Adversarial Attacks in ImageNet", "Authors": ["Yunuo Xiong", "Shujuan Liu", "Hongwei Xiong"], "Categories": "cs.CV cs.LG", "Comments": ["11 pages", "3 figures"]}, "abstract": "The existence of adversarial images has seriously affected the task of image recognition and practical application of deep learning, it is also a key scientific problem that deep learning urgently needs to solve. By far the most effective approach is to train the neural network with a large number of adversarial examples. However, this adversarial training method requires a huge amount of computing resources when applied to ImageNet, and has not yet achieved satisfactory results for high-intensity adversarial attacks. In this paper, we construct an interference neural network by applying additional background images and corresponding labels, and use pre-trained ResNet-152 to efficiently complete the training. Compared with the state-of-the-art results under the PGD attack, it has a better defense effect with much smaller computing resources. This work provides new ideas for academic research and practical applications of effective defense against adversarial attacks.", "url": "https://arxiv.org/abs/2310.05947"}, {"metadata": {"arXiv": "2310.05959", "Date": "Tue, 12 Sep 2023 10:56:16 ", "Title": "Automating global landslide detection with heterogeneous ensemble deep-learning classification", "Authors": ["Alexandra Jarna Ganer{\\o}d", "Gabriele Franch", "Erin Lindsay", "Martina Calovi"], "Categories": "cs.CV cs.LG", "Comments": ["Author 1 and Author 2 contributed equally to this work"]}, "abstract": "With changing climatic conditions, we are already seeing an increase in extreme weather events and their secondary consequences, including landslides. Landslides threaten infrastructure, including roads, railways, buildings, and human life. Hazard-based spatial planning and early warning systems are cost-effective strategies to reduce the risk to society from landslides. However, these both rely on data from previous landslide events, which is often scarce. Many deep learning (DL) models have recently been applied for landside mapping using medium- to high-resolution satellite images as input. However, they often suffer from sensitivity problems, overfitting, and low mapping accuracy. This study addresses some of these limitations by using a diverse global landslide dataset, using different segmentation models, such as Unet, Linknet, PSP-Net, PAN, and DeepLab and based on their performances, building an ensemble model. The ensemble model achieved the highest F1-score (0.69) when combining both Sentinel-1 and Sentinel-2 bands, with the highest average improvement of 6.87 % when the ensemble size was 20. On the other hand, Sentinel-2 bands only performed very well, with an F1 score of 0.61 when the ensemble size is 20 with an improvement of 14.59 % when the ensemble size is 20. This result shows considerable potential in building a robust and reliable monitoring system based on changes in vegetation index dNDVI only.", "url": "https://arxiv.org/abs/2310.05959"}, {"metadata": {"arXiv": "2310.06085", "Date": "Sun, 20 Aug 2023 22:27:54 ", "Title": "Quantile-based Maximum Likelihood Training for Outlier Detection", "Authors": ["Masoud Taghikhah", "Nishant Kumar", "Sini\\v{s}a \\v{S}egvi\\'c", "Abouzar Eslami", "Stefan Gumhold"], "Categories": "cs.CV cs.LG", "Comments": ["Code available at https://github.com/taghikhah/QuantOD"]}, "abstract": "Discriminative learning effectively predicts true object class for image classification. However, it often results in false positives for outliers, posing critical concerns in applications like autonomous driving and video surveillance systems. Previous attempts to address this challenge involved training image classifiers through contrastive learning using actual outlier data or synthesizing outliers for self-supervised learning. Furthermore, unsupervised generative modeling of inliers in pixel space has shown limited success for outlier detection. In this work, we introduce a quantile-based maximum likelihood objective for learning the inlier distribution to improve the outlier separation during inference. Our approach fits a normalizing flow to pre-trained discriminative features and detects the outliers according to the evaluated log-likelihood. The experimental evaluation demonstrates the effectiveness of our method as it surpasses the performance of the state-of-the-art unsupervised methods for outlier detection. The results are also competitive compared with a recent self-supervised approach for outlier detection. Our work allows to reduce dependency on well-sampled negative training data, which is especially important for domains like medical diagnostics or remote sensing.", "url": "https://arxiv.org/abs/2310.06085"}, {"metadata": {"arXiv": "2310.06164", "Date": "Sat, 16 Sep 2023 23:33:15 ", "Title": "DEUX: Active Exploration for Learning Unsupervised Depth Perception", "Authors": ["Marvin Chanc\\'an", "Alex Wong", "Ian Abraham"], "Categories": "cs.CV cs.LG cs.RO"}, "abstract": "Depth perception models are typically trained on non-interactive datasets with predefined camera trajectories. However, this often introduces systematic biases into the learning process correlated to specific camera paths chosen during data acquisition. In this paper, we investigate the role of how data is collected for learning depth completion, from a robot navigation perspective, by leveraging 3D interactive environments. First, we evaluate four depth completion models trained on data collected using conventional navigation techniques. Our key insight is that existing exploration paradigms do not necessarily provide task-specific data points to achieve competent unsupervised depth completion learning. We then find that data collected with respect to photometric reconstruction has a direct positive influence on model performance. As a result, we develop an active, task-informed, depth uncertainty-based motion planning approach for learning depth completion, which we call DEpth Uncertainty-guided eXploration (DEUX). Training with data collected by our approach improves depth completion by an average greater than 18% across four depth completion models compared to existing exploration methods on the MP3D test set. We show that our approach further improves zero-shot generalization, while offering new insights into integrating robot learning-based depth estimation.", "url": "https://arxiv.org/abs/2310.06164"}, {"metadata": {"arXiv": "2310.06234", "Date": "Tue, 10 Oct 2023 01:04:15 ", "Title": "Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing", "Authors": ["Wei Dong", "Dawei Yan", "Zhijun Lin", "Peng Wang"], "Categories": "cs.CV cs.LG", "Comments": ["Paper is accepted to NeurIPS 2023"]}, "abstract": "The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adaptive adapters. This parameter-sharing strategy in adapter design allows us to significantly reduce the number of new parameters while maintaining satisfactory performance, thereby offering a promising approach to compress the adaptation cost. We conduct experiments on 24 downstream image classification tasks using various Vision Transformer variants to evaluate our method. The results demonstrate that our approach achieves compelling transfer learning performance with a reduced parameter count. Our code is available at \\href{https://github.com/DavidYanAnDe/ARC}{https://github.com/DavidYanAnDe/ARC}.", "url": "https://arxiv.org/abs/2310.06234"}, {"metadata": {"arXiv": "2310.06437", "Date": "Tue, 10 Oct 2023 09:06:39 ", "Title": "Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks", "Authors": ["Cong Yang", "Bipin Indurkhya", "John See", "Bo Gao", "Yan Ke", "Zeyd Boukhers", "Zhenyu Yang", "and Marcin Grzegorzek"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted for publication in the International Journal of Computer Vision (IJCV)"]}, "abstract": "Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN) but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target's context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape and image datasets. The GTs are then structurally evaluated with representative methods to build viable baselines for fair comparisons. Experiments demonstrate that GTs generated by our strategy yield promising quality with respect to standard consistency, and also provide a balance between simplicity and completeness.", "url": "https://arxiv.org/abs/2310.06437"}, {"metadata": {"arXiv": "2310.06489", "Date": "Tue, 10 Oct 2023 09:57:19 ", "Title": "Deep Learning for Automatic Detection and Facial Recognition in Japanese Macaques: Illuminating Social Networks", "Authors": ["Julien Paulet (UJM)", "Axel Molina (ENS-PSL)", "Benjamin Beltzung (IPHC)", "Takafumi Suzumura", "Shinya Yamamoto", "C\\'edric Sueur (IPHC", "IUF", "ANTHROPO LAB)"], "Categories": "cs.CV cs.LG cs.SI"}, "abstract": "Individual identification plays a pivotal role in ecology and ethology, notably as a tool for complex social structures understanding. However, traditional identification methods often involve invasive physical tags and can prove both disruptive for animals and time-intensive for researchers. In recent years, the integration of deep learning in research offered new methodological perspectives through automatization of complex tasks. Harnessing object detection and recognition technologies is increasingly used by researchers to achieve identification on video footage. This study represents a preliminary exploration into the development of a non-invasive tool for face detection and individual identification of Japanese macaques (Macaca fuscata) through deep learning. The ultimate goal of this research is, using identifications done on the dataset, to automatically generate a social network representation of the studied population. The current main results are promising: (i) the creation of a Japanese macaques' face detector (Faster-RCNN model), reaching a 82.2% accuracy and (ii) the creation of an individual recognizer for K{\\=o}jima island macaques population (YOLOv8n model), reaching a 83% accuracy. We also created a K{\\=o}jima population social network by traditional methods, based on co-occurrences on videos. Thus, we provide a benchmark against which the automatically generated network will be assessed for reliability. These preliminary results are a testament to the potential of this innovative approach to provide the scientific community with a tool for tracking individuals and social network studies in Japanese macaques.", "url": "https://arxiv.org/abs/2310.06489"}, {"metadata": {"arXiv": "2310.06667", "Date": "Tue, 10 Oct 2023 14:42:32 ", "Title": "SC2GAN: Rethinking Entanglement by Self-correcting Correlated GAN Space", "Authors": ["Zikun Chen", "Han Zhao", "Parham Aarabi", "Ruowei Jiang"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted to the Out Of Distribution Generalization in Computer Vision workshop at ICCV2023"]}, "abstract": "Generative Adversarial Networks (GANs) can synthesize realistic images, with the learned latent space shown to encode rich semantic information with various interpretable directions. However, due to the unstructured nature of the learned latent space, it inherits the bias from the training data where specific groups of visual attributes that are not causally related tend to appear together, a phenomenon also known as spurious correlations, e.g., age and eyeglasses or women and lipsticks. Consequently, the learned distribution often lacks the proper modelling of the missing examples. The interpolation following editing directions for one attribute could result in entangled changes with other attributes. To address this problem, previous works typically adjust the learned directions to minimize the changes in other attributes, yet they still fail on strongly correlated features. In this work, we study the entanglement issue in both the training data and the learned latent space for the StyleGAN2-FFHQ model. We propose a novel framework SC$^2$GAN that achieves disentanglement by re-projecting low-density latent code samples in the original latent space and correcting the editing directions based on both the high-density and low-density regions. By leveraging the original meaningful directions and semantic region-specific layers, our framework interpolates the original latent codes to generate images with attribute combination that appears infrequently, then inverts these samples back to the original latent space. We apply our framework to pre-existing methods that learn meaningful latent directions and showcase its strong capability to disentangle the attributes with small amounts of low-density region samples added.", "url": "https://arxiv.org/abs/2310.06667"}, {"metadata": {"arXiv": "2310.05963", "Date": "Wed, 13 Sep 2023 06:30:08 ", "Title": "CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics", "Authors": ["Yining Luo", "Yingfa Chen", "Zhen Zhang"], "Categories": "cs.LG physics.comp-ph physics.flu-dyn", "Comments": ["33 pages", "11 figures", "preprint"]}, "abstract": "In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solutions of fluid mechanics equations. (3) challenging. It has a certain learning difficulty, prompting to find models with strong learning ability. (4) standardized. CFDBench facilitates a comprehensive and fair comparison of different deep learning methods for CFD. We make appropriate modifications to popular deep neural networks to apply them to CFDBench and enable the accommodation of more changing inputs. The evaluation on CFDBench reveals some new shortcomings of existing works and we propose possible directions for solving such problems.", "url": "https://arxiv.org/abs/2310.05963"}, {"metadata": {"arXiv": "2310.05988", "Date": "Sat, 07 Oct 2023 19:35:07 ", "Title": "A Dual Latent State Learning Approach: Exploiting Regional Network Similarities for QoS Prediction", "Authors": ["Ziliang Wang", "Xiaohong Zhang", "Meng Yan"], "Categories": "cs.LG"}, "abstract": "Individual objects, whether users or services, within a specific region often exhibit similar network states due to their shared origin from the same city or autonomous system (AS). Despite this regional network similarity, many existing techniques overlook its potential, resulting in subpar performance arising from challenges such as data sparsity and label imbalance. In this paper, we introduce the regional-based dual latent state learning network(R2SL), a novel deep learning framework designed to overcome the pitfalls of traditional individual object-based prediction techniques in Quality of Service (QoS) prediction. Unlike its predecessors, R2SL captures the nuances of regional network behavior by deriving two distinct regional network latent states: the city-network latent state and the AS-network latent state. These states are constructed utilizing aggregated data from common regions rather than individual object data. Furthermore, R2SL adopts an enhanced Huber loss function that adjusts its linear loss component, providing a remedy for prevalent label imbalance issues. To cap off the prediction process, a multi-scale perception network is leveraged to interpret the integrated feature map, a fusion of regional network latent features and other pertinent information, ultimately accomplishing the QoS prediction. Through rigorous testing on real-world QoS datasets, R2SL demonstrates superior performance compared to prevailing state-of-the-art methods. Our R2SL approach ushers in an innovative avenue for precise QoS predictions by fully harnessing the regional network similarities inherent in objects.", "url": "https://arxiv.org/abs/2310.05988"}, {"metadata": {"arXiv": "2310.05996", "Date": "Mon, 09 Oct 2023 08:47:12 ", "Title": "A novel Network Science Algorithm for Improving Triage of Patients", "Authors": ["Pietro Hiram Guzzi", "Annamaria De Filippo", "Pierangelo Veltri"], "Categories": "cs.LG"}, "abstract": "Patient triage plays a crucial role in healthcare, ensuring timely and appropriate care based on the urgency of patient conditions. Traditional triage methods heavily rely on human judgment, which can be subjective and prone to errors. Recently, a growing interest has been in leveraging artificial intelligence (AI) to develop algorithms for triaging patients. This paper presents the development of a novel algorithm for triaging patients. It is based on the analysis of patient data to produce decisions regarding their prioritization. The algorithm was trained on a comprehensive data set containing relevant patient information, such as vital signs, symptoms, and medical history. The algorithm was designed to accurately classify patients into triage categories through rigorous preprocessing and feature engineering. Experimental results demonstrate that our algorithm achieved high accuracy and performance, outperforming traditional triage methods. By incorporating computer science into the triage process, healthcare professionals can benefit from improved efficiency, accuracy, and consistency, prioritizing patients effectively and optimizing resource allocation. Although further research is needed to address challenges such as biases in training data and model interpretability, the development of AI-based algorithms for triaging patients shows great promise in enhancing healthcare delivery and patient outcomes.", "url": "https://arxiv.org/abs/2310.05996"}, {"metadata": {"arXiv": "2310.06002", "Date": "Mon, 09 Oct 2023 14:37:56 ", "Title": "LCOT: Linear circular optimal transport", "Authors": ["Rocio Diaz Martin", "Ivan Medri", "Yikun Bai", "Xinran Liu", "Kangbai Yan", "Gustavo K. Rohde", "Soheil Kolouri"], "Categories": "cs.LG"}, "abstract": "The optimal transport problem for measures supported on non-Euclidean spaces has recently gained ample interest in diverse applications involving representation learning. In this paper, we focus on circular probability measures, i.e., probability measures supported on the unit circle, and introduce a new computationally efficient metric for these measures, denoted as Linear Circular Optimal Transport (LCOT). The proposed metric comes with an explicit linear embedding that allows one to apply Machine Learning (ML) algorithms to the embedded measures and seamlessly modify the underlying metric for the ML algorithm to LCOT. We show that the proposed metric is rooted in the Circular Optimal Transport (COT) and can be considered the linearization of the COT metric with respect to a fixed reference measure. We provide a theoretical analysis of the proposed metric and derive the computational complexities for pairwise comparison of circular probability measures. Lastly, through a set of numerical experiments, we demonstrate the benefits of LCOT in learning representations of circular measures.", "url": "https://arxiv.org/abs/2310.06002"}, {"metadata": {"arXiv": "2310.06047", "Date": "Mon, 09 Oct 2023 18:02:38 ", "Title": "Knowledge Distillation for Anomaly Detection", "Authors": ["Adrian Alan Pol", "Ekaterina Govorkova", "Sonja Gronroos", "Nadezda Chernyavskaya", "Philip Harris", "Maurizio Pierini", "Isobel Ojalvo", "Peter Elmer"], "Categories": "cs.LG"}, "abstract": "Unsupervised deep learning techniques are widely used to identify anomalous behaviour. The performance of such methods is a product of the amount of training data and the model size. However, the size is often a limiting factor for the deployment on resource-constrained devices. We present a novel procedure based on knowledge distillation for compressing an unsupervised anomaly detection model into a supervised deployable one and we suggest a set of techniques to improve the detection sensitivity. Compressed models perform comparably to their larger counterparts while significantly reducing the size and memory footprint.", "url": "https://arxiv.org/abs/2310.06047"}, {"metadata": {"arXiv": "2310.06059", "Date": "Mon, 09 Oct 2023 18:12:46 ", "Title": "Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting", "Authors": ["Peng Zhang", "Ting Gao", "Jin Guo", "Jinqiao Duan"], "Categories": "cs.LG math.DS", "Comments": ["12 pages,4 figures"]}, "abstract": "Early warning for epilepsy patients is crucial for their safety and well-being, in terms of preventing or minimizing the severity of seizures. Through the patients' EEG data, we propose a meta learning framework for improving prediction on early ictal signals. To better utilize the meta label corrector method, we fuse the information from both the real data and the augmented data from the latent Stochastic differential equation(SDE). Besides, we also optimally select the latent dynamical system via distribution of transition time between real data and that from the latent SDE. In this way, the extracted tipping dynamical feature is also integrated into the meta network to better label the noisy data. To validate our method, LSTM is implemented as the baseline model. We conduct a series of experiments to predict seizure in various long-term window from 1-2 seconds input data and find surprisingly increment of prediction accuracy.", "url": "https://arxiv.org/abs/2310.06059"}, {"metadata": {"arXiv": "2310.06077", "Date": "Mon, 09 Oct 2023 18:34:29 ", "Title": "Performative Time-Series Forecasting", "Authors": ["Zhiyuan Zhao", "Alexander Rodriguez", "B.Aditya Prakash"], "Categories": "cs.LG", "Comments": ["12 pages (7 main text", "2 reference", "3 appendix)", "3 figures", "4 tables"]}, "abstract": "Time-series forecasting is a critical challenge in various domains and has witnessed substantial progress in recent years. Many real-life scenarios, such as public health, economics, and social applications, involve feedback loops where predictions can influence the predicted outcome, subsequently altering the target variable's distribution. This phenomenon, known as performativity, introduces the potential for 'self-negating' or 'self-fulfilling' predictions. Despite extensive studies in classification problems across domains, performativity remains largely unexplored in the context of time-series forecasting from a machine-learning perspective. In this paper, we formalize performative time-series forecasting (PeTS), addressing the challenge of accurate predictions when performativity-induced distribution shifts are possible. We propose a novel approach, Feature Performative-Shifting (FPS), which leverages the concept of delayed response to anticipate distribution shifts and subsequently predicts targets accordingly. We provide theoretical insights suggesting that FPS can potentially lead to reduced generalization error. We conduct comprehensive experiments using multiple time-series models on COVID-19 and traffic forecasting tasks. The results demonstrate that FPS consistently outperforms conventional time-series forecasting methods, highlighting its efficacy in handling performativity-induced challenges.", "url": "https://arxiv.org/abs/2310.06077"}, {"metadata": {"arXiv": "2310.06083", "Date": "Mon, 09 Oct 2023 18:40:04 ", "Title": "Transformers and Large Language Models for Chemistry and Drug Discovery", "Authors": ["Andres M Bran", "Philippe Schwaller"], "Categories": "cs.LG physics.chem-ph"}, "abstract": "Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology. In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration. The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language. A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language. As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.", "url": "https://arxiv.org/abs/2310.06083"}, {"metadata": {"arXiv": "2310.06112", "Date": "Mon, 09 Oct 2023 19:40:25 ", "Title": "Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach", "Authors": ["Shaopeng Fu", "Di Wang"], "Categories": "cs.LG stat.ML"}, "abstract": "Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). However, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time AT can be detrimental to the robustness of DNNs. This paper presents a theoretical explanation of robust overfitting for DNNs. Specifically, we non-trivially extend the neural tangent kernel (NTK) theory to AT and prove that an adversarially trained wide DNN can be well approximated by a linearized DNN. Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can be derived, which reveals a new AT degeneration phenomenon: a long-term AT will result in a wide DNN degenerates to that obtained without AT and thus cause robust overfitting. Based on our theoretical results, we further design a method namely Adv-NTK, the first AT algorithm for infinite-width DNNs. Experiments on real-world datasets show that Adv-NTK can help infinite-width DNNs enhance comparable robustness to that of their finite-width counterparts, which in turn justifies our theoretical findings. The code is available at https://github.com/fshp971/adv-ntk.", "url": "https://arxiv.org/abs/2310.06112"}, {"metadata": {"arXiv": "2310.06124", "Date": "Mon, 09 Oct 2023 19:59:59 ", "Title": "Factorized Tensor Networks for Multi-Task and Multi-Domain Learning", "Authors": ["Yash Garg", "Nebiyou Yismaw", "Rakib Hyder", "Ashley Prater-Bennette", "M. Salman Asif"], "Categories": "cs.LG cs.CV"}, "abstract": "Multi-task and multi-domain learning methods seek to learn multiple tasks/domains, jointly or one after another, using a single unified network. The key challenge and opportunity is to exploit shared information across tasks and domains to improve the efficiency of the unified network. The efficiency can be in terms of accuracy, storage cost, computation, or sample complexity. In this paper, we propose a factorized tensor network (FTN) that can achieve accuracy comparable to independent single-task/domain networks with a small number of additional parameters. FTN uses a frozen backbone network from a source model and incrementally adds task/domain-specific low-rank tensor factors to the shared frozen network. This approach can adapt to a large number of target domains and tasks without catastrophic forgetting. Furthermore, FTN requires a significantly smaller number of task-specific parameters compared to existing methods. We performed experiments on widely used multi-domain and multi-task datasets. We show the experiments on convolutional-based architecture with different backbones and on transformer-based architecture. We observed that FTN achieves similar accuracy as single-task/domain methods while using only a fraction of additional parameters per task.", "url": "https://arxiv.org/abs/2310.06124"}, {"metadata": {"arXiv": "2310.06139", "Date": "Mon, 09 Oct 2023 20:35:38 ", "Title": "On the Correlation between Random Variables and their Principal Components", "Authors": ["Zenon Gniazdowski"], "Categories": "cs.LG math.ST stat.TH", "Comments": ["15 pages"], "Journal-ref": "Zeszyty Naukowe WWSI, No 28, Vol. 17, 2023, pp. 41-55", "DOI": "10.26348/znwwsi.28.41"}, "abstract": "The article attempts to find an algebraic formula describing the correlation coefficients between random variables and the principal components representing them. As a result of the analysis, starting from selected statistics relating to individual random variables, the equivalents of these statistics relating to a set of random variables were presented in the language of linear algebra, using the concepts of vector and matrix. This made it possible, in subsequent steps, to derive the expected formula. The formula found is identical to the formula used in Factor Analysis to calculate factor loadings. The discussion showed that it is possible to apply this formula to optimize the number of principal components in Principal Component Analysis, as well as to optimize the number of factors in Factor Analysis.", "url": "https://arxiv.org/abs/2310.06139"}, {"metadata": {"arXiv": "2310.06150", "Date": "Mon, 09 Oct 2023 20:58:52 ", "Title": "Latent Diffusion Model for DNA Sequence Generation", "Authors": ["Zehui Li", "Yuhao Ni", "Tim August B. Huygelen", "Akashaditya Das", "Guoxuan Xia", "Guy-Bart Stan", "Yiren Zhao"], "Categories": "cs.LG"}, "abstract": "The harnessing of machine learning, especially deep generative models, has opened up promising avenues in the field of synthetic DNA sequence generation. Whilst Generative Adversarial Networks (GANs) have gained traction for this application, they often face issues such as limited sample diversity and mode collapse. On the other hand, Diffusion Models are a promising new class of generative models that are not burdened with these problems, enabling them to reach the state-of-the-art in domains such as image generation. In light of this, we propose a novel latent diffusion model, DiscDiff, tailored for discrete DNA sequence generation. By simply embedding discrete DNA sequences into a continuous latent space using an autoencoder, we are able to leverage the powerful generative abilities of continuous diffusion models for the generation of discrete data. Additionally, we introduce Fr\\'echet Reconstruction Distance (FReD) as a new metric to measure the sample quality of DNA sequence generations. Our DiscDiff model demonstrates an ability to generate synthetic DNA sequences that align closely with real DNA in terms of Motif Distribution, Latent Embedding Distribution (FReD), and Chromatin Profiles. Additionally, we contribute a comprehensive cross-species dataset of 150K unique promoter-gene sequences from 15 species, enriching resources for future generative modelling in genomics. We will make our code public upon publication.", "url": "https://arxiv.org/abs/2310.06150"}, {"metadata": {"arXiv": "2310.06159", "Date": "Mon, 09 Oct 2023 21:16:57 ", "Title": "Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization", "Authors": ["Cong Ma", "Xingyu Xu", "Tian Tong", "Yuejie Chi"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["Book chapter for \"Explorations in the Mathematics of Data Science - The Inaugural Volume of the Center for Approximation and Mathematical Data Analytics\". arXiv admin note: text overlap with arXiv:2104.14526"]}, "abstract": "Many problems encountered in science and engineering can be formulated as estimating a low-rank object (e.g., matrices and tensors) from incomplete, and possibly corrupted, linear measurements. Through the lens of matrix and tensor factorization, one of the most popular approaches is to employ simple iterative algorithms such as gradient descent (GD) to recover the low-rank factors directly, which allow for small memory and computation footprints. However, the convergence rate of GD depends linearly, and sometimes even quadratically, on the condition number of the low-rank object, and therefore, GD slows down painstakingly when the problem is ill-conditioned. This chapter introduces a new algorithmic approach, dubbed scaled gradient descent (ScaledGD), that provably converges linearly at a constant rate independent of the condition number of the low-rank object, while maintaining the low per-iteration cost of gradient descent for a variety of tasks including sensing, robust principal component analysis and completion. In addition, ScaledGD continues to admit fast global convergence to the minimax-optimal solution, again almost independent of the condition number, from a small random initialization when the rank is over-specified in the presence of Gaussian noise. In total, ScaledGD highlights the power of appropriate preconditioning in accelerating nonconvex statistical estimation, where the iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the symmetry in low-rank factorization without hurting generalization.", "url": "https://arxiv.org/abs/2310.06159"}, {"metadata": {"arXiv": "2310.06161", "Date": "Mon, 09 Oct 2023 21:19:39 ", "Title": "Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness", "Authors": ["Bhavya Vasudeva", "Kameron Shahabi", "Vatsal Sharan"], "Categories": "cs.LG stat.ML", "Comments": ["28 pages", "10 figures", "16 tables"]}, "abstract": "Neural networks (NNs) are known to exhibit simplicity bias where they tend to prefer learning 'simple' features over more 'complex' ones, even when the latter may be more informative. Simplicity bias can lead to the model making biased predictions which have poor out-of-distribution (OOD) generalization. To address this, we propose a framework that encourages the model to use a more diverse set of features to make predictions. We first train a simple model, and then regularize the conditional mutual information with respect to it to obtain the final model. We demonstrate the effectiveness of this framework in various problem settings and real-world applications, showing that it effectively addresses simplicity bias and leads to more features being used, enhances OOD generalization, and improves subgroup robustness and fairness. We complement these results with theoretical analyses of the effect of the regularization and its OOD generalization properties.", "url": "https://arxiv.org/abs/2310.06161"}, {"metadata": {"arXiv": "2310.06177", "Date": "Mon, 09 Oct 2023 22:02:05 ", "Title": "DockGame: Cooperative Games for Multimeric Rigid Protein Docking", "Authors": ["Vignesh Ram Somnath", "Pier Giuseppe Sessa", "Maria Rodriguez Martinez", "Andreas Krause"], "Categories": "cs.LG", "Comments": ["Under Review"]}, "abstract": "Protein interactions and assembly formation are fundamental to most biological processes. Predicting the assembly structure from constituent proteins -- referred to as the protein docking task -- is thus a crucial step in protein design applications. Most traditional and deep learning methods for docking have focused mainly on binary docking, following either a search-based, regression-based, or generative modeling paradigm. In this paper, we focus on the less-studied multimeric (i.e., two or more proteins) docking problem. We introduce DockGame, a novel game-theoretic framework for docking -- we view protein docking as a cooperative game between proteins, where the final assembly structure(s) constitute stable equilibria w.r.t. the underlying game potential. Since we do not have access to the true potential, we consider two approaches - i) learning a surrogate game potential guided by physics-based energy functions and computing equilibria by simultaneous gradient updates, and ii) sampling from the Gibbs distribution of the true potential by learning a diffusion generative model over the action spaces (rotations and translations) of all proteins. Empirically, on the Docking Benchmark 5.5 (DB5.5) dataset, DockGame has much faster runtimes than traditional docking methods, can generate multiple plausible assembly structures, and achieves comparable performance to existing binary docking baselines, despite solving the harder task of coordinating multiple protein chains.", "url": "https://arxiv.org/abs/2310.06177"}, {"metadata": {"arXiv": "2310.06179", "Date": "Mon, 09 Oct 2023 22:07:48 ", "Title": "Automatic Integration for Spatiotemporal Neural Point Processes", "Authors": ["Zihao Zhou", "Rose Yu"], "Categories": "cs.LG stat.ML"}, "abstract": "Learning continuous-time point processes is essential to many discrete event forecasting tasks. However, integration poses a major challenge, particularly for spatiotemporal point processes (STPPs), as it involves calculating the likelihood through triple integrals over space and time. Existing methods for integrating STPP either assume a parametric form of the intensity function, which lacks flexibility; or approximating the intensity with Monte Carlo sampling, which introduces numerical errors. Recent work by Omi et al. [2019] proposes a dual network or AutoInt approach for efficient integration of flexible intensity function. However, the method only focuses on the 1D temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance. We prove consistency of AutoSTPP and validate it on synthetic data and benchmark real world datasets, showcasing its significant advantage in recovering complex intensity functions from irregular spatiotemporal events, particularly when the intensity is sharply localized.", "url": "https://arxiv.org/abs/2310.06179"}, {"metadata": {"arXiv": "2310.06182", "Date": "Mon, 09 Oct 2023 22:20:27 ", "Title": "PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization", "Authors": ["Jiancong Xiao", "Ruoyu Sun", "Zhi-quan Luo"], "Categories": "cs.LG", "Comments": ["NeurIPS 2023"]}, "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is found empirically that adversarially robust generalization is crucial in establishing defense algorithms against adversarial attacks. Therefore, it is interesting to study the theoretical guarantee of robust generalization. This paper focuses on norm-based complexity, based on a PAC-Bayes approach (Neyshabur et al., 2017). The main challenge lies in extending the key ingredient, which is a weight perturbation bound in standard settings, to the robust settings. Existing attempts heavily rely on additional strong assumptions, leading to loose bounds. In this paper, we address this issue and provide a spectrally-normalized robust generalization bound for DNNs. Compared to existing bounds, our bound offers two significant advantages: Firstly, it does not depend on additional assumptions. Secondly, it is considerably tighter, aligning with the bounds of standard generalization. Therefore, our result provides a different perspective on understanding robust generalization: The mismatch terms between standard and robust generalization bounds shown in previous studies do not contribute to the poor robust generalization. Instead, these disparities solely due to mathematical issues. Finally, we extend the main result to adversarial robustness against general non-$\\ell_p$ attacks and other neural network architectures.", "url": "https://arxiv.org/abs/2310.06182"}, {"metadata": {"arXiv": "2310.06205", "Date": "Mon, 09 Oct 2023 23:07:28 ", "Title": "Fair Classifiers that Abstain without Harm", "Authors": ["Tongxin Yin", "Jean-Fran\\c{c}ois Ton", "Ruocheng Guo", "Yuanshun Yao", "Mingyan Liu", "Yang Liu"], "Categories": "cs.LG"}, "abstract": "In critical applications, it is vital for classifiers to defer decision-making to humans. We propose a post-hoc method that makes existing classifiers selectively abstain from predicting certain samples. Our abstaining classifier is incentivized to maintain the original accuracy for each sub-population (i.e. no harm) while achieving a set of group fairness definitions to a user specified degree. To this end, we design an Integer Programming (IP) procedure that assigns abstention decisions for each training sample to satisfy a set of constraints. To generalize the abstaining decisions to test samples, we then train a surrogate model to learn the abstaining decisions based on the IP solutions in an end-to-end manner. We analyze the feasibility of the IP procedure to determine the possible abstention rate for different levels of unfairness tolerance and accuracy constraint for achieving no harm. To the best of our knowledge, this work is the first to identify the theoretical relationships between the constraint parameters and the required abstention rate. Our theoretical results are important since a high abstention rate is often infeasible in practice due to a lack of human resources. Our framework outperforms existing methods in terms of fairness disparity without sacrificing accuracy at similar abstention rates.", "url": "https://arxiv.org/abs/2310.06205"}, {"metadata": {"arXiv": "2310.06217", "Date": "Tue, 10 Oct 2023 00:21:10 ", "Title": "Federated Multi-Level Optimization over Decentralized Networks", "Authors": ["Shuoguang Yang", "Xuezhou Zhang", "Mengdi Wang"], "Categories": "cs.LG math.OC", "Comments": ["arXiv admin note: substantial text overlap with arXiv:2206.10870"]}, "abstract": "Multi-level optimization has gained increasing attention in recent years, as it provides a powerful framework for solving complex optimization problems that arise in many fields, such as meta-learning, multi-player games, reinforcement learning, and nested composition optimization. In this paper, we study the problem of distributed multi-level optimization over a network, where agents can only communicate with their immediate neighbors. This setting is motivated by the need for distributed optimization in large-scale systems, where centralized optimization may not be practical or feasible. To address this problem, we propose a novel gossip-based distributed multi-level optimization algorithm that enables networked agents to solve optimization problems at different levels in a single timescale and share information through network propagation. Our algorithm achieves optimal sample complexity, scaling linearly with the network size, and demonstrates state-of-the-art performance on various applications, including hyper-parameter tuning, decentralized reinforcement learning, and risk-averse optimization.", "url": "https://arxiv.org/abs/2310.06217"}, {"metadata": {"arXiv": "2310.06221", "Date": "Tue, 10 Oct 2023 00:25:21 ", "Title": "Detecting and Learning Out-of-Distribution Data in the Open world: Algorithm and Theory", "Authors": ["Yiyou Sun"], "Categories": "cs.LG", "Comments": ["Ph.D. thesis"]}, "abstract": "This thesis makes considerable contributions to the realm of machine learning, specifically in the context of open-world scenarios where systems face previously unseen data and contexts. Traditional machine learning models are usually trained and tested within a fixed and known set of classes, a condition known as the closed-world setting. While this assumption works in controlled environments, it falls short in real-world applications where new classes or categories of data can emerge dynamically and unexpectedly. To address this, our research investigates two intertwined steps essential for open-world machine learning: Out-of-distribution (OOD) Detection and Open-world Representation Learning (ORL). OOD detection focuses on identifying instances from unknown classes that fall outside the model's training distribution. This process reduces the risk of making overly confident, erroneous predictions about unfamiliar inputs. Moving beyond OOD detection, ORL extends the capabilities of the model to not only detect unknown instances but also learn from and incorporate knowledge about these new classes. By delving into these research problems of open-world learning, this thesis contributes both algorithmic solutions and theoretical foundations, which pave the way for building machine learning models that are not only performant but also reliable in the face of the evolving complexities of the real world.", "url": "https://arxiv.org/abs/2310.06221"}, {"metadata": {"arXiv": "2310.06233", "Date": "Tue, 10 Oct 2023 01:00:13 ", "Title": "Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers", "Authors": ["Zhi-Yong Wang", "Hing Cheung So and Abdelhak M. Zoubir"], "Categories": "cs.LG eess.IV eess.SP"}, "abstract": "To alleviate the bias generated by the l1-norm in the low-rank tensor completion problem, nonconvex surrogates/regularizers have been suggested to replace the tensor nuclear norm, although both can achieve sparsity. However, the thresholding functions of these nonconvex regularizers may not have closed-form expressions and thus iterations are needed, which increases the computational loads. To solve this issue, we devise a framework to generate sparsity-inducing regularizers with closed-form thresholding functions. These regularizers are applied to low-tubal-rank tensor completion, and efficient algorithms based on the alternating direction method of multipliers are developed. Furthermore, convergence of our methods is analyzed and it is proved that the generated sequences are bounded and any limit point is a stationary point. Experimental results using synthetic and real-world datasets show that the proposed algorithms outperform the state-of-the-art methods in terms of restoration performance.", "url": "https://arxiv.org/abs/2310.06233"}, {"metadata": {"arXiv": "2310.06237", "Date": "Tue, 10 Oct 2023 01:21:01 ", "Title": "Differentially Private Multi-Site Treatment Effect Estimation", "Authors": ["Tatsuki Koga", "Kamalika Chaudhuri", "David Page"], "Categories": "cs.LG cs.CR", "Comments": ["16 pages"]}, "abstract": "Patient privacy is a major barrier to healthcare AI. For confidentiality reasons, most patient data remains in silo in separate hospitals, preventing the design of data-driven healthcare AI systems that need large volumes of patient data to make effective decisions. A solution to this is collective learning across multiple sites through federated learning with differential privacy. However, literature in this space typically focuses on differentially private statistical estimation and machine learning, which is different from the causal inference-related problems that arise in healthcare. In this work, we take a fresh look at federated learning with a focus on causal inference; specifically, we look at estimating the average treatment effect (ATE), an important task in causal inference for healthcare applications, and provide a federated analytics approach to enable ATE estimation across multiple sites along with differential privacy (DP) guarantees at each site. The main challenge comes from site heterogeneity -- different sites have different sample sizes and privacy budgets. We address this through a class of per-site estimation algorithms that reports the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate. Our experiments on real and synthetic data show that our method reliably aggregates private statistics across sites and provides better privacy-utility tradeoff under site heterogeneity than baselines.", "url": "https://arxiv.org/abs/2310.06237"}, {"metadata": {"arXiv": "2310.06243", "Date": "Tue, 10 Oct 2023 01:39:04 ", "Title": "Sample-Efficient Multi-Agent RL: An Optimization Perspective", "Authors": ["Nuoya Xiong", "Zhihan Liu", "Zhaoran Wang", "Zhuoran Yang"], "Categories": "cs.LG cs.GT"}, "abstract": "We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under the general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.", "url": "https://arxiv.org/abs/2310.06243"}, {"metadata": {"arXiv": "2310.06253", "Date": "Tue, 10 Oct 2023 01:58:38 ", "Title": "A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning", "Authors": ["Ran Wei", "Nathan Lambert", "Anthony McDonald", "Alfredo Garcia", "Roberto Calandra"], "Categories": "cs.LG"}, "abstract": "Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the \\emph{objective mismatch} between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.", "url": "https://arxiv.org/abs/2310.06253"}, {"metadata": {"arXiv": "2310.06268", "Date": "Tue, 10 Oct 2023 02:45:50 ", "Title": "Bi-Level Offline Policy Optimization with Limited Exploration", "Authors": ["Wenzhuo Zhou"], "Categories": "cs.LG math.ST stat.TH"}, "abstract": "We study offline reinforcement learning (RL) which seeks to learn a good policy based on a fixed, pre-collected dataset. A fundamental challenge behind this task is the distributional shift due to the dataset lacking sufficient exploration, especially under function approximation. To tackle this issue, we propose a bi-level structured policy optimization algorithm that models a hierarchical interaction between the policy (upper-level) and the value function (lower-level). The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch. Subsequently, at the upper level, the policy aims to maximize a conservative value estimate from the confidence set formed at the lower level. This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation. In practice, it can be solved through a computationally efficient, penalized adversarial estimation procedure. Our theoretical regret guarantees do not rely on any data-coverage and completeness-type assumptions, only requiring realizability. These guarantees also demonstrate that the learned policy represents the \"best effort\" among all policies, as no other policies can outperform it. We evaluate our model using a blend of synthetic, benchmark, and real-world datasets for offline RL, showing that it performs competitively with state-of-the-art methods.", "url": "https://arxiv.org/abs/2310.06268"}, {"metadata": {"arXiv": "2310.06282", "Date": "Tue, 10 Oct 2023 03:32:33 ", "Title": "MuseChat: A Conversational Music Recommendation System for Videos", "Authors": ["Zhikang Dong", "Bin Chen", "Xiulong Liu", "Pawel Polak", "Peng Zhang"], "Categories": "cs.LG cs.CV cs.IR"}, "abstract": "We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recommendation with reasoning. Second, we introduce a multi-modal recommendation engine that matches music either by aligning it with visual cues from the video or by harmonizing visual information, feedback from previously recommended music, and the user's textual input. Third, we bridge music representations and textual data with a Large Language Model(Vicuna-7B). This alignment equips MuseChat to deliver music recommendations and their underlying reasoning in a manner resembling human communication. Our evaluations show that MuseChat surpasses existing state-of-the-art models in music retrieval tasks and pioneers the integration of the recommendation process within a natural language framework.", "url": "https://arxiv.org/abs/2310.06282"}, {"metadata": {"arXiv": "2310.06306", "Date": "Tue, 10 Oct 2023 04:44:35 ", "Title": "Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing", "Authors": ["Yingyan Zeng", "Xiaoyu Chen", "Ran Jin"], "Categories": "cs.LG stat.ML"}, "abstract": "Online sensing and computational resources in Industrial Cyber-physical Systems (ICPS) facilitate AI-driven decision-making. Yet, issues with data quality, such as imbalanced classes, hinder AI models trained offline. To address this, AI models are updated online with streaming data for continuous improvement. Supervised learning models, however, face challenges in selecting quality streaming samples for updates due to annotation constraints. Active learning methods in literature offer solutions by focusing on under-represented or well-represented regions. Balancing these strategies in changing manufacturing contexts is challenging. Some acquisition criteria learned by AI dynamically adapt but may not consistently handle frequent changes. We introduce an ensemble active learning method, CBEAL, employing active learning agents specifically for exploration or exploitation. Weights of agents are adjusted based on agent decision effectiveness. CBEAL optimally guides data acquisition, minimizing human annotation. Our theoretical analysis and empirical studies validate CBEAL's efficiency in ICPS manufacturing process modeling.", "url": "https://arxiv.org/abs/2310.06306"}, {"metadata": {"arXiv": "2310.06312", "Date": "Tue, 10 Oct 2023 05:13:10 ", "Title": "Discovering Mixtures of Structural Causal Models from Time Series Data", "Authors": ["Sumanth Varambally", "Yi-An Ma", "Rose Yu"], "Categories": "cs.LG stat.ML"}, "abstract": "In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal graphs. Theoretically, we prove the identifiability of such a model under some mild assumptions.", "url": "https://arxiv.org/abs/2310.06312"}, {"metadata": {"arXiv": "2310.06319", "Date": "Tue, 10 Oct 2023 05:29:33 ", "Title": "Transfer learning-based physics-informed convolutional neural network for simulating flow in porous media with time-varying controls", "Authors": ["Jungang Chen", "Eduardo Gildin", "John E. Killough"], "Categories": "cs.LG cs.CE"}, "abstract": "A physics-informed convolutional neural network is proposed to simulate two phase flow in porous media with time-varying well controls. While most of PICNNs in existing literatures worked on parameter-to-state mapping, our proposed network parameterizes the solution with time-varying controls to establish a control-to-state regression. Firstly, finite volume scheme is adopted to discretize flow equations and formulate loss function that respects mass conservation laws. Neumann boundary conditions are seamlessly incorporated into the semi-discretized equations so no additional loss term is needed. The network architecture comprises two parallel U-Net structures, with network inputs being well controls and outputs being the system states. To capture the time-dependent relationship between inputs and outputs, the network is well designed to mimic discretized state space equations. We train the network progressively for every timestep, enabling it to simultaneously predict oil pressure and water saturation at each timestep. After training the network for one timestep, we leverage transfer learning techniques to expedite the training process for subsequent timestep. The proposed model is used to simulate oil-water porous flow scenarios with varying reservoir gridblocks and aspects including computation efficiency and accuracy are compared against corresponding numerical approaches. The results underscore the potential of PICNN in effectively simulating systems with numerous grid blocks, as computation time does not scale with model dimensionality. We assess the temporal error using 10 different testing controls with variation in magnitude and another 10 with higher alternation frequency with proposed control-to-state architecture. Our observations suggest the need for a more robust and reliable model when dealing with controls that exhibit significant variations in magnitude or frequency.", "url": "https://arxiv.org/abs/2310.06319"}, {"metadata": {"arXiv": "2310.06328", "Date": "Tue, 10 Oct 2023 05:54:00 ", "Title": "Exploit the antenna response consistency to define the alignment criteria for CSI data", "Authors": ["Ke Xu", "Jiangtao Wang", "Hongyuan Zhu", "Dingchang Zheng"], "Categories": "cs.LG eess.SP"}, "abstract": "Self-supervised learning (SSL) for WiFi-based human activity recognition (HAR) holds great promise due to its ability to address the challenge of insufficient labeled data. However, directly transplanting SSL algorithms, especially contrastive learning, originally designed for other domains to CSI data, often fails to achieve the expected performance. We attribute this issue to the inappropriate alignment criteria, which disrupt the semantic distance consistency between the feature space and the input space. To address this challenge, we introduce \\textbf{A}netenna \\textbf{R}esponse \\textbf{C}onsistency (ARC) as a solution to define proper alignment criteria. ARC is designed to retain semantic information from the input space while introducing robustness to real-world noise. We analyze ARC from the perspective of CSI data structure, demonstrating that its optimal solution leads to a direct mapping from input CSI data to action vectors in the feature map. Furthermore, we provide extensive experimental evidence to validate the effectiveness of ARC in improving the performance of self-supervised learning for WiFi-based HAR.", "url": "https://arxiv.org/abs/2310.06328"}, {"metadata": {"arXiv": "2310.06333", "Date": "Tue, 10 Oct 2023 06:03:51 ", "Title": "Learning bounded-degree polytrees with known skeleton", "Authors": ["Davin Choo", "Joy Qiping Yang", "Arnab Bhattacharyya", "Cl\\'ement L. Canonne"], "Categories": "cs.LG cs.DS math.PR math.ST stat.ML stat.TH"}, "abstract": "We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.", "url": "https://arxiv.org/abs/2310.06333"}, {"metadata": {"arXiv": "2310.06341", "Date": "Tue, 10 Oct 2023 06:22:06 ", "Title": "Federated Learning with Reduced Information Leakage and Computation", "Authors": ["Tongxin Yin", "Xueru Zhang", "Mohammad Mahdi Khalili", "Mingyan Liu"], "Categories": "cs.LG"}, "abstract": "Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized clients to collaboratively learn a common model without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as clients' sensitive information can be inferred from intermediate computations. Moreover, such information leakage accumulates substantially over time as the same data is repeatedly used during the iterative learning process. As a result, it can be particularly difficult to balance the privacy-accuracy trade-off when designing privacy-preserving FL algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning framework with first-order approximation applied at every even iteration. Under this framework, half of the FL updates incur no information leakage and require much less computation. We first conduct the theoretical analysis on the convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to preserve privacy. Experiments on real-world data show that Upcycled-FL consistently outperforms existing methods over heterogeneous data, and significantly improves privacy-accuracy trade-off while reducing 48% of the training time on average.", "url": "https://arxiv.org/abs/2310.06341"}, {"metadata": {"arXiv": "2310.06343", "Date": "Tue, 10 Oct 2023 06:26:05 ", "Title": "Boosting Continuous Control with Consistency Policy", "Authors": ["Yuhui Chen", "Haoran Li", "Dongbin Zhao"], "Categories": "cs.LG", "Comments": ["18 pages", "9 pages"]}, "abstract": "Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement learning, and can be seamlessly extended for online RL tasks. Experimental results indicate that CPQL achieves new state-of-the-art performance on 11 offline and 21 online tasks, significantly improving inference speed by nearly 45 times compared to Diffusion-QL. We will release our code later.", "url": "https://arxiv.org/abs/2310.06343"}, {"metadata": {"arXiv": "2310.06367", "Date": "Tue, 10 Oct 2023 07:08:35 ", "Title": "DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening", "Authors": ["Bowen Gao", "Bo Qiang", "Haichuan Tan", "Minsi Ren", "Yinjun Jia", "Minsi Lu", "Jingjing Liu", "Weiying Ma", "Yanyan Lan"], "Categories": "cs.LG"}, "abstract": "Virtual screening, which identifies potential drugs from vast compound databases to bind with a particular protein pocket, is a critical step in AI-assisted drug discovery. Traditional docking methods are highly time-consuming, and can only work with a restricted search library in real-life applications. Recent supervised learning approaches using scoring functions for binding-affinity prediction, although promising, have not yet surpassed docking methods due to their strong dependency on limited data with reliable binding-affinity labels. In this paper, we propose a novel contrastive learning framework, DrugCLIP, by reformulating virtual screening as a dense retrieval task and employing contrastive learning to align representations of binding protein pockets and molecules from a large quantity of pairwise data without explicit binding-affinity scores. We also introduce a biological-knowledge inspired data augmentation strategy to learn better protein-molecule representations. Extensive experiments show that DrugCLIP significantly outperforms traditional docking and supervised learning methods on diverse virtual screening benchmarks with highly reduced computation time, especially in zero-shot setting.", "url": "https://arxiv.org/abs/2310.06367"}, {"metadata": {"arXiv": "2310.06379", "Date": "Tue, 10 Oct 2023 07:43:41 ", "Title": "Initialization Bias of Fourier Neural Operator: Revisiting the Edge of Chaos", "Authors": ["Takeshi Koshizuka", "Masahiro Fujisawa", "Yusuke Tanaka", "and Issei Sato"], "Categories": "cs.LG"}, "abstract": "This paper investigates the initialization bias of the Fourier neural operator (FNO). A mean-field theory for FNO is established, analyzing the behavior of the random FNO from an ``edge of chaos'' perspective. We uncover that the forward and backward propagation behaviors exhibit characteristics unique to FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Building upon this observation, we also propose a FNO version of the He initialization scheme to mitigate the negative initialization bias leading to training instability. Experimental results demonstrate the effectiveness of our initialization scheme, enabling stable training of a 32-layer FNO without the need for additional techniques or significant performance degradation.", "url": "https://arxiv.org/abs/2310.06379"}, {"metadata": {"arXiv": "2310.06380", "Date": "Tue, 10 Oct 2023 07:46:54 ", "Title": "CAST: Cluster-Aware Self-Training for Tabular Data", "Authors": ["Minwook Kim", "Juseong Kim", "Kibeom Kim", "Donggil Kang", "Giltae Song"], "Categories": "cs.LG", "Comments": ["17 pages with appendix"]}, "abstract": "Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density regions to have lower confidence by leveraging prior knowledge for each class within the training data. Extensive empirical evaluations on up to 20 real-world datasets confirm not only the superior performance of CAST but also its robustness in various setups in self-training contexts.", "url": "https://arxiv.org/abs/2310.06380"}, {"metadata": {"arXiv": "2310.06393", "Date": "Tue, 10 Oct 2023 07:57:00 ", "Title": "Harnessing Administrative Data Inventories to Create a Reliable Transnational Reference Database for Crop Type Monitoring", "Authors": ["Maja Schneider and Marco K\\\"orner"], "Categories": "cs.LG cs.IR", "Journal-ref": "IEEE International Geoscience and Remote Sensing Symposium (IGARSS) 2022", "DOI": "10.1109/IGARSS46834.2022.9883089"}, "abstract": "With leaps in machine learning techniques and their applicationon Earth observation challenges has unlocked unprecedented performance across the domain. While the further development of these methods was previously limited by the availability and volume of sensor data and computing resources, the lack of adequate reference data is now constituting new bottlenecks. Since creating such ground-truth information is an expensive and error-prone task, new ways must be devised to source reliable, high-quality reference data on large scales. As an example, we showcase E URO C ROPS, a reference dataset for crop type classification that aggregates and harmonizes administrative data surveyed in different countries with the goal of transnational interoperability.", "url": "https://arxiv.org/abs/2310.06393"}, {"metadata": {"arXiv": "2310.06396", "Date": "Tue, 10 Oct 2023 07:59:23 ", "Title": "Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach", "Authors": ["Kai Zhao", "Qiyu Kang", "Yang Song", "Rui She", "Sijie Wang", "Wee Peng Tay"], "Categories": "cs.LG", "Comments": ["Accepted by Advances in Neural Information Processing Systems (NeurIPS)", "New Orleans", "USA", "Dec. 2023", "spotlight"]}, "abstract": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experiments is available at https://github.com/zknus/NeurIPS-2023-HANG-Robustness.", "url": "https://arxiv.org/abs/2310.06396"}, {"metadata": {"arXiv": "2310.06415", "Date": "Tue, 10 Oct 2023 08:36:21 ", "Title": "Deep reinforcement learning uncovers processes for separating azeotropic mixtures without prior knowledge", "Authors": ["Quirin G\\\"ottl", "Jonathan Pirnay", "Jakob Burger", "Dominik G. Grimm"], "Categories": "cs.LG", "Comments": ["36 pages", "7 figures", "4 tables. G\\\"ottl and Pirnay contributed equally as joint first authors. Burger and Grimm contributed equally as joint last authors"]}, "abstract": "Process synthesis in chemical engineering is a complex planning problem due to vast search spaces, continuous parameters and the need for generalization. Deep reinforcement learning agents, trained without prior knowledge, have shown to outperform humans in various complex planning problems in recent years. Existing work on reinforcement learning for flowsheet synthesis shows promising concepts, but focuses on narrow problems in a single chemical system, limiting its practicality. We present a general deep reinforcement learning approach for flowsheet synthesis. We demonstrate the adaptability of a single agent to the general task of separating binary azeotropic mixtures. Without prior knowledge, it learns to craft near-optimal flowsheets for multiple chemical systems, considering different feed compositions and conceptual approaches. On average, the agent can separate more than 99% of the involved materials into pure components, while autonomously learning fundamental process engineering paradigms. This highlights the agent's planning flexibility, an encouraging step toward true generality.", "url": "https://arxiv.org/abs/2310.06415"}, {"metadata": {"arXiv": "2310.06430", "Date": "Tue, 10 Oct 2023 08:54:14 ", "Title": "Conformal Prediction for Deep Classifier via Label Ranking", "Authors": ["Jianguo Huang", "Huajun Xi", "Linjun Zhang", "Huaxiu Yao", "Yue Qiu", "Hongxin Wei"], "Categories": "cs.LG cs.CV math.ST stat.TH"}, "abstract": "Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. In this paper, we empirically and theoretically show that disregarding the probabilities' value will mitigate the undesirable effect of miscalibrated probability values. Then, we propose a novel algorithm named $\\textit{Sorted Adaptive prediction sets}$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce sets of small size and communicate instance-wise uncertainty. Theoretically, we provide a finite-sample coverage guarantee of SAPS and show that the expected value of set size from SAPS is always smaller than APS. Extensive experiments validate that SAPS not only lessens the prediction sets but also broadly enhances the conditional coverage rate and adaptation of prediction sets.", "url": "https://arxiv.org/abs/2310.06430"}, {"metadata": {"arXiv": "2310.06448", "Date": "Tue, 10 Oct 2023 09:17:17 ", "Title": "Asynchronous Federated Learning with Incentive Mechanism Based on Contract Theory", "Authors": ["Danni Yang", "Yun Ji", "Zhoubin Kou", "Xiaoxiong Zhong", "Sheng Zhang"], "Categories": "cs.LG cs.DC"}, "abstract": "To address the challenges posed by the heterogeneity inherent in federated learning (FL) and to attract high-quality clients, various incentive mechanisms have been employed. However, existing incentive mechanisms are typically utilized in conventional synchronous aggregation, resulting in significant straggler issues. In this study, we propose a novel asynchronous FL framework that integrates an incentive mechanism based on contract theory. Within the incentive mechanism, we strive to maximize the utility of the task publisher by adaptively adjusting clients' local model training epochs, taking into account factors such as time delay and test accuracy. In the asynchronous scheme, considering client quality, we devise aggregation weights and an access control algorithm to facilitate asynchronous aggregation. Through experiments conducted on the MNIST dataset, the simulation results demonstrate that the test accuracy achieved by our framework is 3.12% and 5.84% higher than that achieved by FedAvg and FedProx without any attacks, respectively. The framework exhibits a 1.35% accuracy improvement over the ideal Local SGD under attacks. Furthermore, aiming for the same target accuracy, our framework demands notably less computation time than both FedAvg and FedProx.", "url": "https://arxiv.org/abs/2310.06448"}, {"metadata": {"arXiv": "2310.06483", "Date": "Tue, 10 Oct 2023 09:50:54 ", "Title": "Variance Reduced Online Gradient Descent for Kernelized Pairwise Learning with Limited Memory", "Authors": ["Hilal AlQuabeh", "Bhaskar Mukhoty", "Bin Gu"], "Categories": "cs.LG", "Comments": ["Accepted in ACML2023"]}, "abstract": "Pairwise learning is essential in machine learning, especially for problems involving loss functions defined on pairs of training examples. Online gradient descent (OGD) algorithms have been proposed to handle online pairwise learning, where data arrives sequentially. However, the pairwise nature of the problem makes scalability challenging, as the gradient computation for a new sample involves all past samples. Recent advancements in OGD algorithms have aimed to reduce the complexity of calculating online gradients, achieving complexities less than $O(T)$ and even as low as $O(1)$. However, these approaches are primarily limited to linear models and have induced variance. In this study, we propose a limited memory OGD algorithm that extends to kernel online pairwise learning while improving the sublinear regret. Specifically, we establish a clear connection between the variance of online gradients and the regret, and construct online gradients using the most recent stratified samples with a limited buffer of size of $s$ representing all past data, which have a complexity of $O(sT)$ and employs $O(\\sqrt{T}\\log{T})$ random Fourier features for kernel approximation. Importantly, our theoretical results demonstrate that the variance-reduced online gradients lead to an improved sublinear regret bound. The experiments on real-world datasets demonstrate the superiority of our algorithm over both kernelized and linear online pairwise learning algorithms.", "url": "https://arxiv.org/abs/2310.06483"}, {"metadata": {"arXiv": "2310.06506", "Date": "Tue, 10 Oct 2023 10:26:30 ", "Title": "Runway Sign Classifier: A DAL C Certifiable Machine Learning System", "Authors": ["Konstantin Dmitriev", "Johann Schumann", "Islam Bostanov", "Mostafa Abdelhamid and Florian Holzapfel"], "Categories": "cs.LG"}, "abstract": "In recent years, the remarkable progress of Machine Learning (ML) technologies within the domain of Artificial Intelligence (AI) systems has presented unprecedented opportunities for the aviation industry, paving the way for further advancements in automation, including the potential for single pilot or fully autonomous operation of large commercial airplanes. However, ML technology faces major incompatibilities with existing airborne certification standards, such as ML model traceability and explainability issues or the inadequacy of traditional coverage metrics. Certification of ML-based airborne systems using current standards is problematic due to these challenges. This paper presents a case study of an airborne system utilizing a Deep Neural Network (DNN) for airport sign detection and classification. Building upon our previous work, which demonstrates compliance with Design Assurance Level (DAL) D, we upgrade the system to meet the more stringent requirements of Design Assurance Level C. To achieve DAL C, we employ an established architectural mitigation technique involving two redundant and dissimilar Deep Neural Networks. The application of novel ML-specific data management techniques further enhances this approach. This work is intended to illustrate how the certification challenges of ML-based systems can be addressed for medium criticality airborne applications.", "url": "https://arxiv.org/abs/2310.06506"}, {"metadata": {"arXiv": "2310.06511", "Date": "Tue, 10 Oct 2023 10:48:52 ", "Title": "Self-Supervised Set Representation Learning for Unsupervised Meta-Learning", "Authors": ["Dong Bok Lee", "Seanie Lee", "Joonho Ko", "Kenji Kawaguchi", "Juho Lee", "Sung Ju Hwang"], "Categories": "cs.LG"}, "abstract": "Dataset distillation methods have achieved remarkable success in distilling a large dataset into a small set of representative samples. However, they are not designed to produce a distilled dataset that can be effectively used for facilitating self-supervised pre-training. To this end, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning (SSL). We first prove that a gradient of synthetic samples with respect to a SSL objective in naive bilevel optimization is \\textit{biased} due to the randomness originating from data augmentations or masking. To address this issue, we propose to minimize the mean squared error (MSE) between a model's representations of the synthetic examples and their corresponding learnable target feature representations for the inner objective, which does not introduce any randomness. Our primary motivation is that the model obtained by the proposed inner optimization can mimic the \\textit{self-supervised target model}. To achieve this, we also introduce the MSE between representations of the inner model and the self-supervised target model on the original full dataset for outer optimization. Lastly, assuming that a feature extractor is fixed, we only optimize a linear head on top of the feature extractor, which allows us to reduce the computational cost and obtain a closed-form solution of the head with kernel ridge regression. We empirically validate the effectiveness of our method on various applications involving transfer learning.", "url": "https://arxiv.org/abs/2310.06511"}, {"metadata": {"arXiv": "2310.06514", "Date": "Tue, 10 Oct 2023 10:55:49 ", "Title": "AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments", "Authors": ["Yang Zhang", "Yawei Li", "Hannah Brown", "Mina Rezaei", "Bernd Bischl", "Philip Torr", "Ashkan Khakzar", "Kenji Kawaguchi"], "Categories": "cs.LG", "Comments": ["32 pages including Appendix"]}, "abstract": "Feature attribution explains neural network outputs by identifying relevant input features. How do we know if the identified features are indeed relevant to the network? This notion is referred to as faithfulness, an essential property that reflects the alignment between the identified (attributed) features and the features used by the model. One recent trend to test faithfulness is to design the data such that we know which input features are relevant to the label and then train a model on the designed data. Subsequently, the identified features are evaluated by comparing them with these designed ground truth features. However, this idea has the underlying assumption that the neural network learns to use all and only these designed features, while there is no guarantee that the learning process trains the network in this way. In this paper, we solve this missing link by explicitly designing the neural network by manually setting its weights, along with designing data, so we know precisely which input features in the dataset are relevant to the designed network. Thus, we can test faithfulness in AttributionLab, our designed synthetic environment, which serves as a sanity check and is effective in filtering out attribution methods. If an attribution method is not faithful in a simple controlled environment, it can be unreliable in more complex scenarios. Furthermore, the AttributionLab environment serves as a laboratory for controlled experiments through which we can study feature attribution methods, identify issues, and suggest potential improvements.", "url": "https://arxiv.org/abs/2310.06514"}, {"metadata": {"arXiv": "2310.06522", "Date": "Tue, 10 Oct 2023 11:08:31 ", "Title": "Watt For What: Rethinking Deep Learning's Energy-Performance Relationship", "Authors": ["Shreyank N Gowda", "Xinyue Hao", "Gen Li", "Laura Sevilla-Lara", "Shashank Narayana Gowda"], "Categories": "cs.LG cs.CV"}, "abstract": "Deep learning models have revolutionized various fields, from image recognition to natural language processing, by achieving unprecedented levels of accuracy. However, their increasing energy consumption has raised concerns about their environmental impact, disadvantaging smaller entities in research and exacerbating global energy consumption. In this paper, we explore the trade-off between model accuracy and electricity consumption, proposing a metric that penalizes large consumption of electricity. We conduct a comprehensive study on the electricity consumption of various deep learning models across different GPUs, presenting a detailed analysis of their accuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity consumed, we demonstrate how smaller, more energy-efficient models can significantly expedite research while mitigating environmental concerns. Our results highlight the potential for a more sustainable approach to deep learning, emphasizing the importance of optimizing models for efficiency. This research also contributes to a more equitable research landscape, where smaller entities can compete effectively with larger counterparts. This advocates for the adoption of efficient deep learning practices to reduce electricity consumption, safeguarding the environment for future generations whilst also helping ensure a fairer competitive landscape.", "url": "https://arxiv.org/abs/2310.06522"}, {"metadata": {"arXiv": "2310.06543", "Date": "Tue, 10 Oct 2023 11:42:49 ", "Title": "An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for Travelling Salesman Problems", "Authors": ["Shiqing Liu", "Xueming Yan", "Yaochu Jin"], "Categories": "cs.LG", "Comments": ["35 pages", "7 figures"]}, "abstract": "Recent years have witnessed a surge in research on machine learning for combinatorial optimization since learning-based approaches can outperform traditional heuristics and approximate exact solvers at a lower computation cost. However, most existing work on supervised neural combinatorial optimization focuses on TSP instances with a fixed number of cities and requires large amounts of training samples to achieve a good performance, making them less practical to be applied to realistic optimization scenarios. This work aims to develop a data-driven graph representation learning method for solving travelling salesman problems (TSPs) with various numbers of cities. To this end, we propose an edge-aware graph autoencoder (EdgeGAE) model that can learn to solve TSPs after being trained on solution data of various sizes with an imbalanced distribution. We formulate the TSP as a link prediction task on sparse connected graphs. A residual gated encoder is trained to learn latent edge embeddings, followed by an edge-centered decoder to output link predictions in an end-to-end manner. To improve the model's generalization capability of solving large-scale problems, we introduce an active sampling strategy into the training process. In addition, we generate a benchmark dataset containing 50,000 TSP instances with a size from 50 to 500 cities, following an extremely scale-imbalanced distribution, making it ideal for investigating the model's performance for practical applications. We conduct experiments using different amounts of training data with various scales, and the experimental results demonstrate that the proposed data-driven approach achieves a highly competitive performance among state-of-the-art learning-based methods for solving TSPs.", "url": "https://arxiv.org/abs/2310.06543"}, {"metadata": {"arXiv": "2310.06549", "Date": "Tue, 10 Oct 2023 11:51:12 ", "Title": "Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks", "Authors": ["Lukas Struppek", "Dominik Hintersdorf", "Kristian Kersting"], "Categories": "cs.LG cs.CR cs.CV", "Comments": ["23 pages", "8 tables", "8 figures"]}, "abstract": "Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhancing model resilience against MIAs.", "url": "https://arxiv.org/abs/2310.06549"}, {"metadata": {"arXiv": "2310.06574", "Date": "Tue, 10 Oct 2023 12:35:20 ", "Title": "XAI for Early Crop Classification", "Authors": ["Ayshah Chan", "Maja Schneider", "and Marco K\\\"orner"], "Categories": "cs.LG stat.AP stat.ML"}, "abstract": "We propose an approach for early crop classification through identifying important timesteps with eXplainable AI (XAI) methods. Our approach consists of training a baseline crop classification model to carry out layer-wise relevance propagation (LRP) so that the salient time step can be identified. We chose a selected number of such important time indices to create the bounding region of the shortest possible classification timeframe. We identified the period 21st April 2019 to 9th August 2019 as having the best trade-off in terms of accuracy and earliness. This timeframe only suffers a 0.75% loss in accuracy as compared to using the full timeseries. We observed that the LRP-derived important timesteps also highlight small details in input values that differentiates between different classes and", "url": "https://arxiv.org/abs/2310.06574"}, {"metadata": {"arXiv": "2310.06600", "Date": "Tue, 10 Oct 2023 13:08:50 ", "Title": "Pi-DUAL: Using Privileged Information to Distinguish Clean from Noisy Labels", "Authors": ["Ke Wang", "Guillermo Ortiz-Jimenez", "Rodolphe Jenatton", "Mark Collier", "Efi Kokiopoulou", "Pascal Frossard"], "Categories": "cs.LG cs.CV"}, "abstract": "Label noise is a pervasive problem in deep learning that often compromises the generalization performance of trained models. Recently, leveraging privileged information (PI) -- information available only during training but not at test time -- has emerged as an effective approach to mitigate this issue. Yet, existing PI-based methods have failed to consistently outperform their no-PI counterparts in terms of preventing overfitting to label noise. To address this deficiency, we introduce Pi-DUAL, an architecture designed to harness PI to distinguish clean from wrong labels. Pi-DUAL decomposes the output logits into a prediction term, based on conventional input features, and a noise-fitting term influenced solely by PI. A gating mechanism steered by PI adaptively shifts focus between these terms, allowing the model to implicitly separate the learning paths of clean and wrong labels. Empirically, Pi-DUAL achieves significant performance improvements on key PI benchmarks (e.g., +6.8% on ImageNet-PI), establishing a new state-of-the-art test set accuracy. Additionally, Pi-DUAL is a potent method for identifying noisy samples post-training, outperforming other strong methods at this task. Overall, Pi-DUAL is a simple, scalable and practical approach for mitigating the effects of label noise in a variety of real-world scenarios with PI.", "url": "https://arxiv.org/abs/2310.06600"}, {"metadata": {"arXiv": "2310.06609", "Date": "Tue, 10 Oct 2023 13:23:05 ", "Title": "Discovering Interpretable Physical Models Using Symbolic Regression and Discrete Exterior Calculus", "Authors": ["Simone Manti and Alessandro Lucantonio"], "Categories": "cs.LG cs.DM cs.NE"}, "abstract": "Computational modeling is a key resource to gather insight into physical systems in modern scientific research and engineering. While access to large amount of data has fueled the use of Machine Learning (ML) to recover physical models from experiments and increase the accuracy of physical simulations, purely data-driven models have limited generalization and interpretability. To overcome these limitations, we propose a framework that combines Symbolic Regression (SR) and Discrete Exterior Calculus (DEC) for the automated discovery of physical models starting from experimental data. Since these models consist of mathematical expressions, they are interpretable and amenable to analysis, and the use of a natural, general-purpose discrete mathematical language for physics favors generalization with limited input data. Importantly, DEC provides building blocks for the discrete analogue of field theories, which are beyond the state-of-the-art applications of SR to physical problems. Further, we show that DEC allows to implement a strongly-typed SR procedure that guarantees the mathematical consistency of the recovered models and reduces the search space of symbolic expressions. Finally, we prove the effectiveness of our methodology by re-discovering three models of Continuum Physics from synthetic experimental data: Poisson equation, the Euler's Elastica and the equations of Linear Elasticity. Thanks to their general-purpose nature, the methods developed in this paper may be applied to diverse contexts of physical modeling.", "url": "https://arxiv.org/abs/2310.06609"}, {"metadata": {"arXiv": "2310.06622", "Date": "Tue, 10 Oct 2023 13:39:18 ", "Title": "Robustness May be More Brittle than We Think under Different Degrees of Distribution Shifts", "Authors": ["Kaican Li", "Yifan Zhang", "Lanqing Hong", "Zhenguo Li", "Nevin L. Zhang"], "Categories": "cs.LG"}, "abstract": "Out-of-distribution (OOD) generalization is a complicated problem due to the idiosyncrasies of possible distribution shifts between training and test domains. Most benchmarks employ diverse datasets to address this issue; however, the degree of the distribution shift between the training domains and the test domains of each dataset remains largely fixed. This may lead to biased conclusions that either underestimate or overestimate the actual OOD performance of a model. Our study delves into a more nuanced evaluation setting that covers a broad range of shift degrees. We show that the robustness of models can be quite brittle and inconsistent under different degrees of distribution shifts, and therefore one should be more cautious when drawing conclusions from evaluations under a limited range of degrees. In addition, we observe that large-scale pre-trained models, such as CLIP, are sensitive to even minute distribution shifts of novel downstream tasks. This indicates that while pre-trained representations may help improve downstream in-distribution performance, they could have minimal or even adverse effects on generalization in certain OOD scenarios of the downstream task if not used properly. In light of these findings, we encourage future research to conduct evaluations across a broader range of shift degrees whenever possible.", "url": "https://arxiv.org/abs/2310.06622"}, {"metadata": {"arXiv": "2310.06625", "Date": "Tue, 10 Oct 2023 13:44:09 ", "Title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting", "Authors": ["Yong Liu", "Tengge Hu", "Haoran Zhang", "Haixu Wu", "Shiyu Wang", "Lintao Ma", "Mingsheng Long"], "Categories": "cs.LG"}, "abstract": "The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformer is challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the unified embedding for each temporal token fuses multiple variates with potentially unaligned timestamps and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any adaptation on the basic components. We propose iTransformer that simply inverts the duties of the attention mechanism and the feed-forward network. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves consistent state-of-the-art on several real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting.", "url": "https://arxiv.org/abs/2310.06625"}, {"metadata": {"arXiv": "2310.06639", "Date": "Tue, 10 Oct 2023 14:00:03 ", "Title": "The Lattice Overparametrization Paradigm for the Machine Learning of Lattice Operators", "Authors": ["Diego Marcondes and Junior Barrera"], "Categories": "cs.LG"}, "abstract": "The machine learning of lattice operators has three possible bottlenecks. From a statistical standpoint, it is necessary to design a constrained class of operators based on prior information with low bias, and low complexity relative to the sample size. From a computational perspective, there should be an efficient algorithm to minimize an empirical error over the class. From an understanding point of view, the properties of the learned operator need to be derived, so its behavior can be theoretically understood. The statistical bottleneck can be overcome due to the rich literature about the representation of lattice operators, but there is no general learning algorithm for them. In this paper, we discuss a learning paradigm in which, by overparametrizing a class via elements in a lattice, an algorithm for minimizing functions in a lattice is applied to learn. We present the stochastic lattice gradient descent algorithm as a general algorithm to learn on constrained classes of operators as long as a lattice overparametrization of it is fixed, and we discuss previous works which are proves of concept. Moreover, if there are algorithms to compute the basis of an operator from its overparametrization, then its properties can be deduced and the understanding bottleneck is also overcome. This learning paradigm has three properties that modern methods based on neural networks lack: control, transparency and interpretability. Nowadays, there is an increasing demand for methods with these characteristics, and we believe that mathematical morphology is in a unique position to supply them. The lattice overparametrization paradigm could be a missing piece for it to achieve its full potential within modern machine learning.", "url": "https://arxiv.org/abs/2310.06639"}, {"metadata": {"arXiv": "2310.06643", "Date": "Tue, 10 Oct 2023 14:06:56 ", "Title": "Implicit Variational Inference for High-Dimensional Posteriors", "Authors": ["Anshuk Uppal", "Kristoffer Stensbo-Smidt", "Wouter K. Boomsma", "and Jes Frellsen"], "Categories": "cs.LG stat.ML", "Comments": ["9 pages", "and supplementary"]}, "abstract": "In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notoriously challenging to achieve. To the best of our knowledge, no other method has been shown to accomplish this task for such large models. Through experiments in downstream tasks, we demonstrate that our expressive posteriors outperform state-of-the-art uncertainty quantification methods, validating the effectiveness of our training algorithm and the quality of the learned implicit approximation.", "url": "https://arxiv.org/abs/2310.06643"}, {"metadata": {"arXiv": "2310.06644", "Date": "Tue, 10 Oct 2023 14:07:37 ", "Title": "Zero-Level-Set Encoder for Neural Distance Fields", "Authors": ["Stefan Rhys Jeske and Jonathan Klein and Dominik L. Michels and Jan Bender"], "Categories": "cs.LG cs.GR"}, "abstract": "Neural shape representation generally refers to representing 3D geometry using neural networks, e.g., to compute a signed distance or occupancy value at a specific spatial position. Previous methods tend to rely on the auto-decoder paradigm, which often requires densely-sampled and accurate signed distances to be known during training and testing, as well as an additional optimization loop during inference. This introduces a lot of computational overhead, in addition to having to compute signed distances analytically, even during testing. In this paper, we present a novel encoder-decoder neural network for embedding 3D shapes in a single forward pass. Our architecture is based on a multi-scale hybrid system incorporating graph-based and voxel-based components, as well as a continuously differentiable decoder. Furthermore, the network is trained to solve the Eikonal equation and only requires knowledge of the zero-level set for training and inference. Additional volumetric samples can be generated on-the-fly, and incorporated in an unsupervised manner. This means that in contrast to most previous work, our network is able to output valid signed distance fields without explicit prior knowledge of non-zero distance values or shape occupancy. In other words, our network computes approximate solutions to the boundary-valued Eikonal equation. It also requires only a single forward pass during inference, instead of the common latent code optimization. We further propose a modification of the loss function in case that surface normals are not well defined, e.g., in the context of non-watertight surface-meshes and non-manifold geometry. We finally demonstrate the efficacy, generalizability and scalability of our method on datasets consisting of deforming 3D shapes, single class encoding and multiclass encoding, showcasing a wide range of possible applications.", "url": "https://arxiv.org/abs/2310.06644"}, {"metadata": {"arXiv": "2310.06645", "Date": "Tue, 10 Oct 2023 14:07:49 ", "Title": "Self-Supervised Representation Learning for Online Handwriting Text Classification", "Authors": ["Pouya Mehralian", "Bagher BabaAli", "Ashena Gorgan Mohammadi"], "Categories": "cs.LG cs.CL"}, "abstract": "Self-supervised learning offers an efficient way of extracting rich representations from various types of unlabeled data while avoiding the cost of annotating large-scale datasets. This is achievable by designing a pretext task to form pseudo labels with respect to the modality and domain of the data. Given the evolving applications of online handwritten texts, in this study, we propose the novel Part of Stroke Masking (POSM) as a pretext task for pretraining models to extract informative representations from the online handwriting of individuals in English and Chinese languages, along with two suggested pipelines for fine-tuning the pretrained models. To evaluate the quality of the extracted representations, we use both intrinsic and extrinsic evaluation methods. The pretrained models are fine-tuned to achieve state-of-the-art results in tasks such as writer identification, gender classification, and handedness classification, also highlighting the superiority of utilizing the pretrained models over the models trained from scratch.", "url": "https://arxiv.org/abs/2310.06645"}, {"metadata": {"arXiv": "2310.06661", "Date": "Tue, 10 Oct 2023 14:37:17 ", "Title": "Tertiary Lymphoid Structures Generation through Graph-based Diffusion", "Authors": ["Manuel Madeira", "Dorina Thanou", "Pascal Frossard"], "Categories": "cs.LG"}, "abstract": "Graph-based representation approaches have been proven to be successful in the analysis of biomedical data, due to their capability of capturing intricate dependencies between biological entities, such as the spatial organization of different cell types in a tumor tissue. However, to further enhance our understanding of the underlying governing biological mechanisms, it is important to accurately capture the actual distributions of such complex data. Graph-based deep generative models are specifically tailored to accomplish that. In this work, we leverage state-of-the-art graph-based diffusion models to generate biologically meaningful cell-graphs. In particular, we show that the adopted graph diffusion model is able to accurately learn the distribution of cells in terms of their tertiary lymphoid structures (TLS) content, a well-established biomarker for evaluating the cancer progression in oncology research. Additionally, we further illustrate the utility of the learned generative models for data augmentation in a TLS classification task. To the best of our knowledge, this is the first work that leverages the power of graph diffusion models in generating meaningful biological cell structures.", "url": "https://arxiv.org/abs/2310.06661"}, {"metadata": {"arXiv": "2310.06668", "Date": "Tue, 10 Oct 2023 14:42:34 ", "Title": "Latent Diffusion Counterfactual Explanations", "Authors": ["Karim Farid", "Simon Schrodi", "Max Argus", "Thomas Brox"], "Categories": "cs.LG cs.CV"}, "abstract": "Counterfactual explanations have emerged as a promising method for elucidating the behavior of opaque black-box models. Recently, several works leveraged pixel-space diffusion models for counterfactual generation. To handle noisy, adversarial gradients during counterfactual generation -- causing unrealistic artifacts or mere adversarial perturbations -- they required either auxiliary adversarially robust models or computationally intensive guidance schemes. However, such requirements limit their applicability, e.g., in scenarios with restricted access to the model's training data. To address these limitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE). LDCE harnesses the capabilities of recent class- or text-conditional foundation latent diffusion models to expedite counterfactual generation and focus on the important, semantic parts of the data. Furthermore, we propose a novel consensus guidance mechanism to filter out noisy, adversarial gradients that are misaligned with the diffusion model's implicit classifier. We demonstrate the versatility of LDCE across a wide spectrum of models trained on diverse datasets with different learning paradigms. Finally, we showcase how LDCE can provide insights into model errors, enhancing our understanding of black-box model behavior.", "url": "https://arxiv.org/abs/2310.06668"}, {"metadata": {"arXiv": "2310.06670", "Date": "Tue, 10 Oct 2023 14:46:22 ", "Title": "Domain Generalization by Rejecting Extreme Augmentations", "Authors": ["Masih Aminbeidokhti", "Fidel A. Guerrero Pe\\~na", "Heitor Rapela Medeiros", "Thomas Dubail", "Eric Granger", "Marco Pedersoli"], "Categories": "cs.LG cs.CV"}, "abstract": "Data augmentation is one of the most effective techniques for regularizing deep learning models and improving their recognition performance in a variety of tasks and domains. However, this holds for standard in-domain settings, in which the training and test data follow the same distribution. For the out-of-domain case, where the test data follow a different and unknown distribution, the best recipe for data augmentation is unclear. In this paper, we show that for out-of-domain and domain generalization settings, data augmentation can provide a conspicuous and robust improvement in performance. To do that, we propose a simple training procedure: (i) use uniform sampling on standard data augmentation transformations; (ii) increase the strength transformations to account for the higher data variance expected when working out-of-domain, and (iii) devise a new reward function to reject extreme transformations that can harm the training. With this procedure, our data augmentation scheme achieves a level of accuracy that is comparable to or better than state-of-the-art methods on benchmark domain generalization datasets. Code: \\url{https://github.com/Masseeh/DCAug}", "url": "https://arxiv.org/abs/2310.06670"}, {"metadata": {"arXiv": "2310.06682", "Date": "Tue, 10 Oct 2023 14:57:04 ", "Title": "On the importance of catalyst-adsorbate 3D interactions for relaxed energy predictions", "Authors": ["Alvaro Carbonero", "Alexandre Duval", "Victor Schmidt", "Santiago Miret", "Alex Hernandez-Garcia", "Yoshua Bengio", "David Rolnick"], "Categories": "cs.LG"}, "abstract": "The use of machine learning for material property prediction and discovery has traditionally centered on graph neural networks that incorporate the geometric configuration of all atoms. However, in practice not all this information may be readily available, e.g.~when evaluating the potentially unknown binding of adsorbates to catalyst. In this paper, we investigate whether it is possible to predict a system's relaxed energy in the OC20 dataset while ignoring the relative position of the adsorbate with respect to the electro-catalyst. We consider SchNet, DimeNet++ and FAENet as base architectures and measure the impact of four modifications on model performance: removing edges in the input graph, pooling independent representations, not sharing the backbone weights and using an attention mechanism to propagate non-geometric relative information. We find that while removing binding site information impairs accuracy as expected, modified models are able to predict relaxed energies with remarkably decent MAE. Our work suggests future research directions in accelerated materials discovery where information on reactant configurations can be reduced or altogether omitted.", "url": "https://arxiv.org/abs/2310.06682"}, {"metadata": {"arXiv": "2310.06710", "Date": "Tue, 10 Oct 2023 15:36:58 ", "Title": "Zero-Shot Transfer in Imitation Learning", "Authors": ["Alvaro Cauderan", "Gauthier Boeshertz", "Florian Schwarb", "Calvin Zhang"], "Categories": "cs.LG"}, "abstract": "We present an algorithm that learns to imitate expert behavior and can transfer to previously unseen domains without retraining. Such an algorithm is extremely relevant in real-world applications such as robotic learning because 1) reward functions are difficult to design, 2) learned policies from one domain are difficult to deploy in another domain and 3) learning directly in the real world is either expensive or unfeasible due to security concerns. To overcome these constraints, we combine recent advances in Deep RL by using an AnnealedVAE to learn a disentangled state representation and imitate an expert by learning a single Q-function which avoids adversarial training. We demonstrate the effectiveness of our method in 3 environments ranging in difficulty and the type of transfer knowledge required.", "url": "https://arxiv.org/abs/2310.06710"}, {"metadata": {"arXiv": "2310.06713", "Date": "Tue, 10 Oct 2023 15:38:30 ", "Title": "Interpretable Traffic Event Analysis with Bayesian Networks", "Authors": ["Tong Yuan", "Jian Yang", "Zeyi Wen"], "Categories": "cs.LG stat.AP", "Comments": ["11 pages", "7 figures"], "MSC-class": "62F15", "ACM-class": "G.3"}, "abstract": "Although existing machine learning-based methods for traffic accident analysis can provide good quality results to downstream tasks, they lack interpretability which is crucial for this critical problem. This paper proposes an interpretable framework based on Bayesian Networks for traffic accident prediction. To enable the ease of interpretability, we design a dataset construction pipeline to feed the traffic data into the framework while retaining the essential traffic data information. With a concrete case study, our framework can derive a Bayesian Network from a dataset based on the causal relationships between weather and traffic events across the United States. Consequently, our framework enables the prediction of traffic accidents with competitive accuracy while examining how the probability of these events changes under different conditions, thus illustrating transparent relationships between traffic and weather events. Additionally, the visualization of the network simplifies the analysis of relationships between different variables, revealing the primary causes of traffic accidents and ultimately providing a valuable reference for reducing traffic accidents.", "url": "https://arxiv.org/abs/2310.06713"}, {"metadata": {"arXiv": "2310.06715", "Date": "Tue, 10 Oct 2023 15:42:14 ", "Title": "S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models", "Authors": ["Tiezhi Wang and Nils Strodthoff"], "Categories": "cs.LG eess.SP stat.ML", "Comments": ["11 pages", "1 figure", "code available at https://github.com/AI4HealthUOL/s4sleep"]}, "abstract": "Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components, leading to statistically significant advancements in performance on the extensive SHHS dataset. These improvements are assessed through both statistical and systematic error estimations. We anticipate that the architectural insights gained from this study will not only prove valuable for future research in sleep staging but also hold relevance for other time series annotation tasks.", "url": "https://arxiv.org/abs/2310.06715"}, {"metadata": {"arXiv": "2310.06746", "Date": "Tue, 10 Oct 2023 16:19:20 ", "Title": "Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules", "Authors": ["Ying Wu and Hanzhong Liu and Kai Ren and Xiangyu Chang"], "Categories": "cs.LG stat.ME stat.ML"}, "abstract": "Interpretability is a key concern in estimating heterogeneous treatment effects using machine learning methods, especially for healthcare applications where high-stake decisions are often made. Inspired by the Predictive, Descriptive, Relevant framework of interpretability, we propose causal rule learning which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. Causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously belongs to multiple groups with different average treatment effects? The rule analysis phase outlines a detailed procedure to further analyze each rule in the subset from multiple perspectives, revealing the most promising rules for further validation. The rules themselves, their corresponding subgroup treatment effects, and their weights in the linear combination give us more insights into heterogeneous treatment effects. Simulation and real-world data analysis demonstrate the superior performance of causal rule learning on the interpretable estimation of heterogeneous treatment effect when the ground truth is complex and the sample size is sufficient.", "url": "https://arxiv.org/abs/2310.06746"}, {"metadata": {"arXiv": "2310.06777", "Date": "Tue, 10 Oct 2023 16:51:32 ", "Title": "Information Content Exploration", "Authors": ["Jacob Chmura", "Hasham Burhani", "Xiao Qi Shi"], "Categories": "cs.LG", "Comments": ["12 pages", "12 figures"]}, "abstract": "Sparse reward environments are known to be challenging for reinforcement learning agents. In such environments, efficient and scalable exploration is crucial. Exploration is a means by which an agent gains information about the environment. We expand on this topic and propose a new intrinsic reward that systemically quantifies exploratory behavior and promotes state coverage by maximizing the information content of a trajectory taken by an agent. We compare our method to alternative exploration based intrinsic reward techniques, namely Curiosity Driven Learning and Random Network Distillation. We show that our information theoretic reward induces efficient exploration and outperforms in various games, including Montezuma Revenge, a known difficult task for reinforcement learning. Finally, we propose an extension that maximizes information content in a discretely compressed latent space which boosts sample efficiency and generalizes to continuous state spaces.", "url": "https://arxiv.org/abs/2310.06777"}, {"metadata": {"arXiv": "2310.06790", "Date": "Tue, 10 Oct 2023 17:04:21 ", "Title": "Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches", "Authors": ["C. Ricardo Constante-Amores and Alec J. Linot and Michael D. Graham"], "Categories": "cs.LG"}, "abstract": "Data-driven approximations of the Koopman operator are promising for predicting the time evolution of systems characterized by complex dynamics. Among these methods, the approach known as extended dynamic mode decomposition with dictionary learning (EDMD-DL) has garnered significant attention. Here we present a modification of EDMD-DL that concurrently determines both the dictionary of observables and the corresponding approximation of the Koopman operator. This innovation leverages automatic differentiation to facilitate gradient descent computations through the pseudoinverse. We also address the performance of several alternative methodologies. We assess a 'pure' Koopman approach, which involves the direct time-integration of a linear, high-dimensional system governing the dynamics within the space of observables. Additionally, we explore a modified approach where the system alternates between spaces of states and observables at each time step -- this approach no longer satisfies the linearity of the true Koopman operator representation. For further comparisons, we also apply a state space approach (neural ODEs). We consider systems encompassing two and three-dimensional ordinary differential equation systems featuring steady, oscillatory, and chaotic attractors, as well as partial differential equations exhibiting increasingly complex and intricate behaviors. Our framework significantly outperforms EDMD-DL. Furthermore, the state space approach offers superior performance compared to the 'pure' Koopman approach where the entire time evolution occurs in the space of observables. When the temporal evolution of the Koopman approach alternates between states and observables at each time step, however, its predictions become comparable to those of the state space approach.", "url": "https://arxiv.org/abs/2310.06790"}, {"metadata": {"arXiv": "2310.06793", "Date": "Tue, 10 Oct 2023 17:06:41 ", "Title": "Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning", "Authors": ["Stefan Stojanovic", "Yassir Jedra", "Alexandre Proutiere"], "Categories": "cs.LG stat.ML", "Comments": ["To appear in NeurIPS 2023"]}, "abstract": "We study matrix estimation problems arising in reinforcement learning (RL) with low-rank structure. In low-rank bandits, the matrix to be recovered specifies the expected arm rewards, and for low-rank Markov Decision Processes (MDPs), it may for example characterize the transition kernel of the MDP. In both cases, each entry of the matrix carries important information, and we seek estimation methods with low entry-wise error. Importantly, these methods further need to accommodate for inherent correlations in the available data (e.g. for MDPs, the data consists of system trajectories). We investigate the performance of simple spectral-based matrix estimation approaches: we show that they efficiently recover the singular subspaces of the matrix and exhibit nearly-minimal entry-wise error. These new results on low-rank matrix estimation make it possible to devise reinforcement learning algorithms that fully exploit the underlying low-rank structure. We provide two examples of such algorithms: a regret minimization algorithm for low-rank bandit problems, and a best policy identification algorithm for reward-free RL in low-rank MDPs. Both algorithms yield state-of-the-art performance guarantees.", "url": "https://arxiv.org/abs/2310.06793"}, {"metadata": {"arXiv": "2310.06801", "Date": "Tue, 10 Oct 2023 17:11:20 ", "Title": "Inverse Factorized Q-Learning for Cooperative Multi-agent Imitation Learning", "Authors": ["The Viet Bui and Tien Mai and Thanh Hong Nguyen"], "Categories": "cs.LG cs.MA"}, "abstract": "This paper concerns imitation learning (IL) (i.e, the problem of learning to mimic expert behaviors from demonstrations) in cooperative multi-agent systems. The learning problem under consideration poses several challenges, characterized by high-dimensional state and action spaces and intricate inter-agent dependencies. In a single-agent setting, IL has proven to be done efficiently through an inverse soft-Q learning process given expert demonstrations. However, extending this framework to a multi-agent context introduces the need to simultaneously learn both local value functions to capture local observations and individual actions, and a joint value function for exploiting centralized learning. In this work, we introduce a novel multi-agent IL algorithm designed to address these challenges. Our approach enables the centralized learning by leveraging mixing networks to aggregate decentralized Q functions. A main advantage of this approach is that the weights of the mixing networks can be trained using information derived from global states. We further establish conditions for the mixing networks under which the multi-agent objective function exhibits convexity within the Q function space. We present extensive experiments conducted on some challenging competitive and cooperative multi-agent game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2), which demonstrates the effectiveness of our proposed algorithm compared to existing state-of-the-art multi-agent IL algorithms.", "url": "https://arxiv.org/abs/2310.06801"}, {"metadata": {"arXiv": "2310.05993", "Date": "Sun, 08 Oct 2023 20:18:50 ", "Title": "Measuring reasoning capabilities of ChatGPT", "Authors": ["Adrian Groza"], "Categories": "cs.AI"}, "abstract": "I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks. For experiments, I use the 144 puzzles from the library \\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\\cite{groza:fol}. The library contains puzzles of various types, including arithmetic puzzles, logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling puzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct solutions for these puzzles were checked using the theorem prover Prover9~\\cite{mccune2005release} and the finite models finder Mace4~\\cite{mccune2003mace4} based on human-modelling in Equational First Order Logic. A first output of this study is the benchmark of 100 logical puzzles. For this dataset ChatGPT provided both correct answer and justification for 7\\% only. %, while BARD for 5\\%. Since the dataset seems challenging, the researchers are invited to test the dataset on more advanced or tuned models than ChatGPT3.5 with more crafted prompts. A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models. I have identified 67 such logical faults, among which: inconsistencies, implication does not hold, unsupported claim, lack of commonsense, wrong justification. The 100 solutions generated by ChatGPT contain 698 logical faults. That is on average, 7 fallacies for each reasoning task. A third ouput is the annotated answers of the ChatGPT with the corresponding logical faults. Each wrong statement within the ChatGPT answer was manually annotated, aiming to quantify the amount of faulty text generated by the language model. On average, 26.03\\% from the generated text was a logical fault.", "url": "https://arxiv.org/abs/2310.05993"}, {"metadata": {"arXiv": "2310.06089", "Date": "Mon, 09 Oct 2023 19:06:25 ", "Title": "Predictive auxiliary objectives in deep RL mimic learning in the brain", "Authors": ["Ching Fang", "Kimberly L Stachenfeld"], "Categories": "cs.AI"}, "abstract": "The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the RL system and hippocampus, an area thought to learn a predictive model to support memory-guided behavior. We also connect the encoder network and the value learning network of the RL system to visual cortex and striatum in the brain, respectively. This work demonstrates how representation learning in deep RL systems can provide an interpretable framework for modeling multi-region interactions in the brain. The deep RL perspective taken here also suggests an additional role of the hippocampus in the brain -- that of an auxiliary learning system that benefits representation learning in other regions.", "url": "https://arxiv.org/abs/2310.06089"}, {"metadata": {"arXiv": "2310.06114", "Date": "Mon, 09 Oct 2023 19:42:22 ", "Title": "Learning Interactive Real-World Simulators", "Authors": ["Mengjiao Yang", "Yilun Du", "Kamyar Ghasemipour", "Jonathan Tompson", "Dale Schuurmans", "Pieter Abbeel"], "Categories": "cs.AI", "Comments": ["https://universal-simulator.github.io"]}, "abstract": "Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as \"open the drawer\" and low-level controls such as \"move by x, y\" from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications. Video demos can be found at https://universal-simulator.github.io.", "url": "https://arxiv.org/abs/2310.06114"}, {"metadata": {"arXiv": "2310.06116", "Date": "Mon, 09 Oct 2023 19:47:03 ", "Title": "OptiMUS: Optimization Modeling Using mip Solvers and large language models", "Authors": ["Ali AhmadiTeshnizi", "Wenzhi Gao", "Madeleine Udell"], "Categories": "cs.AI"}, "abstract": "Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS is able to solve 67\\% more problems compared to a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}", "url": "https://arxiv.org/abs/2310.06116"}, {"metadata": {"arXiv": "2310.06167", "Date": "Mon, 09 Oct 2023 21:36:21 ", "Title": "Predictable Artificial Intelligence", "Authors": ["Lexin Zhou", "Pablo A. Moreno-Casares", "Fernando Mart\\'inez-Plumed", "John Burden", "Ryan Burnell", "Lucy Cheke", "C\\`esar Ferri", "Alexandru Marcoci", "Behzad Mehrbakhsh", "Yael Moros-Daval", "Se\\'an \\'O h\\'Eigeartaigh", "Danaja Rutar", "Wout Schellaert", "Konstantinos Voudouris", "Jos\\'e Hern\\'andez-Orallo"], "Categories": "cs.AI", "Comments": ["11 pages excluding references", "4 figures", "and 2 tables. Paper Under Review"], "MSC-class": "ACM-class: I.2"}, "abstract": "We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.", "url": "https://arxiv.org/abs/2310.06167"}, {"metadata": {"arXiv": "2310.06174", "Date": "Mon, 09 Oct 2023 21:57:07 ", "Title": "How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?", "Authors": ["Khanin Sisaengsuwanchai", "Navapat Nananukul", "Mayank Kejriwal"], "Categories": "cs.AI cs.SE"}, "abstract": "Entity Resolution (ER) is the problem of semi-automatically determining when two entities refer to the same underlying entity, with applications ranging from healthcare to e-commerce. Traditional ER solutions required considerable manual expertise, including feature engineering, as well as identification and curation of training data. In many instances, such techniques are highly dependent on the domain. With recent advent in large language models (LLMs), there is an opportunity to make ER much more seamless and domain-independent. However, it is also well known that LLMs can pose risks, and that the quality of their outputs can depend on so-called prompt engineering. Unfortunately, a systematic experimental study on the effects of different prompting methods for addressing ER, using LLMs like ChatGPT, has been lacking thus far. This paper aims to address this gap by conducting such a study. Although preliminary in nature, our results show that prompting can significantly affect the quality of ER, although it affects some metrics more than others, and can also be dataset dependent.", "url": "https://arxiv.org/abs/2310.06174"}, {"metadata": {"arXiv": "2310.06176", "Date": "Mon, 09 Oct 2023 21:58:55 ", "Title": "Factual and Personalized Recommendations using Language Models and Reinforcement Learning", "Authors": ["Jihwan Jeong", "Yinlam Chow", "Guy Tennenholtz", "Chih-Wei Hsu", "Azamat Tulepbergenov", "Mohammad Ghavamzadeh", "Craig Boutilier"], "Categories": "cs.AI"}, "abstract": "Recommender systems (RSs) play a central role in connecting users to content, products, and services, matching candidate items to users based on their preferences. While traditional RSs rely on implicit user feedback signals, conversational RSs interact with users in natural language. In this work, we develop a comPelling, Precise, Personalized, Preference-relevant language model (P4LM) that recommends items to users while putting emphasis on explaining item characteristics and their relevance. P4LM uses the embedding space representation of a user's preferences to generate compelling responses that are factually-grounded and relevant w.r.t. the user's preferences. Moreover, we develop a joint reward function that measures precision, appeal, and personalization, which we use as AI-based feedback in a reinforcement learning-based language model framework. Using the MovieLens 25M dataset, we demonstrate that P4LM delivers compelling, personalized movie narratives to users.", "url": "https://arxiv.org/abs/2310.06176"}, {"metadata": {"arXiv": "2310.06245", "Date": "Tue, 10 Oct 2023 01:44:47 ", "Title": "We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses", "Authors": ["Benjamin Kane and Lenhart Schubert"], "Categories": "cs.AI cs.CL"}, "abstract": "Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be able to specify personas in an explicit form. In previous work, personas have typically been represented as sets of one-off pieces of self-knowledge that are retrieved by the dialogue system for use in generation. However, in realistic human conversations, personas are often revealed through story-like narratives that involve rich habitual knowledge -- knowledge about kinds of events that an agent often participates in (e.g., work activities, hobbies, sporting activities, favorite entertainments, etc.), including typical goals, sub-events, preconditions, and postconditions of those events. We capture such habitual knowledge using an explicit schema representation, and propose an approach to dialogue generation that retrieves relevant schemas to condition a large language model to generate persona-based responses. Furthermore, we demonstrate a method for bootstrapping the creation of such schemas by first generating generic passages from a set of simple facts, and then inducing schemas from the generated passages.", "url": "https://arxiv.org/abs/2310.06245"}, {"metadata": {"arXiv": "2310.06326", "Date": "Tue, 10 Oct 2023 05:50:25 ", "Title": "I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction", "Authors": ["Yusheng Huang", "Zhouhan Lin"], "Categories": "cs.AI"}, "abstract": "Multimodal information extraction is attracting research attention nowadays, which requires aggregating representations from different modalities. In this paper, we present the Intra- and Inter-Sample Relationship Modeling (I2SRM) method for this task, which contains two modules. Firstly, the intra-sample relationship modeling module operates on a single sample and aims to learn effective representations. Embeddings from textual and visual modalities are shifted to bridge the modality gap caused by distinct pre-trained language and image models. Secondly, the inter-sample relationship modeling module considers relationships among multiple samples and focuses on capturing the interactions. An AttnMixup strategy is proposed, which not only enables collaboration among samples but also augments data to improve generalization. We conduct extensive experiments on the multimodal named entity recognition datasets Twitter-2015 and Twitter-2017, and the multimodal relation extraction dataset MNRE. Our proposed method I2SRM achieves competitive results, 77.12% F1-score on Twitter-2015, 88.40% F1-score on Twitter-2017, and 84.12% F1-score on MNRE.", "url": "https://arxiv.org/abs/2310.06326"}, {"metadata": {"arXiv": "2310.06383", "Date": "Tue, 10 Oct 2023 07:47:57 ", "Title": "What Makes for Robust Multi-Modal Models in the Face of Missing Modalities?", "Authors": ["Siting Li", "Chenzhuang Du", "Yue Zhao", "Yu Huang", "Hang Zhao"], "Categories": "cs.AI"}, "abstract": "With the growing success of multi-modal learning, research on the robustness of multi-modal models, especially when facing situations with missing modalities, is receiving increased attention. Nevertheless, previous studies in this domain exhibit certain limitations, as they often lack theoretical insights or their methodologies are tied to specific network architectures or modalities. We model the scenarios of multi-modal models encountering missing modalities from an information-theoretic perspective and illustrate that the performance ceiling in such scenarios can be approached by efficiently utilizing the information inherent in non-missing modalities. In practice, there are two key aspects: (1) The encoder should be able to extract sufficiently good features from the non-missing modality; (2) The extracted features should be robust enough not to be influenced by noise during the fusion process across modalities. To this end, we introduce Uni-Modal Ensemble with Missing Modality Adaptation (UME-MMA). UME-MMA employs uni-modal pre-trained weights for the multi-modal model to enhance feature extraction and utilizes missing modality data augmentation techniques to better adapt to situations with missing modalities. Apart from that, UME-MMA, built on a late-fusion learning framework, allows for the plug-and-play use of various encoders, making it suitable for a wide range of modalities and enabling seamless integration of large-scale pre-trained encoders to further enhance performance. And we demonstrate UME-MMA's effectiveness in audio-visual datasets~(e.g., AV-MNIST, Kinetics-Sound, AVE) and vision-language datasets~(e.g., MM-IMDB, UPMC Food101).", "url": "https://arxiv.org/abs/2310.06383"}, {"metadata": {"arXiv": "2310.06428", "Date": "Tue, 10 Oct 2023 08:53:54 ", "Title": "Proceedings of The first international workshop on eXplainable AI for the Arts (XAIxArts)", "Authors": ["Nick Bryan-Kinns", "Corey Ford", "Alan Chamberlain", "Steven David Benford", "Helen Kennedy", "Zijin Li", "Wu Qiong", "Gus G. Xia", "and Jeba Rezwana"], "Categories": "cs.AI cs.HC cs.SD eess.AS"}, "abstract": "This first international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts. Workshop held at the 15th ACM Conference on Creativity and Cognition (C&C 2023).", "url": "https://arxiv.org/abs/2310.06428"}, {"metadata": {"arXiv": "2310.06441", "Date": "Tue, 10 Oct 2023 09:13:46 ", "Title": "Stepwise functional refoundation of relational concept analysis", "Authors": ["J\\'er\\^ome Euzenat (MOEX)"], "Categories": "cs.AI", "Comments": ["euzenat2023a"]}, "abstract": "Relational concept analysis (RCA) is an extension of formal concept analysis allowing to deal with several related contexts simultaneously. It has been designed for learning description logic theories from data and used within various applications. A puzzling observation about RCA is that it returns a single family of concept lattices although, when the data feature circular dependencies, other solutions may be considered acceptable. The semantics of RCA, provided in an operational way, does not shed light on this issue. In this report, we define these acceptable solutions as those families of concept lattices which belong to the space determined by the initial contexts (well-formed), cannot scale new attributes (saturated), and refer only to concepts of the family (self-supported). We adopt a functional view on the RCA process by defining the space of well-formed solutions and two functions on that space: one expansive and the other contractive. We show that the acceptable solutions are the common fixed points of both functions. This is achieved step-by-step by starting from a minimal version of RCA that considers only one single context defined on a space of contexts and a space of lattices. These spaces are then joined into a single space of context-lattice pairs, which is further extended to a space of indexed families of context-lattice pairs representing the objects manip", "url": "https://arxiv.org/abs/2310.06441"}, {"metadata": {"arXiv": "2310.06484", "Date": "Tue, 10 Oct 2023 09:53:07 ", "Title": "Memory efficient location recommendation through proximity-aware representation", "Authors": ["Xuan Luo", "Rui Lv", "Hui Zhao"], "Categories": "cs.AI"}, "abstract": "Sequential location recommendation plays a huge role in modern life, which can enhance user experience, bring more profit to businesses and assist in government administration. Although methods for location recommendation have evolved significantly thanks to the development of recommendation systems, there is still limited utilization of geographic information, along with the ongoing challenge of addressing data sparsity. In response, we introduce a Proximity-aware based region representation for Sequential Recommendation (PASR for short), built upon the Self-Attention Network architecture. We tackle the sparsity issue through a novel loss function employing importance sampling, which emphasizes informative negative samples during optimization. Moreover, PASR enhances the integration of geographic information by employing a self-attention-based geography encoder to the hierarchical grid and proximity grid at each GPS point. To further leverage geographic information, we utilize the proximity-aware negative samplers to enhance the quality of negative samples. We conducted evaluations using three real-world Location-Based Social Networking (LBSN) datasets, demonstrating that PASR surpasses state-of-the-art sequential location recommendation methods", "url": "https://arxiv.org/abs/2310.06484"}, {"metadata": {"arXiv": "2310.06486", "Date": "Tue, 10 Oct 2023 09:53:59 ", "Title": "Topological RANSAC for instance verification and retrieval without fine-tuning", "Authors": ["Guoyuan An", "Juhyung Seon", "Inkyu An", "Yuchi Huo", "Sung-Eui Yoon"], "Categories": "cs.AI cs.CV cs.IR"}, "abstract": "This paper presents an innovative approach to enhancing explainable image retrieval, particularly in situations where a fine-tuning set is unavailable. The widely-used SPatial verification (SP) method, despite its efficacy, relies on a spatial model and the hypothesis-testing strategy for instance recognition, leading to inherent limitations, including the assumption of planar structures and neglect of topological relations among features. To address these shortcomings, we introduce a pioneering technique that replaces the spatial model with a topological one within the RANSAC process. We propose bio-inspired saccade and fovea functions to verify the topological consistency among features, effectively circumventing the issues associated with SP's spatial model. Our experimental results demonstrate that our method significantly outperforms SP, achieving state-of-the-art performance in non-fine-tuning retrieval. Furthermore, our approach can enhance performance when used in conjunction with fine-tuned features. Importantly, our method retains high explainability and is lightweight, offering a practical and adaptable solution for a variety of real-world applications.", "url": "https://arxiv.org/abs/2310.06486"}, {"metadata": {"arXiv": "2310.06500", "Date": "Tue, 10 Oct 2023 10:17:58 ", "Title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents", "Authors": ["Yuan Li", "Yixuan Zhang", "and Lichao Sun"], "Categories": "cs.AI"}, "abstract": "Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.", "url": "https://arxiv.org/abs/2310.06500"}, {"metadata": {"arXiv": "2310.06513", "Date": "Tue, 10 Oct 2023 10:55:12 ", "Title": "Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction", "Authors": ["Yangqing Fu", "Ming Sun", "Buqing Nie", "Yue Gao"], "Categories": "cs.AI"}, "abstract": "Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.", "url": "https://arxiv.org/abs/2310.06513"}, {"metadata": {"arXiv": "2310.06541", "Date": "Tue, 10 Oct 2023 11:40:20 ", "Title": "Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A Quantum Reinforcement Learning Approach", "Authors": ["Gyu Seon Kim", "JaeHyun Chung", "and Soohyun Park"], "Categories": "cs.AI", "Comments": ["5 pages", "5 figures"]}, "abstract": "The advent of reusable rockets has heralded a new era in space exploration, reducing the costs of launching satellites by a significant factor. Traditional rockets were disposable, but the design of reusable rockets for repeated use has revolutionized the financial dynamics of space missions. The most critical phase of reusable rockets is the landing stage, which involves managing the tremendous speed and attitude for safe recovery. The complexity of this task presents new challenges for control systems, specifically in terms of precision and adaptability. Classical control systems like the proportional-integral-derivative (PID) controller lack the flexibility to adapt to dynamic system changes, making them costly and time-consuming to redesign of controller. This paper explores the integration of quantum reinforcement learning into the control systems of reusable rockets as a promising alternative. Unlike classical reinforcement learning, quantum reinforcement learning uses quantum bits that can exist in superposition, allowing for more efficient information encoding and reducing the number of parameters required. This leads to increased computational efficiency, reduced memory requirements, and more stable and predictable performance. Due to the nature of reusable rockets, which must be light, heavy computers cannot fit into them. In the reusable rocket scenario, quantum reinforcement learning, which has reduced memory requirements due to fewer parameters, is a good solution.", "url": "https://arxiv.org/abs/2310.06541"}, {"metadata": {"arXiv": "2310.06552", "Date": "Tue, 10 Oct 2023 11:56:48 ", "Title": "Automated clinical coding using off-the-shelf large language models", "Authors": ["Joseph S. Boyle", "Antanas Kascenas", "Pat Lok", "Maria Liakata", "Alison Q. O'Neil"], "Categories": "cs.AI cs.CL", "Comments": ["9 pages", "4 figures"], "ACM-class": "I.2.7; I.2.8"}, "abstract": "The task of assigning diagnostic ICD codes to patient hospital admissions is typically performed by expert human coders. Efforts towards automated ICD coding are dominated by supervised deep learning models. However, difficulties in learning to predict the large number of rare codes remain a barrier to adoption in clinical practice. In this work, we leverage off-the-shelf pre-trained generative large language models (LLMs) to develop a practical solution that is suitable for zero-shot and few-shot code assignment. Unsupervised pre-training alone does not guarantee precise knowledge of the ICD ontology and specialist clinical coding task, therefore we frame the task as information extraction, providing a description of each coded concept and asking the model to retrieve related mentions. For efficiency, rather than iterating over all codes, we leverage the hierarchical nature of the ICD ontology to sparsely search for relevant codes. Then, in a second stage, which we term 'meta-refinement', we utilise GPT-4 to select a subset of the relevant labels as predictions. We validate our method using Llama-2, GPT-3.5 and GPT-4 on the CodiEsp dataset of ICD-coded clinical case documents. Our tree-search method achieves state-of-the-art performance on rarer classes, achieving the best macro-F1 of 0.225, whilst achieving slightly lower micro-F1 of 0.157, compared to 0.216 and 0.219 respectively from PLM-ICD. To the best of our knowledge, this is the first method for automated ICD coding requiring no task-specific learning.", "url": "https://arxiv.org/abs/2310.06552"}, {"metadata": {"arXiv": "2310.06603", "Date": "Tue, 10 Oct 2023 13:12:03 ", "Title": "V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric Heterogenous Distillation Network", "Authors": ["Caizhen He", "Hai Wang", "and Long Chen", "Tong Luo", "and Yingfeng Cai"], "Categories": "cs.AI cs.CV cs.RO"}, "abstract": "Object detection is the central issue of intelligent traffic systems, and recent advancements in single-vehicle lidar-based 3D detection indicate that it can provide accurate position information for intelligent agents to make decisions and plan. Compared with single-vehicle perception, multi-view vehicle-road cooperation perception has fundamental advantages, such as the elimination of blind spots and a broader range of perception, and has become a research hotspot. However, the current perception of cooperation focuses on improving the complexity of fusion while ignoring the fundamental problems caused by the absence of single-view outlines. We propose a multi-view vehicle-road cooperation perception system, vehicle-to-everything cooperative perception (V2X-AHD), in order to enhance the identification capability, particularly for predicting the vehicle's shape. At first, we propose an asymmetric heterogeneous distillation network fed with different training data to improve the accuracy of contour recognition, with multi-view teacher features transferring to single-view student features. While the point cloud data are sparse, we propose Spara Pillar, a spare convolutional-based plug-in feature extraction backbone, to reduce the number of parameters and improve and enhance feature extraction capabilities. Moreover, we leverage the multi-head self-attention (MSA) to fuse the single-view feature, and the lightweight design makes the fusion feature a smooth expression. The results of applying our algorithm to the massive open dataset V2Xset demonstrate that our method achieves the state-of-the-art result. The V2X-AHD can effectively improve the accuracy of 3D object detection and reduce the number of network parameters, according to this study, which serves as a benchmark for cooperative perception. The code for this article is available at https://github.com/feeling0414-lab/V2X-AHD.", "url": "https://arxiv.org/abs/2310.06603"}, {"metadata": {"arXiv": "2310.06624", "Date": "Tue, 10 Oct 2023 13:41:41 ", "Title": "BridgeHand2Vec Bridge Hand Representation", "Authors": ["Anna Sztyber-Betley", "Filip Ko{\\l}odziej", "Jan Betley", "Piotr Duszak"], "Categories": "cs.AI", "Journal-ref": "Frontiers in Artificial Intelligence and Applications, Volume 372: ECAI 2023, Pages 2274 - 2281", "DOI": "10.3233/FAIA230526"}, "abstract": "Contract bridge is a game characterized by incomplete information, posing an exciting challenge for artificial intelligence methods. This paper proposes the BridgeHand2Vec approach, which leverages a neural network to embed a bridge player's hand (consisting of 13 cards) into a vector space. The resulting representation reflects the strength of the hand in the game and enables interpretable distances to be determined between different hands. This representation is derived by training a neural network to estimate the number of tricks that a pair of players can take. In the remainder of this paper, we analyze the properties of the resulting vector space and provide examples of its application in reinforcement learning, and opening bid classification. Although this was not our main goal, the neural network used for the vectorization achieves SOTA results on the DDBP2 problem (estimating the number of tricks for two given hands).", "url": "https://arxiv.org/abs/2310.06624"}, {"metadata": {"arXiv": "2310.06656", "Date": "Tue, 10 Oct 2023 14:30:04 ", "Title": "Assessing the Impact of a Supervised Classification Filter on Flow-based Hybrid Network Anomaly Detection", "Authors": ["Dominik Macko", "Patrik Goldschmidt", "Peter Pi\\v{s}tek", "Daniela Chud\\'a"], "Categories": "cs.AI cs.CR cs.NI"}, "abstract": "Constant evolution and the emergence of new cyberattacks require the development of advanced techniques for defense. This paper aims to measure the impact of a supervised filter (classifier) in network anomaly detection. We perform our experiments by employing a hybrid anomaly detection approach in network flow data. For this purpose, we extended a state-of-the-art autoencoder-based anomaly detection method by prepending a binary classifier acting as a prefilter for the anomaly detector. The method was evaluated on the publicly available real-world dataset UGR'16. Our empirical results indicate that the hybrid approach does offer a higher detection rate of known attacks than a standalone anomaly detector while still retaining the ability to detect zero-day attacks. Employing a supervised binary prefilter has increased the AUC metric by over 11%, detecting 30% more attacks while keeping the number of false positives approximately the same.", "url": "https://arxiv.org/abs/2310.06656"}, {"metadata": {"arXiv": "2310.06824", "Date": "Tue, 10 Oct 2023 17:54:39 ", "Title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets", "Authors": ["Samuel Marks and Max Tegmark"], "Categories": "cs.AI"}, "abstract": "Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we curate high-quality datasets of true/false statements and use them to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that language models linearly represent the truth or falsehood of factual statements. We also introduce a novel technique, mass-mean probing, which generalizes better and is more causally implicated in model outputs than other probing techniques.", "url": "https://arxiv.org/abs/2310.06824"}, {"metadata": {"arXiv": "2310.05929", "Date": "Wed, 16 Aug 2023 07:13:22 ", "Title": "Deep Learning based Tomato Disease Detection and Remedy Suggestions using Mobile Application", "Authors": ["Yagya Raj Pandeya", "Samin Karki", "Ishan Dangol", "Nitesh Rajbanshi"], "Categories": "cs.CV cs.AI"}, "abstract": "We have developed a comprehensive computer system to assist farmers who practice traditional farming methods and have limited access to agricultural experts for addressing crop diseases. Our system utilizes artificial intelligence (AI) to identify and provide remedies for vegetable diseases. To ensure ease of use, we have created a mobile application that offers a user-friendly interface, allowing farmers to inquire about vegetable diseases and receive suitable solutions in their local language. The developed system can be utilized by any farmer with a basic understanding of a smartphone. Specifically, we have designed an AI-enabled mobile application for identifying and suggesting remedies for vegetable diseases, focusing on tomato diseases to benefit the local farming community in Nepal. Our system employs state-of-the-art object detection methodology, namely You Only Look Once (YOLO), to detect tomato diseases. The detected information is then relayed to the mobile application, which provides remedy suggestions guided by domain experts. In order to train our system effectively, we curated a dataset consisting of ten classes of tomato diseases. We utilized various data augmentation methods to address overfitting and trained a YOLOv5 object detector. The proposed method achieved a mean average precision of 0.76 and offers an efficient mobile interface for interacting with the AI system. While our system is currently in the development phase, we are actively working towards enhancing its robustness and real-time usability by accumulating more training samples.", "url": "https://arxiv.org/abs/2310.05929"}, {"metadata": {"arXiv": "2310.05934", "Date": "Wed, 23 Aug 2023 04:14:55 ", "Title": "DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion", "Authors": ["Se Jin Park", "Joanna Hong", "Minsu Kim", "Yong Man Ro"], "Categories": "cs.CV cs.AI cs.MM eess.IV"}, "abstract": "Speech-driven 3D facial animation has gained significant attention for its ability to create realistic and expressive facial animations in 3D space based on speech. Learning-based methods have shown promising progress in achieving accurate facial motion synchronized with speech. However, one-to-many nature of speech-to-3D facial synthesis has not been fully explored: while the lip accurately synchronizes with the speech content, other facial attributes beyond speech-related motions are variable with respect to the speech. To account for the potential variance in the facial attributes within a single speech, we propose DF-3DFace, a diffusion-driven speech-to-3D face mesh synthesis. DF-3DFace captures the complex one-to-many relationships between speech and 3D face based on diffusion. It concurrently achieves aligned lip motion by exploiting audio-mesh synchronization and masked conditioning. Furthermore, the proposed method jointly models identity and pose in addition to facial motions so that it can generate 3D face animation without requiring a reference identity mesh and produce natural head poses. We contribute a new large-scale 3D facial mesh dataset, 3D-HDTF to enable the synthesis of variations in identities, poses, and facial motions of 3D face mesh. Extensive experiments demonstrate that our method successfully generates highly variable facial shapes and motions from speech and simultaneously achieves more realistic facial animation than the state-of-the-art methods.", "url": "https://arxiv.org/abs/2310.05934"}, {"metadata": {"arXiv": "2310.05938", "Date": "Thu, 24 Aug 2023 15:04:30 ", "Title": "Component attention network for multimodal dance improvisation recognition", "Authors": ["Jia Fu", "Jiarui Tan", "Wenjie Yin", "Sepideh Pashami", "M{\\aa}rten Bj\\\"orkman"], "Categories": "cs.CV cs.AI cs.SD eess.AS", "Comments": ["Accepted to 25th ACM International Conference on Multimodal Interaction (ICMI 2023)"], "ACM-class": "I.2; I.5.4", "DOI": "10.1145/3577190.3614114"}, "abstract": "Dance improvisation is an active research topic in the arts. Motion analysis of improvised dance can be challenging due to its unique dynamics. Data-driven dance motion analysis, including recognition and generation, is often limited to skeletal data. However, data of other modalities, such as audio, can be recorded and benefit downstream tasks. This paper explores the application and performance of multimodal fusion methods for human motion recognition in the context of dance improvisation. We propose an attention-based model, component attention network (CANet), for multimodal fusion on three levels: 1) feature fusion with CANet, 2) model fusion with CANet and graph convolutional network (GCN), and 3) late fusion with a voting strategy. We conduct thorough experiments to analyze the impact of each modality in different fusion methods and distinguish critical temporal or component features. We show that our proposed model outperforms the two baseline methods, demonstrating its potential for analyzing improvisation in dance.", "url": "https://arxiv.org/abs/2310.05938"}, {"metadata": {"arXiv": "2310.05951", "Date": "Sat, 09 Sep 2023 17:52:40 ", "Title": "Reducing the False Positive Rate Using Bayesian Inference in Autonomous Driving Perception", "Authors": ["Johann J. S. Bastos", "Bruno L. S. da Silva", "Tiago Zanotelli", "Cristiano Premebida", "Gledson Melotti"], "Categories": "cs.CV cs.AI"}, "abstract": "Object recognition is a crucial step in perception systems for autonomous and intelligent vehicles, as evidenced by the numerous research works in the topic. In this paper, object recognition is explored by using multisensory and multimodality approaches, with the intention of reducing the false positive rate (FPR). The reduction of the FPR becomes increasingly important in perception systems since the misclassification of an object can potentially cause accidents. In particular, this work presents a strategy through Bayesian inference to reduce the FPR considering the likelihood function as a cumulative distribution function from Gaussian kernel density estimations, and the prior probabilities as cumulative functions of normalized histograms. The validation of the proposed methodology is performed on the KITTI dataset using deep networks (DenseNet, NasNet, and EfficientNet), and recent 3D point cloud networks (PointNet, and PintNet++), by considering three object-categories (cars, cyclists, pedestrians) and the RGB and LiDAR sensor modalities.", "url": "https://arxiv.org/abs/2310.05951"}, {"metadata": {"arXiv": "2310.06068", "Date": "Mon, 09 Oct 2023 18:19:51 ", "Title": "Augmenting Vision-Based Human Pose Estimation with Rotation Matrix", "Authors": ["Milad Vazan", "Fatemeh Sadat Masoumi", "Ruizhi Ou", "Reza Rawassizadeh"], "Categories": "cs.CV cs.AI", "Comments": ["24 pages"]}, "abstract": "Fitness applications are commonly used to monitor activities within the gym, but they often fail to automatically track indoor activities inside the gym. This study proposes a model that utilizes pose estimation combined with a novel data augmentation method, i.e., rotation matrix. We aim to enhance the classification accuracy of activity recognition based on pose estimation data. Through our experiments, we experiment with different classification algorithms along with image augmentation approaches. Our findings demonstrate that the SVM with SGD optimization, using data augmentation with the Rotation Matrix, yields the most accurate results, achieving a 96% accuracy rate in classifying five physical activities. Conversely, without implementing the data augmentation techniques, the baseline accuracy remains at a modest 64%.", "url": "https://arxiv.org/abs/2310.06068"}, {"metadata": {"arXiv": "2310.06123", "Date": "Mon, 09 Oct 2023 19:57:24 ", "Title": "Text-driven Prompt Generation for Vision-Language Models in Federated Learning", "Authors": ["Chen Qiu", "Xingyu Li", "Chaithanya Kumar Mummadi", "Madan Ravi Ganesh", "Zhenzhen Li", "Lu Peng", "Wan-Yi Lin"], "Categories": "cs.CV cs.AI"}, "abstract": "Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes and is also generalizable to unseen datasets.", "url": "https://arxiv.org/abs/2310.06123"}, {"metadata": {"arXiv": "2310.06344", "Date": "Tue, 10 Oct 2023 06:27:30 ", "Title": "Filter Pruning For CNN With Enhanced Linear Representation Redundancy", "Authors": ["Bojue Wang", "Chunmei Ma", "Bin Liu", "Nianbo Liu", "Jinqi Zhu"], "Categories": "cs.CV cs.AI"}, "abstract": "Structured network pruning excels non-structured methods because they can take advantage of the thriving developed parallel computing techniques. In this paper, we propose a new structured pruning method. Firstly, to create more structured redundancy, we present a data-driven loss function term calculated from the correlation coefficient matrix of different feature maps in the same layer, named CCM-loss. This loss term can encourage the neural network to learn stronger linear representation relations between feature maps during the training from the scratch so that more homogenous parts can be removed later in pruning. CCM-loss provides us with another universal transcendental mathematical tool besides L*-norm regularization, which concentrates on generating zeros, to generate more redundancy but for the different genres. Furthermore, we design a matching channel selection strategy based on principal components analysis to exploit the maximum potential ability of CCM-loss. In our new strategy, we mainly focus on the consistency and integrality of the information flow in the network. Instead of empirically hard-code the retain ratio for each layer, our channel selection strategy can dynamically adjust each layer's retain ratio according to the specific circumstance of a per-trained model to push the prune ratio to the limit. Notably, on the Cifar-10 dataset, our method brings 93.64% accuracy for pruned VGG-16 with only 1.40M parameters and 49.60M FLOPs, the pruned ratios for parameters and FLOPs are 90.6% and 84.2%, respectively. For ResNet-50 trained on the ImageNet dataset, our approach achieves 42.8% and 47.3% storage and computation reductions, respectively, with an accuracy of 76.23%. Our code is available at https://github.com/Bojue-Wang/CCM-LRR.", "url": "https://arxiv.org/abs/2310.06344"}, {"metadata": {"arXiv": "2310.06351", "Date": "Tue, 10 Oct 2023 06:37:03 ", "Title": "Fire Detection From Image and Video Using YOLOv5", "Authors": ["Arafat Islam", "Md. Imtiaz Habib"], "Categories": "cs.CV cs.AI", "Comments": ["6 pages", "6 sections", "unpublished paper"]}, "abstract": "For the detection of fire-like targets in indoor, outdoor and forest fire images, as well as fire detection under different natural lights, an improved YOLOv5 fire detection deep learning algorithm is proposed. The YOLOv5 detection model expands the feature extraction network from three dimensions, which enhances feature propagation of fire small targets identification, improves network performance, and reduces model parameters. Furthermore, through the promotion of the feature pyramid, the top-performing prediction box is obtained. Fire-YOLOv5 attains excellent results compared to state-of-the-art object detection networks, notably in the detection of small targets of fire and smoke with mAP 90.5% and f1 score 88%. Overall, the Fire-YOLOv5 detection model can effectively deal with the inspection of small fire targets, as well as fire-like and smoke-like objects with F1 score 0.88. When the input image size is 416 x 416 resolution, the average detection time is 0.12 s per frame, which can provide real-time forest fire detection. Moreover, the algorithm proposed in this paper can also be applied to small target detection under other complicated situations. The proposed system shows an improved approach in all fire detection metrics such as precision, recall, and mean average precision.", "url": "https://arxiv.org/abs/2310.06351"}, {"metadata": {"arXiv": "2310.06370", "Date": "Tue, 10 Oct 2023 07:20:37 ", "Title": "Advanced Efficient Strategy for Detection of Dark Objects Based on Spiking Network with Multi-Box Detection", "Authors": ["Munawar Ali", "Baoqun Yin", "Hazrat Bilal", "Aakash Kumar", "Ali Muhammad", "Avinash Rohra"], "Categories": "cs.CV cs.AI"}, "abstract": "Several deep learning algorithms have shown amazing performance for existing object detection tasks, but recognizing darker objects is the largest challenge. Moreover, those techniques struggled to detect or had a slow recognition rate, resulting in significant performance losses. As a result, an improved and accurate detection approach is required to address the above difficulty. The whole study proposes a combination of spiked and normal convolution layers as an energy-efficient and reliable object detector model. The proposed model is split into two sections. The first section is developed as a feature extractor, which utilizes pre-trained VGG16, and the second section of the proposal structure is the combination of spiked and normal Convolutional layers to detect the bounding boxes of images. We drew a pre-trained model for classifying detected objects. With state of the art Python libraries, spike layers can be trained efficiently. The proposed spike convolutional object detector (SCOD) has been evaluated on VOC and Ex-Dark datasets. SCOD reached 66.01% and 41.25% mAP for detecting 20 different objects in the VOC-12 and 12 objects in the Ex-Dark dataset. SCOD uses 14 Giga FLOPS for its forward path calculations. Experimental results indicated superior performance compared to Tiny YOLO, Spike YOLO, YOLO-LITE, Tinier YOLO and Center of loc+Xception based on mAP for the VOC dataset.", "url": "https://arxiv.org/abs/2310.06370"}, {"metadata": {"arXiv": "2310.06481", "Date": "Tue, 10 Oct 2023 09:49:06 ", "Title": "An improved ctgan for data processing method of imbalanced disk failure", "Authors": ["Jingbo Jia", "Peng Wu and Hussain Dawood"], "Categories": "cs.AI"}, "abstract": "To address the problem of insufficient failure data generated by disks and the imbalance between the number of normal and failure data. The existing Conditional Tabular Generative Adversarial Networks(CTGAN) deep learning methods have been proven to be effective in solving imbalance disk failure data. But CTGAN cannot learn the internal information of disk failure data very well. In this paper, a fault diagnosis method based on improved CTGAN, a classifier for specific category discrimination is added and a discriminator generate adversarial network based on residual network is proposed. We named it Residual Conditional Tabular Generative Adversarial Networks (RCTGAN). Firstly, to enhance the stability of system a residual network is utilized. RCTGAN uses a small amount of real failure data to synthesize fake fault data; Then, the synthesized data is mixed with the real data to balance the amount of normal and failure data; Finally, four classifier (multilayer perceptron, support vector machine, decision tree, random forest) models are trained using the balanced data set, and the performance of the models is evaluated using G-mean. The experimental results show that the data synthesized by the RCTGAN can further improve the fault diagnosis accuracy of the classifier.", "url": "https://arxiv.org/abs/2310.06481"}, {"metadata": {"arXiv": "2310.05932", "Date": "Mon, 21 Aug 2023 13:22:20 ", "Title": "A Multi-Agent Systems Approach for Peer-to-Peer Energy Trading in Dairy Farming", "Authors": ["Mian Ibad Ali Shah", "Abdul Wahid", "Enda Barrett", "Karl Mason"], "Categories": "cs.MA cs.AI cs.SY", "Comments": ["Proc. of the Artificial Intelligence for Sustainability", "ECAI 2023", "Eunika et al. (eds.)", "Sep 30- Oct 1", "2023", "https://sites.google.com/view/ai4s. 2023"]}, "abstract": "To achieve desired carbon emission reductions, integrating renewable generation and accelerating the adoption of peer-to-peer energy trading is crucial. This is especially important for energy-intensive farming, like dairy farming. However, integrating renewables and peer-to-peer trading presents challenges. To address this, we propose the Multi-Agent Peer-to-Peer Dairy Farm Energy Simulator (MAPDES), enabling dairy farms to participate in peer-to-peer markets. Our strategy reduces electricity costs and peak demand by approximately 30% and 24% respectively, while increasing energy sales by 37% compared to the baseline scenario without P2P trading. This demonstrates the effectiveness of our approach.", "url": "https://arxiv.org/abs/2310.05932"}, {"metadata": {"arXiv": "2310.06303", "Date": "Tue, 10 Oct 2023 04:34:00 ", "Title": "Dobby: A Conversational Service Robot Driven by GPT-4", "Authors": ["Carson Stark", "Bohkyung Chun", "Casey Charleston", "Varsha Ravi", "Luis Pabon", "Surya Sunkari", "Tarun Mohan", "Peter Stone", "and Justin Hart"], "Categories": "cs.RO cs.AI"}, "abstract": "This work introduces a robotics platform which embeds a conversational AI agent in an embodied system for natural language understanding and intelligent decision-making for service tasks; integrating task planning and human-like conversation. The agent is derived from a large language model, which has learned from a vast corpus of general knowledge. In addition to generating dialogue, this agent can interface with the physical world by invoking commands on the robot; seamlessly merging communication and behavior. This system is demonstrated in a free-form tour-guide scenario, in an HRI study combining robots with and without conversational AI capabilities. Performance is measured along five dimensions: overall effectiveness, exploration abilities, scrutinization abilities, receptiveness to personification, and adaptability.", "url": "https://arxiv.org/abs/2310.06303"}, {"metadata": {"arXiv": "2310.06075", "Date": "Mon, 09 Oct 2023 18:31:50 ", "Title": "Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction", "Authors": ["Swati Padhee", "Tanvi Banerjee", "Daniel M. Abrams", "and Nirmish Shah"], "Categories": "cs.AI cs.LG", "Comments": ["8 pages"]}, "abstract": "Sickle Cell Disease (SCD) is a chronic genetic disorder characterized by recurrent acute painful episodes. Opioids are often used to manage these painful episodes; the extent of their use in managing pain in this disorder is an issue of debate. The risk of addiction and side effects of these opioid treatments can often lead to more pain episodes in the future. Hence, it is crucial to forecast future patient pain trajectories to help patients manage their SCD to improve their quality of life without compromising their treatment. It is challenging to obtain many pain records to design forecasting models since it is mainly recorded by patients' self-report. Therefore, it is expensive and painful (due to the need for patient compliance) to solve pain forecasting problems in a purely supervised manner. In light of this challenge, we propose to solve the pain forecasting problem using self-supervised learning methods. Also, clustering such time-series data is crucial for patient phenotyping, anticipating patients' prognoses by identifying \"similar\" patients, and designing treatment guidelines tailored to homogeneous patient subgroups. Hence, we propose a self-supervised learning approach for clustering time-series data, where each cluster comprises patients who share similar future pain profiles. Experiments on five years of real-world datasets show that our models achieve superior performance over state-of-the-art benchmarks and identify meaningful clusters that can be translated into actionable information for clinical decision-making.", "url": "https://arxiv.org/abs/2310.06075"}, {"metadata": {"arXiv": "2310.06100", "Date": "Mon, 09 Oct 2023 19:21:41 ", "Title": "High Dimensional Causal Inference with Variational Backdoor Adjustment", "Authors": ["Daniel Israel", "Aditya Grover", "Guy Van den Broeck"], "Categories": "cs.AI cs.LG stat.ML"}, "abstract": "Backdoor adjustment is a technique in causal inference for estimating interventional quantities from purely observational data. For example, in medical settings, backdoor adjustment can be used to control for confounding and estimate the effectiveness of a treatment. However, high dimensional treatments and confounders pose a series of potential pitfalls: tractability, identifiability, optimization. In this work, we take a generative modeling approach to backdoor adjustment for high dimensional treatments and confounders. We cast backdoor adjustment as an optimization problem in variational inference without reliance on proxy variables and hidden confounders. Empirically, our method is able to estimate interventional likelihood in a variety of high dimensional settings, including semi-synthetic X-ray medical data. To the best of our knowledge, this is the first application of backdoor adjustment in which all the relevant variables are high dimensional.", "url": "https://arxiv.org/abs/2310.06100"}, {"metadata": {"arXiv": "2310.06225", "Date": "Tue, 10 Oct 2023 00:39:04 ", "Title": "GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models", "Authors": ["Bruno Silva", "Leonardo Nunes", "Roberto Estev\\~ao", "Ranveer Chandra"], "Categories": "cs.AI cs.LG"}, "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn credits for renewing agronomist certifications, answering 93% of the questions correctly and outperforming earlier general-purpose models, which achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest performance when compared to human subjects. This performance suggests that GPT-4 could potentially pass on major graduate education admission tests or even earn credits for renewing agronomy certificates. We also explore the models' capacity to address general agriculture-related questions and generate crop management guidelines for Brazilian and Indian farmers, utilizing robust datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate program exams from India. The results suggest that GPT-4, ER, and RAG can contribute meaningfully to agricultural education, assessment, and crop management practice, offering valuable insights to farmers and agricultural professionals.", "url": "https://arxiv.org/abs/2310.06225"}, {"metadata": {"arXiv": "2310.06369", "Date": "Tue, 10 Oct 2023 07:11:25 ", "Title": "Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks", "Authors": ["Sung Moon Ko", "Sumin Lee", "Dae-Woong Jeong", "Woohyung Lim", "Sehui Han"], "Categories": "cs.AI cs.LG", "Comments": ["12+11 pages", "6+1 figures", "0+7 tables"]}, "abstract": "Transfer learning is a crucial technique for handling a small amount of data that is potentially related to other abundant data. However, most of the existing methods are focused on classification tasks using images and language datasets. Therefore, in order to expand the transfer learning scheme to regression tasks, we propose a novel transfer technique based on differential geometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this method, we interpret the latent vectors from the model to exist on a Riemannian curved manifold. We find a proper diffeomorphism between pairs of tasks to ensure that every arbitrary point maps to a locally flat coordinate in the overlapping region, allowing the transfer of knowledge from the source to the target data. This also serves as an effective regularizer for the model to behave in extrapolation regions. In this article, we demonstrate that GATE outperforms conventional methods and exhibits stable behavior in both the latent space and extrapolation regions for various molecular graph datasets.", "url": "https://arxiv.org/abs/2310.06369"}, {"metadata": {"arXiv": "2310.06714", "Date": "Tue, 10 Oct 2023 15:41:26 ", "Title": "Exploring Memorization in Fine-tuned Language Models", "Authors": ["Shenglai Zeng", "Yaxin Li", "Jie Ren", "Yiding Liu", "Han Xu", "Pengfei He", "Yue Xing", "Shuaiqiang Wang", "Jiliang Tang", "Dawei Yin"], "Categories": "cs.AI cs.CL cs.LG"}, "abstract": "LLMs have shown great capabilities in various tasks but also exhibited memorization of training data, thus raising tremendous privacy and copyright concerns. While prior work has studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared with pre-training, fine-tuning typically involves sensitive data and diverse objectives, thus may bring unique memorization behaviors and distinct privacy risks. In this work, we conduct the first comprehensive analysis to explore LMs' memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that fine-tuned memorization presents a strong disparity among tasks. We provide an understanding of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention score distribution. By investigating its memorization behavior, multi-task fine-tuning paves a potential strategy to mitigate fine-tuned memorization.", "url": "https://arxiv.org/abs/2310.06714"}, {"metadata": {"arXiv": "2310.06786", "Date": "Tue, 10 Oct 2023 16:57:28 ", "Title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text", "Authors": ["Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba"], "Categories": "cs.AI cs.CL cs.LG"}, "abstract": "There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, openly released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.", "url": "https://arxiv.org/abs/2310.06786"}, {"metadata": {"arXiv": "2310.06020", "Date": "Mon, 09 Oct 2023 18:00:01 ", "Title": "DyST: Towards Dynamic Neural Scene Representations on Real-World Videos", "Authors": ["Maximilian Seitzer", "Sjoerd van Steenkiste", "Thomas Kipf", "Klaus Greff", "Mehdi S. M. Sajjadi"], "Categories": "cs.CV cs.AI cs.GR cs.LG cs.RO", "Comments": ["Project website: https://dyst-paper.github.io/"]}, "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.", "url": "https://arxiv.org/abs/2310.06020"}, {"metadata": {"arXiv": "2310.06138", "Date": "Mon, 09 Oct 2023 20:32:49 ", "Title": "Layout Sequence Prediction From Noisy Mobile Modality", "Authors": ["Haichao Zhang", "Yi Xu", "Hongsheng Lu", "Takayuki Shimizu", "Yun Fu"], "Categories": "cs.CV cs.AI cs.LG cs.MM cs.RO", "Comments": ["In Proceedings of the 31st ACM International Conference on Multimedia 2023 (MM 23)"], "DOI": "10.1145/3581783.3611936"}, "abstract": "Trajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model predicts layout sequences by implicitly inferring object size and projection status from a single reference timestamp or significantly obstructed sequences. Achieving SOTA results in randomly obstructed experiments and extremely short input experiments, our model illustrates the effectiveness of leveraging noisy mobile data. In summary, our approach offers a promising solution to the challenges faced by layout sequence and trajectory prediction models in real-world settings, paving the way for utilizing sensor data from mobile phones to accurately predict pedestrian bounding box trajectories. To the best of our knowledge, this is the first work that addresses severely obstructed and extremely short layout sequences by combining vision with noisy mobile modality, making it the pioneering work in the field of layout sequence trajectory prediction.", "url": "https://arxiv.org/abs/2310.06138"}, {"metadata": {"arXiv": "2310.06238", "Date": "Tue, 10 Oct 2023 01:22:41 ", "Title": "Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering", "Authors": ["Xiulong Liu and Zhikang Dong and Peng Zhang"], "Categories": "cs.CV cs.AI cs.CL cs.LG cs.MM cs.SD eess.AS"}, "abstract": "In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model's ability to effectively reason across these diverse modalities is compromised, impeding further advancement. In this paper, we meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution. In particular, for binary questions, we strive to ensure that both answers are almost uniformly spread within each question category. As a result, we construct a new dataset, named MUSIC-AVQA v2.0, which is more challenging and we believe could better foster the progress of AVQA task. Furthermore, we present a novel baseline model that delves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0, this model surpasses all the existing benchmarks, improving accuracy by 2% on MUSIC-AVQA v2.0, setting a new state-of-the-art performance.", "url": "https://arxiv.org/abs/2310.06238"}, {"metadata": {"arXiv": "2310.06003", "Date": "Mon, 09 Oct 2023 15:08:32 ", "Title": "Rethinking Memory and Communication Cost for Efficient Large Language Model Training", "Authors": ["Chan Wu", "Hanxiao Zhang", "Lin Ju", "Jinjing Huang", "Youshao Xiao", "Zhaoxin Huan", "Siyuan Li", "Fanzhuang Meng", "Lei Liang", "Xiaolu Zhang and Jun Zhou"], "Categories": "cs.LG cs.AI"}, "abstract": "As model sizes and training datasets continue to increase, large-scale model training frameworks reduce memory consumption by various sharding techniques. However, the huge communication overhead reduces the training efficiency, especially in public cloud environments with varying network bandwidths. In this paper, we rethink the impact of memory consumption and communication overhead on the training speed of large language model, and propose a memory-communication balanced \\underline{Pa}rtial \\underline{R}edundancy \\underline{O}ptimizer (PaRO). PaRO reduces the amount and frequency of inter-group communication by grouping GPU clusters and introducing minor intra-group memory redundancy, thereby improving the training efficiency of the model. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large model training. Our experiments demonstrate that the HO-Ring algorithm improves communication efficiency by 32.6\\% compared to the traditional Ring algorithm. Compared to the baseline ZeRO, PaRO significantly improves training throughput by 1.2x-2.6x and achieves a near-linear scalability. Therefore, the PaRO strategy provides more fine-grained options for the trade-off between memory consumption and communication overhead in different training scenarios.", "url": "https://arxiv.org/abs/2310.06003"}, {"metadata": {"arXiv": "2310.06045", "Date": "Mon, 09 Oct 2023 18:02:11 ", "Title": "Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model", "Authors": ["Yingkai Sha", "Ryan A. Sobash", "David John Gagne II"], "Categories": "cs.LG cs.AI physics.ao-ph"}, "abstract": "An ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overconfident but produces meaningful ensemble spreads that can distinguish good and bad forecasts. The quality of CGAN outputs is also evaluated. Results show that the CGAN outputs behave similarly to a numerical ensemble; they preserved the inter-variable correlations and the contribution of influential predictors as in the original HRRR forecasts. This work provides a novel approach to post-process CAM output using neural networks that can be applied to severe weather prediction.", "url": "https://arxiv.org/abs/2310.06045"}, {"metadata": {"arXiv": "2310.06113", "Date": "Mon, 09 Oct 2023 19:40:54 ", "Title": "When is Agnostic Reinforcement Learning Statistically Tractable?", "Authors": ["Zeyu Jia", "Gene Li", "Alexander Rakhlin", "Ayush Sekhari", "Nathan Srebro"], "Categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "Comments": ["Accepted to NeurIPS 2023"]}, "abstract": "We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\\epsilon$-suboptimal policy with respect to $\\Pi$? Towards that end, we introduce a new complexity measure, called the \\emph{spanning capacity}, that depends solely on the set $\\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \\emph{sunflower} structure, which in conjunction with bounded spanning capacity enables statistically efficient online RL via a new algorithm called POPLER, which takes inspiration from classical importance sampling methods as well as techniques for reachable-state identification and policy evaluation in reward-free exploration.", "url": "https://arxiv.org/abs/2310.06113"}, {"metadata": {"arXiv": "2310.06117", "Date": "Mon, 09 Oct 2023 19:48:55 ", "Title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "Authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le and Denny Zhou"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.", "url": "https://arxiv.org/abs/2310.06117"}, {"metadata": {"arXiv": "2310.06119", "Date": "Mon, 09 Oct 2023 19:52:22 ", "Title": "Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis", "Authors": ["Zezhi Shao", "Fei Wang", "Yongjun Xu", "Wei Wei", "Chengqing Yu", "Zhao Zhang", "Di Yao", "Guangyin Jin", "Xin Cao", "Gao Cong", "Christian S. Jensen", "Xueqi Cheng"], "Categories": "cs.LG cs.AI"}, "abstract": "Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting models on more than 18 datasets. Furthermore, we highlight the heterogeneity among MTS datasets and classify them based on temporal and spatial characteristics. We further prove that neglecting heterogeneity is the primary reason for generating controversies in technical approaches. Moreover, based on the proposed BasicTS and rich heterogeneous MTS datasets, we conduct an exhaustive and reproducible performance and efficiency comparison of popular models, providing insights for researchers in selecting and designing MTS forecasting models.", "url": "https://arxiv.org/abs/2310.06119"}, {"metadata": {"arXiv": "2310.06131", "Date": "Mon, 09 Oct 2023 20:22:43 ", "Title": "Learning Layer-wise Equivariances Automatically using Gradients", "Authors": ["Tycho F.A. van der Ouderaa", "Alexander Immer", "Mark van der Wilk"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in deep networks. We demonstrate the ability to automatically learn layer-wise equivariances on image classification tasks, achieving equivalent or improved performance over baselines with hard-coded symmetry.", "url": "https://arxiv.org/abs/2310.06131"}, {"metadata": {"arXiv": "2310.06147", "Date": "Mon, 09 Oct 2023 20:49:42 ", "Title": "Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond", "Authors": ["Hao Sun"], "Categories": "cs.LG cs.AI"}, "abstract": "Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research. Highlighted Takeaways: 1. RLHF is Online Inverse RL with Offline Demonstration Data. 2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error. 3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evaluation and optimization where feedback is also expensive. 4. The policy learning in RLHF is more challenging than conventional problems studied in IRL due to their high action dimensionality and feedback sparsity. 5. The main superiority of PPO over off-policy value-based methods is its stability gained from (almost) on-policy data and conservative policy updates.", "url": "https://arxiv.org/abs/2310.06147"}, {"metadata": {"arXiv": "2310.06148", "Date": "Mon, 09 Oct 2023 20:51:49 ", "Title": "Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques", "Authors": ["Mike Huisman", "Aske Plaat", "Jan N. van Rijn"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["Accepted at Machine Learning Journal", "Special Issue on Discovery Science 2021"]}, "abstract": "Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data regimes of similar data distribution as the one used for training. Our findings show that both the output layer and the noisy training conditions induced by data scarcity play important roles in facilitating this specialization for MAML. Lastly, we show that the pre-trained features as obtained by the finetuning baseline are more diverse and discriminative than those learned by MAML and Reptile. Due to this lack of diversity and distribution specialization, MAML and Reptile may fail to generalize to out-of-distribution tasks whereas finetuning can fall back on the diversity of the learned features.", "url": "https://arxiv.org/abs/2310.06148"}, {"metadata": {"arXiv": "2310.06171", "Date": "Mon, 09 Oct 2023 21:49:48 ", "Title": "Memory-Consistent Neural Networks for Imitation Learning", "Authors": ["Kaustubh Sridhar", "Souradeep Dutta", "Dinesh Jayaraman", "James Weimer", "Insup Lee"], "Categories": "cs.LG cs.AI cs.RO", "Comments": ["22 pages (9 main pages)"]}, "abstract": "Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, and Diffusion backbones, spanning dexterous robotic manipulation and driving, proprioceptive inputs and visual inputs, and varying sizes and types of demonstration data, we find large and consistent gains in performance, validating that MCNNs are better-suited than vanilla deep neural networks for imitation learning applications. Website: https://sites.google.com/view/mcnn-imitation", "url": "https://arxiv.org/abs/2310.06171"}, {"metadata": {"arXiv": "2310.06218", "Date": "Tue, 10 Oct 2023 00:22:27 ", "Title": "SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration", "Authors": ["Jingyang Xiang and Siqi Li and Jun Chen and Shipeng Bai and Yukai Ma and Guang Dai and Yong Liu"], "Categories": "cs.LG cs.AI", "Comments": ["14 pages", "4 figures", "Accepted by 37th Conference on Neural Information Processing Systems (NeurIPS 2023)"]}, "abstract": "The study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1$\\times$N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a \\emph{Block Sparse Row} matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1$\\times$N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel \\emph{\\textbf{S}oft \\textbf{U}niform \\textbf{B}lock \\textbf{P}runing} (SUBP) approach to train a uniform 1$\\times$N sparse structured network from scratch. Specifically, our approach tends to repeatedly allow pruned blocks to regrow to the network based on block angular redundancy and importance sampling in a uniform manner throughout the training process. It not only makes the model less dependent on pre-training, reduces the model redundancy and the risk of pruning the important blocks permanently but also achieves balanced workload. Empirically, on ImageNet, comprehensive experiments across various CNN architectures show that our SUBP consistently outperforms existing 1$\\times$N and structured sparsity methods based on pre-trained models or training from scratch. Source codes and models are available at \\url{https://github.com/JingyangXiang/SUBP}.", "url": "https://arxiv.org/abs/2310.06218"}, {"metadata": {"arXiv": "2310.06261", "Date": "Tue, 10 Oct 2023 02:08:09 ", "Title": "Self-Discriminative Modeling for Anomalous Graph Detection", "Authors": ["Jinyu Cai", "Yunhe Zhang", "Jicong Fan"], "Categories": "cs.LG cs.AI", "Comments": ["This work was submitted to NeurIPS 2023 but was unfortunately rejected"]}, "abstract": "This paper studies the problem of detecting anomalous graphs using a machine learning model trained on only normal graphs, which has many applications in molecule, biology, and social network data analysis. We present a self-discriminative modeling framework for anomalous graph detection. The key idea, mathematically and numerically illustrated, is to learn a discriminator (classifier) from the given normal graphs together with pseudo-anomalous graphs generated by a model jointly trained, where we never use any true anomalous graphs and we hope that the generated pseudo-anomalous graphs interpolate between normal ones and (real) anomalous ones. Under the framework, we provide three algorithms with different computational efficiencies and stabilities for anomalous graph detection. The three algorithms are compared with several state-of-the-art graph-level anomaly detection baselines on nine popular graph datasets (four with small size and five with moderate size) and show significant improvement in terms of AUC. The success of our algorithms stems from the integration of the discriminative classifier and the well-posed pseudo-anomalous graphs, which provide new insights for anomaly detection. Moreover, we investigate our algorithms for large-scale imbalanced graph datasets. Surprisingly, our algorithms, though fully unsupervised, are able to significantly outperform supervised learning algorithms of anomalous graph detection. The corresponding reason is also analyzed.", "url": "https://arxiv.org/abs/2310.06261"}, {"metadata": {"arXiv": "2310.06286", "Date": "Tue, 10 Oct 2023 03:46:32 ", "Title": "Suppressing Overestimation in Q-Learning through Adversarial Behaviors", "Authors": ["HyeAnn Lee", "Donghwan Lee"], "Categories": "cs.LG cs.AI"}, "abstract": "The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.", "url": "https://arxiv.org/abs/2310.06286"}, {"metadata": {"arXiv": "2310.06301", "Date": "Tue, 10 Oct 2023 04:26:04 ", "Title": "Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition", "Authors": ["Zhongtian Chen", "Edmund Lau", "Jake Mendel", "Susan Wei", "Daniel Murfet"], "Categories": "cs.LG cs.AI", "MSC-class": "62F15, 68T07"}, "abstract": "We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular $k$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.", "url": "https://arxiv.org/abs/2310.06301"}, {"metadata": {"arXiv": "2310.06322", "Date": "Tue, 10 Oct 2023 05:35:02 ", "Title": "Predicting Three Types of Freezing of Gait Events Using Deep Learning Models", "Authors": ["Wen Tao Mo", "Jonathan H. Chan"], "Categories": "cs.LG cs.AI", "Comments": ["5 pages"]}, "abstract": "Freezing of gait is a Parkinson's Disease symptom that episodically inflicts a patient with the inability to step or turn while walking. While medical experts have discovered various triggers and alleviating actions for freezing of gait, the underlying causes and prediction models are still being explored today. Current freezing of gait prediction models that utilize machine learning achieve high sensitivity and specificity in freezing of gait predictions based on time-series data; however, these models lack specifications on the type of freezing of gait events. We develop various deep learning models using the transformer encoder architecture plus Bidirectional LSTM layers and different feature sets to predict the three different types of freezing of gait events. The best performing model achieves a score of 0.427 on testing data, which would rank top 5 in Kaggle's Freezing of Gait prediction competition, hosted by THE MICHAEL J. FOX FOUNDATION. However, we also recognize overfitting in training data that could be potentially improved through pseudo labelling on additional data and model architecture simplification.", "url": "https://arxiv.org/abs/2310.06322"}, {"metadata": {"arXiv": "2310.06387", "Date": "Tue, 10 Oct 2023 07:50:29 ", "Title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations", "Authors": ["Zeming Wei", "Yifei Wang", "Yisen Wang"], "Categories": "cs.LG cs.AI cs.CL cs.CR"}, "abstract": "Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL to influence LLM behavior and provide a new perspective for enhancing the safety and alignment of LLMs.", "url": "https://arxiv.org/abs/2310.06387"}, {"metadata": {"arXiv": "2310.06399", "Date": "Tue, 10 Oct 2023 08:06:32 ", "Title": "Lo-Hi: Practical ML Drug Discovery Benchmark", "Authors": ["Simon Steshin"], "Categories": "cs.LG cs.AI", "Comments": ["29 pages", "Advances in Neural Information Processing Systems", "2023"]}, "abstract": "Finding new drugs is getting harder and harder. One of the hopes of drug discovery is to use machine learning models to predict molecular properties. That is why models for molecular property prediction are being developed and tested on benchmarks such as MoleculeNet. However, existing benchmarks are unrealistic and are too different from applying the models in practice. We have created a new practical \\emph{Lo-Hi} benchmark consisting of two tasks: Lead Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug discovery process. For the Hi task, we designed a novel molecular splitting algorithm that solves the Balanced Vertex Minimum $k$-Cut problem. We tested state-of-the-art and classic ML models, revealing which works better under practical settings. We analyzed modern benchmarks and showed that they are unrealistic and overoptimistic. Review: https://openreview.net/forum?id=H2Yb28qGLV Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023 Lo-Hi splitter library: https://github.com/SteshinSS/lohi_splitter", "url": "https://arxiv.org/abs/2310.06399"}, {"metadata": {"arXiv": "2310.06417", "Date": "Tue, 10 Oct 2023 08:40:47 ", "Title": "Advective Diffusion Transformers for Topological Generalization in Graph Learning", "Authors": ["Qitian Wu", "Chenxiao Yang", "Kaipeng Zeng", "Fan Nie", "Michael Bronstein", "Junchi Yan"], "Categories": "cs.LG cs.AI", "Comments": ["39 pages"]}, "abstract": "Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent graphs, under the assumption of a specific data-generating condition. In addition to these findings, we propose a novel graph encoder backbone, Advective Diffusion Transformer (ADiT), inspired by advective graph diffusion equations that have a closed-form solution backed up with theoretical guarantees of desired generalization under topological distribution shifts. The new model, functioning as a versatile graph Transformer, demonstrates superior performance across a wide range of graph learning tasks.", "url": "https://arxiv.org/abs/2310.06417"}, {"metadata": {"arXiv": "2310.06427", "Date": "Tue, 10 Oct 2023 08:52:16 ", "Title": "TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems", "Authors": ["Zijie Huang", "Wanjia Zhao", "Jingdong Gao", "Ziniu Hu", "Xiao Luo", "Yadi Cao", "Yuanzhou Chen", "Yizhou Sun", "Wei Wang"], "Categories": "cs.LG cs.AI"}, "abstract": "Learning complex multi-agent system dynamics from data is crucial across many domains, such as in physical simulations and material modeling. Extended from purely data-driven approaches, existing physics-informed approaches such as Hamiltonian Neural Network strictly follow energy conservation law to introduce inductive bias, making their learning more sample efficiently. However, many real-world systems do not strictly conserve energy, such as spring systems with frictions. Recognizing this, we turn our attention to a broader physical principle: Time-Reversal Symmetry, which depicts that the dynamics of a system shall remain invariant when traversed back over time. It still helps to preserve energies for conservative systems and in the meanwhile, serves as a strong inductive bias for non-conservative, reversible systems. To inject such inductive bias, in this paper, we propose a simple-yet-effective self-supervised regularization term as a soft constraint that aligns the forward and backward trajectories predicted by a continuous graph neural network-based ordinary differential equation (GraphODE). It effectively imposes time-reversal symmetry to enable more accurate model predictions across a wider range of dynamical systems under classical mechanics. In addition, we further provide theoretical analysis to show that our regularization essentially minimizes higher-order Taylor expansion terms during the ODE integration steps, which enables our model to be more noise-tolerant and even applicable to irreversible systems. Experimental results on a variety of physical systems demonstrate the effectiveness of our proposed method. Particularly, it achieves an MSE improvement of 11.5 % on a challenging chaotic triple-pendulum systems.", "url": "https://arxiv.org/abs/2310.06427"}, {"metadata": {"arXiv": "2310.06452", "Date": "Tue, 10 Oct 2023 09:25:44 ", "Title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity", "Authors": ["Robert Kirk", "Ishita Mediratta", "Christoforos Nalmpantis", "Jelena Luketina", "Eric Hambro", "Edward Grefenstette", "Roberta Raileanu"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT, Anthropic's Claude, or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the trade-off between generalisation and diversity.", "url": "https://arxiv.org/abs/2310.06452"}, {"metadata": {"arXiv": "2310.06648", "Date": "Tue, 10 Oct 2023 14:13:59 ", "Title": "Diversity from Human Feedback", "Authors": ["Ren-Jian Wang", "Ke Xue", "Yutong Wang", "Peng Yang", "Haobo Fu", "Qiang Fu", "Chao Qian"], "Categories": "cs.LG cs.AI cs.NE"}, "abstract": "Diversity plays a significant role in many problems, such as ensemble learning, reinforcement learning, and combinatorial optimization. How to define the diversity measure is a longstanding problem. Many methods rely on expert experience to define a proper behavior space and then obtain the diversity measure, which is, however, challenging in many scenarios. In this paper, we propose the problem of learning a behavior space from human feedback and present a general method called Diversity from Human Feedback (DivHF) to solve it. DivHF learns a behavior descriptor consistent with human preference by querying human feedback. The learned behavior descriptor can be combined with any distance measure to define a diversity measure. We demonstrate the effectiveness of DivHF by integrating it with the Quality-Diversity optimization algorithm MAP-Elites and conducting experiments on the QDax suite. The results show that DivHF learns a behavior space that aligns better with human requirements compared to direct data-driven approaches and leads to more diverse solutions under human preference. Our contributions include formulating the problem, proposing the DivHF method, and demonstrating its effectiveness through experiments.", "url": "https://arxiv.org/abs/2310.06648"}, {"metadata": {"arXiv": "2310.06743", "Date": "Tue, 10 Oct 2023 16:12:17 ", "Title": "Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks", "Authors": ["Marc Ru{\\ss}wurm", "Konstantin Klemmer", "Esther Rolf", "Robin Zbinden", "Devis Tuia"], "Categories": "cs.LG cs.AI"}, "abstract": "Learning feature representations of geographical space is vital for any machine learning model that integrates geolocated data, spanning application domains such as remote sensing, ecology, or epidemiology. Recent work mostly embeds coordinates using sine and cosine projections based on Double Fourier Sphere (DFS) features -- these embeddings assume a rectangular data domain even on global data, which can lead to artifacts, especially at the poles. At the same time, relatively little attention has been paid to the exact design of the neural network architectures these functional embeddings are combined with. This work proposes a novel location encoder for globally distributed geographic data that combines spherical harmonic basis functions, natively defined on spherical surfaces, with sinusoidal representation networks (SirenNets) that can be interpreted as learned Double Fourier Sphere embedding. We systematically evaluate the cross-product of positional embeddings and neural network architectures across various classification and regression benchmarks and synthetic evaluation datasets. In contrast to previous approaches that require the combination of both positional encoding and neural networks to learn meaningful representations, we show that both spherical harmonics and sinusoidal representation networks are competitive on their own but set state-of-the-art performances across tasks when combined. We provide source code at www.github.com/marccoru/locationencoder", "url": "https://arxiv.org/abs/2310.06743"}, {"metadata": {"arXiv": "2310.06756", "Date": "Tue, 10 Oct 2023 16:27:12 ", "Title": "Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory", "Authors": ["Yiting Chen", "Zhanpeng Zhou", "Junchi Yan"], "Categories": "cs.LG cs.AI"}, "abstract": "The behavior of neural networks still remains opaque, and a recently widely noted phenomenon is that networks often achieve similar performance when initialized with different random parameters. This phenomenon has attracted significant attention in measuring the similarity between features learned by distinct networks. However, feature similarity could be vague in describing the same feature since equivalent features hardly exist. In this paper, we expand the concept of equivalent feature and provide the definition of what we call functionally equivalent features. These features produce equivalent output under certain transformations. Using this definition, we aim to derive a more intrinsic metric for the so-called feature complexity regarding the redundancy of features learned by a neural network at each layer. We offer a formal interpretation of our approach through the lens of category theory, a well-developed area in mathematics. To quantify the feature complexity, we further propose an efficient algorithm named Iterative Feature Merging. Our experimental results validate our ideas and theories from various perspectives. We empirically demonstrate that the functionally equivalence widely exists among different features learned by the same neural network and we could reduce the number of parameters of the network without affecting the performance.The IFM shows great potential as a data-agnostic model prune method. We have also drawn several interesting empirical findings regarding the defined feature complexity.", "url": "https://arxiv.org/abs/2310.06756"}, {"metadata": {"arXiv": "2310.06763", "Date": "Tue, 10 Oct 2023 16:39:47 ", "Title": "FABind: Fast and Accurate Protein-Ligand Binding", "Authors": ["Qizhi Pei", "Kaiyuan Gao", "Lijun Wu", "Jinhua Zhu", "Yingce Xia", "Shufang Xie", "Tao Qin", "Kun He", "Tie-Yan Liu", "Rui Yan"], "Categories": "cs.LG cs.AI q-bio.BM", "Comments": ["Neural Information Processing Systems (NIPS 2023)"]}, "abstract": "Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction module, which is also leveraged for docking pose estimation. The model further enhances the docking process by incrementally integrating the predicted pocket to optimize protein-ligand binding, reducing discrepancies between training and inference. Through extensive experiments on benchmark datasets, our proposed $\\mathbf{FABind}$ demonstrates strong advantages in terms of effectiveness and efficiency compared to existing methods. Our code is available at $\\href{https://github.com/QizhiPei/FABind}{Github}$.", "url": "https://arxiv.org/abs/2310.06763"}, {"metadata": {"arXiv": "2310.06771", "Date": "Tue, 10 Oct 2023 16:48:18 ", "Title": "Correlated Noise Provably Beats Independent Noise for Differentially Private Learning", "Authors": ["Christopher A. Choquette-Choo", "Krishnamurthy Dvijotham", "Krishna Pillutla", "Arun Ganesh", "Thomas Steinke", "Abhradeep Thakurta"], "Categories": "cs.LG cs.AI cs.CR math.OC", "Comments": ["Christopher A. Choquette-Choo", "Krishnamurthy Dvijotham", "and Krishna Pillutla contributed equally"]}, "abstract": "Differentially private learning algorithms inject noise into the learning process. While the most common private learning algorithm, DP-SGD, adds independent Gaussian noise in each iteration, recent work on matrix factorization mechanisms has shown empirically that introducing correlations in the noise can greatly improve their utility. We characterize the asymptotic learning utility for any choice of the correlation function, giving precise analytical bounds for linear regression and as the solution to a convex program for general convex functions. We show, using these bounds, how correlated noise provably improves upon vanilla DP-SGD as a function of problem parameters such as the effective dimension and condition number. Moreover, our analytical expression for the near-optimal correlation function circumvents the cubic complexity of the semi-definite program used to optimize the noise correlation matrix in previous work. We validate our theory with experiments on private deep learning. Our work matches or outperforms prior work while being efficient both in terms of compute and memory.", "url": "https://arxiv.org/abs/2310.06771"}, {"metadata": {"arXiv": "2310.06779", "Date": "Tue, 10 Oct 2023 16:54:25 ", "Title": "A Supervised Embedding and Clustering Anomaly Detection method for classification of Mobile Network Faults", "Authors": ["R. Mosayebi", "H. Kia", "A. Kianpour Raki"], "Categories": "cs.LG cs.AI"}, "abstract": "The paper introduces Supervised Embedding and Clustering Anomaly Detection (SEMC-AD), a method designed to efficiently identify faulty alarm logs in a mobile network and alleviate the challenges of manual monitoring caused by the growing volume of alarm logs. SEMC-AD employs a supervised embedding approach based on deep neural networks, utilizing historical alarm logs and their labels to extract numerical representations for each log, effectively addressing the issue of imbalanced classification due to a small proportion of anomalies in the dataset without employing one-hot encoding. The robustness of the embedding is evaluated by plotting the two most significant principle components of the embedded alarm logs, revealing that anomalies form distinct clusters with similar embeddings. Multivariate normal Gaussian clustering is then applied to these components, identifying clusters with a high ratio of anomalies to normal alarms (above 90%) and labeling them as the anomaly group. To classify new alarm logs, we check if their embedded vectors' two most significant principle components fall within the anomaly-labeled clusters. If so, the log is classified as an anomaly. Performance evaluation demonstrates that SEMC-AD outperforms conventional random forest and gradient boosting methods without embedding. SEMC-AD achieves 99% anomaly detection, whereas random forest and XGBoost only detect 86% and 81% of anomalies, respectively. While supervised classification methods may excel in labeled datasets, the results demonstrate that SEMC-AD is more efficient in classifying anomalies in datasets with numerous categorical features, significantly enhancing anomaly detection, reducing operator burden, and improving network maintenance.", "url": "https://arxiv.org/abs/2310.06779"}, {"metadata": {"arXiv": "2310.06794", "Date": "Tue, 10 Oct 2023 17:07:05 ", "Title": "$f$-Policy Gradients: A General Framework for Goal Conditioned RL using $f$-Divergences", "Authors": ["Siddhant Agarwal", "Ishan Durugkar", "Peter Stone", "Amy Zhang"], "Categories": "cs.LG cs.AI cs.RO", "Comments": ["Accepted at NeurIPS 2023"]}, "abstract": "Goal-Conditioned Reinforcement Learning (RL) problems often have access to sparse rewards where the agent receives a reward signal only when it has achieved the goal, making policy optimization a difficult problem. Several works augment this sparse reward with a learned dense reward function, but this can lead to sub-optimal policies if the reward is misaligned. Moreover, recent works have demonstrated that effective shaping rewards for a particular problem can depend on the underlying learning algorithm. This paper introduces a novel way to encourage exploration called $f$-Policy Gradients, or $f$-PG. $f$-PG minimizes the f-divergence between the agent's state visitation distribution and the goal, which we show can lead to an optimal policy. We derive gradients for various f-divergences to optimize this objective. Our learning paradigm provides dense learning signals for exploration in sparse reward settings. We further introduce an entropy-regularized policy optimization objective, that we call $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective. We show that several metric-based shaping rewards like L2 can be used with $s$-MaxEnt RL, providing a common ground to study such metric-based shaping rewards with efficient exploration. We find that $f$-PG has better performance compared to standard policy gradient methods on a challenging gridworld as well as the Point Maze and FetchReach environments. More information on our website https://agarwalsiddhant10.github.io/projects/fpg.html.", "url": "https://arxiv.org/abs/2310.06794"}, {"metadata": {"arXiv": "2310.06835", "Date": "Tue, 10 Oct 2023 17:59:26 ", "Title": "Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement Learning", "Authors": ["Kaustuv Mukherji", "Devendra Parkar", "Lahari Pokala", "Dyuman Aditya", "Paulo Shakarian", "Clark Dorman"], "Categories": "cs.LG cs.AI cs.LO", "Comments": ["Submitted to IEEE International Conference on Semantic Computing"]}, "abstract": "Recent advances in reinforcement learning (RL) have shown much promise across a variety of applications. However, issues such as scalability, explainability, and Markovian assumptions limit its applicability in certain domains. We observe that many of these shortcomings emanate from the simulator as opposed to the RL training algorithms themselves. As such, we propose a semantic proxy for simulation based on a temporal extension to annotated logic. In comparison with two high-fidelity simulators, we show up to three orders of magnitude speed-up while preserving the quality of policy learned in addition to showing the ability to model and leverage non-Markovian dynamics and instantaneous actions while providing an explainable trace describing the outcomes of the agent actions.", "url": "https://arxiv.org/abs/2310.06835"}, {"metadata": {"arXiv": "2310.06585", "Date": "Tue, 10 Oct 2023 12:52:42 ", "Title": "A Black-Box Physics-Informed Estimator based on Gaussian Process Regression for Robot Inverse Dynamics Identification", "Authors": ["Giulio Giacomuzzo", "Alberto Dalla Libera", "Diego Romeres", "Ruggero Carli"], "Categories": "cs.RO cs.AI cs.LG cs.SY eess.SY"}, "abstract": "In this paper, we propose a black-box model based on Gaussian process regression for the identification of the inverse dynamics of robotic manipulators. The proposed model relies on a novel multidimensional kernel, called \\textit{Lagrangian Inspired Polynomial} (\\kernelInitials{}) kernel. The \\kernelInitials{} kernel is based on two main ideas. First, instead of directly modeling the inverse dynamics components, we model as GPs the kinetic and potential energy of the system. The GP prior on the inverse dynamics components is derived from those on the energies by applying the properties of GPs under linear operators. Second, as regards the energy prior definition, we prove a polynomial structure of the kinetic and potential energy, and we derive a polynomial kernel that encodes this property. As a consequence, the proposed model allows also to estimate the kinetic and potential energy without requiring any label on these quantities. Results on simulation and on two real robotic manipulators, namely a 7 DOF Franka Emika Panda and a 6 DOF MELFA RV4FL, show that the proposed model outperforms state-of-the-art black-box estimators based both on Gaussian Processes and Neural Networks in terms of accuracy, generality and data efficiency. The experiments on the MELFA robot also demonstrate that our approach achieves performance comparable to fine-tuned model-based estimators, despite requiring less prior information.", "url": "https://arxiv.org/abs/2310.06585"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
