<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2309.03999", "Date": "Thu, 07 Sep 2023 20:05:39 ", "Title": "Adapting Self-Supervised Representations to Multi-Domain Setups", "Authors": ["Neha Kalibhat", "Sam Sharpe", "Jeremy Goodsitt", "Bayan Bruss", "Soheil Feizi"], "Categories": "cs.CV cs.LG", "Comments": ["Published at BMVC 2023"]}, "abstract": "Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on multi-domain benchmarks including PACS, DomainNet and WILDS. Models trained with DDM show significantly improved generalization (7.4%) to unseen domains compared to baselines. Therefore, DDM can efficiently adapt self-supervised encoders to provide high-quality, generalizable representations for diverse multi-domain data.", "url": "https://arxiv.org/abs/2309.03999"}, {"metadata": {"arXiv": "2309.04001", "Date": "Thu, 07 Sep 2023 20:07:57 ", "Title": "Multimodal Transformer for Material Segmentation", "Authors": ["Md Kaykobad Reza (1)", "Ashley Prater-Bennette (2)", "M. Salman Asif (1) ((1) University of California", "Riverside", "(2) Air Force Research Laboratory)"], "Categories": "cs.CV cs.LG", "Comments": ["9 pages", "3 figures"]}, "abstract": "Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for overall model performance. Furthermore, our ablation studies also highlight the capacity of different input modalities to improve performance in the identification of different types of materials. The code and pretrained models will be made available at https://github.com/csiplab/MMSFormer.", "url": "https://arxiv.org/abs/2309.04001"}, {"metadata": {"arXiv": "2309.04354", "Date": "Fri, 08 Sep 2023 14:24:10 ", "Title": "Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts", "Authors": ["Erik Daxberger", "Floris Weers", "Bowen Zhang", "Tom Gunter", "Ruoming Pang", "Marcin Eichner", "Michael Emmersberger", "Yinfei Yang", "Alexander Toshev", "Xianzhi Du"], "Categories": "cs.CV cs.LG stat.ML"}, "abstract": "Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense counterpart by 3.39% on ImageNet-1k. For an even smaller ViT variant with only 54M FLOPs inference cost, our MoE achieves an improvement of 4.66%.", "url": "https://arxiv.org/abs/2309.04354"}, {"metadata": {"arXiv": "2309.03913", "Date": "Tue, 15 Aug 2023 20:04:18 ", "Title": "A Robust Adaptive Workload Orchestration in Pure Edge Computing", "Authors": ["Zahra Safavifar", "Charafeddine Mechalikh and Fatemeh Golpayegani"], "Categories": "cs.DC cs.LG", "Comments": ["9 pages", "Accepted in ICAART conference"]}, "abstract": "Pure Edge computing (PEC) aims to bring cloud applications and services to the edge of the network to support the growing user demand for time-sensitive applications and data-driven computing. However, mobility and limited computational capacity of edge devices pose challenges in supporting some urgent and computationally intensive tasks with strict response time demands. If the execution results of these tasks exceed the deadline, they become worthless and can cause severe safety issues. Therefore, it is essential to ensure that edge nodes complete as many latency-sensitive tasks as possible.", "url": "https://arxiv.org/abs/2309.03913"}, {"metadata": {"arXiv": "2309.03964", "Date": "Thu, 07 Sep 2023 18:44:58 ", "Title": "REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation", "Authors": ["Skyler Seto", "Barry-John Theobald", "Federico Danieli", "Navdeep Jaitly", "Dan Busbridge"], "Categories": "cs.LG cs.CV", "Comments": ["Accepted at WACV 2024", "17 pages", "7 figures", "11 tables"]}, "abstract": "Fully-test-time adaptation (F-TTA) can mitigate performance loss due to distribution shifts between train and test data (1) without access to the training data, and (2) without knowledge of the model training procedure. In online F-TTA, a pre-trained model is adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization. However, models adapted with online using entropy minimization, are unstable especially in single sample settings, leading to degenerate solutions, and limiting the adoption of TTA inference strategies. Prior works identify noisy, or unreliable, samples as a cause of failure in online F-TTA. One solution is to ignore these samples, which can lead to bias in the update procedure, slow adaptation, and poor generalization. In this work, we present a general framework for improving robustness of F-TTA to these noisy samples, inspired by self-paced learning and robust loss functions. Our proposed approach, Robust Entropy Adaptive Loss Minimization (REALM), achieves better adaptation accuracy than previous approaches throughout the adaptation process on corruptions of CIFAR-10 and ImageNet-1K, demonstrating its effectiveness.", "url": "https://arxiv.org/abs/2309.03964"}, {"metadata": {"arXiv": "2309.03965", "Date": "Thu, 07 Sep 2023 18:46:52 ", "Title": "Improving Resnet-9 Generalization Trained on Small Datasets", "Authors": ["Omar Mohamed Awad and Habib Hajimolahoseini and Michael Lim and Gurpreet Gosal and Walid Ahmed and Yang Liu and Gordon Deng"], "Categories": "cs.LG cs.CV"}, "abstract": "This paper presents our proposed approach that won the first prize at the ICLR competition on Hardware Aware Efficient Training. The challenge is to achieve the highest possible accuracy in an image classification task in less than 10 minutes. The training is done on a small dataset of 5000 images picked randomly from CIFAR-10 dataset. The evaluation is performed by the competition organizers on a secret dataset with 1000 images of the same size. Our approach includes applying a series of technique for improving the generalization of ResNet-9 including: sharpness aware optimization, label smoothing, gradient centralization, input patch whitening as well as metalearning based training. Our experiments show that the ResNet-9 can achieve the accuracy of 88% while trained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets", "url": "https://arxiv.org/abs/2309.03965"}, {"metadata": {"arXiv": "2309.03970", "Date": "Thu, 07 Sep 2023 19:03:28 ", "Title": "Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!", "Authors": ["Rishabh Jain"], "Categories": "cs.LG", "Comments": ["Appeared in IJCAI 2023 Workshop on Explainable Artificial Intelligence (XAI)"]}, "abstract": "Interpretability and explainability of neural networks is continuously increasing in importance, especially within safety-critical domains and to provide the social right to explanation. Concept based explanations align well with how humans reason, proving to be a good way to explain models. Concept Embedding Models (CEMs) are one such concept based explanation architectures. These have shown to overcome the trade-off between explainability and performance. However, they have a key limitation -- they require concept annotations for all their training data. For large datasets, this can be expensive and infeasible. Motivated by this, we propose Automatic Concept Embedding Models (ACEMs), which learn the concept annotations automatically.", "url": "https://arxiv.org/abs/2309.03970"}, {"metadata": {"arXiv": "2309.03974", "Date": "Thu, 07 Sep 2023 19:15:40 ", "Title": "DBsurf: A Discrepancy Based Method for Discrete Stochastic Gradient Estimation", "Authors": ["Pau Mulet Arabi", "Alec Flowers", "Lukas Mauch", "Fabien Cardinaux"], "Categories": "cs.LG", "Comments": ["22 pages", "7 figures"], "ACM-class": "I.2.0"}, "abstract": "Computing gradients of an expectation with respect to the distributional parameters of a discrete distribution is a problem arising in many fields of science and engineering. Typically, this problem is tackled using Reinforce, which frames the problem of gradient estimation as a Monte Carlo simulation. Unfortunately, the Reinforce estimator is especially sensitive to discrepancies between the true probability distribution and the drawn samples, a common issue in low sampling regimes that results in inaccurate gradient estimates. In this paper, we introduce DBsurf, a reinforce-based estimator for discrete distributions that uses a novel sampling procedure to reduce the discrepancy between the samples and the actual distribution. To assess the performance of our estimator, we subject it to a diverse set of tasks. Among existing estimators, DBsurf attains the lowest variance in a least squares problem commonly used in the literature for benchmarking. Furthermore, DBsurf achieves the best results for training variational auto-encoders (VAE) across different datasets and sampling setups. Finally, we apply DBsurf to build a simple and efficient Neural Architecture Search (NAS) algorithm with state-of-the-art performance.", "url": "https://arxiv.org/abs/2309.03974"}, {"metadata": {"arXiv": "2309.04015", "Date": "Thu, 07 Sep 2023 20:53:23 ", "Title": "Optimal Transport with Tempered Exponential Measures", "Authors": ["Ehsan Amid", "Frank Nielsen", "Richard Nock", "and Manfred K. Warmuth"], "Categories": "cs.LG math.OC"}, "abstract": "In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, ``\\`a-la-Kantorovich'', which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, ``\\`a-la-Sinkhorn-Cuturi'', which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that a generalization of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity which is under control up to sparsity patterns. In addition, it fits naturally in the unbalanced optimal transport problem setting as well.", "url": "https://arxiv.org/abs/2309.04015"}, {"metadata": {"arXiv": "2309.04030", "Date": "Thu, 07 Sep 2023 21:57:15 ", "Title": "Brief technical note on linearizing recurrent neural networks (RNNs) before vs after the pointwise nonlinearity", "Authors": ["Marino Pagan", "Adrian Valente", "Srdjan Ostojic", "and Carlos D. Brody"], "Categories": "cs.LG q-bio.NC", "Comments": ["10 pages"]}, "abstract": "Linearization of the dynamics of recurrent neural networks (RNNs) is often used to study their properties. The same RNN dynamics can be written in terms of the ``activations\" (the net inputs to each unit, before its pointwise nonlinearity) or in terms of the ``activities\" (the output of each unit, after its pointwise nonlinearity); the two corresponding linearizations are different from each other. This brief and informal technical note describes the relationship between the two linearizations, between the left and right eigenvectors of their dynamics matrices, and shows that some context-dependent effects are readily apparent under linearization of activity dynamics but not linearization of activation dynamics.", "url": "https://arxiv.org/abs/2309.04030"}, {"metadata": {"arXiv": "2309.04037", "Date": "Thu, 07 Sep 2023 22:15:32 ", "Title": "SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks", "Authors": ["Jinyang Liu", "Sheng Di", "Sian Jin", "Kai Zhao", "Xin Liang", "Zizhong Chen", "Franck Cappello"], "Categories": "cs.LG cs.DC cs.IT math.IT"}, "abstract": "The fast growth of computational power and scales of modern super-computing systems have raised great challenges for the management of exascale scientific data. To maintain the usability of scientific data, error-bound lossy compression is proposed and developed as an essential technique for the size reduction of scientific data with constrained data distortion. Among the diverse datasets generated by various scientific simulations, certain datasets cannot be effectively compressed by existing error-bounded lossy compressors with traditional techniques. The recent success of Artificial Intelligence has inspired several researchers to integrate neural networks into error-bounded lossy compressors. However, those works still suffer from limited compression ratios and/or extremely low efficiencies. To address those issues and improve the compression on the hard-to-compress datasets, in this paper, we propose SRN-SZ, which is a deep learning-based scientific error-bounded lossy compressor leveraging the hierarchical data grid expansion paradigm implemented by super-resolution neural networks. SRN-SZ applies the most advanced super-resolution network HAT for its compression, which is free of time-costing per-data training. In experiments compared with various state-of-the-art compressors, SRN-SZ achieves up to 75% compression ratio improvements under the same error bound and up to 80% compression ratio improvements under the same PSNR than the second-best compressor.", "url": "https://arxiv.org/abs/2309.04037"}, {"metadata": {"arXiv": "2309.04081", "Date": "Fri, 08 Sep 2023 02:42:40 ", "Title": "UER: A Heuristic Bias Addressing Approach for Online Continual Learning", "Authors": ["Huiwei Lin", "Shanshan Feng", "Baoquan Zhang", "Hongliang Qiao", "Xutao Li", "and Yunming Ye"], "Categories": "cs.LG cs.CV", "Comments": ["9 pages", "12 figures", "ACM MM2023"]}, "abstract": "Online continual learning aims to continuously train neural networks from a continuous data stream with a single pass-through data. As the most effective approach, the rehearsal-based methods replay part of previous data. Commonly used predictors in existing methods tend to generate biased dot-product logits that prefer to the classes of current data, which is known as a bias issue and a phenomenon of forgetting. Many approaches have been proposed to overcome the forgetting problem by correcting the bias; however, they still need to be improved in online fashion. In this paper, we try to address the bias issue by a more straightforward and more efficient method. By decomposing the dot-product logits into an angle factor and a norm factor, we empirically find that the bias problem mainly occurs in the angle factor, which can be used to learn novel knowledge as cosine logits. On the contrary, the norm factor abandoned by existing methods helps remember historical knowledge. Based on this observation, we intuitively propose to leverage the norm factor to balance the new and old knowledge for addressing the bias. To this end, we develop a heuristic approach called unbias experience replay (UER). UER learns current samples only by the angle factor and further replays previous samples by both the norm and angle factors. Extensive experiments on three datasets show that UER achieves superior performance over various state-of-the-art methods. The code is in https://github.com/FelixHuiweiLin/UER.", "url": "https://arxiv.org/abs/2309.04081"}, {"metadata": {"arXiv": "2309.04211", "Date": "Fri, 08 Sep 2023 08:47:23 ", "Title": "Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse", "Authors": ["Edward A. Small", "Jeffrey N. Clark", "Christopher J. McWilliams", "Kacper Sokol", "Jeffrey Chan", "Flora D. Salim", "Raul Santos-Rodriguez"], "Categories": "cs.LG cs.CY", "Comments": ["7 pages", "5 figures", "3 appendix pages"]}, "abstract": "Counterfactuals operationalised through algorithmic recourse have become a powerful tool to make artificial intelligence systems explainable. Conceptually, given an individual classified as y -- the factual -- we seek actions such that their prediction becomes the desired class y' -- the counterfactual. This process offers algorithmic recourse that is (1) easy to customise and interpret, and (2) directly aligned with the goals of each individual. However, the properties of a \"good\" counterfactual are still largely debated; it remains an open challenge to effectively locate a counterfactual along with its corresponding recourse. Some strategies use gradient-driven methods, but these offer no guarantees on the feasibility of the recourse and are open to adversarial attacks on carefully created manifolds. This can lead to unfairness and lack of robustness. Other methods are data-driven, which mostly addresses the feasibility problem at the expense of privacy, security and secrecy as they require access to the entire training data set. Here, we introduce LocalFACE, a model-agnostic technique that composes feasible and actionable counterfactual explanations using locally-acquired information at each step of the algorithmic recourse. Our explainer preserves the privacy of users by only leveraging data that it specifically requires to construct actionable algorithmic recourse, and protects the model by offering transparency solely in the regions deemed necessary for the intervention.", "url": "https://arxiv.org/abs/2309.04211"}, {"metadata": {"arXiv": "2309.04222", "Date": "Fri, 08 Sep 2023 09:11:26 ", "Title": "Offline Recommender System Evaluation under Unobserved Confounding", "Authors": ["Olivier Jeunen and Ben London"], "Categories": "cs.LG cs.IR stat.ML", "Comments": ["Accepted at the CONSEQUENCES'23 workshop at RecSys '23"]}, "abstract": "Off-Policy Estimation (OPE) methods allow us to learn and evaluate decision-making policies from logged data. This makes them an attractive choice for the offline evaluation of recommender systems, and several recent works have reported successful adoption of OPE methods to this end. An important assumption that makes this work is the absence of unobserved confounders: random variables that influence both actions and rewards at data collection time. Because the data collection policy is typically under the practitioner's control, the unconfoundedness assumption is often left implicit, and its violations are rarely dealt with in the existing literature. This work aims to highlight the problems that arise when performing off-policy estimation in the presence of unobserved confounders, specifically focusing on a recommendation use-case. We focus on policy-based estimators, where the logging propensities are learned from logged data. We characterise the statistical bias that arises due to confounding, and show how existing diagnostics are unable to uncover such cases. Because the bias depends directly on the true and unobserved logging propensities, it is non-identifiable. As the unconfoundedness assumption is famously untestable, this becomes especially problematic. This paper emphasises this common, yet often overlooked issue. Through synthetic data, we empirically show how na\\\"ive propensity estimation under confounding can lead to severely biased metric estimates that are allowed to fly under the radar. We aim to cultivate an awareness among researchers and practitioners of this important problem, and touch upon potential research directions towards mitigating its effects.", "url": "https://arxiv.org/abs/2309.04222"}, {"metadata": {"arXiv": "2309.04236", "Date": "Fri, 08 Sep 2023 09:54:36 ", "Title": "Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos", "Authors": ["Di Wang", "Xiaotong Liu", "Shao-Bo Lin", "Ding-Xuan Zhou"], "Categories": "cs.LG stat.ML", "Comments": ["46pages", "13figures"]}, "abstract": "Data silos, mainly caused by privacy and interoperability, significantly constrain collaborations among different organizations with similar data for the same purpose. Distributed learning based on divide-and-conquer provides a promising way to settle the data silos, but it suffers from several challenges, including autonomy, privacy guarantees, and the necessity of collaborations. This paper focuses on developing an adaptive distributed kernel ridge regression (AdaDKRR) by taking autonomy in parameter selection, privacy in communicating non-sensitive information, and the necessity of collaborations in performance improvement into account. We provide both solid theoretical verification and comprehensive experiments for AdaDKRR to demonstrate its feasibility and effectiveness. Theoretically, we prove that under some mild conditions, AdaDKRR performs similarly to running the optimal learning algorithms on the whole data, verifying the necessity of collaborations and showing that no other distributed learning scheme can essentially beat AdaDKRR under the same conditions. Numerically, we test AdaDKRR on both toy simulations and two real-world applications to show that AdaDKRR is superior to other existing distributed learning schemes. All these results show that AdaDKRR is a feasible scheme to defend against data silos, which are highly desired in numerous application regions such as intelligent decision-making, pricing forecasting, and performance prediction for products.", "url": "https://arxiv.org/abs/2309.04236"}, {"metadata": {"arXiv": "2309.04284", "Date": "Fri, 08 Sep 2023 12:06:48 ", "Title": "Viewing the process of generating counterfactuals as a source of knowledge -- Application to the Naive Bayes classifier", "Authors": ["Vincent Lemaire", "Nathan Le Boudec", "Fran\\c{c}oise Fessant and Victor Guyomard"], "Categories": "cs.LG", "Comments": ["12 pages"]}, "abstract": "There are now many comprehension algorithms for understanding the decisions of a machine learning algorithm. Among these are those based on the generation of counterfactual examples. This article proposes to view this generation process as a source of creating a certain amount of knowledge that can be stored to be used, later, in different ways. This process is illustrated in the additive model and, more specifically, in the case of the naive Bayes classifier, whose interesting properties for this purpose are shown.", "url": "https://arxiv.org/abs/2309.04284"}, {"metadata": {"arXiv": "2309.04318", "Date": "Fri, 08 Sep 2023 13:31:06 ", "Title": "Generating the Ground Truth: Synthetic Data for Label Noise Research", "Authors": ["Sjoerd de Vries and Dirk Thierens"], "Categories": "cs.LG"}, "abstract": "Most real-world classification tasks suffer from label noise to some extent. Such noise in the data adversely affects the generalization error of learned models and complicates the evaluation of noise-handling methods, as their performance cannot be accurately measured without clean labels. In label noise research, typically either noisy or incomplex simulated data are accepted as a baseline, into which additional noise with known properties is injected. In this paper, we propose SYNLABEL, a framework that aims to improve upon the aforementioned methodologies. It allows for creating a noiseless dataset informed by real data, by either pre-specifying or learning a function and defining it as the ground truth function from which labels are generated. Furthermore, by resampling a number of values for selected features in the function domain, evaluating the function and aggregating the resulting labels, each data point can be assigned a soft label or label distribution. Such distributions allow for direct injection and quantification of label noise. The generated datasets serve as a clean baseline of adjustable complexity into which different types of noise may be introduced. We illustrate how the framework can be applied, how it enables quantification of label noise and how it improves over existing methodologies.", "url": "https://arxiv.org/abs/2309.04318"}, {"metadata": {"arXiv": "2309.04361", "Date": "Fri, 08 Sep 2023 14:41:21 ", "Title": "Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System", "Authors": ["Jonathan D. Boyd", "Joshua H. Tyler", "Anthony M. Murphy", "Donald R. Reising"], "Categories": "cs.LG eess.SP", "Comments": ["18 pages"]}, "abstract": "As power quality becomes a higher priority in the electric utility industry, the amount of disturbance event data continues to grow. Utilities do not have the required personnel to analyze each event by hand. This work presents an automated approach for analyzing power quality events recorded by digital fault recorders and power quality monitors operating within a power transmission system. The automated approach leverages rule-based analytics to examine the time and frequency domain characteristics of the voltage and current signals. Customizable thresholds are set to categorize each disturbance event. The events analyzed within this work include various faults, motor starting, and incipient instrument transformer failure. Analytics for fourteen different event types have been developed. The analytics were tested on 160 signal files and yielded an accuracy of ninety-nine percent. Continuous, nominal signal data analysis is performed using an approach coined as the cyclic histogram. The cyclic histogram process will be integrated into the digital fault recorders themselves to facilitate the detection of subtle signal variations that are too small to trigger a disturbance event and that can occur over hours or days. In addition to reducing memory requirements by a factor of 320, it is anticipated that cyclic histogram processing will aid in identifying incipient events and identifiers. This project is expected to save engineers time by automating the classification of disturbance events and increase the reliability of the transmission system by providing near real time detection and identification of disturbances as well as prevention of problems before they occur.", "url": "https://arxiv.org/abs/2309.04361"}, {"metadata": {"arXiv": "2309.04427", "Date": "Fri, 08 Sep 2023 16:41:25 ", "Title": "Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach", "Authors": ["Sofiane Ouaari", "Ali Burak \\\"Unal", "Mete Akg\\\"un", "Nico Pfeifer"], "Categories": "cs.LG cs.CR"}, "abstract": "Several domains increasingly rely on machine learning in their applications. The resulting heavy dependence on data has led to the emergence of various laws and regulations around data ethics and privacy and growing awareness of the need for privacy-preserving machine learning (ppML). Current ppML techniques utilize methods that are either purely based on cryptography, such as homomorphic encryption, or that introduce noise into the input, such as differential privacy. The main criticism given to those techniques is the fact that they either are too slow or they trade off a model s performance for improved confidentiality. To address this performance reduction, we aim to leverage robust representation learning as a way of encoding our data while optimizing the privacy-utility trade-off. Our method centers on training autoencoders in a multi-objective manner and then concatenating the latent and learned features from the encoding part as the encoded form of our data. Such a deep learning-powered encoding can then safely be sent to a third party for intensive training and hyperparameter tuning. With our proposed framework, we can share our data and use third party tools without being under the threat of revealing its original form. We empirically validate our results on unimodal and multimodal settings, the latter following a vertical splitting system and show improved performance over state-of-the-art.", "url": "https://arxiv.org/abs/2309.04427"}, {"metadata": {"arXiv": "2309.04085", "Date": "Fri, 08 Sep 2023 02:54:31 ", "Title": "Sample-Efficient Co-Design of Robotic Agents Using Multi-fidelity Training on Universal Policy Network", "Authors": ["Kishan R. Nagiredla", "Buddhika L. Semage", "Thommen G. Karimpanal", "Arun Kumar A. V and Santu Rana"], "Categories": "cs.RO cs.LG", "Comments": ["17 pages", "10 figures"]}, "abstract": "Co-design involves simultaneously optimizing the controller and agents physical design. Its inherent bi-level optimization formulation necessitates an outer loop design optimization driven by an inner loop control optimization. This can be challenging when the design space is large and each design evaluation involves data-intensive reinforcement learning process for control optimization. To improve the sample-efficiency we propose a multi-fidelity-based design exploration strategy based on Hyperband where we tie the controllers learnt across the design spaces through a universal policy learner for warm-starting the subsequent controller learning problems. Further, we recommend a particular way of traversing the Hyperband generated design matrix that ensures that the stochasticity of the Hyperband is reduced the most with the increasing warm starting effect of the universal policy learner as it is strengthened with each new design evaluation. Experiments performed on a wide range of agent design problems demonstrate the superiority of our method compared to the baselines. Additionally, analysis of the optimized designs shows interesting design alterations including design simplifications and non-intuitive alterations that have emerged in the biological world.", "url": "https://arxiv.org/abs/2309.04085"}, {"metadata": {"arXiv": "2309.04370", "Date": "Fri, 08 Sep 2023 15:02:46 ", "Title": "Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control", "Authors": ["David DeFazio", "Eisuke Hirota", "Shiqi Zhang"], "Categories": "cs.RO cs.LG", "Comments": ["Accepted to CoRL 2023"]}, "abstract": "Seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. Although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. We demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human. The video can be seen at our project page: https://bu-air-lab.github.io/guide_dog/", "url": "https://arxiv.org/abs/2309.04370"}, {"metadata": {"arXiv": "2309.04272", "Date": "Fri, 08 Sep 2023 11:47:31 ", "Title": "Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity", "Authors": ["Jiduan Wu and Anas Barakat and Ilyas Fatkhullin and Niao He"], "Categories": "eess.SY cs.GT cs.LG cs.SY"}, "abstract": "Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit regularization property. In this work, we propose a simpler nested Zeroth-Order (ZO) algorithm improving sample complexity by several orders of magnitude. Our main result guarantees a $\\widetilde{\\mathcal{O}}(\\epsilon^{-3})$ sample complexity under the same assumptions using a single-point ZO estimator. Furthermore, when the estimator is replaced by a two-point estimator, our method enjoys a better $\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$ sample complexity. Our key improvements rely on a more sample-efficient nested algorithm design and finer control of the ZO natural gradient estimation error.", "url": "https://arxiv.org/abs/2309.04272"}, {"metadata": {"arXiv": "2309.03924", "Date": "Thu, 07 Sep 2023 03:04:50 ", "Title": "Automatic Algorithm Selection for Pseudo-Boolean Optimization with Given Computational Time Limits", "Authors": ["Catalina Pezo and Dorit Hochbaum and Julio Godoy and Roberto Asin-Acha"], "Categories": "cs.AI cs.LO"}, "abstract": "Machine learning (ML) techniques have been proposed to automatically select the best solver from a portfolio of solvers, based on predicted performance. These techniques have been applied to various problems, such as Boolean Satisfiability, Traveling Salesperson, Graph Coloring, and others. These methods, known as meta-solvers, take an instance of a problem and a portfolio of solvers as input. They then predict the best-performing solver and execute it to deliver a solution. Typically, the quality of the solution improves with a longer computational time. This has led to the development of anytime selectors, which consider both the instance and a user-prescribed computational time limit. Anytime meta-solvers predict the best-performing solver within the specified time limit. Constructing an anytime meta-solver is considerably more challenging than building a meta-solver without the \"anytime\" feature. In this study, we focus on the task of designing anytime meta-solvers for the NP-hard optimization problem of Pseudo-Boolean Optimization (PBO), which generalizes Satisfiability and Maximum Satisfiability problems. The effectiveness of our approach is demonstrated via extensive empirical study in which our anytime meta-solver improves dramatically on the performance of Mixed Integer Programming solver Gurobi, which is the best-performing single solver in the portfolio. For example, out of all instances and time limits for which Gurobi failed to find feasible solutions, our meta-solver identified feasible solutions for 47% of these.", "url": "https://arxiv.org/abs/2309.03924"}, {"metadata": {"arXiv": "2309.04069", "Date": "Fri, 08 Sep 2023 01:50:32 ", "Title": "Inferring physical laws by artificial intelligence based causal models", "Authors": ["Jorawar Singh and Kishor Bharti and Arvind"], "Categories": "cs.AI physics.data-an quant-ph", "Comments": ["Latex 12 pages", "16 figures"]}, "abstract": "The advances in Artificial Intelligence (AI) and Machine Learning (ML) have opened up many avenues for scientific research, and are adding new dimensions to the process of knowledge creation. However, even the most powerful and versatile of ML applications till date are primarily in the domain of analysis of associations and boil down to complex data fitting. Judea Pearl has pointed out that Artificial General Intelligence must involve interventions involving the acts of doing and imagining. Any machine assisted scientific discovery thus must include casual analysis and interventions. In this context, we propose a causal learning model of physical principles, which not only recognizes correlations but also brings out casual relationships. We use the principles of causal inference and interventions to study the cause-and-effect relationships in the context of some well-known physical phenomena. We show that this technique can not only figure out associations among data, but is also able to correctly ascertain the cause-and-effect relations amongst the variables, thereby strengthening (or weakening) our confidence in the proposed model of the underlying physical process.", "url": "https://arxiv.org/abs/2309.04069"}, {"metadata": {"arXiv": "2309.04295", "Date": "Fri, 08 Sep 2023 12:34:28 ", "Title": "FIMO: A Challenge Formal Dataset for Automated Theorem Proving", "Authors": ["Chengwu Liu", "Jianhao Shen", "Huajian Xin", "Zhengying Liu", "Ye Yuan", "Haiming Wang", "Wei Ju", "Chuanyang Zheng", "Yichun Yin", "Lin Li", "Ming Zhang", "Qun Liu"], "Categories": "cs.AI"}, "abstract": "We present FIMO, an innovative dataset comprising formal mathematical problem statements sourced from the International Mathematical Olympiad (IMO) Shortlisted Problems. Designed to facilitate advanced automated theorem proving at the IMO level, FIMO is currently tailored for the Lean formal language. It comprises 149 formal problem statements, accompanied by both informal problem descriptions and their corresponding LaTeX-based informal proofs. Through initial experiments involving GPT-4, our findings underscore the existing limitations in current methodologies, indicating a substantial journey ahead before achieving satisfactory IMO-level automated theorem proving outcomes.", "url": "https://arxiv.org/abs/2309.04295"}, {"metadata": {"arXiv": "2309.04105", "Date": "Fri, 08 Sep 2023 03:56:34 ", "Title": "Weakly Supervised Point Clouds Transformer for 3D Object Detection", "Authors": ["Zuojin Tang", "Bo Sun", "Tongwei Ma", "Daosheng Li", "Zhenhui Xu"], "Categories": "cs.CV cs.AI cs.RO", "Comments": ["International Conference on Intelligent Transportation Systems (ITSC)", "2022"], "DOI": "10.1109/ITSC55140.2022.9921926"}, "abstract": "The annotation of 3D datasets is required for semantic-segmentation and object detection in scene understanding. In this paper we present a framework for the weakly supervision of a point clouds transformer that is used for 3D object detection. The aim is to decrease the required amount of supervision needed for training, as a result of the high cost of annotating a 3D datasets. We propose an Unsupervised Voting Proposal Module, which learns randomly preset anchor points and uses voting network to select prepared anchor points of high quality. Then it distills information into student and teacher network. In terms of student network, we apply ResNet network to efficiently extract local characteristics. However, it also can lose much global information. To provide the input which incorporates the global and local information as the input of student networks, we adopt the self-attention mechanism of transformer to extract global features, and the ResNet layers to extract region proposals. The teacher network supervises the classification and regression of the student network using the pre-trained model on ImageNet. On the challenging KITTI datasets, the experimental results have achieved the highest level of average precision compared with the most recent weakly supervised 3D object detectors.", "url": "https://arxiv.org/abs/2309.04105"}, {"metadata": {"arXiv": "2309.04421", "Date": "Fri, 08 Sep 2023 16:32:56 ", "Title": "SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios", "Authors": ["Amr Gomaa and Robin Zitt and Guillermo Reyes and Antonio Kr\\\"uger"], "Categories": "cs.CV cs.AI cs.HC", "Comments": ["Shorter versions are accepted as AutomotiveUI2023 Work in Progress and UIST2023 Poster Papers"], "DOI": "10.1145/3581961.3609889 10.1145/3586182.3616635"}, "abstract": "Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures\\footnote{\\url{https://github.com/amrgomaaelhady/SynthoGestures}}, improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the creation of the data set, our tool accelerates the development of gesture recognition systems for automotive applications.", "url": "https://arxiv.org/abs/2309.04421"}, {"metadata": {"arXiv": "2309.04430", "Date": "Fri, 08 Sep 2023 16:45:56 ", "Title": "Create Your World: Lifelong Text-to-Image Diffusion", "Authors": ["Gan Sun", "Wenqi Liang", "Jiahua Dong", "Jun Li", "Zhengming Ding", "Yang Cong"], "Categories": "cs.CV cs.AI", "Comments": ["15 pages,10 figures"]}, "abstract": "Text-to-image generative models can produce diverse high-quality images of concepts with a text prompt, which have demonstrated excellent ability in image generation, image translation, etc. We in this work study the problem of synthesizing instantiations of a use's own concepts in a never-ending manner, i.e., create your world, where the new concepts from user are quickly learned with a few examples. To achieve this goal, we propose a Lifelong text-to-image Diffusion Model (L2DM), which intends to overcome knowledge \"catastrophic forgetting\" for the past encountered concepts, and semantic \"catastrophic neglecting\" for one or more concepts in the text prompt. In respect of knowledge \"catastrophic forgetting\", our L2DM framework devises a task-aware memory enhancement module and a elastic-concept distillation module, which could respectively safeguard the knowledge of both prior concepts and each past personalized concept. When generating images with a user text prompt, the solution to semantic \"catastrophic neglecting\" is that a concept attention artist module can alleviate the semantic neglecting from concept aspect, and an orthogonal attention module can reduce the semantic binding from attribute aspect. To the end, our model can generate more faithful image across a range of continual text prompts in terms of both qualitative and quantitative metrics, when comparing with the related state-of-the-art models. The code will be released at https://wenqiliang.github.io/.", "url": "https://arxiv.org/abs/2309.04430"}, {"metadata": {"arXiv": "2309.04077", "Date": "Fri, 08 Sep 2023 02:24:37 ", "Title": "SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments", "Authors": ["Abhinav Rajvanshi", "Karan Sikka", "Xiao Lin", "Bhoram Lee", "Han-Pang Chiu and Alvaro Velasquez"], "Categories": "cs.RO cs.AI"}, "abstract": "Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on a new multi-object navigation task, that requires the agent to utilize a massive amount of human knowledge to efficiently search multiple different objects in an unknown environment. SayNav outperforms an oracle based Point-nav baseline, achieving a success rate of 95.35% (vs 56.06% for the baseline), under the ideal settings on this task, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments.", "url": "https://arxiv.org/abs/2309.04077"}, {"metadata": {"arXiv": "2309.04138", "Date": "Fri, 08 Sep 2023 05:33:56 ", "Title": "Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion", "Authors": ["Daegyu Lim", "Myeong-Ju Kim", "Junhyeok Cha", "Donghyeon Kim", "Jaeheung Park"], "Categories": "cs.RO cs.AI", "Comments": ["Accepted by 2023 IROS conference"]}, "abstract": "The estimation of external joint torque and contact wrench is essential for achieving stable locomotion of humanoids and safety-oriented robots. Although the contact wrench on the foot of humanoids can be measured using a force-torque sensor (FTS), FTS increases the cost, inertia, complexity, and failure possibility of the system. This paper introduces a method for learning external joint torque solely using proprioceptive sensors (encoders and IMUs) for a floating base robot. For learning, the GRU network is used and random walking data is collected. Real robot experiments demonstrate that the network can estimate the external torque and contact wrench with significantly smaller errors compared to the model-based method, momentum observer (MOB) with friction modeling. The study also validates that the estimated contact wrench can be utilized for zero moment point (ZMP) feedback control, enabling stable walking. Moreover, even when the robot's feet and the inertia of the upper body are changed, the trained network shows consistent performance with a model-based calibration. This result demonstrates the possibility of removing FTS on the robot, which reduces the disadvantages of hardware sensors. The summary video is available at https://youtu.be/gT1D4tOiKpo.", "url": "https://arxiv.org/abs/2309.04138"}, {"metadata": {"arXiv": "2309.04316", "Date": "Fri, 08 Sep 2023 13:29:05 ", "Title": "Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models", "Authors": ["Leonard B\\\"armann", "Rainer Kartmann", "Fabian Peller-Konrad", "Alex Waibel", "Tamim Asfour"], "Categories": "cs.RO cs.AI", "Comments": ["This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice", "after which this version may no longer be accessible. Submitted to the 2023 IEEE/RAS International Conference on Humanoid Robots (Humanoids). Supplementary video available at https://youtu.be/y5O2mRGtsLM"]}, "abstract": "Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, thus informing the generation of the next statement. Specifically, we introduce incremental prompt learning, which enables the system to interactively learn from its mistakes. For that purpose, the LLM can call another LLM responsible for code-level improvements of the current interaction based on human feedback. The improved interaction is then saved in the robot's memory, and thus retrieved on similar requests. We integrate the system in the robot cognitive architecture of the humanoid robot ARMAR-6 and evaluate our methods both quantitatively (in simulation) and qualitatively (in simulation and real-world) by demonstrating generalized incrementally-learned knowledge.", "url": "https://arxiv.org/abs/2309.04316"}, {"metadata": {"arXiv": "2309.04074", "Date": "Fri, 08 Sep 2023 02:19:14 ", "Title": "Computationally Efficient Data-Driven Discovery and Linear Representation of Nonlinear Systems For Control", "Authors": ["Madhur Tiwari", "George Nehma", "Bethany Lusch"], "Categories": "eess.SY cs.AI cs.SY"}, "abstract": "This work focuses on developing a data-driven framework using Koopman operator theory for system identification and linearization of nonlinear systems for control. Our proposed method presents a deep learning framework with recursive learning. The resulting linear system is controlled using a linear quadratic control. An illustrative example using a pendulum system is presented with simulations on noisy data. We show that our proposed method is trained more efficiently and is more accurate than an autoencoder baseline.", "url": "https://arxiv.org/abs/2309.04074"}, {"metadata": {"arXiv": "2309.03918", "Date": "Wed, 06 Sep 2023 09:43:34 ", "Title": "A recommender for the management of chronic pain in patients undergoing spinal cord stimulation", "Authors": ["Tigran Tchrakian", "Mykhaylo Zayats", "Alessandra Pascale", "Dat Huynh", "Pritish Parida", "Carla Agurto Rios", "Sergiy Zhuk", "Jeffrey L. Rogers", "ENVISION Studies Physician Author Group", "Boston Scientific Research Scientists Consortium"], "Categories": "cs.AI cs.CY cs.LG"}, "abstract": "Spinal cord stimulation (SCS) is a therapeutic approach used for the management of chronic pain. It involves the delivery of electrical impulses to the spinal cord via an implanted device, which when given suitable stimulus parameters can mask or block pain signals. Selection of optimal stimulation parameters usually happens in the clinic under the care of a provider whereas at-home SCS optimization is managed by the patient. In this paper, we propose a recommender system for the management of pain in chronic pain patients undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB) approach to develop a system that recommends SCS settings to patients with the aim of improving their condition. These recommendations, sent directly to patients though a digital health ecosystem, combined with a patient monitoring system closes the therapeutic loop around a chronic pain patient over their entire patient journey. We evaluated the system in a cohort of SCS-implanted ENVISION study subjects (Clinicaltrials.gov ID: NCT03240588) using a combination of quality of life metrics and Patient States (PS), a novel measure of holistic outcomes. SCS recommendations provided statistically significant improvement in clinical outcomes (pain and/or QoL) in 85\\% of all subjects (N=21). Among subjects in moderate PS (N=7) prior to receiving recommendations, 100\\% showed statistically significant improvements and 5/7 had improved PS dwell time. This analysis suggests SCS patients may benefit from SCS recommendations, resulting in additional clinical improvement on top of benefits already received from SCS therapy.", "url": "https://arxiv.org/abs/2309.03918"}, {"metadata": {"arXiv": "2309.04062", "Date": "Fri, 08 Sep 2023 01:36:58 ", "Title": "3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation", "Authors": ["Sungjun Cho", "Dae-Woong Jeong", "Sung Moon Ko", "Jinwoo Kim", "Sehui Han", "Seunghoon Hong", "Honglak Lee", "Moontae Lee"], "Categories": "cs.LG cs.AI physics.chem-ph", "Comments": ["16 pages", "5 figures"]}, "abstract": "Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless application to downstream tasks with no access to accurate conformers. Experiments on real-world molecular property prediction datasets show that the graph encoder trained via D&D can infer 3D information based on the 2D graph and shows superior performance and label-efficiency against other baselines.", "url": "https://arxiv.org/abs/2309.04062"}, {"metadata": {"arXiv": "2309.04082", "Date": "Fri, 08 Sep 2023 02:44:37 ", "Title": "Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning", "Authors": ["Sungjun Cho", "Seunghyuk Cho", "Sungwoo Park", "Hankook Lee", "Honglak Lee", "Moontae Lee"], "Categories": "cs.LG cs.AI", "Comments": ["19 pages", "7 figures"]}, "abstract": "Real-world graphs naturally exhibit hierarchical or cyclical structures that are unfit for the typical Euclidean space. While there exist graph neural networks that leverage hyperbolic or spherical spaces to learn representations that embed such structures more accurately, these methods are confined under the message-passing paradigm, making the models vulnerable against side-effects such as oversmoothing and oversquashing. More recent work have proposed global attention-based graph Transformers that can easily model long-range interactions, but their extensions towards non-Euclidean geometry are yet unexplored. To bridge this gap, we propose Fully Product-Stereographic Transformer, a generalization of Transformers towards operating entirely on the product of constant curvature spaces. When combined with tokenized graph Transformers, our model can learn the curvature appropriate for the input graph in an end-to-end fashion, without the need of additional tuning on different curvature initializations. We also provide a kernelized approach to non-Euclidean attention, which enables our model to run in time and memory cost linear to the number of nodes and edges while respecting the underlying geometry. Experiments on graph reconstruction and node classification demonstrate the benefits of generalizing Transformers to the non-Euclidean domain.", "url": "https://arxiv.org/abs/2309.04082"}, {"metadata": {"arXiv": "2309.04160", "Date": "Fri, 08 Sep 2023 07:01:38 ", "Title": "Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity", "Authors": ["Yinghao Zhu", "Zixiang Wang", "Long He", "Shiyun Xie", "Zixi Chen", "Jingkun An", "Liantao Ma", "Chengwei Pan"], "Categories": "cs.LG cs.AI"}, "abstract": "Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measuring similar patients, our approach designs a feature confidence learner module. This module is sensitive to the missing feature status, enabling the model to better judge the reliability of each feature. Moreover, we propose a novel patient similarity metric that takes feature confidence into account, ensuring that evaluations are not based merely on potentially inaccurate imputed values. Consequently, our work captures dense prototype patient representations with feature-missing-aware calibration process. Comprehensive experiments demonstrate that designed model surpasses established EHR-focused models with a statistically significant improvement on MIMIC-III and MIMIC-IV datasets in-hospital mortality outcome prediction task. The code is publicly available at \\url{https://anonymous.4open.science/r/SparseEHR} to assure the reproducibility.", "url": "https://arxiv.org/abs/2309.04160"}, {"metadata": {"arXiv": "2309.04195", "Date": "Fri, 08 Sep 2023 08:12:29 ", "Title": "Towards Mitigating Architecture Overfitting in Dataset Distillation", "Authors": ["Xuyang Zhong", "Chen Liu"], "Categories": "cs.LG cs.AI"}, "abstract": "Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.", "url": "https://arxiv.org/abs/2309.04195"}, {"metadata": {"arXiv": "2309.04296", "Date": "Fri, 08 Sep 2023 12:36:49 ", "Title": "Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility", "Authors": ["Arian Prabowo", "Kaixuan Chen", "Hao Xue", "Subbu Sethuvenkatraman", "Flora D. Salim"], "Categories": "cs.LG cs.AI cs.SY eess.SY", "Comments": ["10 pages", "2 figures", "5 tables", "BuildSys '23"]}, "abstract": "In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the second longest total lockdown duration globally during the pandemic. Results underscore the crucial role of continual learning in accurate energy forecasting, particularly during Out-of-Distribution periods. Secondary data such as mobility and temperature provided ancillary support to the primary forecasting model. More importantly, while traditional methods struggled to adapt during lockdowns, models featuring at least online learning demonstrated resilience, with lockdown periods posing fewer challenges once armed with adaptive learning techniques. This study contributes valuable methodologies and insights to the ongoing effort to improve energy load forecasting during future Out-of-Distribution periods.", "url": "https://arxiv.org/abs/2309.04296"}, {"metadata": {"arXiv": "2309.04311", "Date": "Fri, 08 Sep 2023 13:17:06 ", "Title": "Federated Learning for Early Dropout Prediction on Healthy Ageing Applications", "Authors": ["Christos Chrysanthos Nikolaidis", "Vasileios Perifanis", "Nikolaos Pavlidis", "Pavlos S. Efraimidis"], "Categories": "cs.LG cs.AI cs.DC"}, "abstract": "The provision of social care applications is crucial for elderly people to improve their quality of life and enables operators to provide early interventions. Accurate predictions of user dropouts in healthy ageing applications are essential since they are directly related to individual health statuses. Machine Learning (ML) algorithms have enabled highly accurate predictions, outperforming traditional statistical methods that struggle to cope with individual patterns. However, ML requires a substantial amount of data for training, which is challenging due to the presence of personal identifiable information (PII) and the fragmentation posed by regulations. In this paper, we present a federated machine learning (FML) approach that minimizes privacy concerns and enables distributed training, without transferring individual data. We employ collaborative training by considering individuals and organizations under FML, which models both cross-device and cross-silo learning scenarios. Our approach is evaluated on a real-world dataset with non-independent and identically distributed (non-iid) data among clients, class imbalance and label ambiguity. Our results show that data selection and class imbalance handling techniques significantly improve the predictive accuracy of models trained under FML, demonstrating comparable or superior predictive performance than traditional ML models.", "url": "https://arxiv.org/abs/2309.04311"}, {"metadata": {"arXiv": "2309.04332", "Date": "Fri, 08 Sep 2023 13:59:18 ", "Title": "Graph Neural Networks Use Graphs When They Shouldn't", "Authors": ["Maya Bechler-Speicher", "Ido Amos", "Ran Gilad-Bachrach", "Amir Globerson"], "Categories": "cs.LG cs.AI"}, "abstract": "Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting. We then provide a theoretical explanation for this phenomenon, via analyzing the implicit bias of gradient-descent-based learning of GNNs in this setting. Finally, based on our empirical and theoretical findings, we propose a graph-editing method to mitigate the tendency of GNNs to overfit graph-structures that should be ignored. We show that this method indeed improves the accuracy of GNNs across multiple benchmarks.", "url": "https://arxiv.org/abs/2309.04332"}, {"metadata": {"arXiv": "2309.04339", "Date": "Fri, 08 Sep 2023 14:08:19 ", "Title": "Online Submodular Maximization via Online Convex Optimization", "Authors": ["T. Si-Salem", "G. \\\"Ozcan", "I. Nikolaou", "E. Terzi", "S. Ioannidis"], "Categories": "cs.LG cs.AI math.OC", "Comments": ["Under review"]}, "abstract": "We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.", "url": "https://arxiv.org/abs/2309.04339"}, {"metadata": {"arXiv": "2309.04344", "Date": "Fri, 08 Sep 2023 14:15:47 ", "Title": "Zero-Shot Robustification of Zero-Shot Models With Foundation Models", "Authors": ["Dyah Adila", "Changho Shin", "Linrong Cai", "Frederic Sala"], "Categories": "cs.LG cs.AI"}, "abstract": "Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and show an average improvement of 15.98% over several zero-shot baselines. Additionally, we demonstrate that RoboShot is compatible with a variety of pretrained and language models.", "url": "https://arxiv.org/abs/2309.04344"}, {"metadata": {"arXiv": "2309.04367", "Date": "Fri, 08 Sep 2023 14:56:22 ", "Title": "Active Learning for Classifying 2D Grid-Based Level Completability", "Authors": ["Mahsa Bazzaz", "Seth Cooper"], "Categories": "cs.LG cs.AI", "Comments": ["4 pages", "3 figures"], "Journal-ref": "IEEE Conference on Games 2023"}, "abstract": "Determining the completability of levels generated by procedural generators such as machine learning models can be challenging, as it can involve the use of solver agents that often require a significant amount of time to analyze and solve levels. Active learning is not yet widely adopted in game evaluations, although it has been used successfully in natural language processing, image and speech recognition, and computer vision, where the availability of labeled data is limited or expensive. In this paper, we propose the use of active learning for learning level completability classification. Through an active learning approach, we train deep-learning models to classify the completability of generated levels for Super Mario Bros., Kid Icarus, and a Zelda-like game. We compare active learning for querying levels to label with completability against random queries. Our results show using an active learning approach to label levels results in better classifier performance with the same amount of labeled data.", "url": "https://arxiv.org/abs/2309.04367"}, {"metadata": {"arXiv": "2309.04381", "Date": "Fri, 08 Sep 2023 15:23:40 ", "Title": "Generalization Bounds: Perspectives from Information Theory and PAC-Bayes", "Authors": ["Fredrik Hellstr\\\"om", "Giuseppe Durisi", "Benjamin Guedj", "Maxim Raginsky"], "Categories": "cs.LG cs.AI cs.IT math.IT math.ST stat.ML stat.TH", "Comments": ["222 pages"]}, "abstract": "A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demonstrate how many proofs in the area share a modular structure, through which the underlying ideas can be intuited. We pay special attention to the conditional mutual information (CMI) framework; analytical studies of the information complexity of learning algorithms; and the application of the proposed methods to deep learning. This monograph is intended to provide a comprehensive introduction to information-theoretic generalization bounds and their connection to PAC-Bayes, serving as a foundation from which the most recent developments are accessible. It is aimed broadly towards researchers with an interest in generalization and theoretical machine learning.", "url": "https://arxiv.org/abs/2309.04381"}, {"metadata": {"arXiv": "2309.04433", "Date": "Fri, 08 Sep 2023 16:55:23 ", "Title": "Variations and Relaxations of Normalizing Flows", "Authors": ["Keegan Kelly", "Lorena Piedras", "Sukrit Rao", "David Roth"], "Categories": "cs.LG cs.AI"}, "abstract": "Normalizing Flows (NFs) describe a class of models that express a complex target distribution as the composition of a series of bijective transformations over a simpler base distribution. By limiting the space of candidate transformations to diffeomorphisms, NFs enjoy efficient, exact sampling and density evaluation, enabling NFs to flexibly behave as both discriminative and generative models. Their restriction to diffeomorphisms, however, enforces that input, output and all intermediary spaces share the same dimension, limiting their ability to effectively represent target distributions with complex topologies. Additionally, in cases where the prior and target distributions are not homeomorphic, Normalizing Flows can leak mass outside of the support of the target. This survey covers a selection of recent works that combine aspects of other generative model classes, such as VAEs and score-based diffusion, and in doing so loosen the strict bijectivity constraints of NFs to achieve a balance of expressivity, training speed, sample efficiency and likelihood tractability.", "url": "https://arxiv.org/abs/2309.04433"}, {"metadata": {"arXiv": "2309.04459", "Date": "Fri, 08 Sep 2023 17:37:05 ", "Title": "Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning", "Authors": ["David Yunis", "Justin Jung", "Falcon Dai", "Matthew Walter"], "Categories": "cs.LG cs.AI cs.RO"}, "abstract": "Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language processing to generate temporally extended actions. Such a method outperforms baselines for skill-generation in several challenging sparse-reward domains, and requires orders-of-magnitude less computation in skill-generation and online rollouts.", "url": "https://arxiv.org/abs/2309.04459"}, {"metadata": {"arXiv": "2309.04470", "Date": "Fri, 08 Sep 2023 17:57:31 ", "Title": "On the Actionability of Outcome Prediction", "Authors": ["Lydia T. Liu", "Solon Barocas", "Jon Kleinberg", "Karen Levy"], "Categories": "cs.LG cs.AI cs.CY", "Comments": ["14 pages", "3 figures"]}, "abstract": "Predicting future outcomes is a prevalent application of machine learning in social impact domains. Examples range from predicting student success in education to predicting disease risk in healthcare. Practitioners recognize that the ultimate goal is not just to predict but to act effectively. Increasing evidence suggests that relying on outcome predictions for downstream interventions may not have desired results. In most domains there exists a multitude of possible interventions for each individual, making the challenge of taking effective action more acute. Even when causal mechanisms connecting the individual's latent states to outcomes is well understood, in any given instance (a specific student or patient), practitioners still need to infer -- from budgeted measurements of latent states -- which of many possible interventions will be most effective for this individual. With this in mind, we ask: when are accurate predictors of outcomes helpful for identifying the most suitable intervention? Through a simple model encompassing actions, latent states, and measurements, we demonstrate that pure outcome prediction rarely results in the most effective policy for taking actions, even when combined with other measurements. We find that except in cases where there is a single decisive action for improving the outcome, outcome prediction never maximizes \"action value\", the utility of taking actions. Making measurements of actionable latent states, where specific actions lead to desired outcomes, considerably enhances the action value compared to outcome prediction, and the degree of improvement depends on action costs and the outcome model. This analysis emphasizes the need to go beyond generic outcome prediction in interventional settings by incorporating knowledge of plausible actions and latent states.", "url": "https://arxiv.org/abs/2309.04470"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
