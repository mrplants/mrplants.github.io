<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2310.12262", "Date": "Wed, 18 Oct 2023 18:57:13 ", "Title": "Improving SCGAN's Similarity Constraint and Learning a Better Disentangled Representation", "Authors": ["Iman Yazdanpanah"], "Categories": "cs.CV cs.LG eess.IV"}, "abstract": "SCGAN adds a similarity constraint between generated images and conditions as a regularization term on generative adversarial networks. Similarity constraint works as a tutor to instruct the generator network to comprehend the difference of representations based on conditions. We understand how SCGAN works on a deeper level. This understanding makes us realize that the similarity constraint functions like the contrastive loss function. We believe that a model with high understanding and intelligence measures the similarity between images based on their structure and high level features, just like humans do. Two major changes we applied to SCGAN in order to make a modified model are using SSIM to measure similarity between images and applying contrastive loss principles to the similarity constraint. The modified model performs better using FID and FactorVAE metrics. The modified model also has better generalisability compared to other models. Keywords Generative Adversarial Nets, Unsupervised Learning, Disentangled Representation Learning, Contrastive Disentanglement, SSIM", "url": "https://arxiv.org/abs/2310.12262"}, {"metadata": {"arXiv": "2310.12945", "Date": "Thu, 19 Oct 2023 17:41:48 ", "Title": "3D-GPT: Procedural 3D Modeling with Large Language Models", "Authors": ["Chunyi Sun", "Junlin Han", "Weijian Deng", "Xinlong Wang", "Zishan Qin", "Stephen Gould"], "Categories": "cs.CV cs.GR cs.LG", "Comments": ["Project page: https://chuny1.github.io/3DGPT/3dgpt.html"]}, "abstract": "In the pursuit of efficient automated content creation, procedural generation, leveraging modifiable parameters and rule-based systems, emerges as a promising approach. Nonetheless, it could be a demanding endeavor, given its intricate nature necessitating a deep understanding of rules, algorithms, and parameters. To reduce workload, we introduce 3D-GPT, a framework utilizing large language models~(LLMs) for instruction-driven 3D modeling. 3D-GPT positions LLMs as proficient problem solvers, dissecting the procedural 3D modeling tasks into accessible segments and appointing the apt agent for each task. 3D-GPT integrates three core agents: the task dispatch agent, the conceptualization agent, and the modeling agent. They collaboratively achieve two objectives. First, it enhances concise initial scene descriptions, evolving them into detailed forms while dynamically adapting the text based on subsequent instructions. Second, it integrates procedural generation, extracting parameter values from enriched text to effortlessly interface with 3D software for asset creation. Our empirical investigations confirm that 3D-GPT not only interprets and executes instructions, delivering reliable results but also collaborates effectively with human designers. Furthermore, it seamlessly integrates with Blender, unlocking expanded manipulation possibilities. Our work highlights the potential of LLMs in 3D modeling, offering a basic framework for future advancements in scene generation and animation.", "url": "https://arxiv.org/abs/2310.12945"}, {"metadata": {"arXiv": "2310.12976", "Date": "Thu, 19 Oct 2023 17:59:37 ", "Title": "On the Hidden Waves of Image", "Authors": ["Yinpeng Chen and Dongdong Chen and Xiyang Dai and Mengchen Liu and Lu Yuan and Zicheng Liu and Youzuo Lin"], "Categories": "cs.CV cs.LG"}, "abstract": "In this paper, we introduce an intriguing phenomenon-the successful reconstruction of images using a set of one-way wave equations with hidden and learnable speeds. Each individual image corresponds to a solution with a unique initial condition, which can be computed from the original image using a visual encoder (e.g., a convolutional neural network). Furthermore, the solution for each image exhibits two noteworthy mathematical properties: (a) it can be decomposed into a collection of special solutions of the same one-way wave equations that are first-order autoregressive, with shared coefficient matrices for autoregression, and (b) the product of these coefficient matrices forms a diagonal matrix with the speeds of the wave equations as its diagonal elements. We term this phenomenon hidden waves, as it reveals that, although the speeds of the set of wave equations and autoregressive coefficient matrices are latent, they are both learnable and shared across images. This represents a mathematical invariance across images, providing a new mathematical perspective to understand images.", "url": "https://arxiv.org/abs/2310.12976"}, {"metadata": {"arXiv": "2310.12370", "Date": "Wed, 18 Oct 2023 22:34:32 ", "Title": "No-Regret Learning in Bilateral Trade via Global Budget Balance", "Authors": ["Martino Bernasconi", "Matteo Castiglioni", "Andrea Celli", "Federico Fusco"], "Categories": "cs.GT cs.LG"}, "abstract": "Bilateral trade revolves around the challenge of facilitating transactions between two strategic agents -- a seller and a buyer -- both of whom have a private valuations for the item. We study the online version of the problem, in which at each time step a new seller and buyer arrive. The learner's task is to set a price for each agent, without any knowledge about their valuations. The sequence of sellers and buyers is chosen by an oblivious adversary. In this setting, known negative results rule out the possibility of designing algorithms with sublinear regret when the learner has to guarantee budget balance for each iteration. In this paper, we introduce the notion of global budget balance, which requires the agent to be budget balance only over the entire time horizon. By requiring global budget balance, we provide the first no-regret algorithms for bilateral trade with adversarial inputs under various feedback models. First, we show that in the full-feedback model the learner can guarantee $\\tilde{O}(\\sqrt{T})$ regret against the best fixed prices in hindsight, which is order-wise optimal. Then, in the case of partial feedback models, we provide an algorithm guaranteeing a $\\tilde{O}(T^{3/4})$ regret upper bound with one-bit feedback, which we complement with a nearly-matching lower bound. Finally, we investigate how these results vary when measuring regret using an alternative benchmark.", "url": "https://arxiv.org/abs/2310.12370"}, {"metadata": {"arXiv": "2310.12243", "Date": "Wed, 18 Oct 2023 18:28:44 ", "Title": "REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes", "Authors": ["Matthew Hull", "Zijie J. Wang", "and Duen Horng Chau"], "Categories": "cs.LG cs.CV"}, "abstract": "Deep Learning models, such as those used in an autonomous vehicle are vulnerable to adversarial attacks where an attacker could place an adversarial object in the environment, leading to mis-classification. Generating these adversarial objects in the digital space has been extensively studied, however successfully transferring these attacks from the digital realm to the physical realm has proven challenging when controlling for real-world environmental factors. In response to these limitations, we introduce REVAMP, an easy-to-use Python library that is the first-of-its-kind tool for creating attack scenarios with arbitrary objects and simulating realistic environmental factors, lighting, reflection, and refraction. REVAMP enables researchers and practitioners to swiftly explore various scenarios within the digital realm by offering a wide range of configurable options for designing experiments and using differentiable rendering to reproduce physically plausible adversarial objects. We will demonstrate and invite the audience to try REVAMP to produce an adversarial texture on a chosen object while having control over various scene parameters. The audience will choose a scene, an object to attack, the desired attack class, and the number of camera positions to use. Then, in real time, we show how this altered texture causes the chosen object to be mis-classified, showcasing the potential of REVAMP in real-world scenarios. REVAMP is open-source and available at https://github.com/poloclub/revamp.", "url": "https://arxiv.org/abs/2310.12243"}, {"metadata": {"arXiv": "2310.12248", "Date": "Wed, 18 Oct 2023 18:33:41 ", "Title": "A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs", "Authors": ["Mateo Perez", "Fabio Somenzi", "Ashutosh Trivedi"], "Categories": "cs.LG cs.LO"}, "abstract": "Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.", "url": "https://arxiv.org/abs/2310.12248"}, {"metadata": {"arXiv": "2310.12294", "Date": "Wed, 18 Oct 2023 19:55:11 ", "Title": "Open-Set Multivariate Time-Series Anomaly Detection", "Authors": ["Thomas Lai", "Thi Kieu Khanh Ho", "Narges Armanfard"], "Categories": "cs.LG", "Comments": ["11 pages", "5 tables", "3 figures"]}, "abstract": "Numerous methods for time series anomaly detection (TSAD) methods have emerged in recent years. Most existing methods are unsupervised and assume the availability of normal training samples only, while few supervised methods have shown superior performance by incorporating labeled anomalous samples in the training phase. However, certain anomaly types are inherently challenging for unsupervised methods to differentiate from normal data, while supervised methods are constrained to detecting anomalies resembling those present during training, failing to generalize to unseen anomaly classes. This paper is the first attempt in providing a novel approach for the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are visible in the training phase, with the objective of detecting both seen and unseen anomaly classes in the test phase. The proposed method, called Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three primary modules: a Feature Extractor to extract meaningful time-series features; a Multi-head Network consisting of Generative-, Deviation-, and Contrastive heads for capturing both seen and unseen anomaly classes; and an Anomaly Scoring module leveraging the insights of the three heads to detect anomalies. Extensive experiments on three real-world datasets consistently show that our approach surpasses existing methods under various experimental settings, thus establishing a new state-of-the-art performance in the TSAD field.", "url": "https://arxiv.org/abs/2310.12294"}, {"metadata": {"arXiv": "2310.12350", "Date": "Wed, 18 Oct 2023 21:51:42 ", "Title": "Equipping Federated Graph Neural Networks with Structure-aware Group Fairness", "Authors": ["Nan Cui", "Xiuling Wang", "Wendy Hui Wang", "Violet Chen and Yue Ning"], "Categories": "cs.LG"}, "abstract": "Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a training graph and statistical fairness metrics of the trained GNN models. Based on the theoretical analysis, we design $\\text{F}^2$GNN which contains two key components: a fairness-aware local model update scheme that enhances group fairness of the local models on the client side, and a fairness-weighted global model update scheme that takes both data bias and fairness metrics of local models into consideration in the aggregation process. We evaluate $\\text{F}^2$GNN empirically versus a number of baseline methods, and demonstrate that $\\text{F}^2$GNN outperforms these baselines in terms of both fairness and model accuracy.", "url": "https://arxiv.org/abs/2310.12350"}, {"metadata": {"arXiv": "2310.12353", "Date": "Wed, 18 Oct 2023 21:57:20 ", "Title": "Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach", "Authors": ["Syed Islam", "Monika Filipovska"], "Categories": "cs.LG", "Comments": ["Transportation Research Board Annual Meeting 2023"]}, "abstract": "Traffic state forecasting is crucial for traffic management and control strategies, as well as user- and system-level decision making in the transportation network. While traffic forecasting has been approached with a variety of techniques over the last couple of decades, most approaches simply rely on endogenous traffic variables for state prediction, despite the evidence that exogenous factors can significantly impact traffic conditions. This paper proposes a multi-dimensional spatio-temporal graph attention-based traffic prediction approach (M-STGAT), which predicts traffic based on past observations of speed, along with lane closure events, temperature, and visibility across the transportation network. The approach is based on a graph attention network architecture, which also learns based on the structure of the transportation network on which these variables are observed. Numerical experiments are performed using traffic speed and lane closure data from the California Department of Transportation (Caltrans) Performance Measurement System (PeMS). The corresponding weather data were downloaded from the National Oceanic and Atmospheric Administration (NOOA) Automated Surface Observing Systems (ASOS). For comparison, the numerical experiments implement three alternative models which do not allow for the multi-dimensional input. The M-STGAT is shown to outperform the three alternative models, when performing tests using our primary data set for prediction with a 30-, 45-, and 60-minute prediction horizon, in terms of three error measures: Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE). However, the model's transferability can vary for different transfer data sets and this aspect may require further investigation.", "url": "https://arxiv.org/abs/2310.12353"}, {"metadata": {"arXiv": "2310.12395", "Date": "Thu, 19 Oct 2023 00:45:05 ", "Title": "Closed-Form Diffusion Models", "Authors": ["Christopher Scarvelis", "Haitz S\\'aez de Oc\\'ariz Borde", "Justin Solomon"], "Categories": "cs.LG stat.ML", "Comments": ["Under review"]}, "abstract": "Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves sampling times competitive with neural SGMs while running on consumer-grade CPUs.", "url": "https://arxiv.org/abs/2310.12395"}, {"metadata": {"arXiv": "2310.12403", "Date": "Thu, 19 Oct 2023 01:15:24 ", "Title": "Cooperative Minibatching in Graph Neural Networks", "Authors": ["Muhammed Fatih Balin", "Dominique LaSalle", "\\\"Umit V. \\c{C}ataly\\\"urek"], "Categories": "cs.LG cs.DC", "Comments": ["Under submission"]}, "abstract": "Significant computational resources are required to train Graph Neural Networks (GNNs) at a large scale, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work per seed vertex as batch sizes increase. Hence, it is favorable for processors equipped with a fast interconnect to work on a large minibatch together as a single larger processor, instead of working on separate smaller minibatches, even though global batch size is identical. We also show how to take advantage of the same phenomenon in serial execution by generating dependent consecutive minibatches. Our experimental evaluations show up to 4x bandwidth savings for fetching vertex embeddings, by simply increasing this dependency without harming model convergence. Combining our proposed approaches, we achieve up to 64% speedup over Independent Minibatching on single-node multi-GPU systems.", "url": "https://arxiv.org/abs/2310.12403"}, {"metadata": {"arXiv": "2310.12421", "Date": "Thu, 19 Oct 2023 02:21:04 ", "Title": "Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling", "Authors": ["Wendy Hui", "Wai Kwong Lau"], "Categories": "cs.LG cs.CY", "Comments": ["11 pages", "2 figures", "6 tables", "R-script in appendix"]}, "abstract": "This paper proposes the use of causal modeling to detect and mitigate algorithmic bias. We provide a brief description of causal modeling and a general overview of our approach. We then use the Adult dataset, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on gender bias and the problem of binary classification. We show that gender bias in the prediction model is statistically significant at the 0.05 level. We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. Furthermore, we show that the overall classification accuracy is improved slightly. Our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as \"lavaan\" in R. Hence, it enhances explainability and promotes trust.", "url": "https://arxiv.org/abs/2310.12421"}, {"metadata": {"arXiv": "2310.12432", "Date": "Thu, 19 Oct 2023 02:49:31 ", "Title": "CAT: Closed-loop Adversarial Training for Safe End-to-End Driving", "Authors": ["Linrui Zhang and Zhenghao Peng and Quanyi Li and Bolei Zhou"], "Categories": "cs.LG", "Comments": ["7th Conference on Robot Learning (CoRL 2023)"]}, "abstract": "Driving safety is a top priority for autonomous vehicles. Orthogonal to prior work handling accident-prone traffic events by algorithm designs at the policy level, we investigate a Closed-loop Adversarial Training (CAT) framework for safe end-to-end driving in this paper through the lens of environment augmentation. CAT aims to continuously improve the safety of driving agents by training the agent on safety-critical scenarios that are dynamically generated over time. A novel resampling technique is developed to turn log-replay real-world driving scenarios into safety-critical ones via probabilistic factorization, where the adversarial traffic generation is modeled as the multiplication of standard motion prediction sub-problems. Consequently, CAT can launch more efficient physical attacks compared to existing safety-critical scenario generation methods and yields a significantly less computational cost in the iterative learning pipeline. We incorporate CAT into the MetaDrive simulator and validate our approach on hundreds of driving scenarios imported from real-world driving datasets. Experimental results demonstrate that CAT can effectively generate adversarial scenarios countering the agent being trained. After training, the agent can achieve superior driving safety in both log-replay and safety-critical traffic scenarios on the held-out test set. Code and data are available at https://metadriverse.github.io/cat.", "url": "https://arxiv.org/abs/2310.12432"}, {"metadata": {"arXiv": "2310.12457", "Date": "Thu, 19 Oct 2023 04:30:14 ", "Title": "MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale", "Authors": ["Haitian Jiang", "Renjie Liu", "Xiao Yan", "Zhenkun Cai", "Minjie Wang", "David Wipf"], "Categories": "cs.LG"}, "abstract": "Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability when applied to the largest publicly-available node classification benchmark exceeding 1TB in size.", "url": "https://arxiv.org/abs/2310.12457"}, {"metadata": {"arXiv": "2310.12461", "Date": "Thu, 19 Oct 2023 04:39:38 ", "Title": "Balanced Group Convolution: An Improved Group Convolution Based on Approximability Estimates", "Authors": ["Youngkyu Lee", "Jongho Park", "Chang-Ock Lee"], "Categories": "cs.LG cs.NA math.NA", "Comments": ["26pages", "2 figures"], "MSC-class": "68W01, 68W40"}, "abstract": "The performance of neural networks has been significantly improved by increasing the number of channels in convolutional layers. However, this increase in performance comes with a higher computational cost, resulting in numerous studies focused on reducing it. One promising approach to address this issue is group convolution, which effectively reduces the computational cost by grouping channels. However, to the best of our knowledge, there has been no theoretical analysis on how well the group convolution approximates the standard convolution. In this paper, we mathematically analyze the approximation of the group convolution to the standard convolution with respect to the number of groups. Furthermore, we propose a novel variant of the group convolution called balanced group convolution, which shows a higher approximation with a small additional computational cost. We provide experimental results that validate our theoretical findings and demonstrate the superior performance of the balanced group convolution over other variants of group convolution.", "url": "https://arxiv.org/abs/2310.12461"}, {"metadata": {"arXiv": "2310.12462", "Date": "Thu, 19 Oct 2023 04:41:01 ", "Title": "Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights", "Authors": ["Yichuan Deng", "Zhao Song", "Shenghao Xie", "Chiwun Yang"], "Categories": "cs.LG cs.CL stat.ML"}, "abstract": "In the realm of deep learning, transformers have emerged as a dominant architecture, particularly in natural language processing tasks. However, with their widespread adoption, concerns regarding the security and privacy of the data processed by these models have arisen. In this paper, we address a pivotal question: Can the data fed into transformers be recovered using their attention weights and outputs? We introduce a theoretical framework to tackle this problem. Specifically, we present an algorithm that aims to recover the input data $X \\in \\mathbb{R}^{d \\times n}$ from given attention weights $W = QK^\\top \\in \\mathbb{R}^{d \\times d}$ and output $B \\in \\mathbb{R}^{n \\times n}$ by minimizing the loss function $L(X)$. This loss function captures the discrepancy between the expected output and the actual output of the transformer. Our findings have significant implications for the Localized Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's design from a security and privacy perspective. This work underscores the importance of understanding and safeguarding the internal workings of transformers to ensure the confidentiality of processed data.", "url": "https://arxiv.org/abs/2310.12462"}, {"metadata": {"arXiv": "2310.12487", "Date": "Thu, 19 Oct 2023 05:47:28 ", "Title": "Improved Operator Learning by Orthogonal Attention", "Authors": ["Zipeng Xiao", "Zhongkai Hao", "Bokai Lin", "Zhijie Deng", "Hang Su"], "Categories": "cs.LG", "Comments": ["14 pages", "5 figures"]}, "abstract": "Neural operators, as an efficient surrogate model for learning the solutions of PDEs, have received extensive attention in the field of scientific machine learning. Among them, attention-based neural operators have become one of the mainstreams in related research. However, existing approaches overfit the limited training data due to the considerable number of parameters in the attention mechanism. To address this, we develop an orthogonal attention based on the eigendecomposition of the kernel integral operator and the neural approximation of eigenfunctions. The orthogonalization naturally poses a proper regularization effect on the resulting neural operator, which aids in resisting overfitting and boosting generalization. Experiments on six standard neural operator benchmark datasets comprising both regular and irregular geometries show that our method can outperform competing baselines with decent margins.", "url": "https://arxiv.org/abs/2310.12487"}, {"metadata": {"arXiv": "2310.12494", "Date": "Thu, 19 Oct 2023 05:56:25 ", "Title": "SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models", "Authors": ["Emmanuel Klu", "Sameer Sethi", "DJ Passey and Donald Martin Jr"], "Categories": "cs.LG", "Comments": ["Preprint"]}, "abstract": "Understanding the long-term impact of algorithmic interventions on society is vital to achieving responsible AI. Traditional evaluation strategies often fall short due to the complex, adaptive and dynamic nature of society. While reinforcement learning (RL) can be a powerful approach for optimizing decisions in dynamic settings, the difficulty of realistic environment design remains a barrier to building robust agents that perform well in practical settings. To address this issue we tap into the field of system dynamics (SD) as a complementary method that incorporates collaborative simulation model specification practices. We introduce SDGym, a low-code library built on the OpenAI Gym framework which enables the generation of custom RL environments based on SD simulation models. Through a feasibility study we validate that well specified, rich RL environments can be generated from preexisting SD models and a few lines of configuration code. We demonstrate the capabilities of the SDGym environment using an SD model of the electric vehicle adoption problem. We compare two SD simulators, PySD and BPTK-Py for parity, and train a D4PG agent using the Acme framework to showcase learning and environment interaction. Our preliminary findings underscore the dual potential of SD to improve RL environment design and for RL to improve dynamic policy discovery within SD models. By open-sourcing SDGym, the intent is to galvanize further research and promote adoption across the SD and RL communities, thereby catalyzing collaboration in this emerging interdisciplinary space.", "url": "https://arxiv.org/abs/2310.12494"}, {"metadata": {"arXiv": "2310.12498", "Date": "Thu, 19 Oct 2023 06:04:48 ", "Title": "Quasi Manhattan Wasserstein Distance", "Authors": ["Evan Unit Lim"], "Categories": "cs.LG cs.NA math.NA"}, "abstract": "The Quasi Manhattan Wasserstein Distance (QMWD) is a metric designed to quantify the dissimilarity between two matrices by combining elements of the Wasserstein Distance with specific transformations. It offers improved time and space complexity compared to the Manhattan Wasserstein Distance (MWD) while maintaining accuracy. QMWD is particularly advantageous for large datasets or situations with limited computational resources. This article provides a detailed explanation of QMWD, its computation, complexity analysis, and comparisons with WD and MWD.", "url": "https://arxiv.org/abs/2310.12498"}, {"metadata": {"arXiv": "2310.12515", "Date": "Thu, 19 Oct 2023 06:32:12 ", "Title": "WeaveNet for Approximating Two-sided Matching Problems", "Authors": ["Shusaku Sone", "Jiaxin Ma", "Atsushi Hashimoto", "Naoya Chiba", "Yoshitaka Ushiku"], "Categories": "cs.LG"}, "abstract": "Matching, a task to optimally assign limited resources under constraints, is a fundamental technology for society. The task potentially has various objectives, conditions, and constraints; however, the efficient neural network architecture for matching is underexplored. This paper proposes a novel graph neural network (GNN), \\textit{WeaveNet}, designed for bipartite graphs. Since a bipartite graph is generally dense, general GNN architectures lose node-wise information by over-smoothing when deeply stacked. Such a phenomenon is undesirable for solving matching problems. WeaveNet avoids it by preserving edge-wise information while passing messages densely to reach a better solution. To evaluate the model, we approximated one of the \\textit{strongly NP-hard} problems, \\textit{fair stable matching}. Despite its inherent difficulties and the network's general purpose design, our model reached a comparative performance with state-of-the-art algorithms specially designed for stable matching for small numbers of agents.", "url": "https://arxiv.org/abs/2310.12515"}, {"metadata": {"arXiv": "2310.12526", "Date": "Thu, 19 Oct 2023 07:03:51 ", "Title": "Parallel Bayesian Optimization Using Satisficing Thompson Sampling for Time-Sensitive Black-Box Optimization", "Authors": ["Xiaobin Song", "Benben Jiang"], "Categories": "cs.LG cs.SY eess.SY"}, "abstract": "Bayesian optimization (BO) is widely used for black-box optimization problems, and have been shown to perform well in various real-world tasks. However, most of the existing BO methods aim to learn the optimal solution, which may become infeasible when the parameter space is extremely large or the problem is time-sensitive. In these contexts, switching to a satisficing solution that requires less information can result in better performance. In this work, we focus on time-sensitive black-box optimization problems and propose satisficing Thompson sampling-based parallel Bayesian optimization (STS-PBO) approaches, including synchronous and asynchronous versions. We shift the target from an optimal solution to a satisficing solution that is easier to learn. The rate-distortion theory is introduced to construct a loss function that balances the amount of information that needs to be learned with sub-optimality, and the Blahut-Arimoto algorithm is adopted to compute the target solution that reaches the minimum information rate under the distortion limit at each step. Both discounted and undiscounted Bayesian cumulative regret bounds are theoretically derived for the proposed STS-PBO approaches. The effectiveness of the proposed methods is demonstrated on a fast-charging design problem of Lithium-ion batteries. The results are accordant with theoretical analyses, and show that our STS-PBO methods outperform both sequential counterparts and parallel BO with traditional Thompson sampling in both synchronous and asynchronous settings.", "url": "https://arxiv.org/abs/2310.12526"}, {"metadata": {"arXiv": "2310.12553", "Date": "Thu, 19 Oct 2023 08:02:40 ", "Title": "Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers", "Authors": ["uya Yoshikawa", "Tomoharu Iwata"], "Categories": "cs.LG cs.CV stat.ML"}, "abstract": "The quality of explanations for the predictions of complex machine learning predictors is often measured using insertion and deletion metrics, which assess the faithfulness of the explanations, i.e., how correctly the explanations reflect the predictor's behavior. To improve the faithfulness, we propose insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which optimizes differentiable predictors to improve both insertion and deletion scores of the explanations while keeping their predictive accuracy. Since the original insertion and deletion metrics are indifferentiable with respect to the explanations and directly unavailable for gradient-based optimization, we extend the metrics to be differentiable and use them to formalize insertion and deletion metric-based regularizers. The experimental results on image and tabular datasets show that the deep neural networks-based predictors fine-tuned using ID-ExpO enable popular post-hoc explainers to produce more faithful and easy-to-interpret explanations while keeping high predictive accuracy.", "url": "https://arxiv.org/abs/2310.12553"}, {"metadata": {"arXiv": "2310.12560", "Date": "Thu, 19 Oct 2023 08:10:57 ", "Title": "Fast Model Debias with Machine Unlearning", "Authors": ["Ruizhe Chen", "Jianfei Yang", "Huimin Xiong", "Jianhong Bai", "Tianxiang Hu", "Jin Hao", "Yang Feng", "Joey Tianyi Zhou", "Jian Wu", "Zuozhu Liu"], "Categories": "cs.LG", "Comments": ["accepted by NIPS 2023"]}, "abstract": "Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. Moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets along with experiments with large language models demonstrate that our method achieves superior or competing accuracies compared with state-of-the-art methods while attaining significantly fewer biases and requiring much less debiasing cost. Notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice.", "url": "https://arxiv.org/abs/2310.12560"}, {"metadata": {"arXiv": "2310.12565", "Date": "Thu, 19 Oct 2023 08:18:10 ", "Title": "Open-World Lifelong Graph Learning", "Authors": ["Marcel Hoffmann", "Lukas Galke", "Ansgar Scherp"], "Categories": "cs.LG", "DOI": "10.1109/IJCNN54540.2023.10191071"}, "abstract": "We study the problem of lifelong graph learning in an open-world scenario, where a model needs to deal with new tasks and potentially unknown classes. We utilize Out-of-Distribution (OOD) detection methods to recognize new classes and adapt existing non-graph OOD detection methods to graph data. Crucially, we suggest performing new class detection by combining OOD detection methods with information aggregated from the graph neighborhood. Most OOD detection methods avoid determining a crisp threshold for deciding whether a vertex is OOD. To tackle this problem, we propose a Weakly-supervised Relevance Feedback (Open-WRF) method, which decreases the sensitivity to thresholds in OOD detection. We evaluate our approach on six benchmark datasets. Our results show that the proposed neighborhood aggregation method for OOD scores outperforms existing methods independent of the underlying graph neural network. Furthermore, we demonstrate that our Open-WRF method is more robust to threshold selection and analyze the influence of graph neighborhood on OOD detection. The aggregation and threshold methods are compatible with arbitrary graph neural networks and OOD detection methods, making our approach versatile and applicable to many real-world applications.", "url": "https://arxiv.org/abs/2310.12565"}, {"metadata": {"arXiv": "2310.12568", "Date": "Thu, 19 Oct 2023 08:21:12 ", "Title": "Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models", "Authors": ["Sami Hamdan", "Shammi More", "Leonard Sasse", "Vera Komeyer", "Kaustubh R. Patil and Federico Raimondo (for the Alzheimer's Disease Neuroimaging Initiative)"], "Categories": "cs.LG q-bio.NC", "Comments": ["13 pages", "5 figures"]}, "abstract": "The fast-paced development of machine learning (ML) methods coupled with its increasing adoption in research poses challenges for researchers without extensive training in ML. In neuroscience, for example, ML can help understand brain-behavior relationships, diagnose diseases, and develop biomarkers using various data sources like magnetic resonance imaging and electroencephalography. The primary objective of ML is to build models that can make accurate predictions on unseen data. Researchers aim to prove the existence of such generalizable models by evaluating performance using techniques such as cross-validation (CV), which uses systematic subsampling to estimate the generalization performance. Choosing a CV scheme and evaluating an ML pipeline can be challenging and, if used improperly, can lead to overestimated results and incorrect interpretations. We created julearn, an open-source Python library, that allow researchers to design and evaluate complex ML pipelines without encountering in common pitfalls. In this manuscript, we present the rationale behind julearn's design, its core features, and showcase three examples of previously-published research projects that can be easily implemented using this novel library. Julearn aims to simplify the entry into the ML world by providing an easy-to-use environment with built in guards against some of the most common ML pitfalls. With its design, unique features and simple interface, it poses as a useful Python-based library for research projects.", "url": "https://arxiv.org/abs/2310.12568"}, {"metadata": {"arXiv": "2310.12595", "Date": "Thu, 19 Oct 2023 09:03:41 ", "Title": "Causal Similarity-Based Hierarchical Bayesian Models", "Authors": ["Sophie Wharrie", "Samuel Kaski"], "Categories": "cs.LG stat.ML"}, "abstract": "The key challenge underlying machine learning is generalisation to new data. This work studies generalisation for datasets consisting of related tasks that may differ in causal mechanisms. For example, observational medical data for complex diseases suffers from heterogeneity in causal mechanisms of disease across patients, creating challenges for machine learning algorithms that need to generalise to new patients outside of the training dataset. Common approaches for learning supervised models with heterogeneous datasets include learning a global model for the entire dataset, learning local models for each tasks' data, or utilising hierarchical, meta-learning and multi-task learning approaches to learn how to generalise from data pooled across multiple tasks. In this paper we propose causal similarity-based hierarchical Bayesian models to improve generalisation to new tasks by learning how to pool data from training tasks with similar causal mechanisms. We apply this general modelling principle to Bayesian neural networks and compare a variety of methods for estimating causal task similarity (for both known and unknown causal models). We demonstrate the benefits of our approach and applicability to real world problems through a range of experiments on simulated and real data.", "url": "https://arxiv.org/abs/2310.12595"}, {"metadata": {"arXiv": "2310.12612", "Date": "Thu, 19 Oct 2023 09:40:30 ", "Title": "How a student becomes a teacher: learning and forgetting through Spectral methods", "Authors": ["Lorenzo Giambagli", "Lorenzo Buffoni", "Lorenzo Chicchi", "Duccio Fanelli"], "Categories": "cs.LG cs.NE stat.ML", "Comments": ["10 pages + references + supplemental material. Poster presentation at NeurIPS 2023"], "Journal-ref": "NeurIPS 2023"}, "abstract": "In theoretical ML, the teacher-student paradigm is often employed as an effective metaphor for real-life tuition. The above scheme proves particularly relevant when the student network is overparameterized as compared to the teacher network. Under these operating conditions, it is tempting to speculate that the student ability to handle the given task could be eventually stored in a sub-portion of the whole network. This latter should be to some extent reminiscent of the frozen teacher structure, according to suitable metrics, while being approximately invariant across different architectures of the student candidate network. Unfortunately, state-of-the-art conventional learning techniques could not help in identifying the existence of such an invariant subnetwork, due to the inherent degree of non-convexity that characterizes the examined problem. In this work, we take a leap forward by proposing a radically different optimization scheme which builds on a spectral representation of the linear transfer of information between layers. The gradient is hence calculated with respect to both eigenvalues and eigenvectors with negligible increase in terms of computational and complexity load, as compared to standard training algorithms. Working in this framework, we could isolate a stable student substructure, that mirrors the true complexity of the teacher in terms of computing neurons, path distribution and topological attributes. When pruning unimportant nodes of the trained student, as follows a ranking that reflects the optimized eigenvalues, no degradation in the recorded performance is seen above a threshold that corresponds to the effective teacher size. The observed behavior can be pictured as a genuine second-order phase transition that bears universality traits.", "url": "https://arxiv.org/abs/2310.12612"}, {"metadata": {"arXiv": "2310.12660", "Date": "Thu, 19 Oct 2023 11:33:33 ", "Title": "Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic", "Authors": ["Rustem Takhanov", "Maxat Tezekbayev", "Artur Pak", "Arman Bolatov", "Zhenisbek Assylbekov"], "Categories": "cs.LG"}, "abstract": "Classes of target functions containing a large number of approximately orthogonal elements are known to be hard to learn by the Statistical Query algorithms. Recently this classical fact re-emerged in a theory of gradient-based optimization of neural networks. In the novel framework, the hardness of a class is usually quantified by the variance of the gradient with respect to a random choice of a target function. A set of functions of the form $x\\to ax \\bmod p$, where $a$ is taken from ${\\mathbb Z}_p$, has attracted some attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of $p$-periodic functions on ${\\mathbb Z}$ and is tightly connected with a class of high-frequency periodic functions on the real line. We present a mathematical analysis of limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We highlight that the variance of the gradient is negligibly small in both cases when either a frequency or the prime base $p$ is large. This in turn prevents such a learning algorithm from being successful.", "url": "https://arxiv.org/abs/2310.12660"}, {"metadata": {"arXiv": "2310.12663", "Date": "Thu, 19 Oct 2023 11:41:52 ", "Title": "Knowledge from Uncertainty in Evidential Deep Learning", "Authors": ["Cai Davies", "Marc Roig Vilamala", "Alun D. Preece", "Federico Cerutti", "Lance M. Kaplan", "Supriyo Chakraborty"], "Categories": "cs.LG"}, "abstract": "This work reveals an evidential signal that emerges from the uncertainty value in Evidential Deep Learning (EDL). EDL is one example of a class of uncertainty-aware deep learning approaches designed to provide confidence (or epistemic uncertainty) about the current test sample. In particular for computer vision and bidirectional encoder large language models, the `evidential signal' arising from the Dirichlet strength in EDL can, in some cases, discriminate between classes, which is particularly strong when using large language models. We hypothesise that the KL regularisation term causes EDL to couple aleatoric and epistemic uncertainty. In this paper, we empirically investigate the correlations between misclassification and evaluated uncertainty, and show that EDL's `evidential signal' is due to misclassification bias. We critically evaluate EDL with other Dirichlet-based approaches, namely Generative Evidential Neural Networks (EDL-GEN) and Prior Networks, and show theoretically and empirically the differences between these loss functions. We conclude that EDL's coupling of uncertainty arises from these differences due to the use (or lack) of out-of-distribution samples during training.", "url": "https://arxiv.org/abs/2310.12663"}, {"metadata": {"arXiv": "2310.12671", "Date": "Thu, 19 Oct 2023 12:00:33 ", "Title": "Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff", "Authors": ["Freek Holvoet", "Katrien Antonio and Roel Henckaerts"], "Categories": "cs.LG q-fin.RM"}, "abstract": "Insurers usually turn to generalized linear models for modelling claim frequency and severity data. Due to their success in other fields, machine learning techniques are gaining popularity within the actuarial toolbox. Our paper contributes to the literature on frequency-severity insurance pricing with machine learning via deep learning structures. We present a benchmark study on four insurance data sets with frequency and severity targets in the presence of multiple types of input features. We compare in detail the performance of: a generalized linear model on binned input data, a gradient-boosted tree model, a feed-forward neural network (FFNN), and the combined actuarial neural network (CANN). Our CANNs combine a baseline prediction established with a GLM and GBM, respectively, with a neural network correction. We explain the data preprocessing steps with specific focus on the multiple types of input features typically present in tabular insurance data sets, such as postal codes, numeric and categorical covariates. Autoencoders are used to embed the categorical variables into the neural network and we explore their potential advantages in a frequency-severity setting. Finally, we construct global surrogate models for the neural nets' frequency and severity models. These surrogates enable the translation of the essential insights captured by the FFNNs or CANNs to GLMs. As such, a technical tariff table results that can easily be deployed in practice.", "url": "https://arxiv.org/abs/2310.12671"}, {"metadata": {"arXiv": "2310.12680", "Date": "Thu, 19 Oct 2023 12:18:24 ", "Title": "On the Optimization and Generalization of Multi-head Attention", "Authors": ["Puneesh Deora", "Rouzbeh Ghaderi", "Hossein Taheri", "Christos Thrampoulidis"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["48 page; presented in the Workshop on High-dimensional Learning Dynamics", "ICML 2023"]}, "abstract": "The training and generalization dynamics of the Transformer's core mechanism, namely the Attention mechanism, remain under-explored. Besides, existing analyses primarily focus on single-head attention. Inspired by the demonstrated benefits of overparameterization when training fully-connected networks, we investigate the potential optimization and generalization advantages of using multiple attention heads. Towards this goal, we derive convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model, under a suitable realizability condition on the data. We then establish primitive conditions on the initialization that ensure realizability holds. Finally, we demonstrate that these conditions are satisfied for a simple tokenized-mixture model. We expect the analysis can be extended to various data-model and architecture variations.", "url": "https://arxiv.org/abs/2310.12680"}, {"metadata": {"arXiv": "2310.12713", "Date": "Thu, 19 Oct 2023 13:13:41 ", "Title": "Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness", "Authors": ["Yaohua Liu", "Jiaxin Gao", "Zhu Liu", "Xianghao Jiao", "Xin Fan", "Risheng Liu"], "Categories": "cs.LG", "Comments": ["16 Pages"]}, "abstract": "In light of the vulnerability of deep learning models to adversarial samples and the ensuing security issues, a range of methods, including Adversarial Training (AT) as a prominent representative, aimed at enhancing model robustness against various adversarial attacks, have seen rapid development. However, existing methods essentially assist the current state of target model to defend against parameter-oriented adversarial attacks with explicit or implicit computation burdens, which also suffers from unstable convergence behavior due to inconsistency of optimization trajectories. Diverging from previous work, this paper reconsiders the update rule of target model and corresponding deficiency to defend based on its current state. By introducing the historical state of the target model as a proxy, which is endowed with much prior information for defense, we formulate a two-stage update rule, resulting in a general adversarial defense framework, which we refer to as `LAST' ({\\bf L}earn from the P{\\bf ast}). Besides, we devise a Self Distillation (SD) based defense objective to constrain the update process of the proxy model without the introduction of larger teacher models. Experimentally, we demonstrate consistent and significant performance enhancements by refining a series of single-step and multi-step AT methods (e.g., up to $\\bf 9.2\\%$ and $\\bf 20.5\\%$ improvement of Robust Accuracy (RA) on CIFAR10 and CIFAR100 datasets, respectively) across various datasets, backbones and attack modalities, and validate its ability to enhance training stability and ameliorate catastrophic overfitting issues meanwhile.", "url": "https://arxiv.org/abs/2310.12713"}, {"metadata": {"arXiv": "2310.12746", "Date": "Thu, 19 Oct 2023 13:50:56 ", "Title": "TabuLa: Harnessing Language Models for Tabular Data Synthesis", "Authors": ["Zilong Zhao", "Robert Birke and Lydia Chen"], "Categories": "cs.LG"}, "abstract": "Given the ubiquitous use of tabular data in industries and the growing concerns in data privacy and security, tabular data synthesis emerges as a critical research area. The recent state-of-the-art methods show that large language models (LLMs) can be adopted to generate realistic tabular data. As LLMs pre-process tabular data as full text, they have the advantage of avoiding the curse of dimensionality associated with one-hot encoding high-dimensional data. However, their long training time and limited re-usability on new tasks prevent them from replacing exiting tabular generative models. In this paper, we propose Tabula, a tabular data synthesizer based on the language model structure. Through Tabula, we demonstrate the inherent limitation of employing pre-trained language models designed for natural language processing (NLP) in the context of tabular data synthesis. Our investigation delves into the development of a dedicated foundational model tailored specifically for tabular data synthesis. Additionally, we propose a token sequence compression strategy to significantly reduce training time while preserving the quality of synthetic data. Extensive experiments on six datasets demonstrate that using a language model structure without loading the well-trained model weights yields a better starting model for tabular data synthesis. Moreover, the Tabula model, previously trained on other tabular data, serves as an excellent foundation model for new tabular data synthesis tasks. Additionally, the token sequence compression method substantially reduces the model's training time. Results show that Tabula averagely reduces 46.2% training time per epoch comparing to current LLMs-based state-of-the-art algorithm and consistently achieves even higher synthetic data utility.", "url": "https://arxiv.org/abs/2310.12746"}, {"metadata": {"arXiv": "2310.12752", "Date": "Thu, 19 Oct 2023 13:57:38 ", "Title": "Discretize Relaxed Solution of Spectral Clustering via a Non-Heuristic Algorithm", "Authors": ["Hongyuan Zhang and Xuelong Li"], "Categories": "cs.LG"}, "abstract": "Spectral clustering and its extensions usually consist of two steps: (1) constructing a graph and computing the relaxed solution; (2) discretizing relaxed solutions. Although the former has been extensively investigated, the discretization techniques are mainly heuristic methods, e.g., k-means, spectral rotation. Unfortunately, the goal of the existing methods is not to find a discrete solution that minimizes the original objective. In other words, the primary drawback is the neglect of the original objective when computing the discrete solution. Inspired by the first-order optimization algorithms, we propose to develop a first-order term to bridge the original problem and discretization algorithm, which is the first non-heuristic to the best of our knowledge. Since the non-heuristic method is aware of the original graph cut problem, the final discrete solution is more reliable and achieves the preferable loss value. We also theoretically show that the continuous optimum is beneficial to discretization algorithms though simply finding its closest discrete solution is an existing heuristic algorithm which is also unreliable. Sufficient experiments significantly show the superiority of our method.", "url": "https://arxiv.org/abs/2310.12752"}, {"metadata": {"arXiv": "2310.12771", "Date": "Thu, 27 Jul 2023 17:34:26 ", "Title": "Stochastic Average Gradient : A Simple Empirical Investigation", "Authors": ["Pascal Junior Tikeng Notsawo"], "Categories": "cs.LG math.OC", "Comments": ["37 pages", "52 figures. arXiv admin note: substantial text overlap with arXiv:1309.2388 by other authors"], "ACM-class": "G.1.6; I.2.6; I.2.8"}, "abstract": "Despite the recent growth of theoretical studies and empirical successes of neural networks, gradient backpropagation is still the most widely used algorithm for training such networks. On the one hand, we have deterministic or full gradient (FG) approaches that have a cost proportional to the amount of training data used but have a linear convergence rate, and on the other hand, stochastic gradient (SG) methods that have a cost independent of the size of the dataset, but have a less optimal convergence rate than the determinist approaches. To combine the cost of the stochastic approach with the convergence rate of the deterministic approach, a stochastic average gradient (SAG) has been proposed. SAG is a method for optimizing the sum of a finite number of smooth convex functions. Like SG methods, the SAG method's iteration cost is independent of the number of terms in the sum. In this work, we propose to compare SAG to some standard optimizers used in machine learning. SAG converges faster than other optimizers on simple toy problems and performs better than many other optimizers on simple machine learning problems. We also propose a combination of SAG with the momentum algorithm and Adam. These combinations allow empirically higher speed and obtain better performance than the other methods, especially when the landscape of the function to optimize presents obstacles or is ill-conditioned.", "url": "https://arxiv.org/abs/2310.12771"}, {"metadata": {"arXiv": "2310.12785", "Date": "Thu, 19 Oct 2023 14:35:26 ", "Title": "A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off Pareto Frontier", "Authors": ["Hua Tang", "Lu Cheng", "Ninghao Liu", "Mengnan Du"], "Categories": "cs.LG cs.CY"}, "abstract": "While the accuracy-fairness trade-off has been frequently observed in the literature of fair machine learning, rigorous theoretical analyses have been scarce. To demystify this long-standing challenge, this work seeks to develop a theoretical framework by characterizing the shape of the accuracy-fairness trade-off Pareto frontier (FairFrontier), determined by a set of all optimal Pareto classifiers that no other classifiers can dominate. Specifically, we first demonstrate the existence of the trade-off in real-world scenarios and then propose four potential categories to characterize the important properties of the accuracy-fairness Pareto frontier. For each category, we identify the necessary conditions that lead to corresponding trade-offs. Experimental results on synthetic data suggest insightful findings of the proposed framework: (1) When sensitive attributes can be fully interpreted by non-sensitive attributes, FairFrontier is mostly continuous. (2) Accuracy can suffer a \\textit{sharp} decline when over-pursuing fairness. (3) Eliminate the trade-off via a two-step streamlined approach. The proposed research enables an in-depth understanding of the accuracy-fairness trade-off, pushing current fair machine-learning research to a new frontier.", "url": "https://arxiv.org/abs/2310.12785"}, {"metadata": {"arXiv": "2310.12793", "Date": "Thu, 19 Oct 2023 14:50:46 ", "Title": "OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift", "Authors": ["Lin Li", "Yifei Wang", "Chawin Sitawarin", "Michael Spratling"], "Categories": "cs.LG cs.CV", "Comments": ["in submission"]}, "abstract": "Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear way, under many distribution shifts. The latter enables the prediction of OOD robustness from ID robustness. Based on this, we are able to predict the upper limit of OOD robustness for existing robust training schemes. The results suggest that achieving OOD robustness requires designing novel methods beyond the conventional ones. Last, we discover that extra data, data augmentation, advanced model architectures and particular regularization approaches can improve OOD robustness. Noticeably, the discovered training schemes, compared to the baseline, exhibit dramatically higher robustness under threat shift while keeping high ID robustness, demonstrating new promising solutions for robustness against both multi-attack and unforeseen attacks.", "url": "https://arxiv.org/abs/2310.12793"}, {"metadata": {"arXiv": "2310.12803", "Date": "Thu, 19 Oct 2023 14:59:25 ", "Title": "Causal-structure Driven Augmentations for Text OOD Generalization", "Authors": ["Amir Feder", "Yoav Wald", "Claudia Shi", "Suchi Saria", "David Blei"], "Categories": "cs.LG cs.CL", "Comments": ["Forthcoming in NeurIPS 2023"]}, "abstract": "The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic data, we demonstrate that our method for simulating interventions improves out-of-distribution (OOD) accuracy compared to baseline invariant learning algorithms.", "url": "https://arxiv.org/abs/2310.12803"}, {"metadata": {"arXiv": "2310.12805", "Date": "Thu, 19 Oct 2023 15:01:16 ", "Title": "Detection and Evaluation of bias-inducing Features in Machine learning", "Authors": ["Moses Openja", "Gabriel Laberge", "Foutse Khomh"], "Categories": "cs.LG", "Comments": ["65 pages", "manuscript accepted at EMSE journal", "manuscript number", "EMSE-D-22-00330R3"]}, "abstract": "The cause-to-effect analysis can help us decompose all the likely causes of a problem, such as an undesirable business situation or unintended harm to the individual(s). This implies that we can identify how the problems are inherited, rank the causes to help prioritize fixes, simplify a complex problem and visualize them. In the context of machine learning (ML), one can use cause-to-effect analysis to understand the reason for the biased behavior of the system. For example, we can examine the root causes of biases by checking each feature for a potential cause of bias in the model. To approach this, one can apply small changes to a given feature or a pair of features in the data, following some guidelines and observing how it impacts the decision made by the model (i.e., model prediction). Therefore, we can use cause-to-effect analysis to identify the potential bias-inducing features, even when these features are originally are unknown. This is important since most current methods require a pre-identification of sensitive features for bias assessment and can actually miss other relevant bias-inducing features, which is why systematic identification of such features is necessary. Moreover, it often occurs that to achieve an equitable outcome, one has to take into account sensitive features in the model decision. Therefore, it should be up to the domain experts to decide based on their knowledge of the context of a decision whether bias induced by specific features is acceptable or not. In this study, we propose an approach for systematically identifying all bias-inducing features of a model to help support the decision-making of domain experts. We evaluated our technique using four well-known datasets to showcase how our contribution can help spearhead the standard procedure when developing, testing, maintaining, and deploying fair/equitable machine learning systems.", "url": "https://arxiv.org/abs/2310.12805"}, {"metadata": {"arXiv": "2310.12809", "Date": "Thu, 19 Oct 2023 15:06:31 ", "Title": "Hierarchical Forecasting at Scale", "Authors": ["Olivier Sprangers", "Wander Wadman", "Sebastian Schelter", "Maarten de Rijke"], "Categories": "cs.LG"}, "abstract": "Existing hierarchical forecasting techniques scale poorly when the number of time series increases. We propose to learn a coherent forecast for millions of time series with a single bottom-level forecast model by using a sparse loss function that directly optimizes the hierarchical product and/or temporal structure. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen cross-sectional or temporal hierarchy. In addition, removing the need for a post-processing step as required in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline. On the public M5 dataset, our sparse hierarchical loss function performs up to 10% (RMSE) better compared to the baseline loss function. We implement our sparse hierarchical loss function within an existing forecasting model at bol, a large European e-commerce platform, resulting in an improved forecasting performance of 2% at the product level. Finally, we found an increase in forecasting performance of about 5-10% when evaluating the forecasting performance across the cross-sectional hierarchies that we defined. These results demonstrate the usefulness of our sparse hierarchical loss applied to a production forecasting system at a major e-commerce platform.", "url": "https://arxiv.org/abs/2310.12809"}, {"metadata": {"arXiv": "2310.12862", "Date": "Thu, 19 Oct 2023 16:11:49 ", "Title": "Fine-Tuning Generative Models as an Inference Method for Robotic Tasks", "Authors": ["Orr Krupnik", "Elisei Shafer", "Tom Jurgenson", "Aviv Tamar"], "Categories": "cs.LG cs.RO", "Comments": ["7th Conference on Robot Learning", "2023. Project website at https://www.orrkrup.com/mace"]}, "abstract": "Adaptable models could greatly benefit robotic agents operating in the real world, allowing them to deal with novel and varying conditions. While approaches such as Bayesian inference are well-studied frameworks for adapting models to evidence, we build on recent advances in deep generative models which have greatly affected many areas of robotics. Harnessing modern GPU acceleration, we investigate how to quickly adapt the sample generation of neural network models to observations in robotic tasks. We propose a simple and general method that is applicable to various deep generative models and robotic environments. The key idea is to quickly fine-tune the model by fitting it to generated samples matching the observed evidence, using the cross-entropy method. We show that our method can be applied to both autoregressive models and variational autoencoders, and demonstrate its usability in object shape inference from grasping, inverse kinematics calculation, and point cloud completion.", "url": "https://arxiv.org/abs/2310.12862"}, {"metadata": {"arXiv": "2310.12929", "Date": "Thu, 19 Oct 2023 17:28:37 ", "Title": "Probabilistic Modeling of Human Teams to Infer False Beliefs", "Authors": ["Paulo Soares", "Adarsh Pyarelal", "Kobus Barnard"], "Categories": "cs.LG", "Comments": ["8 pages", "7 figures", "presented in the 2021 AAAI Fall Symposium"]}, "abstract": "We develop a probabilistic graphical model (PGM) for artificially intelligent (AI) agents to infer human beliefs during a simulated urban search and rescue (USAR) scenario executed in a Minecraft environment with a team of three players. The PGM approach makes observable states and actions explicit, as well as beliefs and intentions grounded by evidence about what players see and do over time. This approach also supports inferring the effect of interventions, which are vital if AI agents are to assist human teams. The experiment incorporates manipulations of players' knowledge, and the virtual Minecraft-based testbed provides access to several streams of information, including the objects in the players' field of view. The participants are equipped with a set of marker blocks that can be placed near room entrances to signal the presence or absence of victims in the rooms to their teammates. In each team, one of the members is given a different legend for the markers than the other two, which may mislead them about the state of the rooms; that is, they will hold a false belief. We extend previous works in this field by introducing ToMCAT, an AI agent that can reason about individual and shared mental states. We find that the players' behaviors are affected by what they see in their in-game field of view, their beliefs about the meaning of the markers, and their beliefs about which meaning the team decided to adopt. In addition, we show that ToMCAT's beliefs are consistent with the players' actions and that it can infer false beliefs with accuracy significantly better than chance and comparable to inferences made by human observers.", "url": "https://arxiv.org/abs/2310.12929"}, {"metadata": {"arXiv": "2310.12934", "Date": "Thu, 19 Oct 2023 17:31:40 ", "Title": "Generative Flow Networks as Entropy-Regularized RL", "Authors": ["Daniil Tiapkin", "Nikita Morozov", "Alexey Naumov", "Dmitry Vetrov"], "Categories": "cs.LG stat.ML"}, "abstract": "The recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating reinforcement learning principles into the realm of generative flow networks.", "url": "https://arxiv.org/abs/2310.12934"}, {"metadata": {"arXiv": "2310.12952", "Date": "Thu, 19 Oct 2023 17:52:04 ", "Title": "Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning", "Authors": ["Amey Pasarkar and Adji Bousso Dieng"], "Categories": "cs.LG physics.chem-ph q-bio.PE", "Comments": ["Code for evaluating diversity using the Vendi scores can be found at https://github.com/vertaix/Vendi-Score. Code for using the scores within Vendi Sampling can be found at https://github.com/vertaix/Vendi-Sampling"]}, "abstract": "Measuring diversity accurately is important for many scientific fields, including machine learning (ML), ecology, and chemistry. The Vendi Score was introduced as a generic similarity-based diversity metric that extends the Hill number of order q=1 by leveraging ideas from quantum statistical mechanics. Contrary to many diversity metrics in ecology, the Vendi Score accounts for similarity and does not require knowledge of the prevalence of the categories in the collection to be evaluated for diversity. However, the Vendi Score treats each item in a given collection with a level of sensitivity proportional to the item's prevalence. This is undesirable in settings where there is a significant imbalance in item prevalence. In this paper, we extend the other Hill numbers using similarity to provide flexibility in allocating sensitivity to rare or common items. This leads to a family of diversity metrics -- Vendi scores with different levels of sensitivity -- that can be used in a variety of applications. We study the properties of the scores in a synthetic controlled setting where the ground truth diversity is known. We then test their utility in improving molecular simulations via Vendi Sampling. Finally, we use the Vendi scores to better understand the behavior of image generative models in terms of memorization, duplication, diversity, and sample quality.", "url": "https://arxiv.org/abs/2310.12952"}, {"metadata": {"arXiv": "2310.12969", "Date": "Thu, 19 Oct 2023 17:58:59 ", "Title": "Demystifying the Myths and Legends of Nonconvex Convergence of SGD", "Authors": ["Aritra Dutta", "El Houcine Bergou", "Soumia Boucherouite", "Nicklas Werge", "Melih Kandemir", "Xin Li"], "Categories": "cs.LG cs.NA math.NA math.OC"}, "abstract": "Stochastic gradient descent (SGD) and its variants are the main workhorses for solving large-scale optimization problems with nonconvex objective functions. Although the convergence of SGDs in the (strongly) convex case is well-understood, their convergence for nonconvex functions stands on weak mathematical foundations. Most existing studies on the nonconvex convergence of SGD show the complexity results based on either the minimum of the expected gradient norm or the functional sub-optimality gap (for functions with extra structural property) by searching the entire range of iterates. Hence the last iterations of SGDs do not necessarily maintain the same complexity guarantee. This paper shows that an $\\epsilon$-stationary point exists in the final iterates of SGDs, given a large enough total iteration budget, $T$, not just anywhere in the entire range of iterates -- a much stronger result than the existing one. Additionally, our analyses allow us to measure the density of the $\\epsilon$-stationary points in the final iterates of SGD, and we recover the classical $O(\\frac{1}{\\sqrt{T}})$ asymptotic rate under various existing assumptions on the objective function and the bounds on the stochastic gradient. As a result of our analyses, we addressed certain myths and legends related to the nonconvex convergence of SGD and posed some thought-provoking questions that could set new directions for research.", "url": "https://arxiv.org/abs/2310.12969"}, {"metadata": {"arXiv": "2310.12977", "Date": "Thu, 19 Oct 2023 17:59:44 ", "Title": "Training Dynamics of Deep Network Linear Regions", "Authors": ["Ahmed Imtiaz Humayun", "Randall Balestriero", "Richard Baraniuk"], "Categories": "cs.LG", "Comments": ["14 pages", "14 figures"]}, "abstract": "The study of Deep Network (DN) training dynamics has largely focused on the evolution of the loss function, evaluated on or around train and test set data points. In fact, many DN phenomenon were first introduced in literature with that respect, e.g., double descent, grokking. In this study, we look at the training dynamics of the input space partition or linear regions formed by continuous piecewise affine DNs, e.g., networks with (leaky)ReLU nonlinearities. First, we present a novel statistic that encompasses the local complexity (LC) of the DN based on the concentration of linear regions inside arbitrary dimensional neighborhoods around data points. We observe that during training, the LC around data points undergoes a number of phases, starting with a decreasing trend after initialization, followed by an ascent and ending with a final descending trend. Using exact visualization methods, we come across the perplexing observation that during the final LC descent phase of training, linear regions migrate away from training and test samples towards the decision boundary, making the DN input-output nearly linear everywhere else. We also observe that the different LC phases are closely related to the memorization and generalization performance of the DN, especially during grokking.", "url": "https://arxiv.org/abs/2310.12977"}, {"metadata": {"arXiv": "2310.12359", "Date": "Wed, 18 Oct 2023 22:09:29 ", "Title": "MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits", "Authors": ["Yuhang Zhang", "Marcos Quinones-Grueiro", "Zhiyao Zhang", "Yanbing Wang", "William Barbour", "Gautam Biswas and Daniel Work"], "Categories": "cs.MA cs.LG"}, "abstract": "Variable speed limit (VSL) control is a promising traffic management strategy for enhancing safety and mobility. This work introduces MARVEL, a multi-agent reinforcement learning (MARL) framework for implementing large-scale VSL control on freeway corridors using only commonly available data. The agents learn through a reward structure that incorporates adaptability to traffic conditions, safety, and mobility; enabling coordination among the agents. The proposed framework scales to cover corridors with many gantries thanks to a parameter sharing among all VSL agents. The agents are trained in a microsimulation environment based on a short freeway stretch with 8 gantries spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no control scenario and enhances traffic mobility by 14.6% compared to a state-of-the-practice algorithm that has been deployed on I-24. An explainability analysis is undertaken to explore the learned policy under different traffic conditions and the results provide insights into the decision-making process of agents. Finally, we test the policy learned from the simulation-based experiments on real input data from I-24 to illustrate the potential deployment capability of the learned policy.", "url": "https://arxiv.org/abs/2310.12359"}, {"metadata": {"arXiv": "2310.12547", "Date": "Thu, 19 Oct 2023 07:54:30 ", "Title": "PGA: Personalizing Grasping Agents with Single Human-Robot Interaction", "Authors": ["Junghyun Kim", "Gi-Cheon Kang", "Jaein Kim", "Seoyun Yang", "Minjoon Jung", "Byoung-Tak Zhang"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["7 pages", "under review"]}, "abstract": "Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that ground and grasp objects based on natural language instructions. While robots capable of recognizing personal objects like \"my wallet\" can interact more naturally with non-expert users, current LCRG systems primarily limit robots to understanding only generic expressions. To this end, we introduce a task scenario GraspMine with a novel dataset that aims to locate and grasp personal objects given personal indicators via learning from a single human-robot interaction. To address GraspMine, we propose Personalized Grasping Agent (PGA), that learns personal objects by propagating user-given information through a Reminiscence-a collection of raw images from the user's environment. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in the Reminiscence by our proposed label propagation algorithm. Harnessing the information acquired from the interactions and the pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding model to grasp personal objects. Experiments on GraspMine show that PGA significantly outperforms baseline methods both in offline and online settings, signifying its effectiveness and personalization applicability on real-world scenarios. Finally, qualitative analysis shows the effectiveness of PGA through a detailed investigation of results in each phase.", "url": "https://arxiv.org/abs/2310.12547"}, {"metadata": {"arXiv": "2310.12290", "Date": "Wed, 18 Oct 2023 19:43:38 ", "Title": "Fact-based Agent modeling for Multi-Agent Reinforcement Learning", "Authors": ["Baofu Fang", "Caiming Zheng and Hao Wang"], "Categories": "cs.AI"}, "abstract": "In multi-agent systems, agents need to interact and collaborate with other agents in environments. Agent modeling is crucial to facilitate agent interactions and make adaptive cooperation strategies. However, it is challenging for agents to model the beliefs, behaviors, and intentions of other agents in non-stationary environment where all agent policies are learned simultaneously. In addition, the existing methods realize agent modeling through behavior cloning which assume that the local information of other agents can be accessed during execution or training. However, this assumption is infeasible in unknown scenarios characterized by unknown agents, such as competition teams, unreliable communication and federated learning due to privacy concerns. To eliminate this assumption and achieve agent modeling in unknown scenarios, Fact-based Agent modeling (FAM) method is proposed in which fact-based belief inference (FBI) network models other agents in partially observable environment only based on its local information. The reward and observation obtained by agents after taking actions are called facts, and FAM uses facts as reconstruction target to learn the policy representation of other agents through a variational autoencoder. We evaluate FAM on various Multiagent Particle Environment (MPE) and compare the results with several state-of-the-art MARL algorithms. Experimental results show that compared with baseline methods, FAM can effectively improve the efficiency of agent policy learning by making adaptive cooperation strategies in multi-agent reinforcement learning tasks, while achieving higher returns in complex competitive-cooperative mixed scenarios.", "url": "https://arxiv.org/abs/2310.12290"}, {"metadata": {"arXiv": "2310.12386", "Date": "Wed, 18 Oct 2023 23:53:51 ", "Title": "Online Learning and Planning in Cognitive Hierarchies", "Authors": ["Bernhard Hengst", "Maurice Pagnucco", "David Rajaratnam", "Claude Sammut", "Michael Thielscher"], "Categories": "cs.AI cs.RO"}, "abstract": "Complex robot behaviour typically requires the integration of multiple robotic and Artificial Intelligence (AI) techniques and components. Integrating such disparate components into a coherent system, while also ensuring global properties and behaviours, is a significant challenge for cognitive robotics. Using a formal framework to model the interactions between components can be an important step in dealing with this challenge. In this paper we extend an existing formal framework [Clark et al., 2016] to model complex integrated reasoning behaviours of robotic systems; from symbolic planning through to online learning of policies and transition systems. Furthermore the new framework allows for a more flexible modelling of the interactions between different reasoning components.", "url": "https://arxiv.org/abs/2310.12386"}, {"metadata": {"arXiv": "2310.12397", "Date": "Thu, 19 Oct 2023 00:56:37 ", "Title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems", "Authors": ["Kaya Stechly", "Matthew Marquez", "Subbarao Kambhampati"], "Categories": "cs.AI", "Comments": ["18 pages", "3 figures"]}, "abstract": "There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.", "url": "https://arxiv.org/abs/2310.12397"}, {"metadata": {"arXiv": "2310.12638", "Date": "Thu, 19 Oct 2023 10:53:06 ", "Title": "PSYCHIC: A Neuro-Symbolic Framework for Knowledge Graph Question-Answering Grounding", "Authors": ["Hanna Abi Akl"], "Categories": "cs.AI", "Comments": ["10 pages", "3 figures", "2 tables", "accepted for the Scholarly-QALD challenge at the International Semantic Web Conference (ISWC) 2023"]}, "abstract": "The Scholarly Question Answering over Linked Data (Scholarly QALD) at The International Semantic Web Conference (ISWC) 2023 challenge presents two sub-tasks to tackle question answering (QA) over knowledge graphs (KGs). We answer the KGQA over DBLP (DBLP-QUAD) task by proposing a neuro-symbolic (NS) framework based on PSYCHIC, an extractive QA model capable of identifying the query and entities related to a KG question. Our system achieved a F1 score of 00.18% on question answering and came in third place for entity linking (EL) with a score of 71.00%.", "url": "https://arxiv.org/abs/2310.12638"}, {"metadata": {"arXiv": "2310.12616", "Date": "Thu, 19 Oct 2023 09:49:58 ", "Title": "Cross-attention Spatio-temporal Context Transformer for Semantic Segmentation of Historical Maps", "Authors": ["Sidi Wu", "Yizi Chen", "Konrad Schindler", "Lorenz Hurni"], "Categories": "cs.CV cs.AI", "DOI": "10.1145/3589132.3625572"}, "abstract": "Historical maps provide useful spatio-temporal information on the Earth's surface before modern earth observation techniques came into being. To extract information from maps, neural networks, which gain wide popularity in recent years, have replaced hand-crafted map processing methods and tedious manual labor. However, aleatoric uncertainty, known as data-dependent uncertainty, inherent in the drawing/scanning/fading defects of the original map sheets and inadequate contexts when cropping maps into small tiles considering the memory limits of the training process, challenges the model to make correct predictions. As aleatoric uncertainty cannot be reduced even with more training data collected, we argue that complementary spatio-temporal contexts can be helpful. To achieve this, we propose a U-Net-based network that fuses spatio-temporal features with cross-attention transformers (U-SpaTem), aggregating information at a larger spatial range as well as through a temporal sequence of images. Our model achieves a better performance than other state-or-art models that use either temporal or spatial contexts. Compared with pure vision transformers, our model is more lightweight and effective. To the best of our knowledge, leveraging both spatial and temporal contexts have been rarely explored before in the segmentation task. Even though our application is on segmenting historical maps, we believe that the method can be transferred into other fields with similar problems like temporal sequences of satellite images. Our code is freely accessible at https://github.com/chenyizi086/wu.2023.sigspatial.git.", "url": "https://arxiv.org/abs/2310.12616"}, {"metadata": {"arXiv": "2310.12630", "Date": "Thu, 19 Oct 2023 10:27:08 ", "Title": "Heart Disease Detection using Vision-Based Transformer Models from ECG Images", "Authors": ["Zeynep Hilal Kilimci", "Mustafa Yalcin", "Ayhan Kucukmanisa and Amit Kumar Mishra"], "Categories": "cs.CV cs.AI"}, "abstract": "Heart disease, also known as cardiovascular disease, is a prevalent and critical medical condition characterized by the impairment of the heart and blood vessels, leading to various complications such as coronary artery disease, heart failure, and myocardial infarction. The timely and accurate detection of heart disease is of paramount importance in clinical practice. Early identification of individuals at risk enables proactive interventions, preventive measures, and personalized treatment strategies to mitigate the progression of the disease and reduce adverse outcomes. In recent years, the field of heart disease detection has witnessed notable advancements due to the integration of sophisticated technologies and computational approaches. These include machine learning algorithms, data mining techniques, and predictive modeling frameworks that leverage vast amounts of clinical and physiological data to improve diagnostic accuracy and risk stratification. In this work, we propose to detect heart disease from ECG images using cutting-edge technologies, namely vision transformer models. These models are Google-Vit, Microsoft-Beit, and Swin-Tiny. To the best of our knowledge, this is the initial endeavor concentrating on the detection of heart diseases through image-based ECG data by employing cuttingedge technologies namely, transformer models. To demonstrate the contribution of the proposed framework, the performance of vision transformer models are compared with state-of-the-art studies. Experiment results show that the proposed framework exhibits remarkable classification results.", "url": "https://arxiv.org/abs/2310.12630"}, {"metadata": {"arXiv": "2310.12480", "Date": "Thu, 19 Oct 2023 05:36:00 ", "Title": "GRAPE-S: Near Real-Time Coalition Formation for Multiple Service Collectives", "Authors": ["Grace Diehl and Julie A. Adams"], "Categories": "cs.MA cs.AI"}, "abstract": "Robotic collectives for military and disaster response applications require coalition formation algorithms to partition robots into appropriate task teams. Collectives' missions will often incorporate tasks that require multiple high-level robot behaviors or services, which coalition formation must accommodate. The highly dynamic and unstructured application domains also necessitate that coalition formation algorithms produce near optimal solutions (i.e., >95% utility) in near real-time (i.e., <5 minutes) with very large collectives (i.e., hundreds of robots). No previous coalition formation algorithm satisfies these requirements. An initial evaluation found that traditional auction-based algorithms' runtimes are too long, even though the centralized simulator incorporated ideal conditions unlikely to occur in real-world deployments (i.e., synchronization across robots and perfect, instantaneous communication). The hedonic game-based GRAPE algorithm can produce solutions in near real-time, but cannot be applied to multiple service collectives. This manuscript integrates GRAPE and a services model, producing GRAPE-S and Pair-GRAPE-S. These algorithms and two auction baselines were evaluated using a centralized simulator with up to 1000 robots, and via the largest distributed coalition formation simulated evaluation to date, with up to 500 robots. The evaluations demonstrate that auctions transfer poorly to distributed collectives, resulting in excessive runtimes and low utility solutions. GRAPE-S satisfies the target domains' coalition formation requirements, producing near optimal solutions in near real-time, and Pair-GRAPE-S more than satisfies the domain requirements, producing optimal solutions in near real-time. GRAPE-S and Pair-GRAPE-S are the first algorithms demonstrated to support near real-time coalition formation for very large, distributed collectives with multiple services.", "url": "https://arxiv.org/abs/2310.12480"}, {"metadata": {"arXiv": "2310.12309", "Date": "Wed, 18 Oct 2023 20:18:05 ", "Title": "A Unifying Framework for Learning Argumentation Semantics", "Authors": ["Zlatina Mileva", "Antonis Bikakis", "Fabio Aurelio D'Asaro", "Mark Law", "Alessandra Russo"], "Categories": "cs.AI cs.LG"}, "abstract": "Argumentation is a very active research field of Artificial Intelligence concerned with the representation and evaluation of arguments used in dialogues between humans and/or artificial agents. Acceptability semantics of formal argumentation systems define the criteria for the acceptance or rejection of arguments. Several software systems, known as argumentation solvers, have been developed to compute the accepted/rejected arguments using such criteria. These include systems that learn to identify the accepted arguments using non-interpretable methods. In this paper we present a novel framework, which uses an Inductive Logic Programming approach to learn the acceptability semantics for several abstract and structured argumentation frameworks in an interpretable way. Through an empirical evaluation we show that our framework outperforms existing argumentation solvers, thus opening up new future research directions in the area of formal argumentation and human-machine dialogues.", "url": "https://arxiv.org/abs/2310.12309"}, {"metadata": {"arXiv": "2310.12567", "Date": "Thu, 19 Oct 2023 08:19:28 ", "Title": "Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark", "Authors": ["Jiaming Ji", "Borong Zhang", "Jiayi Zhou", "Xuehai Pan", "Weidong Huang", "Ruiyang Sun", "Yiran Geng", "Yifan Zhong", "Juntao Dai", "Yaodong Yang"], "Categories": "cs.AI cs.LG"}, "abstract": "Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for safer, more reliable, and responsible real-world applications. The website of this project can be accessed at https://sites.google.com/view/safety-gymnasium.", "url": "https://arxiv.org/abs/2310.12567"}, {"metadata": {"arXiv": "2310.12773", "Date": "Thu, 19 Oct 2023 14:22:03 ", "Title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback", "Authors": ["Josef Dai", "Xuehai Pan", "Ruiyang Sun", "Jiaming Ji", "Xinbo Xu", "Mickel Liu", "Yizhou Wang", "Yaodong Yang"], "Categories": "cs.AI cs.LG"}, "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.", "url": "https://arxiv.org/abs/2310.12773"}, {"metadata": {"arXiv": "2310.12819", "Date": "Thu, 19 Oct 2023 15:16:43 ", "Title": "Hybrid Search for Efficient Planning with Completeness Guarantees", "Authors": ["Kalle Kujanp\\\"a\\\"a", "Joni Pajarinen", "Alexander Ilin"], "Categories": "cs.AI cs.LG", "Comments": ["NeurIPS 2023 Poster"]}, "abstract": "Solving complex planning problems has been a long-standing challenge in computer science. Learning-based subgoal search methods have shown promise in tackling these problems, but they often suffer from a lack of completeness guarantees, meaning that they may fail to find a solution even if one exists. In this paper, we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces. Specifically, we augment the high-level search with low-level actions to execute a multi-level (hybrid) search, which we call complete subgoal search. This solution achieves the best of both worlds: the practical efficiency of high-level search and the completeness of low-level search. We apply the proposed search method to a recently proposed subgoal search algorithm and evaluate the algorithm trained on offline data on complex planning problems. We demonstrate that our complete subgoal search not only guarantees completeness but can even improve performance in terms of search expansions for instances that the high-level could solve without low-level augmentations. Our approach makes it possible to apply subgoal-level planning for systems where completeness is a critical requirement.", "url": "https://arxiv.org/abs/2310.12819"}, {"metadata": {"arXiv": "2310.12274", "Date": "Wed, 18 Oct 2023 19:18:19 ", "Title": "An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning", "Authors": ["Chen Jin", "Ryutaro Tanno", "Amrutha Saseendran", "Tom Diethe", "Philip Teare"], "Categories": "cs.CV cs.AI cs.CL cs.GR cs.LG", "Comments": ["Project page: https://github.com/lxasqjc/MCPL"]}, "abstract": "Textural Inversion, a prompt learning method, learns a singular embedding for a new \"word\" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new \"words\" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new \"words\" with known words. We evaluate via image generation, editing, and attention visualisation with diverse images. Extensive quantitative comparisons demonstrate that our method can learn more semantically disentangled concepts with enhanced word-concept correlation. Additionally, we introduce a novel dataset and evaluation protocol tailored for this new task of learning object-level concepts.", "url": "https://arxiv.org/abs/2310.12274"}, {"metadata": {"arXiv": "2310.12345", "Date": "Wed, 18 Oct 2023 21:43:37 ", "Title": "ClusT3: Information Invariant Test-Time Training", "Authors": ["Gustavo A. Vargas Hakim and David Osowiechi and Mehrdad Noori and Milad Cheraghalikhani and Ismail Ben Ayed and Christian Desrosiers"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Deep Learning models have shown remarkable performance in a broad range of vision tasks. However, they are often vulnerable against domain shifts at test-time. Test-time training (TTT) methods have been developed in an attempt to mitigate these vulnerabilities, where a secondary task is solved at training time simultaneously with the main task, to be later used as an self-supervised proxy task at test-time. In this work, we propose a novel unsupervised TTT technique based on the maximization of Mutual Information between multi-scale feature maps and a discrete latent representation, which can be integrated to the standard training as an auxiliary clustering task. Experimental results demonstrate competitive classification performance on different popular test-time adaptation benchmarks.", "url": "https://arxiv.org/abs/2310.12345"}, {"metadata": {"arXiv": "2310.12817", "Date": "Thu, 19 Oct 2023 15:12:44 ", "Title": "2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision", "Authors": ["Cheng-Kun Yang", "Min-Hung Chen", "Yung-Yu Chuang", "Yen-Yu Lin"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["ICCV 2023 (main + supp). Website: https://jimmy15923.github.io/mit_web/"]}, "abstract": "We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are iteratively enriched by each other. Experiments show that it performs favorably against existing weakly supervised point cloud segmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The project page will be available at https://jimmy15923.github.io/mit_web/.", "url": "https://arxiv.org/abs/2310.12817"}, {"metadata": {"arXiv": "2310.12973", "Date": "Thu, 19 Oct 2023 17:59:05 ", "Title": "Frozen Transformers in Language Models Are Effective Visual Encoder Layers", "Authors": ["Ziqi Pang", "Ziyang Xie", "Yunze Man", "Yu-Xiong Wang"], "Categories": "cs.CV cs.AI cs.CL cs.LG", "Comments": ["23 pages", "13 figures. Code at https://github.com/ziqipang/LM4VisualEncoding"]}, "abstract": "This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language. Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -- employing a frozen transformer block from pre-trained LLMs as a constituent encoder layer to directly process visual tokens. Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multi-modal vision-language setup with associated language prompts, inputs, or outputs. We demonstrate that our approach consistently enhances performance across a diverse range of tasks, encompassing pure 2D and 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval). Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks. We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding -- the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect. This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions. We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms. Code is available at https://github.com/ziqipang/LM4VisualEncoding.", "url": "https://arxiv.org/abs/2310.12973"}, {"metadata": {"arXiv": "2310.12168", "Date": "Tue, 10 Oct 2023 10:48:27 ", "Title": "RK-core: An Established Methodology for Exploring the Hierarchical Structure within Datasets", "Authors": ["Yao Lu", "Yutian Huang", "Jiaqi Nie", "Zuohui Chen", "Qi Xuan"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Recently, the field of machine learning has undergone a transition from model-centric to data-centric. The advancements in diverse learning tasks have been propelled by the accumulation of more extensive datasets, subsequently facilitating the training of larger models on these datasets. However, these datasets remain relatively under-explored. To this end, we introduce a pioneering approach known as RK-core, to empower gaining a deeper understanding of the intricate hierarchical structure within datasets. Across several benchmark datasets, we find that samples with low coreness values appear less representative of their respective categories, and conversely, those with high coreness values exhibit greater representativeness. Correspondingly, samples with high coreness values make a more substantial contribution to the performance in comparison to those with low coreness values. Building upon this, we further employ RK-core to analyze the hierarchical structure of samples with different coreset selection methods. Remarkably, we find that a high-quality coreset should exhibit hierarchical diversity instead of solely opting for representative samples. The code is available at https://github.com/yaolu-zjut/Kcore.", "url": "https://arxiv.org/abs/2310.12168"}, {"metadata": {"arXiv": "2310.12184", "Date": "Wed, 18 Oct 2023 04:13:48 ", "Title": "Architectural Implications of GNN Aggregation Programming Abstractions", "Authors": ["Yingjie Qi", "Jianlei Yang", "Ao Zhou", "Tong Qiao and Chunming Hu"], "Categories": "cs.LG cs.AI cs.PF", "Comments": ["4 pages", "to be published in IEEE Computer Architecture Letters (CAL)"]}, "abstract": "Graph neural networks (GNNs) have gained significant popularity due to the powerful capability to extract useful representations from graph data. As the need for efficient GNN computation intensifies, a variety of programming abstractions designed for optimizing GNN Aggregation have emerged to facilitate acceleration. However, there is no comprehensive evaluation and analysis upon existing abstractions, thus no clear consensus on which approach is better. In this letter, we classify existing programming abstractions for GNN Aggregation by the dimension of data organization and propagation method. By constructing these abstractions on a state-of-the-art GNN library, we perform a thorough and detailed characterization study to compare their performance and efficiency, and provide several insights on future GNN acceleration based on our analysis.", "url": "https://arxiv.org/abs/2310.12184"}, {"metadata": {"arXiv": "2310.12244", "Date": "Wed, 18 Oct 2023 18:30:07 ", "Title": "A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm", "Authors": ["Haizhou Shi", "Hao Wang"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at NeurIPS 2023"]}, "abstract": "Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL **unifies** various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different **fixed** coefficients; based on insights from this unification, our UDIL allows **adaptive** coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthetic and real-world datasets. Code will be available at https://github.com/Wang-ML-Lab/unified-continual-learning.", "url": "https://arxiv.org/abs/2310.12244"}, {"metadata": {"arXiv": "2310.12281", "Date": "Wed, 18 Oct 2023 19:27:39 ", "Title": "Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning", "Authors": ["Soheila Farokhi", "Aswani Yaramala", "Jiangtao Huang", "Muhammad F. A. Khan", "Xiaojun Qi", "Hamid Karimi"], "Categories": "cs.LG cs.AI"}, "abstract": "In recent years, Massive Open Online Courses (MOOCs) have gained significant traction as a rapidly growing phenomenon in online learning. Unlike traditional classrooms, MOOCs offer a unique opportunity to cater to a diverse audience from different backgrounds and geographical locations. Renowned universities and MOOC-specific providers, such as Coursera, offer MOOC courses on various subjects. Automated assessment tasks like grade and early dropout predictions are necessary due to the high enrollment and limited direct interaction between teachers and learners. However, current automated assessment approaches overlook the structural links between different entities involved in the downstream tasks, such as the students and courses. Our hypothesis suggests that these structural relationships, manifested through an interaction graph, contain valuable information that can enhance the performance of the task at hand. To validate this, we construct a unique knowledge graph for a large MOOC dataset, which will be publicly available to the research community. Furthermore, we utilize graph embedding techniques to extract latent structural information encoded in the interactions between entities in the dataset. These techniques do not require ground truth labels and can be utilized for various tasks. Finally, by combining entity-specific features, behavioral features, and extracted structural features, we enhance the performance of predictive machine learning models in student assignment grade prediction. Our experiments demonstrate that structural features can significantly improve the predictive performance of downstream assessment tasks. The code and data are available in \\url{https://github.com/DSAatUSU/MOOPer_grade_prediction}", "url": "https://arxiv.org/abs/2310.12281"}, {"metadata": {"arXiv": "2310.12298", "Date": "Wed, 18 Oct 2023 19:58:54 ", "Title": "Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization", "Authors": ["Siddharth Singh", "Zachary Sating", "Abhinav Bhatele"], "Categories": "cs.LG cs.AI cs.DC"}, "abstract": "Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate the distinct advantages of using Jorge, outperforming state-of-the-art optimizers such as SGD, AdamW, and Shampoo across multiple deep learning models, both in terms of sample efficiency and wall-clock time.", "url": "https://arxiv.org/abs/2310.12298"}, {"metadata": {"arXiv": "2310.12387", "Date": "Wed, 18 Oct 2023 23:58:54 ", "Title": "Learning to Solve Climate Sensor Placement Problems with a Transformer", "Authors": ["Chen Wang", "Victoria Huang", "Gang Chen", "Hui Ma", "Bryce Chen", "and Jochen Schmidt"], "Categories": "cs.LG cs.AI"}, "abstract": "The optimal placement of sensors for environmental monitoring and disaster management is a challenging problem due to its NP-hard nature. Traditional methods for sensor placement involve exact, approximation, or heuristic approaches, with the latter being the most widely used. However, heuristic methods are limited by expert intuition and experience. Deep learning (DL) has emerged as a promising approach for generating heuristic algorithms automatically. In this paper, we introduce a novel sensor placement approach focused on learning improvement heuristics using deep reinforcement learning (RL) methods. Our approach leverages an RL formulation for learning improvement heuristics, driven by an actor-critic algorithm for training the policy network. We compare our method with several state-of-the-art approaches by conducting comprehensive experiments, demonstrating the effectiveness and superiority of our proposed approach in producing high-quality solutions. Our work presents a promising direction for applying advanced DL and RL techniques to challenging climate sensor placement problems.", "url": "https://arxiv.org/abs/2310.12387"}, {"metadata": {"arXiv": "2310.12407", "Date": "Thu, 19 Oct 2023 01:41:11 ", "Title": "Classification-Aided Robust Multiple Target Tracking Using Neural Enhanced Message Passing", "Authors": ["Xianglong Bai and Zengfu Wang and Quan Pan and Tao Yun and Hua Lan"], "Categories": "cs.LG cs.AI cs.SY eess.SY", "Comments": ["15 pages"]}, "abstract": "We address the challenge of tracking an unknown number of targets in strong clutter environments using measurements from a radar sensor. Leveraging the range-Doppler spectra information, we identify the measurement classes, which serve as additional information to enhance clutter rejection and data association, thus bolstering the robustness of target tracking. We first introduce a novel neural enhanced message passing approach, where the beliefs obtained by the unified message passing are fed into the neural network as additional information. The output beliefs are then utilized to refine the original beliefs. Then, we propose a classification-aided robust multiple target tracking algorithm, employing the neural enhanced message passing technique. This algorithm is comprised of three modules: a message-passing module, a neural network module, and a Dempster-Shafer module. The message-passing module is used to represent the statistical model by the factor graph and infers target kinematic states, visibility states, and data associations based on the spatial measurement information. The neural network module is employed to extract features from range-Doppler spectra and derive beliefs on whether a measurement is target-generated or clutter-generated. The Dempster-Shafer module is used to fuse the beliefs obtained from both the factor graph and the neural network. As a result, our proposed algorithm adopts a model-and-data-driven framework, effectively enhancing clutter suppression and data association, leading to significant improvements in multiple target tracking performance. We validate the effectiveness of our approach using both simulated and real data scenarios, demonstrating its capability to handle challenging tracking scenarios in practical radar applications.", "url": "https://arxiv.org/abs/2310.12407"}, {"metadata": {"arXiv": "2310.12408", "Date": "Thu, 19 Oct 2023 01:45:37 ", "Title": "Provable Guarantees for Neural Networks via Gradient Feature Learning", "Authors": ["Zhenmei Shi", "Junyi Wei", "Yingyu Liang"], "Categories": "cs.LG cs.AI", "Comments": ["NeurIPS 2023", "71 pages"]}, "abstract": "Neural networks have achieved remarkable empirical performance, while the current theoretical analysis is not adequate for understanding their success, e.g., the Neural Tangent Kernel approach fails to capture their key feature learning ability, while recent analyses on feature learning are typically problem-specific. This work proposes a unified analysis framework for two-layer networks trained by gradient descent. The framework is centered around the principle of feature learning from gradients, and its effectiveness is demonstrated by applications in several prototypical problems, such as mixtures of Gaussians and parity functions. The framework also sheds light on interesting network learning phenomena such as feature learning beyond kernels and the lottery ticket hypothesis.", "url": "https://arxiv.org/abs/2310.12408"}, {"metadata": {"arXiv": "2310.12451", "Date": "Thu, 19 Oct 2023 04:08:19 ", "Title": "MTS-LOF: Medical Time-Series Representation Learning via Occlusion-Invariant Features", "Authors": ["Huayu Li", "Ana S. Carreon-Rascon", "Xiwen Chen", "Geng Yuan", "and Ao Li"], "Categories": "cs.LG cs.AI"}, "abstract": "Medical time series data are indispensable in healthcare, providing critical insights for disease diagnosis, treatment planning, and patient management. The exponential growth in data complexity, driven by advanced sensor technologies, has presented challenges related to data labeling. Self-supervised learning (SSL) has emerged as a transformative approach to address these challenges, eliminating the need for extensive human annotation. In this study, we introduce a novel framework for Medical Time Series Representation Learning, known as MTS-LOF. MTS-LOF leverages the strengths of contrastive learning and Masked Autoencoder (MAE) methods, offering a unique approach to representation learning for medical time series data. By combining these techniques, MTS-LOF enhances the potential of healthcare applications by providing more sophisticated, context-rich representations. Additionally, MTS-LOF employs a multi-masking strategy to facilitate occlusion-invariant feature learning. This approach allows the model to create multiple views of the data by masking portions of it. By minimizing the discrepancy between the representations of these masked patches and the fully visible patches, MTS-LOF learns to capture rich contextual information within medical time series datasets. The results of experiments conducted on diverse medical time series datasets demonstrate the superiority of MTS-LOF over other methods. These findings hold promise for significantly enhancing healthcare applications by improving representation learning. Furthermore, our work delves into the integration of joint-embedding SSL and MAE techniques, shedding light on the intricate interplay between temporal and structural dependencies in healthcare data. This understanding is crucial, as it allows us to grasp the complexities of healthcare data analysis.", "url": "https://arxiv.org/abs/2310.12451"}, {"metadata": {"arXiv": "2310.12508", "Date": "Thu, 19 Oct 2023 06:17:17 ", "Title": "SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation", "Authors": ["Chongyu Fan", "Jiancheng Liu", "Yihua Zhang", "Dennis Wei", "Eric Wong", "Sijia Liu"], "Categories": "cs.LG cs.AI"}, "abstract": "With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, SalUn yields a stability advantage in high-variance random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from generating harmful images, SalUn achieves nearly 100% unlearning accuracy, outperforming current state-of-the-art baselines like Erased Stable Diffusion and Forget-Me-Not.", "url": "https://arxiv.org/abs/2310.12508"}, {"metadata": {"arXiv": "2310.12527", "Date": "Thu, 19 Oct 2023 07:04:29 ", "Title": "Testing the Consistency of Performance Scores Reported for Binary Classification Problems", "Authors": ["Attila Fazekas and Gy\\\"orgy Kov\\'acs"], "Categories": "cs.LG cs.AI", "MSC-class": "68T01", "ACM-class": "I.2.1"}, "abstract": "Binary classification is a fundamental task in machine learning, with applications spanning various scientific domains. Whether scientists are conducting fundamental research or refining practical applications, they typically assess and rank classification techniques based on performance metrics such as accuracy, sensitivity, and specificity. However, reported performance scores may not always serve as a reliable basis for research ranking. This can be attributed to undisclosed or unconventional practices related to cross-validation, typographical errors, and other factors. In a given experimental setup, with a specific number of positive and negative test items, most performance scores can assume specific, interrelated values. In this paper, we introduce numerical techniques to assess the consistency of reported performance scores and the assumed experimental setup. Importantly, the proposed approach does not rely on statistical inference but uses numerical methods to identify inconsistencies with certainty. Through three different applications related to medicine, we demonstrate how the proposed techniques can effectively detect inconsistencies, thereby safeguarding the integrity of research fields. To benefit the scientific community, we have made the consistency tests available in an open-source Python package.", "url": "https://arxiv.org/abs/2310.12527"}, {"metadata": {"arXiv": "2310.12632", "Date": "Thu, 19 Oct 2023 10:35:50 ", "Title": "Towards a Deep Learning-based Online Quality Prediction System for Welding Processes", "Authors": ["Yannik Hahn", "Robert Maack", "Guido Buchholz", "Marion Purrio", "Matthias Angerhausen", "Hasan Tercan", "Tobias Meisen"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted for CIRP CMS '23"]}, "abstract": "The digitization of manufacturing processes enables promising applications for machine learning-assisted quality assurance. A widely used manufacturing process that can strongly benefit from data-driven solutions is \\ac{GMAW}. The welding process is characterized by complex cause-effect relationships between material properties, process conditions and weld quality. In non-laboratory environments with frequently changing process parameters, accurate determination of weld quality by destructive testing is economically unfeasible. Deep learning offers the potential to identify the relationships in available process data and predict the weld quality from process observations. In this paper, we present a concept for a deep learning based predictive quality system in \\ac{GMAW}. At its core, the concept involves a pipeline consisting of four major phases: collection and management of multi-sensor data (e.g. current and voltage), real-time processing and feature engineering of the time series data by means of autoencoders, training and deployment of suitable recurrent deep learning models for quality predictions, and model evolutions under changing process conditions using continual learning. The concept provides the foundation for future research activities in which we will realize an online predictive quality system for running production.", "url": "https://arxiv.org/abs/2310.12632"}, {"metadata": {"arXiv": "2310.12688", "Date": "Thu, 19 Oct 2023 12:35:30 ", "Title": "Compression of Recurrent Neural Networks using Matrix Factorization", "Authors": ["Lucas Maison", "H\\'elion du Mas des Bourboux", "Thomas Courtat"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Compressing neural networks is a key step when deploying models for real-time or embedded applications. Factorizing the model's matrices using low-rank approximations is a promising method for achieving compression. While it is possible to set the rank before training, this approach is neither flexible nor optimal. In this work, we propose a post-training rank-selection method called Rank-Tuning that selects a different rank for each matrix. Used in combination with training adaptations, our method achieves high compression rates with no or little performance degradation. Our numerical experiments on signal processing tasks show that we can compress recurrent neural networks up to 14x with at most 1.4% relative performance reduction.", "url": "https://arxiv.org/abs/2310.12688"}, {"metadata": {"arXiv": "2310.12690", "Date": "Thu, 19 Oct 2023 12:38:09 ", "Title": "Neurosymbolic Grounding for Compositional World Models", "Authors": ["Atharva Sehgal", "Arya Grayeli", "Jennifer J. Sun", "Swarat Chaudhuri"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual \"atoms.\" The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-pushing domain, we show that the framework establishes a new state-of-the-art for CG in world modeling.", "url": "https://arxiv.org/abs/2310.12690"}, {"metadata": {"arXiv": "2310.12800", "Date": "Thu, 19 Oct 2023 14:55:51 ", "Title": "Exploring Graph Neural Networks for Indian Legal Judgment Prediction", "Authors": ["Mann Khatri", "Mirza Yusuf", "Yaman Kumar", "Rajiv Ratn Shah and Ponnurangam Kumaraguru"], "Categories": "cs.LG cs.AI"}, "abstract": "The burdensome impact of a skewed judges-to-cases ratio on the judicial system manifests in an overwhelming backlog of pending cases alongside an ongoing influx of new ones. To tackle this issue and expedite the judicial process, the proposition of an automated system capable of suggesting case outcomes based on factual evidence and precedent from past cases gains significance. This research paper centres on developing a graph neural network-based model to address the Legal Judgment Prediction (LJP) problem, recognizing the intrinsic graph structure of judicial cases and making it a binary node classification problem. We explored various embeddings as model features, while nodes such as time nodes and judicial acts were added and pruned to evaluate the model's performance. The study is done while considering the ethical dimension of fairness in these predictions, considering gender and name biases. A link prediction task is also conducted to assess the model's proficiency in anticipating connections between two specified nodes. By harnessing the capabilities of graph neural networks and incorporating fairness analyses, this research aims to contribute insights towards streamlining the adjudication process, enhancing judicial efficiency, and fostering a more equitable legal landscape, ultimately alleviating the strain imposed by mounting case backlogs. Our best-performing model with XLNet pre-trained embeddings as its features gives the macro F1 score of 75% for the LJP task. For link prediction, the same set of features is the best performing giving ROC of more than 80%", "url": "https://arxiv.org/abs/2310.12800"}, {"metadata": {"arXiv": "2310.12808", "Date": "Thu, 19 Oct 2023 15:02:45 ", "Title": "Model Merging by Uncertainty-Based Gradient Matching", "Authors": ["Nico Daheim", "Thomas M\\\"ollenhoff", "Edoardo Maria Ponti", "Iryna Gurevych", "Mohammad Emtiyaz Khan"], "Categories": "cs.LG cs.AI cs.CL", "Comments": ["Preprint. Under review"]}, "abstract": "Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.", "url": "https://arxiv.org/abs/2310.12808"}, {"metadata": {"arXiv": "2310.12900", "Date": "Thu, 19 Oct 2023 16:52:12 ", "Title": "Personalized human mobility prediction for HuMob challenge", "Authors": ["Masahiro Suzuki", "Shomu Furuta", "Yusuke Fukazawa"], "Categories": "cs.LG cs.AI"}, "abstract": "We explain the methodology used to create the data submitted to HuMob Challenge, a data analysis competition for human mobility prediction. We adopted a personalized model to predict the individual's movement trajectory from their data, instead of predicting from the overall movement, based on the hypothesis that human movement is unique to each person. We devised the features such as the date and time, activity time, days of the week, time of day, and frequency of visits to POI (Point of Interest). As additional features, we incorporated the movement of other individuals with similar behavior patterns through the employment of clustering. The machine learning model we adopted was the Support Vector Regression (SVR). We performed accuracy through offline assessment and carried out feature selection and parameter tuning. Although overall dataset provided consists of 100,000 users trajectory, our method use only 20,000 target users data, and do not need to use other 80,000 data. Despite the personalized model's traditional feature engineering approach, this model yields reasonably good accuracy with lower computational cost.", "url": "https://arxiv.org/abs/2310.12900"}, {"metadata": {"arXiv": "2310.12920", "Date": "Thu, 19 Oct 2023 17:14:29 ", "Title": "Generative Marginalization Models", "Authors": ["Sulin Liu", "Peter J. Ramadge", "Ryan P. Adams"], "Categories": "cs.LG cs.AI"}, "abstract": "We introduce marginalization models (MaMs), a new family of generative models for high-dimensional discrete data. They offer scalable and flexible generative modeling with tractable likelihoods by explicitly modeling all induced marginal distributions. Marginalization models enable fast evaluation of arbitrary marginal probabilities with a single forward pass of the neural network, which overcomes a major limitation of methods with exact marginal inference, such as autoregressive models (ARMs). We propose scalable methods for learning the marginals, grounded in the concept of \"marginalization self-consistency\". Unlike previous methods, MaMs support scalable training of any-order generative models for high-dimensional problems under the setting of energy-based training, where the goal is to match the learned distribution to a given desired probability (specified by an unnormalized (log) probability function such as energy function or reward function). We demonstrate the effectiveness of the proposed model on a variety of discrete data distributions, including binary images, language, physical systems, and molecules, for maximum likelihood and energy-based training settings. MaMs achieve orders of magnitude speedup in evaluating the marginal probabilities on both settings. For energy-based training tasks, MaMs enable any-order generative modeling of high-dimensional problems beyond the capability of previous methods. Code is at https://github.com/PrincetonLIPS/MaM.", "url": "https://arxiv.org/abs/2310.12920"}, {"metadata": {"arXiv": "2310.12921", "Date": "Thu, 19 Oct 2023 17:17:06 ", "Title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning", "Authors": ["Juan Rocamonde", "Victoriano Montesinos", "Elvis Nava", "Ethan Perez", "David Lindner"], "Categories": "cs.LG cs.AI"}, "abstract": "Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second ``baseline'' prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.", "url": "https://arxiv.org/abs/2310.12921"}, {"metadata": {"arXiv": "2310.12941", "Date": "Thu, 19 Oct 2023 17:39:02 ", "Title": "The Foundation Model Transparency Index", "Authors": ["Rishi Bommasani", "Kevin Klyman", "Shayne Longpre", "Sayash Kapoor", "Nestor Maslej", "Betty Xiong", "Daniel Zhang", "Percy Liang"], "Categories": "cs.LG cs.AI", "Comments": ["Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). Project page: https://crfm.stanford.edu/fmti"]}, "abstract": "Foundation models have rapidly permeated society, catalyzing a wave of generative AI applications spanning enterprise and consumer-facing contexts. While the societal impact of foundation models is growing, transparency is on the decline, mirroring the opacity that has plagued past digital technologies (e.g. social media). Reversing this trend is essential: transparency is a vital precondition for public accountability, scientific innovation, and effective governance. To assess the transparency of the foundation model ecosystem and help improve transparency over time, we introduce the Foundation Model Transparency Index. The Foundation Model Transparency Index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). We score 10 major foundation model developers (e.g. OpenAI, Google, Meta) against the 100 indicators to assess their transparency. To facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. GPT-4 for OpenAI, PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. Overall, the Foundation Model Transparency Index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.", "url": "https://arxiv.org/abs/2310.12941"}, {"metadata": {"arXiv": "2310.12955", "Date": "Thu, 19 Oct 2023 17:54:39 ", "Title": "Towards Robust Offline Reinforcement Learning under Diverse Data Corruption", "Authors": ["Rui Yang", "Han Zhong", "Jiawei Xu", "Amy Zhang", "Chongjie Zhang", "Lei Han", "Tong Zhang"], "Categories": "cs.LG cs.AI", "Comments": ["31 pages", "17 figures"]}, "abstract": "Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tackle this challenge, we draw inspiration from robust statistics to employ the Huber loss to handle the heavy-tailedness and utilize quantile estimators to balance penalization for corrupted data and learning stability. By incorporating these simple yet effective modifications into IQL, we propose a more robust offline RL approach named Robust IQL (RIQL). Extensive experiments demonstrate that RIQL exhibits highly robust performance when subjected to diverse data corruption scenarios.", "url": "https://arxiv.org/abs/2310.12955"}, {"metadata": {"arXiv": "2310.12956", "Date": "Thu, 19 Oct 2023 17:55:06 ", "Title": "Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems", "Authors": ["David T. Hoffmann", "Simon Schrodi", "Nadine Behrmann", "Volker Fischer", "Thomas Brox"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "In this work, we study rapid, step-wise improvements of the loss in transformers when being confronted with multi-step decision tasks. We found that transformers struggle to learn the intermediate tasks, whereas CNNs have no such issue on the tasks we studied. When transformers learn the intermediate task, they do this rapidly and unexpectedly after both training and validation loss saturated for hundreds of epochs. We call these rapid improvements Eureka-moments, since the transformer appears to suddenly learn a previously incomprehensible task. Similar leaps in performance have become known as Grokking. In contrast to Grokking, for Eureka-moments, both the validation and the training loss saturate before rapidly improving. We trace the problem back to the Softmax function in the self-attention block of transformers and show ways to alleviate the problem. These fixes improve training speed. The improved models reach 95% of the baseline model in just 20% of training steps while having a much higher likelihood to learn the intermediate task, lead to higher final accuracy and are more robust to hyper-parameters.", "url": "https://arxiv.org/abs/2310.12956"}, {"metadata": {"arXiv": "2310.12967", "Date": "Thu, 19 Oct 2023 17:58:11 ", "Title": "Does Your Model Think Like an Engineer? Explainable AI for Bearing Fault Detection with Deep Learning", "Authors": ["Thomas Decker", "Michael Lebacher", "and Volker Tresp"], "Categories": "cs.LG cs.AI", "Comments": ["2023 IEEE International Conference on Acoustics", "Speech and Signal Processing (ICASSP)"], "DOI": "10.1109/ICASSP49357.2023.10096396"}, "abstract": "Deep Learning has already been successfully applied to analyze industrial sensor data in a variety of relevant use cases. However, the opaque nature of many well-performing methods poses a major obstacle for real-world deployment. Explainable AI (XAI) and especially feature attribution techniques promise to enable insights about how such models form their decision. But the plain application of such methods often fails to provide truly informative and problem-specific insights to domain experts. In this work, we focus on the specific task of detecting faults in rolling element bearings from vibration signals. We propose a novel and domain-specific feature attribution framework that allows us to evaluate how well the underlying logic of a model corresponds with expert reasoning. Utilizing the framework we are able to validate the trustworthiness and to successfully anticipate the generalization ability of different well-performing deep learning models. Our methodology demonstrates how signal processing tools can effectively be used to enhance Explainable AI techniques and acts as a template for similar problems.", "url": "https://arxiv.org/abs/2310.12967"}, {"metadata": {"arXiv": "2310.12975", "Date": "Thu, 19 Oct 2023 17:59:21 ", "Title": "Variational Inference for SDEs Driven by Fractional Noise", "Authors": ["Rembert Daems and Manfred Opper and Guillaume Crevecoeur and Tolga Birdal"], "Categories": "cs.LG cs.AI cs.CV stat.AP stat.ML", "Comments": ["24 pages", "under review"]}, "abstract": "We present a novel variational framework for performing inference in (neural) stochastic differential equations (SDEs) driven by Markov-approximate fractional Brownian motion (fBM). SDEs offer a versatile tool for modeling real-world continuous-time dynamic systems with inherent noise and randomness. Combining SDEs with the powerful inference capabilities of variational methods, enables the learning of representative function distributions through stochastic gradient descent. However, conventional SDEs typically assume the underlying noise to follow a Brownian motion (BM), which hinders their ability to capture long-term dependencies. In contrast, fractional Brownian motion (fBM) extends BM to encompass non-Markovian dynamics, but existing methods for inferring fBM parameters are either computationally demanding or statistically inefficient. In this paper, building upon the Markov approximation of fBM, we derive the evidence lower bound essential for efficient variational inference of posterior path measures, drawing from the well-established field of stochastic analysis. Additionally, we provide a closed-form expression to determine optimal approximation coefficients. Furthermore, we propose the use of neural networks to learn the drift, diffusion and control terms within our variational posterior, leading to the variational training of neural-SDEs. In this framework, we also optimize the Hurst index, governing the nature of our fractional noise. Beyond validation on synthetic data, we contribute a novel architecture for variational latent video prediction,-an approach that, to the best of our knowledge, enables the first variational neural-SDE application to video perception.", "url": "https://arxiv.org/abs/2310.12975"}, {"metadata": {"arXiv": "2310.12238", "Date": "Wed, 18 Oct 2023 18:26:01 ", "Title": "Few-Shot In-Context Imitation Learning via Implicit Graph Alignment", "Authors": ["Vitalis Vosylius and Edward Johns"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["Published at CoRL 2023. Videos are available on our project webpage at https://www.robot-learning.uk/implicit-graph-alignment"]}, "abstract": "Consider the following problem: given a few demonstrations of a task across a few different objects, how can a robot learn to perform that same task on new, previously unseen objects? This is challenging because the large variety of objects within a class makes it difficult to infer the task-relevant relationship between the new objects and the objects in the demonstrations. We address this by formulating imitation learning as a conditional alignment problem between graph representations of objects. Consequently, we show that this conditioning allows for in-context learning, where a robot can perform a task on a set of new objects immediately after the demonstrations, without any prior knowledge about the object class or any further training. In our experiments, we explore and validate our design choices, and we show that our method is highly effective for few-shot learning of several real-world, everyday tasks, whilst outperforming baselines. Videos are available on our project webpage at https://www.robot-learning.uk/implicit-graph-alignment.", "url": "https://arxiv.org/abs/2310.12238"}, {"metadata": {"arXiv": "2310.12609", "Date": "Thu, 19 Oct 2023 09:39:07 ", "Title": "Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning", "Authors": ["Junwoo Chang", "Hyunwoo Ryu", "Jiwoo Kim", "Soochul Yoo", "Joohwan Seo", "Nikhil Prakash", "Jongeun Choi", "Roberto Horowitz"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["8 pages", "6 figures"]}, "abstract": "Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.", "url": "https://arxiv.org/abs/2310.12609"}, {"metadata": {"arXiv": "2310.12931", "Date": "Thu, 19 Oct 2023 17:31:01 ", "Title": "Eureka: Human-Level Reward Design via Coding Large Language Models", "Authors": ["Yecheng Jason Ma", "William Liang", "Guanzhi Wang", "De-An Huang", "Osbert Bastani", "Dinesh Jayaraman", "Yuke Zhu", "Linxi Fan", "Anima Anandkumar"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["Project website and open-source code: https://eureka-research.github.io/"]}, "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.", "url": "https://arxiv.org/abs/2310.12931"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
