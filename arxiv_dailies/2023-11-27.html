<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2311.13608", "Date": "Tue, 21 Nov 2023 18:09:30 ", "Title": "Breathing Life Into Sketches Using Text-to-Video Priors", "Authors": ["Rinon Gal", "Yael Vinker", "Yuval Alaluf", "Amit H. Bermano", "Daniel Cohen-Or", "Ariel Shamir", "Gal Chechik"], "Categories": "cs.CV cs.GR cs.LG", "Comments": ["Project page: https://livesketch.github.io/"]}, "abstract": "A sketch is one of the most intuitive and versatile tools humans use to convey their ideas visually. An animated sketch opens another dimension to the expression of ideas and is widely used by designers for a variety of purposes. Animating sketches is a laborious process, requiring extensive experience and professional design skills. In this work, we present a method that automatically adds motion to a single-subject sketch (hence, \"breathing life into it\"), merely by providing a text prompt indicating the desired motion. The output is a short animation provided in vector representation, which can be easily edited. Our method does not require extensive training, but instead leverages the motion prior of a large pretrained text-to-video diffusion model using a score-distillation loss to guide the placement of strokes. To promote natural and smooth motion and to better preserve the sketch's appearance, we model the learned motion through two components. The first governs small local deformations and the second controls global affine transformations. Surprisingly, we find that even models that struggle to generate sketch videos on their own can still serve as a useful backbone for animating abstract representations.", "url": "https://arxiv.org/abs/2311.13608"}, {"metadata": {"arXiv": "2311.13613", "Date": "Wed, 22 Nov 2023 03:45:30 ", "Title": "Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning", "Authors": ["Xin Zhang", "Jiawei Du", "Yunsong Li", "Weiying Xie", "Joey Tianyi Zhou"], "Categories": "cs.CV cs.LG"}, "abstract": "Dataset pruning aims to construct a coreset capable of achieving performance comparable to the original, full dataset. Most existing dataset pruning methods rely on snapshot-based criteria to identify representative samples, often resulting in poor generalization across various pruning and cross-architecture scenarios. Recent studies have addressed this issue by expanding the scope of training dynamics considered, including factors such as forgetting event and probability change, typically using an averaging approach. However, these works struggle to integrate a broader range of training dynamics without overlooking well-generalized samples, which may not be sufficiently highlighted in an averaging manner. In this study, we propose a novel dataset pruning method termed as Temporal Dual-Depth Scoring (TDDS), to tackle this problem. TDDS utilizes a dual-depth strategy to achieve a balance between incorporating extensive training dynamics and identifying representative samples for dataset pruning. In the first depth, we estimate the series of each sample's individual contributions spanning the training progress, ensuring comprehensive integration of training dynamics. In the second depth, we focus on the variability of the sample-wise contributions identified in the first depth to highlight well-generalized samples. Extensive experiments conducted on CIFAR and ImageNet datasets verify the superiority of TDDS over previous SOTA methods. Specifically on CIFAR-100, our method achieves 54.51% accuracy with only 10% training data, surpassing random selection by 7.83% and other comparison methods by at least 12.69%.", "url": "https://arxiv.org/abs/2311.13613"}, {"metadata": {"arXiv": "2311.13833", "Date": "Thu, 23 Nov 2023 07:33:38 ", "Title": "Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models", "Authors": ["Saman Motamed and Danda Pani Paudel and Luc Van Gool"], "Categories": "cs.CV cs.CL cs.LG"}, "abstract": "Diffusion models have revolutionized generative content creation and text-to-image (T2I) diffusion models in particular have increased the creative freedom of users by allowing scene synthesis using natural language. T2I models excel at synthesizing concepts such as nouns, appearances, and styles. To enable customized content creation based on a few example images of a concept, methods such as Textual Inversion and DreamBooth invert the desired concept and enable synthesizing it in new scenes. However, inverting more general concepts that go beyond object appearance and style (adjectives and verbs) through natural language, remains a challenge. Two key characteristics of these concepts contribute to the limitations of current inversion methods. 1) Adjectives and verbs are entangled with nouns (subject) and can hinder appearance-based inversion methods, where the subject appearance leaks into the concept embedding and 2) describing such concepts often extends beyond single word embeddings (being frozen in ice, walking on a tightrope, etc.) that current methods do not handle. In this study, we introduce Lego, a textual inversion method designed to invert subject entangled concepts from a few example images. Lego disentangles concepts from their associated subjects using a simple yet effective Subject Separation step and employs a Context Loss that guides the inversion of single/multi-embedding concepts. In a thorough user study, Lego-generated concepts were preferred over 70% of the time when compared to the baseline. Additionally, visual question answering using a large language model suggested Lego-generated concepts are better aligned with the text description of the concept.", "url": "https://arxiv.org/abs/2311.13833"}, {"metadata": {"arXiv": "2311.14024", "Date": "Thu, 23 Nov 2023 14:28:28 ", "Title": "Creating and Benchmarking a Synthetic Dataset for Cloud Optical Thickness Estimation", "Authors": ["Aleksis Pirinen", "Nosheen Abid", "Nuria Agues Paszkowsky", "Thomas Ohlson Timoudas", "Ronald Scheirer", "Chiara Ceccobello", "Gy\\\"orgy Kov\\'acs", "Anders Persson"], "Categories": "cs.CV cs.LG", "Comments": ["Code", "data and models available at https://github.com/aleksispi/ml-cloud-opt-thick"]}, "abstract": "Cloud formations often obscure optical satellite-based monitoring of the Earth's surface, thus limiting Earth observation (EO) activities such as land cover mapping, ocean color analysis, and cropland monitoring. The integration of machine learning (ML) methods within the remote sensing domain has significantly improved performance on a wide range of EO tasks, including cloud detection and filtering, but there is still much room for improvement. A key bottleneck is that ML methods typically depend on large amounts of annotated data for training, which is often difficult to come by in EO contexts. This is especially true for the task of cloud optical thickness (COT) estimation. A reliable estimation of COT enables more fine-grained and application-dependent control compared to using pre-specified cloud categories, as is commonly done in practice. To alleviate the COT data scarcity problem, in this work we propose a novel synthetic dataset for COT estimation, where top-of-atmosphere radiances have been simulated for 12 of the spectral bands of the Multi-Spectral Instrument (MSI) sensor onboard Sentinel-2 platforms. These data points have been simulated under consideration of different cloud types, COTs, and ground surface and atmospheric profiles. Extensive experimentation of training several ML models to predict COT from the measured reflectivity of the spectral bands demonstrates the usefulness of our proposed dataset. Generalization to real data is also demonstrated on two satellite image datasets -- one that is publicly available, and one which we have collected and annotated. The synthetic data, the newly collected real dataset, code and models have been made publicly available at https://github.com/aleksispi/ml-cloud-opt-thick.", "url": "https://arxiv.org/abs/2311.14024"}, {"metadata": {"arXiv": "2311.14029", "Date": "Thu, 23 Nov 2023 14:33:53 ", "Title": "Understanding the Vulnerability of CLIP to Image Compression", "Authors": ["Cangxiong Chen", "Vinay P. Namboodiri", "Julian Padget"], "Categories": "cs.CV cs.LG", "Comments": ["R0-FoMo: Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models at NeurIPS 2023"]}, "abstract": "CLIP is a widely used foundational vision-language model that is used for zero-shot image recognition and other image-text alignment tasks. We demonstrate that CLIP is vulnerable to change in image quality under compression. This surprising result is further analysed using an attribution method-Integrated Gradients. Using this attribution method, we are able to better understand both quantitatively and qualitatively exactly the nature in which the compression affects the zero-shot recognition accuracy of this model. We evaluate this extensively on CIFAR-10 and STL-10. Our work provides the basis to understand this vulnerability of CLIP and can help us develop more effective methods to improve the robustness of CLIP and other vision-language models.", "url": "https://arxiv.org/abs/2311.14029"}, {"metadata": {"arXiv": "2311.14063", "Date": "Thu, 23 Nov 2023 15:42:00 ", "Title": "Do VSR Models Generalize Beyond LRS3?", "Authors": ["Yasser Abdelaziz Dahou Djilali", "Sanath Narayan", "Eustache Le Bihan", "Haithem Boussaid", "Ebtessam Almazrouei", "Merouane Debbah"], "Categories": "cs.CV cs.CL cs.LG"}, "abstract": "The Lip Reading Sentences-3 (LRS3) benchmark has primarily been the focus of intense research in visual speech recognition (VSR) during the last few years. As a result, there is an increased risk of overfitting to its excessively used test set, which is only one hour duration. To alleviate this issue, we build a new VSR test set named WildVSR, by closely following the LRS3 dataset creation processes. We then evaluate and analyse the extent to which the current VSR models generalize to the new test data. We evaluate a broad range of publicly available VSR models and find significant drops in performance on our test set, compared to their corresponding LRS3 results. Our results suggest that the increase in word error rates is caused by the models inability to generalize to slightly harder and in the wild lip sequences than those found in the LRS3 test set. Our new test benchmark is made public in order to enable future research towards more robust VSR models.", "url": "https://arxiv.org/abs/2311.14063"}, {"metadata": {"arXiv": "2311.14271", "Date": "Fri, 24 Nov 2023 04:15:10 ", "Title": "Segmentation-Based Parametric Painting", "Authors": ["Manuel Ladron de Guevara", "Matthew Fisher", "Aaron Hertzmann"], "Categories": "cs.CV cs.LG", "Comments": ["8 pages"]}, "abstract": "We introduce a novel image-to-painting method that facilitates the creation of large-scale, high-fidelity paintings with human-like quality and stylistic variation. To process large images and gain control over the painting process, we introduce a segmentation-based painting process and a dynamic attention map approach inspired by human painting strategies, allowing optimization of brush strokes to proceed in batches over different image regions, thereby capturing both large-scale structure and fine details, while also allowing stylistic control over detail. Our optimized batch processing and patch-based loss framework enable efficient handling of large canvases, ensuring our painted outputs are both aesthetically compelling and functionally superior as compared to previous methods, as confirmed by rigorous evaluations. Code available at: https://github.com/manuelladron/semantic\\_based\\_painting.git", "url": "https://arxiv.org/abs/2311.14271"}, {"metadata": {"arXiv": "2311.14272", "Date": "Fri, 24 Nov 2023 04:16:32 ", "Title": "CRISP: Hybrid Structured Sparsity for Class-aware Model Pruning", "Authors": ["Shivam Aggarwal", "Kuluhan Binici", "Tulika Mitra"], "Categories": "cs.CV cs.AR cs.LG", "Comments": ["6 pages", "accepted in Design", "Automation & Test in Europe Conference & Exhibition (DATE) 2024"]}, "abstract": "Machine learning pipelines for classification tasks often train a universal model to achieve accuracy across a broad range of classes. However, a typical user encounters only a limited selection of classes regularly. This disparity provides an opportunity to enhance computational efficiency by tailoring models to focus on user-specific classes. Existing works rely on unstructured pruning, which introduces randomly distributed non-zero values in the model, making it unsuitable for hardware acceleration. Alternatively, some approaches employ structured pruning, such as channel pruning, but these tend to provide only minimal compression and may lead to reduced model accuracy. In this work, we propose CRISP, a novel pruning framework leveraging a hybrid structured sparsity pattern that combines both fine-grained N:M structured sparsity and coarse-grained block sparsity. Our pruning strategy is guided by a gradient-based class-aware saliency score, allowing us to retain weights crucial for user-specific classes. CRISP achieves high accuracy with minimal memory consumption for popular models like ResNet-50, VGG-16, and MobileNetV2 on ImageNet and CIFAR-100 datasets. Moreover, CRISP delivers up to 14$\\times$ reduction in latency and energy consumption compared to existing pruning methods while maintaining comparable accuracy. Our code is available at https://github.com/shivmgg/CRISP/.", "url": "https://arxiv.org/abs/2311.14272"}, {"metadata": {"arXiv": "2311.14301", "Date": "Fri, 24 Nov 2023 06:22:38 ", "Title": "GeoViT: A Versatile Vision Transformer Architecture for Geospatial Image Analysis", "Authors": ["Madhav Khirwar", "Ankur Narang"], "Categories": "cs.CV cs.LG", "Comments": ["Extended Abstract", "Preprint"]}, "abstract": "Greenhouse gases are pivotal drivers of climate change, necessitating precise quantification and source identification to foster mitigation strategies. We introduce GeoViT, a compact vision transformer model adept in processing satellite imagery for multimodal segmentation, classification, and regression tasks targeting CO2 and NO2 emissions. Leveraging GeoViT, we attain superior accuracy in estimating power generation rates, fuel type, plume coverage for CO2, and high-resolution NO2 concentration mapping, surpassing previous state-of-the-art models while significantly reducing model size. GeoViT demonstrates the efficacy of vision transformer architectures in harnessing satellite-derived data for enhanced GHG emission insights, proving instrumental in advancing climate change monitoring and emission regulation efforts globally.", "url": "https://arxiv.org/abs/2311.14301"}, {"metadata": {"arXiv": "2311.14388", "Date": "Fri, 24 Nov 2023 10:07:14 ", "Title": "A Parameterized Generative Adversarial Network Using Cyclic Projection for Explainable Medical Image Classification", "Authors": ["Xiangyu Xiong", "Yue Sun", "Xiaohong Liu", "ChanTong Lam", "Tong Tong", "Hao Chen", "Qinquan Gao", "Wei Ke", "Tao Tan"], "Categories": "cs.CV cs.LG", "Comments": ["5 pages", "4 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice", "after which this version may no longer be accessible"]}, "abstract": "Although current data augmentation methods are successful to alleviate the data insufficiency, conventional augmentation are primarily intra-domain while advanced generative adversarial networks (GANs) generate images remaining uncertain, particularly in small-scale datasets. In this paper, we propose a parameterized GAN (ParaGAN) that effectively controls the changes of synthetic samples among domains and highlights the attention regions for downstream classification. Specifically, ParaGAN incorporates projection distance parameters in cyclic projection and projects the source images to the decision boundary to obtain the class-difference maps. Our experiments show that ParaGAN can consistently outperform the existing augmentation methods with explainable classification on two small-scale medical datasets.", "url": "https://arxiv.org/abs/2311.14388"}, {"metadata": {"arXiv": "2311.14450", "Date": "Fri, 24 Nov 2023 12:57:34 ", "Title": "Segment (Almost) Nothing: Prompt-Agnostic Adversarial Attacks on Segmentation Models", "Authors": ["Francesco Croce", "Matthias Hein"], "Categories": "cs.CV cs.CR cs.LG"}, "abstract": "General purpose segmentation models are able to generate (semantic) segmentation masks from a variety of prompts, including visual (points, boxed, etc.) and textual (object names) ones. In particular, input images are pre-processed by an image encoder to obtain embedding vectors which are later used for mask predictions. Existing adversarial attacks target the end-to-end tasks, i.e. aim at altering the segmentation mask predicted for a specific image-prompt pair. However, this requires running an individual attack for each new prompt for the same image. We propose instead to generate prompt-agnostic adversarial attacks by maximizing the $\\ell_2$-distance, in the latent space, between the embedding of the original and perturbed images. Since the encoding process only depends on the image, distorted image representations will cause perturbations in the segmentation masks for a variety of prompts. We show that even imperceptible $\\ell_\\infty$-bounded perturbations of radius $\\epsilon=1/255$ are often sufficient to drastically modify the masks predicted with point, box and text prompts by recently proposed foundation models for segmentation. Moreover, we explore the possibility of creating universal, i.e. non image-specific, attacks which can be readily applied to any input without further computational cost.", "url": "https://arxiv.org/abs/2311.14450"}, {"metadata": {"arXiv": "2311.14485", "Date": "Fri, 24 Nov 2023 13:48:37 ", "Title": "Towards Interpretable Classification of Leukocytes based on Deep Learning", "Authors": ["Stefan R\\\"ohrl and Johannes Groll and Manuel Lengl and Simon Schumann and Christian Klenk and Dominik Heim and Martin Knopp and Oliver Hayden and Klaus Diepold"], "Categories": "cs.CV cs.LG", "Comments": ["Presented at the 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH) @ ICML 2023"]}, "abstract": "Label-free approaches are attractive in cytological imaging due to their flexibility and cost efficiency. They are supported by machine learning methods, which, despite the lack of labeling and the associated lower contrast, can classify cells with high accuracy where the human observer has little chance to discriminate cells. In order to better integrate these workflows into the clinical decision making process, this work investigates the calibration of confidence estimation for the automated classification of leukocytes. In addition, different visual explanation approaches are compared, which should bring machine decision making closer to professional healthcare applications. Furthermore, we were able to identify general detection patterns in neural networks and demonstrate the utility of the presented approaches in different scenarios of blood cell analysis.", "url": "https://arxiv.org/abs/2311.14485"}, {"metadata": {"arXiv": "2311.14094", "Date": "Thu, 23 Nov 2023 16:39:55 ", "Title": "Robust Decision Aggregation with Second-order Information", "Authors": ["Yuqi Pan", "Zhaohua Chen", "Yuqing Kong"], "Categories": "cs.GT cs.LG"}, "abstract": "We consider a decision aggregation problem with two experts who each make a binary recommendation after observing a private signal about an unknown binary world state. An agent, who does not know the joint information structure between signals and states, sees the experts' recommendations and aims to match the action with the true state. Under the scenario, we study whether supplemented additionally with second-order information (each expert's forecast on the other's recommendation) could enable a better aggregation. We adopt a minimax regret framework to evaluate the aggregator's performance, by comparing it to an omniscient benchmark that knows the joint information structure. With general information structures, we show that second-order information provides no benefit. No aggregator can improve over a trivial aggregator, which always follows the first expert's recommendation. However, positive results emerge when we assume experts' signals are conditionally independent given the world state. When the aggregator is deterministic, we present a robust aggregator that leverages second-order information, which can significantly outperform counterparts without it. Second, when two experts are homogeneous, by adding a non-degenerate assumption on the signals, we demonstrate that random aggregators using second-order information can surpass optimal ones without it. In the remaining settings, the second-order information is not beneficial. We also extend the above results to the setting when the aggregator's utility function is more general.", "url": "https://arxiv.org/abs/2311.14094"}, {"metadata": {"arXiv": "2311.13623", "Date": "Wed, 22 Nov 2023 09:21:28 ", "Title": "Density Distribution-based Learning Framework for Addressing Online Continual Learning Challenges", "Authors": ["Shilin Zhang", "Jiahui Wang"], "Categories": "cs.LG cs.CV", "Comments": ["10 pages", "6 figures"]}, "abstract": "In this paper, we address the challenges of online Continual Learning (CL) by introducing a density distribution-based learning framework. CL, especially the Class Incremental Learning, enables adaptation to new test distributions while continuously learning from a single-pass training data stream, which is more in line with the practical application requirements of real-world scenarios. However, existing CL methods often suffer from catastrophic forgetting and higher computing costs due to complex algorithm designs, limiting their practical use. Our proposed framework overcomes these limitations by achieving superior average accuracy and time-space efficiency, bridging the performance gap between CL and classical machine learning. Specifically, we adopt an independent Generative Kernel Density Estimation (GKDE) model for each CL task. During the testing stage, the GKDEs utilize a self-reported max probability density value to determine which one is responsible for predicting incoming test instances. A GKDE-based learning objective can ensure that samples with the same label are grouped together, while dissimilar instances are pushed farther apart. Extensive experiments conducted on multiple CL datasets validate the effectiveness of our proposed framework. Our method outperforms popular CL approaches by a significant margin, while maintaining competitive time-space efficiency, making our framework suitable for real-world applications. Code will be available at https://github.com/xxxx/xxxx.", "url": "https://arxiv.org/abs/2311.13623"}, {"metadata": {"arXiv": "2311.13624", "Date": "Wed, 22 Nov 2023 09:58:01 ", "Title": "A Theoretical Insight into Attack and Defense of Gradient Leakage in Transformer", "Authors": ["Chenyang Li", "Zhao Song", "Weixin Wang", "Chiwun Yang"], "Categories": "cs.LG cs.CR"}, "abstract": "The Deep Leakage from Gradient (DLG) attack has emerged as a prevalent and highly effective method for extracting sensitive training data by inspecting exchanged gradients. This approach poses a substantial threat to the privacy of individuals and organizations alike. This research presents a comprehensive analysis of the gradient leakage method when applied specifically to transformer-based models. Through meticulous examination, we showcase the capability to accurately recover data solely from gradients and rigorously investigate the conditions under which gradient attacks can be executed, providing compelling evidence. Furthermore, we reevaluate the approach of introducing additional noise on gradients as a protective measure against gradient attacks. To address this, we outline a theoretical proof that analyzes the associated privacy costs within the framework of differential privacy. Additionally, we affirm the convergence of the Stochastic Gradient Descent (SGD) algorithm under perturbed gradients. The primary objective of this study is to augment the understanding of gradient leakage attack and defense strategies while actively contributing to the development of privacy-preserving techniques specifically tailored for transformer-based models. By shedding light on the vulnerabilities and countermeasures associated with gradient leakage, this research aims to foster advancements in safeguarding sensitive data and upholding privacy in the context of transformer-based models.", "url": "https://arxiv.org/abs/2311.13624"}, {"metadata": {"arXiv": "2311.13648", "Date": "Wed, 22 Nov 2023 19:04:05 ", "Title": "Evaluating Pretrained models for Deployable Lifelong Learning", "Authors": ["Kiran Lekkala", "Eshan Bhargava", "Laurent Itti"], "Categories": "cs.LG", "Comments": ["In submission to CoLLA 2024. Also published in the Proceedings of WACV 2024 Workshop on Pretraining"]}, "abstract": "We create a novel benchmark for evaluating a Deployable Lifelong Learning system for Visual Reinforcement Learning (RL) that is pretrained on a curated dataset, and propose a novel Scalable Lifelong Learning system capable of retaining knowledge from the previously learnt RL tasks. Our benchmark measures the efficacy of a deployable Lifelong Learning system that is evaluated on scalability, performance and resource utilization. Our proposed system, once pretrained on the dataset, can be deployed to perform continual learning on unseen tasks. Our proposed method consists of a Few Shot Class Incremental Learning (FSCIL) based task-mapper and an encoder/backbone trained entirely using the pretrain dataset. The policy parameters corresponding to the recognized task are then loaded to perform the task. We show that this system can be scaled to incorporate a large number of tasks due to the small memory footprint and fewer computational resources. We perform experiments on our DeLL (Deployment for Lifelong Learning) benchmark on the Atari games to determine the efficacy of the system.", "url": "https://arxiv.org/abs/2311.13648"}, {"metadata": {"arXiv": "2311.13665", "Date": "Wed, 22 Nov 2023 19:39:37 ", "Title": "A Joint Gradient and Loss Based Clustered Federated Learning Design", "Authors": ["Licheng Lin", "Mingzhe Chen", "Zhaohui Yang", "Yusen Wu", "Yuchen Liu"], "Categories": "cs.LG"}, "abstract": "In this paper, a novel clustered FL framework that enables distributed edge devices with non-IID data to independently form several clusters in a distributed manner and implement FL training within each cluster is proposed. In particular, our designed clustered FL algorithm must overcome two challenges associated with FL training. First, the server has limited FL training information (i.e., the parameter server can only obtain the FL model information of each device) and limited computational power for finding the differences among a large amount of devices. Second, each device does not have the data information of other devices for device clustering and can only use global FL model parameters received from the server and its data information to determine its cluster identity, which will increase the difficulty of device clustering. To overcome these two challenges, we propose a joint gradient and loss based distributed clustering method in which each device determines its cluster identity considering the gradient similarity and training loss. The proposed clustering method not only considers how a local FL model of one device contributes to each cluster but also the direction of gradient descent thus improving clustering speed. By delegating clustering decisions to edge devices, each device can fully leverage its private data information to determine its own cluster identity, thereby reducing clustering overhead and improving overall clustering performance. Simulation results demonstrate that our proposed clustered FL algorithm can reduce clustering iterations by up to 99% compared to the existing baseline.", "url": "https://arxiv.org/abs/2311.13665"}, {"metadata": {"arXiv": "2311.13687", "Date": "Wed, 22 Nov 2023 20:47:52 ", "Title": "Beat-Aligned Spectrogram-to-Sequence Generation of Rhythm-Game Charts", "Authors": ["Jayeon Yi and Sungho Lee and Kyogu Lee"], "Categories": "cs.LG cs.MM cs.SD eess.AS", "Comments": ["ISMIR 2023 LBD. Demo videos and code at stet-stet.github.io/goct"]}, "abstract": "In the heart of \"rhythm games\" - games where players must perform actions in sync with a piece of music - are \"charts\", the directives to be given to players. We newly formulate chart generation as a sequence generation task and train a Transformer using a large dataset. We also introduce tempo-informed preprocessing and training procedures, some of which are suggested to be integral for a successful training. Our model is found to outperform the baselines on a large dataset, and is also found to benefit from pretraining and finetuning.", "url": "https://arxiv.org/abs/2311.13687"}, {"metadata": {"arXiv": "2311.13695", "Date": "Wed, 22 Nov 2023 21:07:45 ", "Title": "BackboneLearn: A Library for Scaling Mixed-Integer Optimization-Based Machine Learning", "Authors": ["Vassilis Digalakis Jr and Christos Ziakas"], "Categories": "cs.LG math.OC stat.ML"}, "abstract": "We present BackboneLearn: an open-source software package and framework for scaling mixed-integer optimization (MIO) problems with indicator variables to high-dimensional problems. This optimization paradigm can naturally be used to formulate fundamental problems in interpretable supervised learning (e.g., sparse regression and decision trees), in unsupervised learning (e.g., clustering), and beyond; BackboneLearn solves the aforementioned problems faster than exact methods and with higher accuracy than commonly used heuristics. The package is built in Python and is user-friendly and easily extensible: users can directly implement a backbone algorithm for their MIO problem at hand. The source code of BackboneLearn is available on GitHub (link: https://github.com/chziakas/backbone_learn).", "url": "https://arxiv.org/abs/2311.13695"}, {"metadata": {"arXiv": "2311.13745", "Date": "Thu, 23 Nov 2023 00:27:13 ", "Title": "Sample-Efficient Training for Diffusion", "Authors": ["Shivam Gupta", "Aditya Parulekar", "Eric Price", "Zhiyang Xun"], "Categories": "cs.LG cs.CV cs.IT math.IT math.ST stat.ML stat.TH"}, "abstract": "Score-based diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability. Recently, a number of theoretical works \\citep{chen2022, Chen2022ImprovedAO, Chenetal23flowode, benton2023linear} have shown that diffusion models can efficiently sample, assuming $L^2$-accurate score estimates. The score-matching objective naturally approximates the true score in $L^2$, but the sample complexity of existing bounds depends \\emph{polynomially} on the data radius and desired Wasserstein accuracy. By contrast, the time complexity of sampling is only logarithmic in these parameters. We show that estimating the score in $L^2$ \\emph{requires} this polynomial dependence, but that a number of samples that scales polylogarithmically in the Wasserstein accuracy actually do suffice for sampling. We show that with a polylogarithmic number of samples, the ERM of the score-matching objective is $L^2$ accurate on all but a probability $\\delta$ fraction of the true distribution, and that this weaker guarantee is sufficient for efficient sampling.", "url": "https://arxiv.org/abs/2311.13745"}, {"metadata": {"arXiv": "2311.13766", "Date": "Thu, 23 Nov 2023 01:43:00 ", "Title": "A Unified Framework for Fair Spectral Clustering With Effective Graph Learning", "Authors": ["Xiang Zhang", "Qiao Wang"], "Categories": "cs.LG cs.CY"}, "abstract": "We consider the problem of spectral clustering under group fairness constraints, where samples from each sensitive group are approximately proportionally represented in each cluster. Traditional fair spectral clustering (FSC) methods consist of two consecutive stages, i.e., performing fair spectral embedding on a given graph and conducting $k$means to obtain discrete cluster labels. However, in practice, the graph is usually unknown, and we need to construct the underlying graph from potentially noisy data, the quality of which inevitably affects subsequent fair clustering performance. Furthermore, performing FSC through separate steps breaks the connections among these steps, leading to suboptimal results. To this end, we first theoretically analyze the effect of the constructed graph on FSC. Motivated by the analysis, we propose a novel graph construction method with a node-adaptive graph filter to learn graphs from noisy data. Then, all independent stages of conventional FSC are integrated into a single objective function, forming an end-to-end framework that inputs raw data and outputs discrete cluster labels. An algorithm is developed to jointly and alternately update the variables in each stage. Finally, we conduct extensive experiments on synthetic, benchmark, and real data, which show that our model is superior to state-of-the-art fair clustering methods.", "url": "https://arxiv.org/abs/2311.13766"}, {"metadata": {"arXiv": "2311.13774", "Date": "Thu, 23 Nov 2023 02:19:32 ", "Title": "Learning Hierarchical Polynomials with Three-Layer Neural Networks", "Authors": ["Zihao Wang", "Eshaan Nichani", "Jason D. Lee"], "Categories": "cs.LG stat.ML", "Comments": ["57 pages"]}, "abstract": "We study the problem of learning hierarchical polynomials over the standard Gaussian distribution with three-layer neural networks. We specifically consider target functions of the form $h = g \\circ p$ where $p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a degree $k$ polynomial and $g: \\mathbb{R} \\rightarrow \\mathbb{R}$ is a degree $q$ polynomial. This function class generalizes the single-index model, which corresponds to $k=1$, and is a natural class of functions possessing an underlying hierarchical structure. Our main result shows that for a large subclass of degree $k$ polynomials $p$, a three-layer neural network trained via layerwise gradient descent on the square loss learns the target $h$ up to vanishing test error in $\\widetilde{\\mathcal{O}}(d^k)$ samples and polynomial time. This is a strict improvement over kernel methods, which require $\\widetilde \\Theta(d^{kq})$ samples, as well as existing guarantees for two-layer networks, which require the target function to be low-rank. Our result also generalizes prior works on three-layer neural networks, which were restricted to the case of $p$ being a quadratic. When $p$ is indeed a quadratic, we achieve the information-theoretically optimal sample complexity $\\widetilde{\\mathcal{O}}(d^2)$, which is an improvement over prior work~\\citep{nichani2023provable} requiring a sample size of $\\widetilde\\Theta(d^4)$. Our proof proceeds by showing that during the initial stage of training the network performs feature learning to recover the feature $p$ with $\\widetilde{\\mathcal{O}}(d^k)$ samples. This work demonstrates the ability of three-layer neural networks to learn complex features and as a result, learn a broad class of hierarchical functions.", "url": "https://arxiv.org/abs/2311.13774"}, {"metadata": {"arXiv": "2311.13817", "Date": "Thu, 23 Nov 2023 05:52:28 ", "Title": "Molecular Identification and Peak Assignment: Leveraging Multi-Level Multimodal Alignment on NMR", "Authors": ["Hao Xu", "Zhengyang Zhou", "Pengyu Hong"], "Categories": "cs.LG physics.chem-ph q-bio.QM"}, "abstract": "Nuclear magnetic resonance (NMR) spectroscopy plays an essential role across various scientific disciplines, providing valuable insights into molecular dynamics and interactions. Despite the promise of AI-enhanced NMR prediction models, challenges persist in the interpretation of spectra for tasks such as molecular retrieval, isomer recognition, and peak assignment. In response, this paper introduces Multi-Level Multimodal Alignment with Knowledge-Guided Instance-Wise Discrimination (K-M3AID) to establish meaningful correspondences between two heterogeneous modalities: molecular graphs (structures) and NMR spectra. In particular, K-M3AID employs a dual-coordinated contrastive learning architecture, and incorporates a graph-level alignment module, a node-level alignment module, and a communication channel. Notably, the framework introduces knowledge-guided instance-wise discrimination into contrastive learning within the node-level alignment module, significantly enhancing accuracy in cross-modal alignment. Additionally, K-M3AID showcases its capability of meta-learning by demonstrating that skills acquired during node-level alignment positively impact graph-level alignment. Empirical validation underscores K-M3AID's effectiveness in addressing multiple zero-shot tasks, offering a promising solution to bridge the gap between structural information and spectral data in complex NMR scenarios.", "url": "https://arxiv.org/abs/2311.13817"}, {"metadata": {"arXiv": "2311.13864", "Date": "Thu, 23 Nov 2023 09:08:43 ", "Title": "Which Matters Most in Making Fund Investment Decisions? A Multi-granularity Graph Disentangled Learning Framework", "Authors": ["Chunjing Gan", "Binbin Hu", "Bo Huang", "Tianyu Zhao", "Yingru Lin", "Wenliang Zhong", "Zhiqiang Zhang", "Jun Zhou", "Chuan Shi"], "Categories": "cs.LG", "Comments": ["Accepted by SIGIR 2023"]}, "abstract": "In this paper, we highlight that both conformity and risk preference matter in making fund investment decisions beyond personal interest and seek to jointly characterize these aspects in a disentangled manner. Consequently, we develop a novel M ulti-granularity Graph Disentangled Learning framework named MGDL to effectively perform intelligent matching of fund investment products. Benefiting from the well-established fund graph and the attention module, multi-granularity user representations are derived from historical behaviors to separately express personal interest, conformity and risk preference in a fine-grained way. To attain stronger disentangled representations with specific semantics, MGDL explicitly involve two self-supervised signals, i.e., fund type based contrasts and fund popularity. Extensive experiments in offline and online environments verify the effectiveness of MGDL.", "url": "https://arxiv.org/abs/2311.13864"}, {"metadata": {"arXiv": "2311.13870", "Date": "Thu, 23 Nov 2023 09:27:08 ", "Title": "L(M)V-IQL: Multiple Intention Inverse Reinforcement Learning for Animal Behavior Characterization", "Authors": ["Hao Zhu", "Brice De La Crompe", "Gabriel Kalweit", "Artur Schneider", "Maria Kalweit", "Ilka Diester", "Joschka Boedecker"], "Categories": "cs.LG q-bio.NC"}, "abstract": "In advancing the understanding of decision-making processes, mathematical models, particularly Inverse Reinforcement Learning (IRL), have proven instrumental in reconstructing animal's multiple intentions amidst complex behaviors. Given the recent development of a continuous-time multi-intention IRL framework, there has been persistent inquiry into inferring discrete time-varying reward functions with multiple intention IRL approaches. To tackle the challenge, we introduce the Latent (Markov) Variable Inverse Q-learning (L(M)V-IQL) algorithms, a novel IRL framework tailored for accommodating discrete intrinsic rewards. Leveraging an Expectation-Maximization approach, we cluster observed trajectories into distinct intentions and independently solve the IRL problem for each. Demonstrating the efficacy of L(M)V-IQL through simulated experiments and its application to different real mouse behavior datasets, our approach surpasses current benchmarks in animal behavior prediction, producing interpretable reward functions. This advancement holds promise for neuroscience and psychology, contributing to a deeper understanding of animal decision-making and uncovering underlying brain mechanisms.", "url": "https://arxiv.org/abs/2311.13870"}, {"metadata": {"arXiv": "2311.13877", "Date": "Thu, 23 Nov 2023 09:57:35 ", "Title": "Locally Optimal Descent for Dynamic Stepsize Scheduling", "Authors": ["Gilad Yehudai", "Alon Cohen", "Amit Daniely", "Yoel Drori", "Tomer Koren", "Mariano Schain"], "Categories": "cs.LG math.OC stat.ML"}, "abstract": "We introduce a novel dynamic learning-rate scheduling scheme grounded in theory with the goal of simplifying the manual and time-consuming tuning of schedules in practice. Our approach is based on estimating the locally-optimal stepsize, guaranteeing maximal descent in the direction of the stochastic gradient of the current step. We first establish theoretical convergence bounds for our method within the context of smooth non-convex stochastic optimization, matching state-of-the-art bounds while only assuming knowledge of the smoothness parameter. We then present a practical implementation of our algorithm and conduct systematic experiments across diverse datasets and optimization algorithms, comparing our scheme with existing state-of-the-art learning-rate schedulers. Our findings indicate that our method needs minimal tuning when compared to existing approaches, removing the need for auxiliary manual schedules and warm-up phases and achieving comparable performance with drastically reduced parameter tuning.", "url": "https://arxiv.org/abs/2311.13877"}, {"metadata": {"arXiv": "2311.13883", "Date": "Thu, 23 Nov 2023 10:13:07 ", "Title": "Leveraging Optimal Transport via Projections on Subspaces for Machine Learning Applications", "Authors": ["Cl\\'ement Bonet"], "Categories": "cs.LG", "Comments": ["PhD Thesis"]}, "abstract": "Optimal Transport has received much attention in Machine Learning as it allows to compare probability distributions by exploiting the geometry of the underlying space. However, in its original formulation, solving this problem suffers from a significant computational burden. Thus, a meaningful line of work consists at proposing alternatives to reduce this burden while still enjoying its properties. In this thesis, we focus on alternatives which use projections on subspaces. The main such alternative is the Sliced-Wasserstein distance, which we first propose to extend to Riemannian manifolds in order to use it in Machine Learning applications for which using such spaces has been shown to be beneficial in the recent years. We also study sliced distances between positive measures in the so-called unbalanced OT problem. Back to the original Euclidean Sliced-Wasserstein distance between probability measures, we study the dynamic of gradient flows when endowing the space with this distance in place of the usual Wasserstein distance. Then, we investigate the use of the Busemann function, a generalization of the inner product in metric spaces, in the space of probability measures. Finally, we extend the subspace detour approach to incomparable spaces using the Gromov-Wasserstein distance.", "url": "https://arxiv.org/abs/2311.13883"}, {"metadata": {"arXiv": "2311.13887", "Date": "Thu, 23 Nov 2023 10:18:21 ", "Title": "Unsupervised Learning for Topological Classification of Transportation Networks", "Authors": ["Sina Sabzekar", "Mohammad Reza Valipour Malakshah", "Zahra Amini"], "Categories": "cs.LG"}, "abstract": "With increasing urbanization, transportation plays an increasingly critical role in city development. The number of studies on modeling, optimization, simulation, and data analysis of transportation systems is on the rise. Many of these studies utilize transportation test networks to represent real-world transportation systems in urban areas, examining the efficacy of their proposed approaches. Each of these networks exhibits unique characteristics in their topology, making their applications distinct for various study objectives. Despite their widespread use in research, there is a lack of comprehensive study addressing the classification of these networks based on their topological characteristics. This study aims to fill this gap by employing unsupervised learning methods, particularly clustering. We present a comprehensive framework for evaluating various topological network characteristics. Additionally, we employ two dimensionality reduction techniques, namely Principal Component Analysis (PCA) and Isometric Feature Mapping (ISOMAP), to reduce overlaps of highly correlated features and enhance the interpretability of the subsequent classification results. We then utilize two clustering algorithms, K-means and HDBSCAN, to classify 14 transportation networks. The PCA method, followed by the K-means clustering approach, outperforms other alternatives with a Silhouette score of $0.510$, enabling the classification of transportation networks into five clusters. We also provide a detailed discussion on the resulting classification.", "url": "https://arxiv.org/abs/2311.13887"}, {"metadata": {"arXiv": "2311.13949", "Date": "Thu, 23 Nov 2023 12:02:58 ", "Title": "Optimal Power Flow in Highly Renewable Power System Based on Attention Neural Networks", "Authors": ["Chen Li", "Alexander Kies", "Kai Zhou", "Markus Schlott", "Omar El Sayed", "Mariia Bilousova and Horst Stoecker"], "Categories": "cs.LG cs.SY eess.SY", "Comments": ["Submitted to Elsevier"]}, "abstract": "The Optimal Power Flow (OPF) problem is pivotal for power system operations, guiding generator output and power distribution to meet demand at minimized costs, while adhering to physical and engineering constraints. The integration of renewable energy sources, like wind and solar, however, poses challenges due to their inherent variability. This variability, driven largely by changing weather conditions, demands frequent recalibrations of power settings, thus necessitating recurrent OPF resolutions. This task is daunting using traditional numerical methods, particularly for extensive power systems. In this work, we present a cutting-edge, physics-informed machine learning methodology, trained using imitation learning and historical European weather datasets. Our approach directly correlates electricity demand and weather patterns with power dispatch and generation, circumventing the iterative requirements of traditional OPF solvers. This offers a more expedient solution apt for real-time applications. Rigorous evaluations on aggregated European power systems validate our method's superiority over existing data-driven techniques in OPF solving. By presenting a quick, robust, and efficient solution, this research sets a new standard in real-time OPF resolution, paving the way for more resilient power systems in the era of renewable energy.", "url": "https://arxiv.org/abs/2311.13949"}, {"metadata": {"arXiv": "2311.13950", "Date": "Thu, 23 Nov 2023 12:03:02 ", "Title": "Object Location Prediction in Real-time using LSTM Neural Network and Polynomial Regression", "Authors": ["Petar Stojkovi\\'c", "Predrag Tadi\\'c"], "Categories": "cs.LG cs.RO"}, "abstract": "This paper details the design and implementation of a system for predicting and interpolating object location coordinates. Our solution is based on processing inertial measurements and global positioning system data through a Long Short-Term Memory (LSTM) neural network and polynomial regression. LSTM is a type of recurrent neural network (RNN) particularly suited for processing data sequences and avoiding the long-term dependency problem. We employed data from real-world vehicles and the global positioning system (GPS) sensors. A critical pre-processing step was developed to address varying sensor frequencies and inconsistent GPS time steps and dropouts. The LSTM-based system's performance was compared with the Kalman Filter. The system was tuned to work in real-time with low latency and high precision. We tested our system on roads under various driving conditions, including acceleration, turns, deceleration, and straight paths. We tested our proposed solution's accuracy and inference time and showed that it could perform in real-time. Our LSTM-based system yielded an average error of 0.11 meters with an inference time of 2 ms. This represents a 76\\% reduction in error compared to the traditional Kalman filter method, which has an average error of 0.46 meters with a similar inference time to the LSTM-based system.", "url": "https://arxiv.org/abs/2311.13950"}, {"metadata": {"arXiv": "2311.13959", "Date": "Thu, 23 Nov 2023 12:17:45 ", "Title": "RankFeat\\&RankWeight: Rank-1 Feature/Weight Removal for Out-of-distribution Detection", "Authors": ["Yue Song", "Nicu Sebe", "Wei Wang"], "Categories": "cs.LG cs.CV", "Comments": ["submitted to T-PAMI"]}, "abstract": "The task of out-of-distribution (OOD) detection is crucial for deploying machine learning models in real-world settings. In this paper, we observe that the singular value distributions of the in-distribution (ID) and OOD features are quite different: the OOD feature matrix tends to have a larger dominant singular value than the ID feature, and the class predictions of OOD samples are largely determined by it. This observation motivates us to propose \\texttt{RankFeat}, a simple yet effective \\emph{post hoc} approach for OOD detection by removing the rank-1 matrix composed of the largest singular value and the associated singular vectors from the high-level feature. \\texttt{RankFeat} achieves \\emph{state-of-the-art} performance and reduces the average false positive rate (FPR95) by 17.90\\% compared with the previous best method. The success of \\texttt{RankFeat} motivates us to investigate whether a similar phenomenon would exist in the parameter matrices of neural networks. We thus propose \\texttt{RankWeight} which removes the rank-1 weight from the parameter matrices of a single deep layer. Our \\texttt{RankWeight}is also \\emph{post hoc} and only requires computing the rank-1 matrix once. As a standalone approach, \\texttt{RankWeight} has very competitive performance against other methods across various backbones. Moreover, \\texttt{RankWeight} enjoys flexible compatibility with a wide range of OOD detection methods. The combination of \\texttt{RankWeight} and \\texttt{RankFeat} refreshes the new \\emph{state-of-the-art} performance, achieving the FPR95 as low as 16.13\\% on the ImageNet-1k benchmark. Extensive ablation studies and comprehensive theoretical analyses are presented to support the empirical results.", "url": "https://arxiv.org/abs/2311.13959"}, {"metadata": {"arXiv": "2311.13978", "Date": "Thu, 23 Nov 2023 12:47:43 ", "Title": "MedISure: Towards Assuring Machine Learning-based Medical Image Classifiers using Mixup Boundary Analysis", "Authors": ["Adam Byfield", "William Poulett", "Ben Wallace", "Anusha Jose", "Shatakshi Tyagi", "Smita Shembekar", "Adnan Qayyum", "Junaid Qadir", "and Muhammad Bilal"], "Categories": "cs.LG eess.IV"}, "abstract": "Machine learning (ML) models are becoming integral in healthcare technologies, presenting a critical need for formal assurance to validate their safety, fairness, robustness, and trustworthiness. These models are inherently prone to errors, potentially posing serious risks to patient health and could even cause irreparable harm. Traditional software assurance techniques rely on fixed code and do not directly apply to ML models since these algorithms are adaptable and learn from curated datasets through a training process. However, adapting established principles, such as boundary testing using synthetic test data can effectively bridge this gap. To this end, we present a novel technique called Mix-Up Boundary Analysis (MUBA) that facilitates evaluating image classifiers in terms of prediction fairness. We evaluated MUBA for two important medical imaging tasks -- brain tumour classification and breast cancer classification -- and achieved promising results. This research aims to showcase the importance of adapting traditional assurance principles for assessing ML models to enhance the safety and reliability of healthcare technologies. To facilitate future research, we plan to publicly release our code for MUBA.", "url": "https://arxiv.org/abs/2311.13978"}, {"metadata": {"arXiv": "2311.14014", "Date": "Thu, 23 Nov 2023 14:11:01 ", "Title": "On the Hyperparameter Landscapes of Machine Learning Algorithms", "Authors": ["Mingyu Huang", "Ke Li"], "Categories": "cs.LG"}, "abstract": "Despite the recent success in a plethora of hyperparameter optimization (HPO) methods for machine learning (ML) models, the intricate interplay between model hyperparameters (HPs) and predictive losses (a.k.a fitness), which is a key prerequisite for understanding HPO, remain notably underexplored in our community. This results in limited explainability in the HPO process, rendering a lack of human trust and difficulties in pinpointing algorithm bottlenecks. In this paper, we aim to shed light on this black box by conducting large-scale fitness landscape analysis (FLA) on 1,500 HP loss landscapes of 6 ML models with more than 11 model configurations, across 67 datasets and different levels of fidelities. We reveal the first unified, comprehensive portrait of their topographies in terms of smoothness, neutrality and modality. We also show that such properties are highly transferable across datasets and fidelities, providing fundamental evidence for the success of multi-fidelity and transfer learning methods. These findings are made possible by developing a dedicated FLA framework that incorporates a combination of visual and quantitative measures. We further demonstrate the potential of this framework by analyzing the NAS-Bench-101 landscape, and we believe it is able to faciliate fundamental understanding of a broader range of AutoML tasks.", "url": "https://arxiv.org/abs/2311.14014"}, {"metadata": {"arXiv": "2311.14033", "Date": "Thu, 23 Nov 2023 14:38:10 ", "Title": "Multivariate Scenario Generation of Day-Ahead Electricity Prices using Normalizing Flows", "Authors": ["Hannes Hilger", "Dirk Witthaut", "Manuel Dahmen", "Leonardo Rydin Gorjao", "Julius Trebbien", "Eike Cramer"], "Categories": "cs.LG", "Comments": ["15 pages", "8 figures"]}, "abstract": "Trading on electricity markets requires accurate information about the realization of electricity prices and the uncertainty attached to the predictions. We present a probabilistic forecasting approach for day-ahead electricity prices using the fully data-driven deep generative model called normalizing flows. Our modeling approach generates full-day scenarios of day-ahead electricity prices based on conditional features such as residual load forecasts. Furthermore, we propose extended feature sets of prior realizations and a periodic retraining scheme that allows the normalizing flow to adapt to the changing conditions of modern electricity markets. In particular, we investigate the impact of the energy crisis ensuing from the Russian invasion of Ukraine. Our results highlight that the normalizing flow generates high-quality scenarios that reproduce the true price distribution and yield highly accurate forecasts. Additionally, our analysis highlights how our improvements towards adaptations in changing regimes allow the normalizing flow to adapt to changing market conditions and enables continued sampling of high-quality day-ahead price scenarios.", "url": "https://arxiv.org/abs/2311.14033"}, {"metadata": {"arXiv": "2311.14037", "Date": "Thu, 23 Nov 2023 14:42:43 ", "Title": "AdapterFL: Adaptive Heterogeneous Federated Learning for Resource-constrained Mobile Computing Systems", "Authors": ["Ruixuan Liu and Ming Hu and Zeke Xia and Jun Xia and Pengyu Zhang and Yihao Huang and Yang Liu and Mingsong Chen"], "Categories": "cs.LG"}, "abstract": "Federated Learning (FL) enables collaborative learning of large-scale distributed clients without data sharing. However, due to the disparity of computing resources among massive mobile computing devices, the performance of traditional homogeneous model-based Federated Learning (FL) is seriously limited. On the one hand, to achieve model training in all the diverse clients, mobile computing systems can only use small low-performance models for collaborative learning. On the other hand, devices with high computing resources cannot train a high-performance large model with their insufficient raw data. To address the resource-constrained problem in mobile computing systems, we present a novel heterogeneous FL approach named AdapterFL, which uses a model reassemble strategy to facilitate collaborative training of massive heterogeneous mobile devices adaptively. Specifically, we select multiple candidate heterogeneous models based on the computing performance of massive mobile devices and then divide each heterogeneous model into two partitions. By reassembling the partitions, we can generate models with varied sizes that are combined by the partial parameters of the large model with the partial parameters of the small model. Using these reassembled models for FL training, we can train the partial parameters of the large model using low-performance devices. In this way, we can alleviate performance degradation in large models due to resource constraints. The experimental results show that AdapterFL can achieve up to 12\\% accuracy improvement compared to the state-of-the-art heterogeneous federated learning methods in resource-constrained scenarios.", "url": "https://arxiv.org/abs/2311.14037"}, {"metadata": {"arXiv": "2311.14056", "Date": "Thu, 23 Nov 2023 15:19:30 ", "Title": "DPSUR: Accelerating Differentially Private Stochastic Gradient Descent Using Selective Update and Release", "Authors": ["Jie Fu", "Qingqing Ye", "Haibo Hu", "Zhili Chen", "Lulu Wang", "Kuncan Wang", "Ran Xun"], "Categories": "cs.LG cs.CR", "Comments": ["This paper has been accepted by VLDB 2024"]}, "abstract": "Machine learning models are known to memorize private data to reduce their training loss, which can be inadvertently exploited by privacy attacks such as model inversion and membership inference. To protect against these attacks, differential privacy (DP) has become the de facto standard for privacy-preserving machine learning, particularly those popular training algorithms using stochastic gradient descent, such as DPSGD. Nonetheless, DPSGD still suffers from severe utility loss due to its slow convergence. This is partially caused by the random sampling, which brings bias and variance to the gradient, and partially by the Gaussian noise, which leads to fluctuation of gradient updates. Our key idea to address these issues is to apply selective updates to the model training, while discarding those useless or even harmful updates. Motivated by this, this paper proposes DPSUR, a Differentially Private training framework based on Selective Updates and Release, where the gradient from each iteration is evaluated based on a validation test, and only those updates leading to convergence are applied to the model. As such, DPSUR ensures the training in the right direction and thus can achieve faster convergence than DPSGD. The main challenges lie in two aspects -- privacy concerns arising from gradient evaluation, and gradient selection strategy for model update. To address the challenges, DPSUR introduces a clipping strategy for update randomization and a threshold mechanism for gradient selection. Experiments conducted on MNIST, FMNIST, CIFAR-10, and IMDB datasets show that DPSUR significantly outperforms previous works in terms of convergence speed and model utility.", "url": "https://arxiv.org/abs/2311.14056"}, {"metadata": {"arXiv": "2311.14077", "Date": "Thu, 23 Nov 2023 16:08:52 ", "Title": "RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation", "Authors": ["Yiming Wang", "Yuxuan Song", "Minkai Xu", "Rui Wang", "Hao Zhou", "Weiying Ma"], "Categories": "cs.LG q-bio.QM"}, "abstract": "Retrosynthesis poses a fundamental challenge in biopharmaceuticals, aiming to aid chemists in finding appropriate reactant molecules and synthetic pathways given determined product molecules. With the reactant and product represented as 2D graphs, retrosynthesis constitutes a conditional graph-to-graph generative task. Inspired by the recent advancements in discrete diffusion models for graph generation, we introduce Retrosynthesis Diffusion (RetroDiff), a novel diffusion-based method designed to address this problem. However, integrating a diffusion-based graph-to-graph framework while retaining essential chemical reaction template information presents a notable challenge. Our key innovation is to develop a multi-stage diffusion process. In this method, we decompose the retrosynthesis procedure to first sample external groups from the dummy distribution given products and then generate the external bonds to connect the products and generated groups. Interestingly, such a generation process is exactly the reverse of the widely adapted semi-template retrosynthesis procedure, i.e. from reaction center identification to synthon completion, which significantly reduces the error accumulation. Experimental results on the benchmark have demonstrated the superiority of our method over all other semi-template methods.", "url": "https://arxiv.org/abs/2311.14077"}, {"metadata": {"arXiv": "2311.14079", "Date": "Thu, 23 Nov 2023 16:14:24 ", "Title": "Empirical Comparison between Cross-Validation and Mutation-Validation in Model Selection", "Authors": ["Jinyang Yu", "Sami Hamdan", "Leonard Sasse", "Abigail Morrison", "Kaustubh R. Patil"], "Categories": "cs.LG stat.ML"}, "abstract": "Mutation validation (MV) is a recently proposed approach for model selection, garnering significant interest due to its unique characteristics and potential benefits compared to the widely used cross-validation (CV) method. In this study, we empirically compared MV and $k$-fold CV using benchmark and real-world datasets. By employing Bayesian tests, we compared generalization estimates yielding three posterior probabilities: practical equivalence, CV superiority, and MV superiority. We also evaluated the differences in the capacity of the selected models and computational efficiency. We found that both MV and CV select models with practically equivalent generalization performance across various machine learning algorithms and the majority of benchmark datasets. MV exhibited advantages in terms of selecting simpler models and lower computational costs. However, in some cases MV selected overly simplistic models leading to underfitting and showed instability in hyperparameter selection. These limitations of MV became more evident in the evaluation of a real-world neuroscientific task of predicting sex at birth using brain functional connectivity.", "url": "https://arxiv.org/abs/2311.14079"}, {"metadata": {"arXiv": "2311.14090", "Date": "Thu, 23 Nov 2023 16:36:03 ", "Title": "Class Uncertainty: A Measure to Mitigate Class Imbalance", "Authors": ["Z. S. Baltaci", "K. Oksuz", "S. Kuzucu", "K. Tezoren", "B. K. Konar", "A. Ozkan", "E. Akbas", "S. Kalkan"], "Categories": "cs.LG cs.CV"}, "abstract": "Class-wise characteristics of training examples affect the performance of deep classifiers. A well-studied example is when the number of training examples of classes follows a long-tailed distribution, a situation that is likely to yield sub-optimal performance for under-represented classes. This class imbalance problem is conventionally addressed by approaches relying on the class-wise cardinality of training examples, such as data resampling. In this paper, we demonstrate that considering solely the cardinality of classes does not cover all issues causing class imbalance. To measure class imbalance, we propose \"Class Uncertainty\" as the average predictive uncertainty of the training examples, and we show that this novel measure captures the differences across classes better than cardinality. We also curate SVCI-20 as a novel dataset in which the classes have equal number of training examples but they differ in terms of their hardness; thereby causing a type of class imbalance which cannot be addressed by the approaches relying on cardinality. We incorporate our \"Class Uncertainty\" measure into a diverse set of ten class imbalance mitigation methods to demonstrate its effectiveness on long-tailed datasets as well as on our SVCI-20. Code and datasets will be made available.", "url": "https://arxiv.org/abs/2311.14090"}, {"metadata": {"arXiv": "2311.14101", "Date": "Thu, 23 Nov 2023 17:01:16 ", "Title": "Subnetwork Ensembles", "Authors": ["Tim Whitaker"], "Categories": "cs.LG cs.NE", "Comments": ["116 Pages", "21 figures", "Accepted PhD Dissertation"]}, "abstract": "Neural network ensembles have been effectively used to improve generalization by combining the predictions of multiple independently trained models. However, the growing scale and complexity of deep neural networks have led to these methods becoming prohibitively expensive and time consuming to implement. Low-cost ensemble methods have become increasingly important as they can alleviate the need to train multiple models from scratch while retaining the generalization benefits that traditional ensemble learning methods afford. This dissertation introduces and formalizes a low-cost framework for constructing Subnetwork Ensembles, where a collection of child networks are formed by sampling, perturbing, and optimizing subnetworks from a trained parent model. We explore several distinct methodologies for generating child networks and we evaluate their efficacy through a variety of ablation studies and established benchmarks. Our findings reveal that this approach can greatly improve training efficiency, parametric utilization, and generalization performance while minimizing computational cost. Subnetwork Ensembles offer a compelling framework for exploring how we can build better systems by leveraging the unrealized potential of deep neural networks.", "url": "https://arxiv.org/abs/2311.14101"}, {"metadata": {"arXiv": "2311.14108", "Date": "Thu, 23 Nov 2023 17:09:12 ", "Title": "MINTY: Rule-based Models that Minimize the Need for Imputing Features with Missing Values", "Authors": ["Lena Stempfle and Fredrik D. Johansson"], "Categories": "cs.LG"}, "abstract": "Rule models are often preferred in prediction tasks with tabular inputs as they can be easily interpreted using natural language and provide predictive performance on par with more complex models. However, most rule models' predictions are undefined or ambiguous when some inputs are missing, forcing users to rely on statistical imputation models or heuristics like zero imputation, undermining the interpretability of the models. In this work, we propose fitting concise yet precise rule models that learn to avoid relying on features with missing values and, therefore, limit their reliance on imputation at test time. We develop MINTY, a method that learns rules in the form of disjunctions between variables that act as replacements for each other when one or more is missing. This results in a sparse linear rule model, regularized to have small dependence on features with missing values, that allows a trade-off between goodness of fit, interpretability, and robustness to missing values at test time. We demonstrate the value of MINTY in experiments using synthetic and real-world data sets and find its predictive performance comparable or favorable to baselines, with smaller reliance on features with missing values.", "url": "https://arxiv.org/abs/2311.14108"}, {"metadata": {"arXiv": "2311.14120", "Date": "Thu, 23 Nov 2023 17:30:31 ", "Title": "Weight fluctuations in (deep) linear neural networks and a derivation of the inverse-variance flatness relation", "Authors": ["Markus Gross", "Arne P. Raulf", "Christoph R\\\"ath"], "Categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech", "Comments": ["25 pages", "7 figures"]}, "abstract": "We investigate the stationary (late-time) training regime of single- and two-layer linear neural networks within the continuum limit of stochastic gradient descent (SGD) for synthetic Gaussian data. In the case of a single-layer network in the weakly oversampled regime, the spectrum of the noise covariance matrix deviates notably from the Hessian, which can be attributed to the broken detailed balance of SGD dynamics. The weight fluctuations are in this case generally anisotropic, but experience an isotropic loss. For a two-layer network, we obtain the stochastic dynamics of the weights in each layer and analyze the associated stationary covariances. We identify the inter-layer coupling as a new source of anisotropy for the weight fluctuations. In contrast to the single-layer case, the weight fluctuations experience an anisotropic loss, the flatness of which is inversely related to the fluctuation variance. We thereby provide an analytical derivation of the recently observed inverse variance-flatness relation in a deep linear network model.", "url": "https://arxiv.org/abs/2311.14120"}, {"metadata": {"arXiv": "2311.14131", "Date": "Thu, 23 Nov 2023 17:59:48 ", "Title": "Exactly conservative physics-informed neural networks and deep operator networks for dynamical systems", "Authors": ["Elsa Cardoso-Bihlo and Alex Bihlo"], "Categories": "cs.LG cs.NA math.NA", "Comments": ["12 pages", "6 figures", "1 algorithm"]}, "abstract": "We introduce a method for training exactly conservative physics-informed neural networks and physics-informed deep operator networks for dynamical systems. The method employs a projection-based technique that maps a candidate solution learned by the neural network solver for any given dynamical system possessing at least one first integral onto an invariant manifold. We illustrate that exactly conservative physics-informed neural network solvers and physics-informed deep operator networks for dynamical systems vastly outperform their non-conservative counterparts for several real-world problems from the mathematical sciences.", "url": "https://arxiv.org/abs/2311.14131"}, {"metadata": {"arXiv": "2311.14136", "Date": "Thu, 23 Nov 2023 18:06:05 ", "Title": "A Blockchain Solution for Collaborative Machine Learning over IoT", "Authors": ["Carlos Beis-Penedo and Francisco Troncoso-Pastoriza and Rebeca P. D\\'iaz-Redondo and Ana Fern\\'andez-Vilas and Manuel Fern\\'andez-Veiga and Mart\\'in Gonz\\'alez Soto"], "Categories": "cs.LG cs.CR cs.NI"}, "abstract": "The rapid growth of Internet of Things (IoT) devices and applications has led to an increased demand for advanced analytics and machine learning techniques capable of handling the challenges associated with data privacy, security, and scalability. Federated learning (FL) and blockchain technologies have emerged as promising approaches to address these challenges by enabling decentralized, secure, and privacy-preserving model training on distributed data sources. In this paper, we present a novel IoT solution that combines the incremental learning vector quantization algorithm (XuILVQ) with Ethereum blockchain technology to facilitate secure and efficient data sharing, model training, and prototype storage in a distributed environment. Our proposed architecture addresses the shortcomings of existing blockchain-based FL solutions by reducing computational and communication overheads while maintaining data privacy and security. We assess the performance of our system through a series of experiments, showcasing its potential to enhance the accuracy and efficiency of machine learning tasks in IoT settings.", "url": "https://arxiv.org/abs/2311.14136"}, {"metadata": {"arXiv": "2311.14137", "Date": "Thu, 23 Nov 2023 18:08:15 ", "Title": "Privacy-Preserving Algorithmic Recourse", "Authors": ["Sikha Pentyala", "Shubham Sharma", "Sanjay Kariyappa", "Freddy Lecue", "Daniele Magazzeni"], "Categories": "cs.LG cs.CR", "Comments": ["Accepted at 3rd International Workshop on Explainable AI in Finance", "ICAIF 2023"]}, "abstract": "When individuals are subject to adverse outcomes from machine learning models, providing a recourse path to help achieve a positive outcome is desirable. Recent work has shown that counterfactual explanations - which can be used as a means of single-step recourse - are vulnerable to privacy issues, putting an individuals' privacy at risk. Providing a sequential multi-step path for recourse can amplify this risk. Furthermore, simply adding noise to recourse paths found from existing methods can impact the realism and actionability of the path for an end-user. In this work, we address privacy issues when generating realistic recourse paths based on instance-based counterfactual explanations, and provide PrivRecourse: an end-to-end privacy preserving pipeline that can provide realistic recourse paths. PrivRecourse uses differentially private (DP) clustering to represent non-overlapping subsets of the private dataset. These DP cluster centers are then used to generate recourse paths by forming a graph with cluster centers as the nodes, so that we can generate realistic - feasible and actionable - recourse paths. We empirically evaluate our approach on finance datasets and compare it to simply adding noise to data instances, and to using DP synthetic data, to generate the graph. We observe that PrivRecourse can provide paths that are private and realistic.", "url": "https://arxiv.org/abs/2311.14137"}, {"metadata": {"arXiv": "2311.14182", "Date": "Thu, 23 Nov 2023 20:03:51 ", "Title": "Gradient-based bilevel optimization for multi-penalty Ridge regression through matrix differential calculus", "Authors": ["Gabriele Maroni", "Loris Cannelli", "Dario Piga"], "Categories": "cs.LG stat.ML"}, "abstract": "Common regularization algorithms for linear regression, such as LASSO and Ridge regression, rely on a regularization hyperparameter that balances the tradeoff between minimizing the fitting error and the norm of the learned model coefficients. As this hyperparameter is scalar, it can be easily selected via random or grid search optimizing a cross-validation criterion. However, using a scalar hyperparameter limits the algorithm's flexibility and potential for better generalization. In this paper, we address the problem of linear regression with l2-regularization, where a different regularization hyperparameter is associated with each input variable. We optimize these hyperparameters using a gradient-based approach, wherein the gradient of a cross-validation criterion with respect to the regularization hyperparameters is computed analytically through matrix differential calculus. Additionally, we introduce two strategies tailored for sparse model learning problems aiming at reducing the risk of overfitting to the validation data. Numerical examples demonstrate that our multi-hyperparameter regularization approach outperforms LASSO, Ridge, and Elastic Net regression. Moreover, the analytical computation of the gradient proves to be more efficient in terms of computational time compared to automatic differentiation, especially when handling a large number of input variables. Application to the identification of over-parameterized Linear Parameter-Varying models is also presented.", "url": "https://arxiv.org/abs/2311.14182"}, {"metadata": {"arXiv": "2311.14195", "Date": "Thu, 23 Nov 2023 20:31:48 ", "Title": "Touch Analysis: An Empirical Evaluation of Machine Learning Classification Algorithms on Touch Data", "Authors": ["Melodee Montgomery", "Prosenjit Chatterjee", "John Jenkins", "and Kaushik Roy"], "Categories": "cs.LG cs.NE"}, "abstract": "Our research aims at classifying individuals based on their unique interactions on touchscreen-based smartphones. In this research, we use Touch-Analytics datasets, which include 41 subjects and 30 different behavioral features. Furthermore, we derived new features from the raw data to improve the overall authentication performance. Previous research has already been done on the Touch-Analytics datasets with the state-of-the-art classifiers, including Support Vector Machine (SVM) and k-nearest neighbor (kNN), and achieved equal error rates (EERs) between 0% to 4%. Here, we propose a novel Deep Neural Net (DNN) architecture to classify the individuals correctly. The proposed DNN architecture has three dense layers and uses many-to-many mapping techniques. When we combine the new features with the existing ones, SVM and kNN achieved the classification accuracy of 94.7% and 94.6%, respectively. This research explored seven other classifiers and out of them, the decision tree and our proposed DNN classifiers resulted in the highest accuracy of 100%. The others included: Logistic Regression (LR), Linear Discriminant Analysis (LDA), Gaussian Naive Bayes (NB), Neural Network, and VGGNet with the following accuracy scores of 94.7%, 95.9%, 31.9%, 88.8%, and 96.1%, respectively.", "url": "https://arxiv.org/abs/2311.14195"}, {"metadata": {"arXiv": "2311.14214", "Date": "Thu, 23 Nov 2023 22:08:29 ", "Title": "Extending Variability-Aware Model Selection with Bias Detection in Machine Learning Projects", "Authors": ["Cristina Tavares", "Nathalia Nascimento", "Paulo Alencar", "Donald Cowan"], "Categories": "cs.LG cs.SE", "Comments": ["IEEE BigData 2023"]}, "abstract": "Data science projects often involve various machine learning (ML) methods that depend on data, code, and models. One of the key activities in these projects is the selection of a model or algorithm that is appropriate for the data analysis at hand. ML model selection depends on several factors, which include data-related attributes such as sample size, functional requirements such as the prediction algorithm type, and non-functional requirements such as performance and bias. However, the factors that influence such selection are often not well understood and explicitly represented. This paper describes ongoing work on extending an adaptive variability-aware model selection method with bias detection in ML projects. The method involves: (i) modeling the variability of the factors that affect model selection using feature models based on heuristics proposed in the literature; (ii) instantiating our variability model with added features related to bias (e.g., bias-related metrics); and (iii) conducting experiments that illustrate the method in a specific case study to illustrate our approach based on a heart failure prediction project. The proposed approach aims to advance the state of the art by making explicit factors that influence model selection, particularly those related to bias, as well as their interactions. The provided representations can transform model selection in ML projects into a non ad hoc, adaptive, and explainable process.", "url": "https://arxiv.org/abs/2311.14214"}, {"metadata": {"arXiv": "2311.14222", "Date": "Thu, 23 Nov 2023 23:02:10 ", "Title": "Risk Bounds of Accelerated SGD for Overparameterized Linear Regression", "Authors": ["Xuheng Li and Yihe Deng and Jingfeng Wu and Dongruo Zhou and Quanquan Gu"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["85 pages", "5 figures"]}, "abstract": "Accelerated stochastic gradient descent (ASGD) is a workhorse in deep learning and often achieves better generalization performance than SGD. However, existing optimization theory can only explain the faster convergence of ASGD, but cannot explain its better generalization. In this paper, we study the generalization of ASGD for overparameterized linear regression, which is possibly the simplest setting of learning with overparameterization. We establish an instance-dependent excess risk bound for ASGD within each eigen-subspace of the data covariance matrix. Our analysis shows that (i) ASGD outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate of exponential decay for bias error, while in the subspace of large eigenvalues, its bias error decays slower than SGD; and (ii) the variance error of ASGD is always larger than that of SGD. Our result suggests that ASGD can outperform SGD when the difference between the initialization and the true weight vector is mostly confined to the subspace of small eigenvalues. Additionally, when our analysis is specialized to linear regression in the strongly convex setting, it yields a tighter bound for bias error than the best-known result.", "url": "https://arxiv.org/abs/2311.14222"}, {"metadata": {"arXiv": "2311.14237", "Date": "Fri, 24 Nov 2023 00:36:17 ", "Title": "Pseudo-label Correction for Instance-dependent Noise Using Teacher-student Framework", "Authors": ["Eugene Kim"], "Categories": "cs.LG cs.CV"}, "abstract": "The high capacity of deep learning models to learn complex patterns poses a significant challenge when confronted with label noise. The inability to differentiate clean and noisy labels ultimately results in poor generalization. We approach this problem by reassigning the label for each image using a new teacher-student based framework termed P-LC (pseudo-label correction). Traditional teacher-student networks are composed of teacher and student classifiers for knowledge distillation. In our novel approach, we reconfigure the teacher network into a triple encoder, leveraging the triplet loss to establish a pseudo-label correction system. As the student generates pseudo labels for a set of given images, the teacher learns to choose between the initially assigned labels and the pseudo labels. Experiments on MNIST, Fashion-MNIST, and SVHN demonstrate P-LC's superior performance over existing state-of-the-art methods across all noise levels, most notably in high noise. In addition, we introduce a noise level estimation to help assess model performance and inform the need for additional data cleaning procedures.", "url": "https://arxiv.org/abs/2311.14237"}, {"metadata": {"arXiv": "2311.14255", "Date": "Fri, 24 Nov 2023 02:42:42 ", "Title": "Out-of-Distribution Generalized Dynamic Graph Neural Network with Disentangled Intervention and Invariance Promotion", "Authors": ["Zeyang Zhang", "Xin Wang", "Ziwei Zhang", "Haoyang Li", "Wenwu Zhu"], "Categories": "cs.LG"}, "abstract": "Dynamic graph neural networks (DyGNNs) have demonstrated powerful predictive abilities by exploiting graph structural and temporal dynamics. However, the existing DyGNNs fail to handle distribution shifts, which naturally exist in dynamic graphs, mainly because the patterns exploited by DyGNNs may be variant with respect to labels under distribution shifts. In this paper, we propose Disentangled Intervention-based Dynamic graph Attention networks with Invariance Promotion (I-DIDA) to handle spatio-temporal distribution shifts in dynamic graphs by discovering and utilizing invariant patterns, i.e., structures and features whose predictive abilities are stable across distribution shifts. Specifically, we first propose a disentangled spatio-temporal attention network to capture the variant and invariant patterns. By utilizing the disentangled patterns, we design a spatio-temporal intervention mechanism to create multiple interventional distributions and an environment inference module to infer the latent spatio-temporal environments, and minimize the variance of predictions among these intervened distributions and environments, so that our model can make predictions based on invariant patterns with stable predictive abilities under distribution shifts. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines under distribution shifts. Our work is the first study of spatio-temporal distribution shifts in dynamic graphs, to the best of our knowledge.", "url": "https://arxiv.org/abs/2311.14255"}, {"metadata": {"arXiv": "2311.14304", "Date": "Fri, 24 Nov 2023 06:27:25 ", "Title": "AdaMedGraph: Adaboosting Graph Neural Networks for Personalized Medicine", "Authors": ["Jie Lian", "Xufang Luo", "Caihua Shan", "Dongqi Han", "Varut Vardhanabhuti", "Dongsheng Li"], "Categories": "cs.LG", "Comments": ["Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023", "December 10th", "2023", "New Orleans", "United States", "9 pages"]}, "abstract": "Precision medicine tailored to individual patients has gained significant attention in recent times. Machine learning techniques are now employed to process personalized data from various sources, including images, genetics, and assessments. These techniques have demonstrated good outcomes in many clinical prediction tasks. Notably, the approach of constructing graphs by linking similar patients and then applying graph neural networks (GNNs) stands out, because related information from analogous patients are aggregated and considered for prediction. However, selecting the appropriate edge feature to define patient similarity and construct the graph is challenging, given that each patient is depicted by high-dimensional features from diverse sources. Previous studies rely on human expertise to select the edge feature, which is neither scalable nor efficient in pinpointing crucial edge features for complex diseases. In this paper, we propose a novel algorithm named \\ours, which can automatically select important features to construct multiple patient similarity graphs, and train GNNs based on these graphs as weak learners in adaptive boosting. \\ours{} is evaluated on two real-world medical scenarios and shows superiors performance.", "url": "https://arxiv.org/abs/2311.14304"}, {"metadata": {"arXiv": "2311.14332", "Date": "Fri, 24 Nov 2023 08:15:11 ", "Title": "GATGPT: A Pre-trained Large Language Model with Graph Attention Network for Spatiotemporal Imputation", "Authors": ["Yakun Chen", "Xianzhi Wang", "Guandong Xu"], "Categories": "cs.LG stat.ML"}, "abstract": "The analysis of spatiotemporal data is increasingly utilized across diverse domains, including transportation, healthcare, and meteorology. In real-world settings, such data often contain missing elements due to issues like sensor malfunctions and data transmission errors. The objective of spatiotemporal imputation is to estimate these missing values by understanding the inherent spatial and temporal relationships in the observed multivariate time series. Traditionally, spatiotemporal imputation has relied on specific, intricate architectures designed for this purpose, which suffer from limited applicability and high computational complexity. In contrast, our approach integrates pre-trained large language models (LLMs) into spatiotemporal imputation, introducing a groundbreaking framework, GATGPT. This framework merges a graph attention mechanism with LLMs. We maintain most of the LLM parameters unchanged to leverage existing knowledge for learning temporal patterns, while fine-tuning the upper layers tailored to various applications. The graph attention component enhances the LLM's ability to understand spatial relationships. Through tests on three distinct real-world datasets, our innovative approach demonstrates comparable results to established deep learning benchmarks.", "url": "https://arxiv.org/abs/2311.14332"}, {"metadata": {"arXiv": "2311.14333", "Date": "Fri, 24 Nov 2023 08:15:54 ", "Title": "Cycle Invariant Positional Encoding for Graph Representation Learning", "Authors": ["Zuoyu Yan", "Tengfei Ma", "Liangcai Gao", "Zhi Tang", "Chao Chen", "Yusu Wang"], "Categories": "cs.LG", "Comments": ["Accepted as oral presentation in the Learning on Graphs Conference (LoG 2023)"]}, "abstract": "Cycles are fundamental elements in graph-structured data and have demonstrated their effectiveness in enhancing graph learning models. To encode such information into a graph learning framework, prior works often extract a summary quantity, ranging from the number of cycles to the more sophisticated persistence diagram summaries. However, more detailed information, such as which edges are encoded in a cycle, has not yet been used in graph neural networks. In this paper, we make one step towards addressing this gap, and propose a structure encoding module, called CycleNet, that encodes cycle information via edge structure encoding in a permutation invariant manner. To efficiently encode the space of all cycles, we start with a cycle basis (i.e., a minimal set of cycles generating the cycle space) which we compute via the kernel of the 1-dimensional Hodge Laplacian of the input graph. To guarantee the encoding is invariant w.r.t. the choice of cycle basis, we encode the cycle information via the orthogonal projector of the cycle basis, which is inspired by BasisNet proposed by Lim et al. We also develop a more efficient variant which however requires that the input graph has a unique shortest cycle basis. To demonstrate the effectiveness of the proposed module, we provide some theoretical understandings of its expressive power. Moreover, we show via a range of experiments that networks enhanced by our CycleNet module perform better in various benchmarks compared to several existing SOTA models.", "url": "https://arxiv.org/abs/2311.14333"}, {"metadata": {"arXiv": "2311.14361", "Date": "Fri, 24 Nov 2023 09:03:52 ", "Title": "Deciphering and integrating invariants for neural operator learning with various physical mechanisms", "Authors": ["Rui Zhang", "Qi Meng", "Zhi-Ming Ma"], "Categories": "cs.LG cs.NA math.NA physics.comp-ph"}, "abstract": "Neural operators have been explored as surrogate models for simulating physical systems to overcome the limitations of traditional partial differential equation (PDE) solvers. However, most existing operator learning methods assume that the data originate from a single physical mechanism, limiting their applicability and performance in more realistic scenarios. To this end, we propose Physical Invariant Attention Neural Operator (PIANO) to decipher and integrate the physical invariants (PI) for operator learning from the PDE series with various physical mechanisms. PIANO employs self-supervised learning to extract physical knowledge and attention mechanisms to integrate them into dynamic convolutional layers. Compared to existing techniques, PIANO can reduce the relative error by 13.6\\%-82.2\\% on PDE forecasting tasks across varying coefficients, forces, or boundary conditions. Additionally, varied downstream tasks reveal that the PI embeddings deciphered by PIANO align well with the underlying invariants in the PDE systems, verifying the physical significance of PIANO. The source code will be publicly available at: https://github.com/optray/PIANO.", "url": "https://arxiv.org/abs/2311.14361"}, {"metadata": {"arXiv": "2311.14387", "Date": "Fri, 24 Nov 2023 10:07:10 ", "Title": "Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling", "Authors": ["Mingze Wang", "Zeping Min", "Lei Wu"], "Categories": "cs.LG math.OC", "Comments": ["39 pages"]}, "abstract": "In this work, we investigate the margin-maximization bias exhibited by gradient-based algorithms in classifying linearly separable data. We present an in-depth analysis of the specific properties of the velocity field associated with (normalized) gradients, focusing on their role in margin maximization. Inspired by this analysis, we propose a novel algorithm called Progressive Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at an {\\em exponential rate}. This stands in stark contrast to all existing algorithms, which maximize the margin at a slow {\\em polynomial rate}. Specifically, we identify mild conditions on data distribution under which existing algorithms such as gradient descent (GD) and normalized gradient descent (NGD) {\\em provably fail} in maximizing the margin efficiently. To validate our theoretical findings, we present both synthetic and real-world experiments. Notably, PRGD also shows promise in enhancing the generalization performance when applied to linearly non-separable datasets and deep neural networks.", "url": "https://arxiv.org/abs/2311.14387"}, {"metadata": {"arXiv": "2311.14395", "Date": "Fri, 24 Nov 2023 10:23:57 ", "Title": "Multi-scale Semantic Correlation Mining for Visible-Infrared Person Re-Identification", "Authors": ["Ke Cheng", "Xuecheng Hua", "Hu Lu", "Juanjuan Tu", "Yuanquan Wang", "Shitong Wang"], "Categories": "cs.LG cs.CV"}, "abstract": "The main challenge in the Visible-Infrared Person Re-Identification (VI-ReID) task lies in how to extract discriminative features from different modalities for matching purposes. While the existing well works primarily focus on minimizing the modal discrepancies, the modality information can not thoroughly be leveraged. To solve this problem, a Multi-scale Semantic Correlation Mining network (MSCMNet) is proposed to comprehensively exploit semantic features at multiple scales and simultaneously reduce modality information loss as small as possible in feature extraction. The proposed network contains three novel components. Firstly, after taking into account the effective utilization of modality information, the Multi-scale Information Correlation Mining Block (MIMB) is designed to explore semantic correlations across multiple scales. Secondly, in order to enrich the semantic information that MIMB can utilize, a quadruple-stream feature extractor (QFE) with non-shared parameters is specifically designed to extract information from different dimensions of the dataset. Finally, the Quadruple Center Triplet Loss (QCT) is further proposed to address the information discrepancy in the comprehensive features. Extensive experiments on the SYSU-MM01, RegDB, and LLCM datasets demonstrate that the proposed MSCMNet achieves the greatest accuracy.", "url": "https://arxiv.org/abs/2311.14395"}, {"metadata": {"arXiv": "2311.14402", "Date": "Fri, 24 Nov 2023 10:49:49 ", "Title": "TEA: Test-time Energy Adaptation", "Authors": ["Yige Yuan", "Bingbing Xu", "Liang Hou", "Fei Sun", "Huawei Shen", "Xueqi Cheng"], "Categories": "cs.LG", "Comments": ["16 pages", "10 figures", "7 tables"]}, "abstract": "Test-time adaptation (TTA) aims to improve model generalizability when test data diverges from training distribution, offering the distinct advantage of not requiring access to training data and processes, especially valuable in the context of large pre-trained models. However, current TTA methods fail to address the fundamental issue: covariate shift, i.e., the decreased generalizability can be attributed to the model's reliance on the marginal distribution of the training data, which may impair model calibration and introduce confirmation bias. To address this, we propose a novel energy-based perspective, enhancing the model's perception of target data distributions without requiring access to training data or processes. Building on this perspective, we introduce $\\textbf{T}$est-time $\\textbf{E}$nergy $\\textbf{A}$daptation ($\\textbf{TEA}$), which transforms the trained classifier into an energy-based model and aligns the model's distribution with the test data's, enhancing its ability to perceive test distributions and thus improving overall generalizability. Extensive experiments across multiple tasks, benchmarks and architectures demonstrate TEA's superior generalization performance against state-of-the-art methods. Further in-depth analyses reveal that TEA can equip the model with a comprehensive perception of test distribution, ultimately paving the way toward improved generalization and calibration.", "url": "https://arxiv.org/abs/2311.14402"}, {"metadata": {"arXiv": "2311.14404", "Date": "Fri, 24 Nov 2023 10:56:09 ", "Title": "BHGNN-RT: Network embedding for directed heterogeneous graphs", "Authors": ["Xiyang Sun", "Fumiyasu Komaki"], "Categories": "cs.LG"}, "abstract": "Networks are one of the most valuable data structures for modeling problems in the real world. However, the most recent node embedding strategies have focused on undirected graphs, with limited attention to directed graphs, especially directed heterogeneous graphs. In this study, we first investigated the network properties of directed heterogeneous graphs. Based on network analysis, we proposed an embedding method, a bidirectional heterogeneous graph neural network with random teleport (BHGNN-RT), for directed heterogeneous graphs, that leverages bidirectional message-passing process and network heterogeneity. With the optimization of teleport proportion, BHGNN-RT is beneficial to overcome the over-smoothing problem. Extensive experiments on various datasets were conducted to verify the efficacy and efficiency of BHGNN-RT. Furthermore, we investigated the effects of message components, model layer, and teleport proportion on model performance. The performance comparison with all other baselines illustrates that BHGNN-RT achieves state-of-the-art performance, outperforming the benchmark methods in both node classification and unsupervised clustering tasks.", "url": "https://arxiv.org/abs/2311.14404"}, {"metadata": {"arXiv": "2311.14410", "Date": "Fri, 24 Nov 2023 11:06:22 ", "Title": "Unveiling The Factors of Aesthetic Preferences with Explainable AI", "Authors": ["Derya Soydaner and Johan Wagemans"], "Categories": "cs.LG"}, "abstract": "The allure of aesthetic appeal in images captivates our senses, yet the underlying intricacies of aesthetic preferences remain elusive. In this study, we pioneer a novel perspective by utilizing machine learning models that focus on aesthetic attributes known to influence preferences. Through a data mining approach, our models process these attributes as inputs to predict the aesthetic scores of images. Moreover, to delve deeper and obtain interpretable explanations regarding the factors driving aesthetic preferences, we utilize the popular Explainable AI (XAI) technique known as SHapley Additive exPlanations (SHAP). Our methodology involves employing various machine learning models, including Random Forest, XGBoost, Support Vector Regression, and Multilayer Perceptron, to compare their performances in accurately predicting aesthetic scores, and consistently observing results in conjunction with SHAP. We conduct experiments on three image aesthetic benchmarks, providing insights into the roles of attributes and their interactions. Ultimately, our study aims to shed light on the complex nature of aesthetic preferences in images through machine learning and provides a deeper understanding of the attributes that influence aesthetic judgements.", "url": "https://arxiv.org/abs/2311.14410"}, {"metadata": {"arXiv": "2311.14412", "Date": "Fri, 24 Nov 2023 11:12:26 ", "Title": "A Comparison of PDF Projection with Normalizing Flows and SurVAE", "Authors": ["Paul M. Baggenstoss and Felix Govaers"], "Categories": "cs.LG stat.ME"}, "abstract": "Normalizing flows (NF) recently gained attention as a way to construct generative networks with exact likelihood calculation out of composable layers. However, NF is restricted to dimension-preserving transformations. Surjection VAE (SurVAE) has been proposed to extend NF to dimension-altering transformations. Such networks are desirable because they are expressive and can be precisely trained. We show that the approaches are a re-invention of PDF projection, which appeared over twenty years earlier and is much further developed.", "url": "https://arxiv.org/abs/2311.14412"}, {"metadata": {"arXiv": "2311.14464", "Date": "Fri, 24 Nov 2023 13:19:06 ", "Title": "Finite Volume Features, Global Geometry Representations, and Residual Training for Deep Learning-based CFD Simulation", "Authors": ["Loh Sher En Jessica", "Naheed Anjum Arafat", "Wei Xian Lim", "Wai Lee Chan and Adams Wai Kin Kong"], "Categories": "cs.LG cs.CE physics.flu-dyn"}, "abstract": "Computational fluid dynamics (CFD) simulation is an irreplaceable modelling step in many engineering designs, but it is often computationally expensive. Some graph neural network (GNN)-based CFD methods have been proposed. However, the current methods inherit the weakness of traditional numerical simulators, as well as ignore the cell characteristics in the mesh used in the finite volume method, a common method in practical CFD applications. Specifically, the input nodes in these GNN methods have very limited information about any object immersed in the simulation domain and its surrounding environment. Also, the cell characteristics of the mesh such as cell volume, face surface area, and face centroid are not included in the message-passing operations in the GNN methods. To address these weaknesses, this work proposes two novel geometric representations: Shortest Vector (SV) and Directional Integrated Distance (DID). Extracted from the mesh, the SV and DID provide global geometry perspective to each input node, thus removing the need to collect this information through message-passing. This work also introduces the use of Finite Volume Features (FVF) in the graph convolutions as node and edge attributes, enabling its message-passing operations to adjust to different nodes. Finally, this work is the first to demonstrate how residual training, with the availability of low-resolution data, can be adopted to improve the flow field prediction accuracy. Experimental results on two datasets with five different state-of-the-art GNN methods for CFD indicate that SV, DID, FVF and residual training can effectively reduce the predictive error of current GNN-based methods by as much as 41%.", "url": "https://arxiv.org/abs/2311.14464"}, {"metadata": {"arXiv": "2311.14468", "Date": "Fri, 24 Nov 2023 13:21:35 ", "Title": "Efficient Gradient Estimation via Adaptive Sampling and Importance Sampling", "Authors": ["Corentin Sala\\\"un", "Xingchang Huang", "Iliyan Georgiev", "Niloy J. Mitra", "Gurprit Singh"], "Categories": "cs.LG", "Comments": ["15 pages", "10 figures"]}, "abstract": "Machine learning problems rely heavily on stochastic gradient descent (SGD) for optimization. The effectiveness of SGD is contingent upon accurately estimating gradients from a mini-batch of data samples. Instead of the commonly used uniform sampling, adaptive or importance sampling reduces noise in gradient estimation by forming mini-batches that prioritize crucial data points. Previous research has suggested that data points should be selected with probabilities proportional to their gradient norm. Nevertheless, existing algorithms have struggled to efficiently integrate importance sampling into machine learning frameworks. In this work, we make two contributions. First, we present an algorithm that can incorporate existing importance functions into our framework. Second, we propose a simplified importance function that relies solely on the loss gradient of the output layer. By leveraging our proposed gradient estimation techniques, we observe improved convergence in classification and regression tasks with minimal computational overhead. We validate the effectiveness of our adaptive and importance-sampling approach on image and point-cloud datasets.", "url": "https://arxiv.org/abs/2311.14468"}, {"metadata": {"arXiv": "2311.14469", "Date": "Fri, 24 Nov 2023 13:23:54 ", "Title": "Fault Detection in Telecom Networks using Bi-level Federated Graph Neural Networks", "Authors": ["R. Bourgerie", "T. Zanouda"], "Categories": "cs.LG cs.NI", "Comments": ["This paper has been accepted as part of the The 2nd International Workshop on Federated Learning with Graph Data", "colocated at EEE ICDM 2023"]}, "abstract": "5G and Beyond Networks become increasingly complex and heterogeneous, with diversified and high requirements from a wide variety of emerging applications. The complexity and diversity of Telecom networks place an increasing strain on maintenance and operation efforts. Moreover, the strict security and privacy requirements present a challenge for mobile operators to leverage network data. To detect network faults, and mitigate future failures, prior work focused on leveraging traditional ML/DL methods to locate anomalies in networks. The current approaches, although powerful, do not consider the intertwined nature of embedded and software-intensive Radio Access Network systems. In this paper, we propose a Bi-level Federated Graph Neural Network anomaly detection and diagnosis model that is able to detect anomalies in Telecom networks in a privacy-preserving manner, while minimizing communication costs. Our method revolves around conceptualizing Telecom data as a bi-level temporal Graph Neural Networks. The first graph captures the interactions between different RAN nodes that are exposed to different deployment scenarios in the network, while each individual Radio Access Network node is further elaborated into its software (SW) execution graph. Additionally, we use Federated Learning to address privacy and security limitations. Furthermore, we study the performance of anomaly detection model under three settings: (1) Centralized (2) Federated Learning and (3) Personalized Federated Learning using real-world data from an operational network. Our comprehensive experiments showed that Personalized Federated Temporal Graph Neural Networks method outperforms the most commonly used techniques for Anomaly Detection.", "url": "https://arxiv.org/abs/2311.14469"}, {"metadata": {"arXiv": "2311.14533", "Date": "Fri, 24 Nov 2023 14:56:36 ", "Title": "Comparing Feature Engineering and End-to-End Deep Learning for Autism Spectrum Disorder Assessment based on Fullbody-Tracking", "Authors": ["Alberto Altozano", "Maria Eleonora Minissi", "Mariano Alca\\~niz", "Javier Mar\\'in-Morales"], "Categories": "cs.LG", "Comments": ["This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice", "after which this version may no longer be accessible"]}, "abstract": "Autism Spectrum Disorder (ASD) is characterized by challenges in social communication and restricted patterns, with motor abnormalities gaining traction for early detection. However, kinematic analysis in ASD is limited, often lacking robust validation and relying on hand-crafted features for single tasks, leading to inconsistencies across studies. Thus, end-to-end models have become promising methods to overcome the need for feature engineering. Our aim is to assess both approaches across various kinematic tasks to measure the efficacy of commonly used features in ASD assessment, while comparing them to end-to-end models. Specifically, we developed a virtual reality environment with multiple motor tasks and trained models using both classification approaches. We prioritized a reliable validation framework with repeated cross-validation. Our comparative analysis revealed that hand-crafted features outperformed our deep learning approach in specific tasks, achieving a state-of-the-art area under the curve (AUC) of 0.90$\\pm$0.06. Conversely, end-to-end models provided more consistent results with less variability across all VR tasks, demonstrating domain generalization and reliability, with a maximum task AUC of 0.89$\\pm$0.06. These findings show that end-to-end models enable less variable and context-independent ASD assessments without requiring domain knowledge or task specificity. However, they also recognize the effectiveness of hand-crafted features in specific task scenarios.", "url": "https://arxiv.org/abs/2311.14533"}, {"metadata": {"arXiv": "2311.14534", "Date": "Fri, 24 Nov 2023 15:03:55 ", "Title": "Finding Foundation Models for Time Series Classification with a PreText Task", "Authors": ["Ali Ismail-Fawaz", "Maxime Devanne", "Stefano Berretti", "Jonathan Weber", "Germain Forestier"], "Categories": "cs.LG"}, "abstract": "Over the past decade, Time Series Classification (TSC) has gained an increasing attention. While various methods were explored, deep learning - particularly through Convolutional Neural Networks (CNNs)-stands out as an effective approach. However, due to the limited availability of training data, defining a foundation model for TSC that overcomes the overfitting problem is still a challenging task. The UCR archive, encompassing a wide spectrum of datasets ranging from motion recognition to ECG-based heart disease detection, serves as a prime example for exploring this issue in diverse TSC scenarios. In this paper, we address the overfitting challenge by introducing pre-trained domain foundation models. A key aspect of our methodology is a novel pretext task that spans multiple datasets. This task is designed to identify the originating dataset of each time series sample, with the goal of creating flexible convolution filters that can be applied across different datasets. The research process consists of two phases: a pre-training phase where the model acquires general features through the pretext task, and a subsequent fine-tuning phase for specific dataset classifications. Our extensive experiments on the UCR archive demonstrate that this pre-training strategy significantly outperforms the conventional training approach without pre-training. This strategy effectively reduces overfitting in small datasets and provides an efficient route for adapting these models to new datasets, thus advancing the capabilities of deep learning in TSC.", "url": "https://arxiv.org/abs/2311.14534"}, {"metadata": {"arXiv": "2311.14581", "Date": "Fri, 24 Nov 2023 16:12:43 ", "Title": "Example-Based Explanations of Random Forest Predictions", "Authors": ["Henrik Bostr\\\"om"], "Categories": "cs.LG", "Comments": ["Submitted to 22nd International Symposium on Intelligent Data Analysis", "IDA 2024"]}, "abstract": "A random forest prediction can be computed by the scalar product of the labels of the training examples and a set of weights that are determined by the leafs of the forest into which the test object falls; each prediction can hence be explained exactly by the set of training examples for which the weights are non-zero. The number of examples used in such explanations is shown to vary with the dimensionality of the training set and hyperparameters of the random forest algorithm. This means that the number of examples involved in each prediction can to some extent be controlled by varying these parameters. However, for settings that lead to a required predictive performance, the number of examples involved in each prediction may be unreasonably large, preventing the user to grasp the explanations. In order to provide more useful explanations, a modified prediction procedure is proposed, which includes only the top-weighted examples. An investigation on regression and classification tasks shows that the number of examples used in each explanation can be substantially reduced while maintaining, or even improving, predictive performance compared to the standard prediction procedure.", "url": "https://arxiv.org/abs/2311.14581"}, {"metadata": {"arXiv": "2311.14601", "Date": "Fri, 24 Nov 2023 16:43:17 ", "Title": "A Metalearned Neural Circuit for Nonparametric Bayesian Inference", "Authors": ["Jake C. Snell", "Gianluca Bencomo", "Thomas L. Griffiths"], "Categories": "cs.LG cs.NE stat.ML", "Comments": ["13 pages", "3 figures. Code available at https://github.com/jakesnell/neural-circuits"]}, "abstract": "Most applications of machine learning to classification assume a closed set of balanced classes. This is at odds with the real world, where class occurrence statistics often follow a long-tailed power-law distribution and it is unlikely that all classes are seen in a single sample. Nonparametric Bayesian models naturally capture this phenomenon, but have significant practical barriers to widespread adoption, namely implementation complexity and computational inefficiency. To address this, we present a method for extracting the inductive bias from a nonparametric Bayesian model and transferring it to an artificial neural network. By simulating data with a nonparametric Bayesian prior, we can metalearn a sequence model that performs inference over an unlimited set of classes. After training, this \"neural circuit\" has distilled the corresponding inductive bias and can successfully perform sequential inference over an open set of classes. Our experimental results show that the metalearned neural circuit achieves comparable or better performance than particle filter-based methods for inference in these models while being faster and simpler to use than methods that explicitly incorporate Bayesian nonparametric inference.", "url": "https://arxiv.org/abs/2311.14601"}, {"metadata": {"arXiv": "2311.14632", "Date": "Fri, 24 Nov 2023 17:56:44 ", "Title": "Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach", "Authors": ["Xinwei Zhang", "Zhiqi Bu", "Zhiwei Steven Wu", "Mingyi Hong"], "Categories": "cs.LG cs.CR"}, "abstract": "Differentially Private Stochastic Gradient Descent with gradient clipping (DPSGD-GC) is a powerful tool for training deep learning models using sensitive data, providing both a solid theoretical privacy guarantee and high efficiency. However, using DPSGD-GC to ensure Differential Privacy (DP) comes at the cost of model performance degradation due to DP noise injection and gradient clipping. Existing research has extensively analyzed the theoretical convergence of DPSGD-GC, and has shown that it only converges when using large clipping thresholds that are dependent on problem-specific parameters. Unfortunately, these parameters are often unknown in practice, making it hard to choose the optimal clipping threshold. Therefore, in practice, DPSGD-GC suffers from degraded performance due to the {\\it constant} bias introduced by the clipping. In our work, we propose a new error-feedback (EF) DP algorithm as an alternative to DPSGD-GC, which not only offers a diminishing utility bound without inducing a constant clipping bias, but more importantly, it allows for an arbitrary choice of clipping threshold that is independent of the problem. We establish an algorithm-specific DP analysis for our proposed algorithm, providing privacy guarantees based on R{\\'e}nyi DP. Additionally, we demonstrate that under mild conditions, our algorithm can achieve nearly the same utility bound as DPSGD without gradient clipping. Our empirical results on Cifar-10/100 and E2E datasets, show that the proposed algorithm achieves higher accuracies than DPSGD while maintaining the same level of DP guarantee.", "url": "https://arxiv.org/abs/2311.14632"}, {"metadata": {"arXiv": "2311.14645", "Date": "Fri, 24 Nov 2023 18:27:26 ", "Title": "A General Framework for User-Guided Bayesian Optimization", "Authors": ["Carl Hvarfner and Frank Hutter and Luigi Nardi"], "Categories": "cs.LG stat.ML", "Comments": ["18 pages", "11 figures"]}, "abstract": "The optimization of expensive-to-evaluate black-box functions is prevalent in various scientific disciplines. Bayesian optimization is an automatic, general and sample-efficient method to solve these problems with minimal knowledge of the underlying function dynamics. However, the ability of Bayesian optimization to incorporate prior knowledge or beliefs about the function at hand in order to accelerate the optimization is limited, which reduces its appeal for knowledgeable practitioners with tight budgets. To allow domain experts to customize the optimization routine, we propose ColaBO, the first Bayesian-principled framework for incorporating prior beliefs beyond the typical kernel structure, such as the likely location of the optimizer or the optimal value. The generality of ColaBO makes it applicable across different Monte Carlo acquisition functions and types of user beliefs. We empirically demonstrate ColaBO's ability to substantially accelerate optimization when the prior information is accurate, and to retain approximately default performance when it is misleading.", "url": "https://arxiv.org/abs/2311.14645"}, {"metadata": {"arXiv": "2311.14646", "Date": "Fri, 24 Nov 2023 18:27:41 ", "Title": "More is Better in Modern Machine Learning: when Infinite Overparameterization is Optimal and Overfitting is Obligatory", "Authors": ["James B. Simon", "Dhruva Karkada", "Nikhil Ghosh", "Mikhail Belkin"], "Categories": "cs.LG stat.ML"}, "abstract": "In our era of enormous neural networks, empirical progress has been driven by the philosophy that more is better. Recent deep learning practice has found repeatedly that larger model size, more data, and more computation (resulting in lower training loss) improves performance. In this paper, we give theoretical backing to these empirical observations by showing that these three properties hold in random feature (RF) regression, a class of models equivalent to shallow networks with only the last layer trained. Concretely, we first show that the test risk of RF regression decreases monotonically with both the number of features and the number of samples, provided the ridge penalty is tuned optimally. In particular, this implies that infinite width RF architectures are preferable to those of any finite width. We then proceed to demonstrate that, for a large class of tasks characterized by powerlaw eigenstructure, training to near-zero training loss is obligatory: near-optimal performance can only be achieved when the training error is much smaller than the test error. Grounding our theory in real-world data, we find empirically that standard computer vision tasks with convolutional neural tangent kernels clearly fall into this class. Taken together, our results tell a simple, testable story of the benefits of overparameterization, overfitting, and more data in random feature models.", "url": "https://arxiv.org/abs/2311.14646"}, {"metadata": {"arXiv": "2311.14649", "Date": "Fri, 24 Nov 2023 18:31:11 ", "Title": "Learning in Deep Factor Graphs with Gaussian Belief Propagation", "Authors": ["Seth Nabarro", "Mark van der Wilk", "Andrew J Davison"], "Categories": "cs.LG stat.ML"}, "abstract": "We propose an approach to do learning in Gaussian factor graphs. We treat all relevant quantities (inputs, outputs, parameters, latents) as random variables in a graphical model, and view both training and prediction as inference problems with different observed nodes. Our experiments show that these problems can be efficiently solved with belief propagation (BP), whose updates are inherently local, presenting exciting opportunities for distributed and asynchronous training. Our approach can be scaled to deep networks and provides a natural means to do continual learning: use the BP-estimated parameter marginals of the current task as parameter priors for the next. On a video denoising task we demonstrate the benefit of learnable parameters over a classical factor graph approach and we show encouraging performance of deep factor graphs for continual image classification on MNIST.", "url": "https://arxiv.org/abs/2311.14649"}, {"metadata": {"arXiv": "2311.14652", "Date": "Fri, 24 Nov 2023 18:35:00 ", "Title": "One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space", "Authors": ["Raghav Addanki", "Chenyang Li", "Zhao Song", "Chiwun Yang"], "Categories": "cs.LG cs.CL stat.ML"}, "abstract": "Deploying Large Language Models (LLMs) in streaming applications that involve long contexts, particularly for extended dialogues and text analysis, is of paramount importance but presents two significant challenges. Firstly, the memory consumption is substantial during the decoding phase due to the caching of Key and Value states (KV) of previous tokens. Secondly, attention computation is time-consuming with a time complexity of $O(n^2)$ for the generation of each token. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n \\times d}$, the polynomial method approximates the attention output $T \\in \\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in \\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$ computation within $n^{1+o(1)}$ time executions. Despite this, storing the Key and Value matrices $K, V \\in \\mathbb{R}^{n \\times d}$ still necessitates $O( n d)$ space, leading to significant memory usage. In response to these challenges, we introduce a new algorithm that only reads one pass of the data in streaming fashion. This method employs sublinear space $o(n)$ to store three sketch matrices, alleviating the need for exact $K, V$ storage. Notably, our algorithm exhibits exceptional memory-efficient performance with super-long tokens. As the token length $n$ increases, our error guarantee diminishes while the memory usage remains nearly constant. This unique attribute underscores the potential of our technique in efficiently handling LLMs in streaming applications.", "url": "https://arxiv.org/abs/2311.14652"}, {"metadata": {"arXiv": "2311.14653", "Date": "Fri, 24 Nov 2023 18:37:52 ", "Title": "Data-driven Prior Learning for Bayesian Optimisation", "Authors": ["Sigrid Passano Hellan and Christopher G. Lucas and Nigel H. Goddard"], "Categories": "cs.LG stat.ML", "Comments": ["To be presented at the NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World"]}, "abstract": "Transfer learning for Bayesian optimisation has generally assumed a strong similarity between optimisation tasks, with at least a subset having similar optimal inputs. This assumption can reduce computational costs, but it is violated in a wide range of optimisation problems where transfer learning may nonetheless be useful. We replace this assumption with a weaker one only requiring the shape of the optimisation landscape to be similar, and analyse the recent method Prior Learning for Bayesian Optimisation - PLeBO - in this setting. By learning priors for the hyperparameters of the Gaussian process surrogate model we can better approximate the underlying function, especially for few function evaluations. We validate the learned priors and compare to a breadth of transfer learning approaches, using synthetic data and a recent air pollution optimisation problem as benchmarks. We show that PLeBO and prior transfer find good inputs in fewer evaluations.", "url": "https://arxiv.org/abs/2311.14653"}, {"metadata": {"arXiv": "2311.14658", "Date": "Fri, 24 Nov 2023 18:46:54 ", "Title": "Convergence Analysis for Learning Orthonormal Deep Linear Neural Networks", "Authors": ["Zhen Qin", "Xuwei Tan", "Zhihui Zhu"], "Categories": "cs.LG math.OC"}, "abstract": "Enforcing orthonormal or isometric property for the weight matrices has been shown to enhance the training of deep neural networks by mitigating gradient exploding/vanishing and increasing the robustness of the learned networks. However, despite its practical performance, the theoretical analysis of orthonormality in neural networks is still lacking; for example, how orthonormality affects the convergence of the training process. In this letter, we aim to bridge this gap by providing convergence analysis for training orthonormal deep linear neural networks. Specifically, we show that Riemannian gradient descent with an appropriate initialization converges at a linear rate for training orthonormal deep linear neural networks with a class of loss functions. Unlike existing works that enforce orthonormal weight matrices for all the layers, our approach excludes this requirement for one layer, which is crucial to establish the convergence guarantee. Our results shed light on how increasing the number of hidden layers can impact the convergence speed. Experimental results validate our theoretical analysis.", "url": "https://arxiv.org/abs/2311.14658"}, {"metadata": {"arXiv": "2311.13988", "Date": "Thu, 23 Nov 2023 13:14:31 ", "Title": "Docking Multirotors in Close Proximity using Learnt Downwash Models", "Authors": ["Ajay Shankar", "Heedo Woo", "Amanda Prorok"], "Categories": "cs.RO cs.LG cs.SY eess.SY", "Comments": ["Presented at International Symposium on Experimental Robotics (ISER) 2023"]}, "abstract": "Unmodeled aerodynamic disturbances pose a key challenge for multirotor flight when multiple vehicles are in close proximity to each other. However, certain missions \\textit{require} two multirotors to approach each other within 1-2 body-lengths of each other and hold formation -- we consider one such practical instance: vertically docking two multirotors in the air. In this leader-follower setting, the follower experiences significant downwash interference from the leader in its final docking stages. To compensate for this, we employ a learnt downwash model online within an optimal feedback controller to accurately track a docking maneuver and then hold formation. Through real-world flights with different maneuvers, we demonstrate that this compensation is crucial for reducing the large vertical separation otherwise required by conventional/naive approaches. Our evaluations show a tracking error of less than 0.06m for the follower (a 3-4x reduction) when approaching vertically within two body-lengths of the leader. Finally, we deploy the complete system to effect a successful physical docking between two airborne multirotors in a single smooth planned trajectory.", "url": "https://arxiv.org/abs/2311.13988"}, {"metadata": {"arXiv": "2311.14421", "Date": "Fri, 24 Nov 2023 11:47:08 ", "Title": "Approximation of Convex Envelope Using Reinforcement Learning", "Authors": ["Vivek S. Borkar", "Adit Akarsh"], "Categories": "eess.SY cs.LG cs.SY"}, "abstract": "Oberman gave a stochastic control formulation of the problem of estimating the convex envelope of a non-convex function. Based on this, we develop a reinforcement learning scheme to approximate the convex envelope, using a variant of Q-learning for controlled optimal stopping. It shows very promising results on a standard library of test problems.", "url": "https://arxiv.org/abs/2311.14421"}, {"metadata": {"arXiv": "2311.13712", "Date": "Wed, 22 Nov 2023 22:15:17 ", "Title": "Data Acquisition: A New Frontier in Data-centric AI", "Authors": ["Lingjiao Chen", "Bilge Acun", "Newsha Ardalani", "Yifan Sun", "Feiyang Kang", "Hanrui Lyu", "Yongchan Kwon", "Ruoxi Jia", "Carole-Jean Wu", "Matei Zaharia and James Zou"], "Categories": "cs.AI"}, "abstract": "As Machine Learning (ML) systems continue to grow, the demand for relevant and comprehensive datasets becomes imperative. There is limited study on the challenges of data acquisition due to ad-hoc processes and lack of consistent methodologies. We first present an investigation of current data marketplaces, revealing lack of platforms offering detailed information about datasets, transparent pricing, standardized data formats. With the objective of inciting participation from the data-centric AI community, we then introduce the DAM challenge, a benchmark to model the interaction between the data providers and acquirers. The benchmark was released as a part of DataPerf. Our evaluation of the submitted strategies underlines the need for effective data acquisition strategies in ML.", "url": "https://arxiv.org/abs/2311.13712"}, {"metadata": {"arXiv": "2311.13720", "Date": "Wed, 22 Nov 2023 22:27:47 ", "Title": "Towards More Likely Models for AI Planning", "Authors": ["Turgay Caglar", "Sirine Belhaj", "Tathagata Chakraborti", "Michael Katz", "Sarath Sreedharan"], "Categories": "cs.AI", "Comments": ["27 pages"]}, "abstract": "This is the first work to look at the application of large language models (LLMs) for the purpose of model space edits in automated planning tasks. To set the stage for this sangam, we explore two different flavors of model space problems that have been studied in the AI planning literature and explore the effect of an LLM on those tasks. We empirically demonstrate how the performance of an LLM contrasts with combinatorial search (CS) - an approach that has been traditionally used to solve model space tasks in planning, both with the LLM in the role of a standalone model space reasoner as well as in the role of a statistical signal in concert with the CS approach as part of a two-stage process. Our experiments show promising results suggesting further forays of LLMs into the exciting world of model space reasoning for planning tasks in the future.", "url": "https://arxiv.org/abs/2311.13720"}, {"metadata": {"arXiv": "2311.13782", "Date": "Thu, 23 Nov 2023 02:57:04 ", "Title": "Scalable AI Generative Content for Vehicular Network Semantic Communication", "Authors": ["Hao Feng", "Yi Yang", "Zhu Han"], "Categories": "cs.AI"}, "abstract": "Perceiving vehicles in a driver's blind spot is vital for safe driving. The detection of potentially dangerous vehicles in these blind spots can benefit from vehicular network semantic communication technology. However, efficient semantic communication involves a trade-off between accuracy and delay, especially in bandwidth-limited situations. This paper unveils a scalable Artificial Intelligence Generated Content (AIGC) system that leverages an encoder-decoder architecture. This system converts images into textual representations and reconstructs them into quality-acceptable images, optimizing transmission for vehicular network semantic communication. Moreover, when bandwidth allows, auxiliary information is integrated. The encoder-decoder aims to maintain semantic equivalence with the original images across various tasks. Then the proposed approach employs reinforcement learning to enhance the reliability of the generated contents. Experimental results suggest that the proposed method surpasses the baseline in perceiving vehicles in blind spots and effectively compresses communication data. While this method is specifically designed for driving scenarios, this encoder-decoder architecture also holds potential for wide use across various semantic communication scenarios.", "url": "https://arxiv.org/abs/2311.13782"}, {"metadata": {"arXiv": "2311.13811", "Date": "Thu, 23 Nov 2023 05:20:18 ", "Title": "Education distillation:getting student models to learn in shcools", "Authors": ["Ling Feng", "Danyang Li", "Tianhao Wu", "Xuliang Duan"], "Categories": "cs.AI"}, "abstract": "Knowledge distillation is one of the methods for model compression, and existing knowledge distillation techniques focus on how to improve the distillation algorithm so as to enhance the distillation efficdiency. This paper introduces dynamic incremental learning into knowledge distillation and proposes a distillation strategy for education distillation. Specifically, it is proposed to look at fragmented student models divided from the full student model as low models. As the grade level rises, fragmented student models deepen in conjunction with designed teaching reference layers, while learning and distilling from more teacher models. By moving from lower to higher grades, fragmented student models were gradually integrated into a complete target student model, and the performance of the student models gradually improved from lower to senior grades of the stage. Education distillation strategies combined with distillation algorithms outperform the results of single distillation algorithms on the public dataset CIFAR100,Caltech256, Food-101 dataset.", "url": "https://arxiv.org/abs/2311.13811"}, {"metadata": {"arXiv": "2311.13852", "Date": "Thu, 23 Nov 2023 08:42:18 ", "Title": "A Cross Attention Approach to Diagnostic Explainability using Clinical Practice Guidelines for Depression", "Authors": ["Sumit Dalal", "Deepa Tilwani", "Manas Gaur", "Sarika Jain", "Valerie Shalin", "and Amit Seth"], "Categories": "cs.AI"}, "abstract": "The lack of explainability using relevant clinical knowledge hinders the adoption of Artificial Intelligence-powered analysis of unstructured clinical dialogue. A wealth of relevant, untapped Mental Health (MH) data is available in online communities, providing the opportunity to address the explainability problem with substantial potential impact as a screening tool for both online and offline applications. We develop a method to enhance attention in popular transformer models and generate clinician-understandable explanations for classification by incorporating external clinical knowledge. Inspired by how clinicians rely on their expertise when interacting with patients, we leverage relevant clinical knowledge to model patient inputs, providing meaningful explanations for classification. This will save manual review time and engender trust. We develop such a system in the context of MH using clinical practice guidelines (CPG) for diagnosing depression, a mental health disorder of global concern. We propose an application-specific language model called ProcesS knowledge-infused cross ATtention (PSAT), which incorporates CPGs when computing attention. Through rigorous evaluation on three expert-curated datasets related to depression, we demonstrate application-relevant explainability of PSAT. PSAT also surpasses the performance of nine baseline models and can provide explanations where other baselines fall short. We transform a CPG resource focused on depression, such as the Patient Health Questionnaire (e.g. PHQ-9) and related questions, into a machine-readable ontology using SNOMED-CT. With this resource, PSAT enhances the ability of models like GPT-3.5 to generate application-relevant explanations.", "url": "https://arxiv.org/abs/2311.13852"}, {"metadata": {"arXiv": "2311.13884", "Date": "Thu, 23 Nov 2023 10:14:58 ", "Title": "Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach", "Authors": ["Bin Zhang", "Hangyu Mao", "Jingqing Ruan", "Ying Wen", "Yang Li", "Shao Zhang", "Zhiwei Xu", "Dapeng Li", "Ziyue Li", "Rui Zhao", "Lijuan Li", "Guoliang Fan"], "Categories": "cs.AI", "Comments": ["11pages", "8 figures"]}, "abstract": "The significant advancements in large language models (LLMs) have presented novel opportunities for tackling planning and decision-making within multi-agent systems. However, as the number of agents increases, the issues of hallucination in LLMs and coordination in multi-agent systems (MAS) have become increasingly pronounced. Additionally, the efficient utilization of tokens becomes a critical consideration when employing LLMs to facilitate the interactions of large numbers of agents. In this paper, we present a novel framework aimed at enhancing coordination and decision-making capabilities of LLMs within large-scale multi-agent environments. Our approach draws inspiration from the actor-critic framework employed in multi-agent reinforcement learning, and we develop a modular and token-efficient solution that effectively addresses challenges presented by LLMs and MAS. Through evaluations conducted in experiments involving system resource allocation and robot grid transportation, we demonstrate the considerable advantages afforded by our proposed approach.", "url": "https://arxiv.org/abs/2311.13884"}, {"metadata": {"arXiv": "2311.13905", "Date": "Thu, 23 Nov 2023 10:39:40 ", "Title": "A DRL solution to help reduce the cost in waiting time of securing a traffic light for cyclists", "Authors": ["Lucas Magnana (AGORA)", "Herv\\'e Rivano (AGORA)", "Nicolas Chiabaut"], "Categories": "cs.AI"}, "abstract": "Cyclists prefer to use infrastructure that separates them from motorized traffic. Using a traffic light to segregate car and bike flows, with the addition of bike-specific green phases, is a lightweight and cheap solution that can be deployed dynamically to assess the opportunity of a heavier infrastructure such as a separate bike lane. To compensate for the increased waiting time induced by these new phases, we introduce in this paper a deep reinforcement learning solution that adapts the green phase cycle of a traffic light to the traffic. Vehicle counter data are used to compare the DRL approach with the actuated traffic light control algorithm over whole days. Results show that DRL achieves better minimization of vehicle waiting time at almost all hours. Our DRL approach is also robust to moderate changes in bike traffic. The code of this paper is available at https://github.com/LucasMagnana/A-DRL-solution-to-help-reduce-the-cost-in-waiting-time-of-securing-a-traffic-light-for-cyclists.", "url": "https://arxiv.org/abs/2311.13905"}, {"metadata": {"arXiv": "2311.13960", "Date": "Thu, 23 Nov 2023 12:18:39 ", "Title": "Human Machine Co-Creation. A Complementary Cognitive Approach to Creative Character Design Process Using GANs", "Authors": ["Mohammad Lataifeh", "Xavier A Carrascoa", "Ashraf M Elnagara", "Naveed Ahmeda", "Imran Junejo"], "Categories": "cs.AI"}, "abstract": "Recent advances in Generative Adversarial Networks GANs applications continue to attract the attention of researchers in different fields. In such a framework, two neural networks compete adversely to generate new visual contents indistinguishable from the original dataset. The objective of this research is to create a complementary codesign process between humans and machines to augment character designers abilities in visualizing and creating new characters for multimedia projects such as games and animation. Driven by design cognitive scaffolding, the proposed approach aims to inform the process of perceiving, knowing, and making. The machine generated concepts are used as a launching platform for character designers to conceptualize new characters. A labelled dataset of 22,000 characters was developed for this work and deployed using different GANs to evaluate the most suited for the context, followed by mixed methods evaluation for the machine output and human derivations. The discussed results substantiate the value of the proposed cocreation framework and elucidate how the generated concepts are used as cognitive substances that interact with designers competencies in a versatile manner to influence the creative processes of conceptualizing novel characters.", "url": "https://arxiv.org/abs/2311.13960"}, {"metadata": {"arXiv": "2311.14003", "Date": "Thu, 23 Nov 2023 13:38:43 ", "Title": "Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandit", "Authors": ["Tian Huang", "Ke Li"], "Categories": "cs.AI"}, "abstract": "Optimization problems find widespread use in both single-objective and multi-objective scenarios. In practical applications, users aspire for solutions that converge to the region of interest (ROI) along the Pareto front (PF). While the conventional approach involves approximating a fitness function or an objective function to reflect user preferences, this paper explores an alternative avenue. Specifically, we aim to discover a method that sidesteps the need for calculating the fitness function, relying solely on human feedback. Our proposed approach entails conducting direct preference learning facilitated by an active dueling bandit algorithm. The experimental phase is structured into three sessions. Firstly, we assess the performance of our active dueling bandit algorithm. Secondly, we implement our proposed method within the context of Multi-objective Evolutionary Algorithms (MOEAs). Finally, we deploy our method in a practical problem, specifically in protein structure prediction (PSP). This research presents a novel interactive preference-based MOEA framework that not only addresses the limitations of traditional techniques but also unveils new possibilities for optimization problems.", "url": "https://arxiv.org/abs/2311.14003"}, {"metadata": {"arXiv": "2311.14005", "Date": "Thu, 23 Nov 2023 13:41:22 ", "Title": "When Side-Channel Attacks Break the Black-Box Property of Embedded Artificial Intelligence", "Authors": ["Benoit Coqueret", "Mathieu Carbone", "Olivier Sentieys", "Gabriel Zaid"], "Categories": "cs.AI cs.CR", "Journal-ref": "AISec'23: Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security. November 2023. Pages 127-138", "DOI": "10.1145/3605764.3623903"}, "abstract": "Artificial intelligence, and specifically deep neural networks (DNNs), has rapidly emerged in the past decade as the standard for several tasks from specific advertising to object detection. The performance offered has led DNN algorithms to become a part of critical embedded systems, requiring both efficiency and reliability. In particular, DNNs are subject to malicious examples designed in a way to fool the network while being undetectable to the human observer: the adversarial examples. While previous studies propose frameworks to implement such attacks in black box settings, those often rely on the hypothesis that the attacker has access to the logits of the neural network, breaking the assumption of the traditional black box. In this paper, we investigate a real black box scenario where the attacker has no access to the logits. In particular, we propose an architecture-agnostic attack which solve this constraint by extracting the logits. Our method combines hardware and software attacks, by performing a side-channel attack that exploits electromagnetic leakages to extract the logits for a given input, allowing an attacker to estimate the gradients and produce state-of-the-art adversarial examples to fool the targeted neural network. Through this example of adversarial attack, we demonstrate the effectiveness of logits extraction using side-channel as a first step for more general attack frameworks requiring either the logits or the confidence scores.", "url": "https://arxiv.org/abs/2311.14005"}, {"metadata": {"arXiv": "2311.14030", "Date": "Thu, 23 Nov 2023 14:36:30 ", "Title": "PrivateLoRA For Efficient Privacy Preserving LLM", "Authors": ["Yiming Wang", "Yu Lin", "Xiaodong Zeng", "Guannan Zhang"], "Categories": "cs.AI cs.CR"}, "abstract": "End users face a choice between privacy and efficiency in current Large Language Model (LLM) service paradigms. In cloud-based paradigms, users are forced to compromise data locality for generation quality and processing speed. Conversely, edge device paradigms maintain data locality but fail to deliver satisfactory performance. In this work, we propose a novel LLM service paradigm that distributes privacy-sensitive computation on edge devices and shared computation in the cloud. Only activations are transmitted between the central cloud and edge devices to ensure data locality. Our core innovation, PrivateLoRA, addresses the challenging communication overhead by exploiting the low rank of residual activations, achieving over 95% communication reduction. Consequently, PrivateLoRA effectively maintains data locality and is extremely resource efficient. Under standard 5G networks, PrivateLoRA achieves throughput over 300% of device-only solutions for 7B models and over 80% of an A100 GPU for 33B models. PrivateLoRA also provides tuning performance comparable to LoRA for advanced personalization. Our approach democratizes access to state-of-the-art generative AI for edge devices, paving the way for more tailored LLM experiences for the general public. To our knowledge, our proposed framework is the first efficient and privacy-preserving LLM solution in the literature.", "url": "https://arxiv.org/abs/2311.14030"}, {"metadata": {"arXiv": "2311.14057", "Date": "Thu, 23 Nov 2023 15:22:22 ", "Title": "Assessing the Impact of Noise on Quantum Neural Networks: An Experimental Analysis", "Authors": ["Erik B. Terres Escudero", "Danel Arias Alamo", "Oier Mentxaka G\\'omez", "Pablo Garc\\'ia Bringas"], "Categories": "cs.AI", "Journal-ref": "Hybrid Artificial Intelligent Systems. HAIS 2023. Lecture Notes in Computer Science(), vol 14001. Springer, Cham", "DOI": "10.1007/978-3-031-40725-3_27"}, "abstract": "In the race towards quantum computing, the potential benefits of quantum neural networks (QNNs) have become increasingly apparent. However, Noisy Intermediate-Scale Quantum (NISQ) processors are prone to errors, which poses a significant challenge for the execution of complex algorithms or quantum machine learning. To ensure the quality and security of QNNs, it is crucial to explore the impact of noise on their performance. This paper provides a comprehensive analysis of the impact of noise on QNNs, examining the Mottonen state preparation algorithm under various noise models and studying the degradation of quantum states as they pass through multiple layers of QNNs. Additionally, the paper evaluates the effect of noise on the performance of pre-trained QNNs and highlights the challenges posed by noise models in quantum computing. The findings of this study have significant implications for the development of quantum software, emphasizing the importance of prioritizing stability and noise-correction measures when developing QNNs to ensure reliable and trustworthy results. This paper contributes to the growing body of literature on quantum computing and quantum machine learning, providing new insights into the impact of noise on QNNs and paving the way towards the development of more robust and efficient quantum algorithms.", "url": "https://arxiv.org/abs/2311.14057"}, {"metadata": {"arXiv": "2311.14058", "Date": "Thu, 23 Nov 2023 15:26:29 ", "Title": "Identification for Tree-shaped Structural Causal Models in Polynomial Time", "Authors": ["Aaryan Gupta and Markus Bl\\\"aser"], "Categories": "cs.AI cs.DS"}, "abstract": "Linear structural causal models (SCMs) are used to express and analyse the relationships between random variables. Direct causal effects are represented as directed edges and confounding factors as bidirected edges. Identifying the causal parameters from correlations between the nodes is an open problem in artificial intelligence. In this paper, we study SCMs whose directed component forms a tree. Van der Zander et al. (AISTATS'22, PLMR 151, pp. 6770--6792, 2022) give a PSPACE-algorithm for the identification problem in this case, which is a significant improvement over the general Gr\\\"obner basis approach, which has doubly-exponential time complexity in the number of structural parameters. In this work, we present a randomized polynomial-time algorithm, which solves the identification problem for tree-shaped SCMs. For every structural parameter, our algorithms decides whether it is generically identifiable, generically 2-identifiable, or generically unidentifiable. (No other cases can occur.) In the first two cases, it provides one or two fractional affine square root terms of polynomials (FASTPs) for the corresponding parameter, respectively.", "url": "https://arxiv.org/abs/2311.14058"}, {"metadata": {"arXiv": "2311.14061", "Date": "Thu, 23 Nov 2023 15:37:19 ", "Title": "Towards Explainable Strategy Templates using NLP Transformers", "Authors": ["Pallavi Bagga", "Kostas Stathis"], "Categories": "cs.AI", "Journal-ref": "Workshop Workshop on Explainable AI in Finance, November 27, 2023, ACM, New York, USA"}, "abstract": "This paper bridges the gap between mathematical heuristic strategies learned from Deep Reinforcement Learning (DRL) in automated agent negotiation, and comprehensible, natural language explanations. Our aim is to make these strategies more accessible to non-experts. By leveraging traditional Natural Language Processing (NLP) techniques and Large Language Models (LLMs) equipped with Transformers, we outline how parts of DRL strategies composed of parts within strategy templates can be transformed into user-friendly, human-like English narratives. To achieve this, we present a top-level algorithm that involves parsing mathematical expressions of strategy templates, semantically interpreting variables and structures, generating rule-based primary explanations, and utilizing a Generative Pre-trained Transformer (GPT) model to refine and contextualize these explanations. Subsequent customization for varied audiences and meticulous validation processes in an example illustrate the applicability and potential of this approach.", "url": "https://arxiv.org/abs/2311.14061"}, {"metadata": {"arXiv": "2311.14109", "Date": "Thu, 23 Nov 2023 17:09:48 ", "Title": "Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training", "Authors": ["Cheng Tan", "Jingxuan Wei", "Zhangyang Gao", "Linzhuang Sun", "Siyuan Li", "Xihong Yang", "Stan Z. Li"], "Categories": "cs.AI"}, "abstract": "Multimodal reasoning is a challenging task that requires models to reason across multiple modalities to answer questions. Existing approaches have made progress by incorporating language and visual modalities into a two-stage reasoning framework, separating rationale generation from answer inference. However, these approaches often fall short due to the inadequate quality of the generated rationales. In this work, we delve into the importance of rationales in model reasoning. We observe that when rationales are completely accurate, the model's accuracy significantly improves, highlighting the need for high-quality rationale generation. Motivated by this, we propose MC-CoT, a self-consistency training strategy that generates multiple rationales and answers, subsequently selecting the most accurate through a voting process. This approach not only enhances the quality of generated rationales but also leads to more accurate and robust answers. Through extensive experiments, we demonstrate that our approach significantly improves model performance across various benchmarks. Remarkably, we show that even smaller base models, when equipped with our proposed approach, can achieve results comparable to those of larger models, illustrating the potential of our approach in harnessing the power of rationales for improved multimodal reasoning. The code is available at https://github.com/chengtan9907/mc-cot.", "url": "https://arxiv.org/abs/2311.14109"}, {"metadata": {"arXiv": "2311.14270", "Date": "Fri, 24 Nov 2023 04:12:50 ", "Title": "Efficient Open-world Reinforcement Learning via Knowledge Distillation and Autonomous Rule Discovery", "Authors": ["Ekaterina Nikonova", "Cheng Xue", "Jochen Renz"], "Categories": "cs.AI"}, "abstract": "Deep reinforcement learning suffers from catastrophic forgetting and sample inefficiency making it less applicable to the ever-changing real world. However, the ability to use previously learned knowledge is essential for AI agents to quickly adapt to novelties. Often, certain spatial information observed by the agent in the previous interactions can be leveraged to infer task-specific rules. Inferred rules can then help the agent to avoid potentially dangerous situations in the previously unseen states and guide the learning process increasing agent's novelty adaptation speed. In this work, we propose a general framework that is applicable to deep reinforcement learning agents. Our framework provides the agent with an autonomous way to discover the task-specific rules in the novel environments and self-supervise it's learning. We provide a rule-driven deep Q-learning agent (RDQ) as one possible implementation of that framework. We show that RDQ successfully extracts task-specific rules as it interacts with the world and uses them to drastically increase its learning efficiency. In our experiments, we show that the RDQ agent is significantly more resilient to the novelties than the baseline agents, and is able to detect and adapt to novel situations faster.", "url": "https://arxiv.org/abs/2311.14270"}, {"metadata": {"arXiv": "2311.14315", "Date": "Fri, 24 Nov 2023 07:06:16 ", "Title": "Robust Domain Misinformation Detection via Multi-modal Feature Alignment", "Authors": ["Hui Liu", "Wenya Wang", "Hao Sun", "Anderson Rocha", "and Haoliang Li"], "Categories": "cs.AI", "Comments": ["Accepted by TIFS 2023"], "DOI": "10.1109/TIFS.2023.3326368"}, "abstract": "Social media misinformation harms individuals and societies and is potentialized by fast-growing multi-modal content (i.e., texts and images), which accounts for higher \"credibility\" than text-only news pieces. Although existing supervised misinformation detection methods have obtained acceptable performances in key setups, they may require large amounts of labeled data from various events, which can be time-consuming and tedious. In turn, directly training a model by leveraging a publicly available dataset may fail to generalize due to domain shifts between the training data (a.k.a. source domains) and the data from target domains. Most prior work on domain shift focuses on a single modality (e.g., text modality) and ignores the scenario where sufficient unlabeled target domain data may not be readily available in an early stage. The lack of data often happens due to the dynamic propagation trend (i.e., the number of posts related to fake news increases slowly before catching the public attention). We propose a novel robust domain and cross-modal approach (\\textbf{RDCM}) for multi-modal misinformation detection. It reduces the domain shift by aligning the joint distribution of textual and visual modalities through an inter-domain alignment module and bridges the semantic gap between both modalities through a cross-modality alignment module. We also propose a framework that simultaneously considers application scenarios of domain generalization (in which the target domain data is unavailable) and domain adaptation (in which unlabeled target domain data is available). Evaluation results on two public multi-modal misinformation detection datasets (Pheme and Twitter Datasets) evince the superiority of the proposed model. The formal implementation of this paper can be found in this link: https://github.com/less-and-less-bugs/RDCM", "url": "https://arxiv.org/abs/2311.14315"}, {"metadata": {"arXiv": "2311.14378", "Date": "Fri, 24 Nov 2023 09:52:49 ", "Title": "Ethical implications of ChatGPT in higher education: A scoping review", "Authors": ["Ming Li", "Ariunaa Enkhtur", "Fei Cheng", "Beverley Anne Yamamoto"], "Categories": "cs.AI cs.CY", "Comments": ["Work in progress"]}, "abstract": "This scoping review explores the ethical challenges of using ChatGPT in education, focusing particularly on issues related to higher education. By reviewing recent academic articles written in English, Chinese, and Japanese, we aimed to provide a comprehensive overview of relevant research while identifying gaps for future considerations. Drawing on Arksey and O'Malley's (2005) five-stage scoping review framework, we identified research questions, search terms, and conducted article search from four databases in the target three languages. Each article was reviewed by at least two researchers identifying the main ethical issues of utilizing AI in education, particularly higher education. Our analysis of ethical issues followed the framework developed by DeepMind (Weiginger et al., 2021) to identify six main areas of ethical concern in Language Models. The majority of papers were concerned with misinformation harms (n=25) and/or human-computer interaction related harms (n=24). Given the rapid deployment of Generative Artificial Intelligence (GAI), it is imperative for educators to conduct more empirical studies to develop sound ethical policies for the use of GAI.", "url": "https://arxiv.org/abs/2311.14378"}, {"metadata": {"arXiv": "2311.14401", "Date": "Fri, 24 Nov 2023 10:37:30 ", "Title": "Prototype of deployment of Federated Learning with IoT devices", "Authors": ["Pablo Garc\\'ia Santaclara and Ana Fern\\'andez Vilas and Rebeca P. D\\'iaz Redondo"], "Categories": "cs.AI", "Journal-ref": "Proceedings of the 19th ACM International Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, Ubiquitous Networks. October 2022. Pages 9-16", "DOI": "10.1145/3551663.3558681"}, "abstract": "In the age of technology, data is an increasingly important resource. This importance is growing in the field of Artificial Intelligence (AI), where sub fields such as Machine Learning (ML) need more and more data to achieve better results. Internet of Things (IoT) is the connection of sensors and smart objects to collect and exchange data, in addition to achieving many other tasks. A huge amount of the resource desired, data, is stored in mobile devices, sensors and other Internet of Things (IoT) devices, but remains there due to data protection restrictions. At the same time these devices do not have enough data or computational capacity to train good models. Moreover, transmitting, storing and processing all this data on a centralised server is problematic. Federated Learning (FL) provides an innovative solution that allows devices to learn in a collaborative way. More importantly, it accomplishes this without violating data protection laws. FL is currently growing, and there are several solutions that implement it. This article presents a prototype of a FL solution where the IoT devices used were raspberry pi boards. The results compare the performance of a solution of this type with those obtained in traditional approaches. In addition, the FL solution performance was tested in a hostile environment. A convolutional neural network (CNN) and a image data set were used. The results show the feasibility and usability of these techniques, although in many cases they do not reach the performance of traditional approaches.", "url": "https://arxiv.org/abs/2311.14401"}, {"metadata": {"arXiv": "2311.14426", "Date": "Fri, 24 Nov 2023 11:59:11 ", "Title": "Human-Machine Cooperative Multimodal Learning Method for Cross-subject Olfactory Preference Recognition", "Authors": ["Xiuxin Xia", "Yuchen Guo", "Yanwei Wang", "Yuchao Yang", "Yan Shi and Hong Men"], "Categories": "cs.AI", "Comments": ["14 pages", "13 figures"]}, "abstract": "Odor sensory evaluation has a broad application in food, clothing, cosmetics, and other fields. Traditional artificial sensory evaluation has poor repeatability, and the machine olfaction represented by the electronic nose (E-nose) is difficult to reflect human feelings. Olfactory electroencephalogram (EEG) contains odor and individual features associated with human olfactory preference, which has unique advantages in odor sensory evaluation. However, the difficulty of cross-subject olfactory EEG recognition greatly limits its application. It is worth noting that E-nose and olfactory EEG are more advantageous in representing odor information and individual emotions, respectively. In this paper, an E-nose and olfactory EEG multimodal learning method is proposed for cross-subject olfactory preference recognition. Firstly, the olfactory EEG and E-nose multimodal data acquisition and preprocessing paradigms are established. Secondly, a complementary multimodal data mining strategy is proposed to effectively mine the common features of multimodal data representing odor information and the individual features in olfactory EEG representing individual emotional information. Finally, the cross-subject olfactory preference recognition is achieved in 24 subjects by fusing the extracted common and individual features, and the recognition effect is superior to the state-of-the-art recognition methods. Furthermore, the advantages of the proposed method in cross-subject olfactory preference recognition indicate its potential for practical odor evaluation applications.", "url": "https://arxiv.org/abs/2311.14426"}, {"metadata": {"arXiv": "2311.14457", "Date": "Fri, 24 Nov 2023 13:11:07 ", "Title": "How to ensure a safe control strategy? Towards a SRL for urban transit autonomous operation", "Authors": ["Zicong Zhao"], "Categories": "cs.AI", "Comments": ["18 pages,11 figures,5 tables"]}, "abstract": "Deep reinforcement learning has gradually shown its latent decision-making ability in urban rail transit autonomous operation. However, since reinforcement learning can not neither guarantee safety during learning nor execution, this is still one of the major obstacles to the practical application of reinforcement learning. Given this drawback, reinforcement learning applied in the safety-critical autonomous operation domain remains challenging without generating a safe control command sequence that avoids overspeed operations. Therefore, a SSA-DRL framework is proposed in this paper for safe intelligent control of urban rail transit autonomous operation trains. The proposed framework is combined with linear temporal logic, reinforcement learning and Monte Carlo tree search and consists of four mainly module: a post-posed shielding, a searching tree module, a DRL framework and an additional actor. Furthermore, the output of the framework can meet speed constraint, schedule constraint and optimize the operation process. Finally, the proposed SSA-DRL framework for decision-making in urban rail transit autonomous operation is evaluated in sixteen different sections, and its effectiveness is demonstrated through an ablation experiment and comparison with the scheduled operation plan.", "url": "https://arxiv.org/abs/2311.14457"}, {"metadata": {"arXiv": "2311.14563", "Date": "Fri, 24 Nov 2023 15:50:37 ", "Title": "Electric Vehicles coordination for grid balancing using multi-objective Harris Hawks Optimization", "Authors": ["Cristina Bianca Pop", "Tudor Cioara", "Viorica Chifu", "Ionut Anghel", "Francesco Bellesini"], "Categories": "cs.AI cs.NE cs.SY eess.SY", "Comments": ["Submitted to Elsevier journal"]}, "abstract": "The rise of renewables coincides with the shift towards Electrical Vehicles (EVs) posing technical and operational challenges for the energy balance of the local grid. Nowadays, the energy grid cannot deal with a spike in EVs usage leading to a need for more coordinated and grid aware EVs charging and discharging strategies. However, coordinating power flow from multiple EVs into the grid requires sophisticated algorithms and load-balancing strategies as the complexity increases with more control variables and EVs, necessitating large optimization and decision search spaces. In this paper, we propose an EVs fleet coordination model for the day ahead aiming to ensure a reliable energy supply and maintain a stable local grid, by utilizing EVs to store surplus energy and discharge it during periods of energy deficit. The optimization problem is addressed using Harris Hawks Optimization (HHO) considering criteria related to energy grid balancing, time usage preference, and the location of EV drivers. The EVs schedules, associated with the position of individuals from the population, are adjusted through exploration and exploitation operations, and their technical and operational feasibility is ensured, while the rabbit individual is updated with a non-dominated EV schedule selected per iteration using a roulette wheel algorithm. The solution is evaluated within the framework of an e-mobility service in Terni city. The results indicate that coordinated charging and discharging of EVs not only meet balancing service requirements but also align with user preferences with minimal deviations.", "url": "https://arxiv.org/abs/2311.14563"}, {"metadata": {"arXiv": "2311.14570", "Date": "Fri, 24 Nov 2023 15:59:14 ", "Title": "RAISE -- Radiology AI Safety, an End-to-end lifecycle approach", "Authors": ["M. Jorge Cardoso", "Julia Moosbauer", "Tessa S. Cook", "B. Selnur Erdal", "Brad Genereaux", "Vikash Gupta", "Bennett A. Landman", "Tiarna Lee", "Parashkev Nachev", "Elanchezhian Somasundaram", "Ronald M. Summers", "Khaled Younis", "Sebastien Ourselin", "Franz MJ Pfister"], "Categories": "cs.AI physics.med-ph", "Comments": ["14 pages", "3 figures"]}, "abstract": "The integration of AI into radiology introduces opportunities for improved clinical care provision and efficiency but it demands a meticulous approach to mitigate potential risks as with any other new technology. Beginning with rigorous pre-deployment evaluation and validation, the focus should be on ensuring models meet the highest standards of safety, effectiveness and efficacy for their intended applications. Input and output guardrails implemented during production usage act as an additional layer of protection, identifying and addressing individual failures as they occur. Continuous post-deployment monitoring allows for tracking population-level performance (data drift), fairness, and value delivery over time. Scheduling reviews of post-deployment model performance and educating radiologists about new algorithmic-driven findings is critical for AI to be effective in clinical practice. Recognizing that no single AI solution can provide absolute assurance even when limited to its intended use, the synergistic application of quality assurance at multiple levels - regulatory, clinical, technical, and ethical - is emphasized. Collaborative efforts between stakeholders spanning healthcare systems, industry, academia, and government are imperative to address the multifaceted challenges involved. Trust in AI is an earned privilege, contingent on a broad set of goals, among them transparently demonstrating that the AI adheres to the same rigorous safety, effectiveness and efficacy standards as other established medical technologies. By doing so, developers can instil confidence among providers and patients alike, enabling the responsible scaling of AI and the realization of its potential benefits. The roadmap presented herein aims to expedite the achievement of deployable, reliable, and safe AI in radiology.", "url": "https://arxiv.org/abs/2311.14570"}, {"metadata": {"arXiv": "2311.13614", "Date": "Wed, 22 Nov 2023 04:52:58 ", "Title": "HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data", "Authors": ["Qifan Yu", "Juncheng Li", "Longhui Wei", "Liang Pang", "Wentao Ye", "Bosheng Qin", "Siliang Tang", "Qi Tian", "Yueting Zhuang"], "Categories": "cs.CV cs.AI", "Comments": ["10 pages", "6 figures"]}, "abstract": "Multi-modal Large Language Models (MLLMs) tuned on machine-generated instruction-following data have demonstrated remarkable performance in various multi-modal understanding and generation tasks. However, the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored. This work aims to investigate various hallucinations (i.e., object, relation, attribute hallucinations) and mitigate those hallucinatory toxicities in large-scale machine-generated visual instruction datasets. Drawing on the human ability to identify factual errors, we present a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm. We use our framework to identify and eliminate hallucinations in the training data automatically. Interestingly, HalluciDoctor also indicates that spurious correlations arising from long-tail object co-occurrences contribute to hallucinations. Based on that, we execute counterfactual visual instruction expansion to balance data distribution, thereby enhancing MLLMs' resistance to hallucinations. Comprehensive experiments on hallucination evaluation benchmarks show that our method successfully mitigates 44.6% hallucinations relatively and maintains competitive performance compared to LLaVA.The source code will be released at \\url{https://github.com/Yuqifan1117/HalluciDoctor}.", "url": "https://arxiv.org/abs/2311.13614"}, {"metadata": {"arXiv": "2311.13627", "Date": "Wed, 22 Nov 2023 17:44:24 ", "Title": "Vamos: Versatile Action Models for Video Understanding", "Authors": ["Shijie Wang", "Qi Zhao", "Minh Quan Do", "Nakul Agarwal", "Kwonjoon Lee", "Chen Sun"], "Categories": "cs.CV cs.AI", "Comments": ["Under submission. Code and models will be released at https://brown-palm.github.io/Vamos/"]}, "abstract": "What makes good video representations for video understanding, such as anticipating future activities, or answering video-conditioned questions? While earlier approaches focus on end-to-end learning directly from video pixels, we propose to revisit text-based representations, such as discrete action labels, or free-form video captions, which are interpretable and can be directly consumed by large language models (LLMs). Intuitively, different video understanding tasks may require representations that are complementary and at different granularities. To this end, we propose versatile action models (Vamos), a learning framework powered by a large language model as the \"reasoner\", and can flexibly leverage visual embeddings, action labels, and free-form descriptions extracted from videos as its input. We evaluate Vamos on four complementary video understanding benchmarks, Ego4D, Next-QA, IntentQA, and EgoSchema, on its capability to model temporal dynamics, encode visual history, and perform reasoning. Surprisingly, we observe that text-based representations consistently achieve competitive performance on all benchmarks, and that visual embeddings provide marginal or no performance improvement, demonstrating the effectiveness of text-based video representation in the LLM era. We perform extensive ablation study and qualitative analysis to support our observations, and achieve state-of-the-art performance on three benchmarks.", "url": "https://arxiv.org/abs/2311.13627"}, {"metadata": {"arXiv": "2311.13752", "Date": "Thu, 23 Nov 2023 00:57:35 ", "Title": "3D-MIR: A Benchmark and Empirical Study on 3D Medical Image Retrieval in Radiology", "Authors": ["Asma Ben Abacha", "Alberto Santamaria-Pang", "Ho Hin Lee", "Jameson Merkow", "Qin Cai", "Surya Teja Devarakonda", "Abdullah Islam", "Julia Gong", "Matthew P. Lungren", "Thomas Lin", "Noel C Codella", "Ivan Tarapov"], "Categories": "cs.CV cs.AI"}, "abstract": "The increasing use of medical imaging in healthcare settings presents a significant challenge due to the increasing workload for radiologists, yet it also offers opportunity for enhancing healthcare outcomes if effectively leveraged. 3D image retrieval holds potential to reduce radiologist workloads by enabling clinicians to efficiently search through diagnostically similar or otherwise relevant cases, resulting in faster and more precise diagnoses. However, the field of 3D medical image retrieval is still emerging, lacking established evaluation benchmarks, comprehensive datasets, and thorough studies. This paper attempts to bridge this gap by introducing a novel benchmark for 3D Medical Image Retrieval (3D-MIR) that encompasses four different anatomies imaged with computed tomography. Using this benchmark, we explore a diverse set of search strategies that use aggregated 2D slices, 3D volumes, and multi-modal embeddings from popular multi-modal foundation models as queries. Quantitative and qualitative assessments of each approach are provided alongside an in-depth discussion that offers insight for future research. To promote the advancement of this field, our benchmark, dataset, and code are made publicly available.", "url": "https://arxiv.org/abs/2311.13752"}, {"metadata": {"arXiv": "2311.13928", "Date": "Thu, 23 Nov 2023 11:29:16 ", "Title": "Parameter Exchange for Robust Dynamic Domain Generalization", "Authors": ["Luojun Lin", "Zhifeng Shen", "Zhishu Sun", "Yuanlong Yu", "Lei Zhang", "Weijie Chen"], "Categories": "cs.CV cs.AI", "Comments": ["Accepted by ACM MM 2023. Source code: https://github.com/MetaVisionLab/PE"], "DOI": "10.1145/3581783.3612318"}, "abstract": "Agnostic domain shift is the main reason of model degradation on the unknown target domains, which brings an urgent need to develop Domain Generalization (DG). Recent advances at DG use dynamic networks to achieve training-free adaptation on the unknown target domains, termed Dynamic Domain Generalization (DDG), which compensates for the lack of self-adaptability in static models with fixed weights. The parameters of dynamic networks can be decoupled into a static and a dynamic component, which are designed to learn domain-invariant and domain-specific features, respectively. Based on the existing arts, in this work, we try to push the limits of DDG by disentangling the static and dynamic components more thoroughly from an optimization perspective. Our main consideration is that we can enable the static component to learn domain-invariant features more comprehensively by augmenting the domain-specific information. As a result, the more comprehensive domain-invariant features learned by the static component can then enforce the dynamic component to focus more on learning adaptive domain-specific features. To this end, we propose a simple yet effective Parameter Exchange (PE) method to perturb the combination between the static and dynamic components. We optimize the model using the gradients from both the perturbed and non-perturbed feed-forward jointly to implicitly achieve the aforementioned disentanglement. In this way, the two components can be optimized in a mutually-beneficial manner, which can resist the agnostic domain shifts and improve the self-adaptability on the unknown target domain. Extensive experiments show that PE can be easily plugged into existing dynamic networks to improve their generalization ability without bells and whistles.", "url": "https://arxiv.org/abs/2311.13928"}, {"metadata": {"arXiv": "2311.14073", "Date": "Thu, 23 Nov 2023 16:04:41 ", "Title": "Learning Saliency From Fixations", "Authors": ["Yasser Abdelaziz Dahou Djilali", "Kevin McGuiness", "Noel O'Connor"], "Categories": "cs.CV cs.AI"}, "abstract": "We present a novel approach for saliency prediction in images, leveraging parallel decoding in transformers to learn saliency solely from fixation maps. Models typically rely on continuous saliency maps, to overcome the difficulty of optimizing for the discrete fixation map. We attempt to replicate the experimental setup that generates saliency datasets. Our approach treats saliency prediction as a direct set prediction problem, via a global loss that enforces unique fixations prediction through bipartite matching and a transformer encoder-decoder architecture. By utilizing a fixed set of learned fixation queries, the cross-attention reasons over the image features to directly output the fixation points, distinguishing it from other modern saliency predictors. Our approach, named Saliency TRansformer (SalTR), achieves metric scores on par with state-of-the-art approaches on the Salicon and MIT300 benchmarks.", "url": "https://arxiv.org/abs/2311.14073"}, {"metadata": {"arXiv": "2311.14175", "Date": "Thu, 23 Nov 2023 19:44:50 ", "Title": "Appearance-based gaze estimation enhanced with synthetic images using deep neural networks", "Authors": ["Dmytro Herashchenko and Igor Farka\\v{s}"], "Categories": "cs.CV cs.AI", "Comments": ["6 pages", "10 figures", "accepted to 2023 IEEE Symposium Series on Computational Intelligence"]}, "abstract": "Human eye gaze estimation is an important cognitive ingredient for successful human-robot interaction, enabling the robot to read and predict human behavior. We approach this problem using artificial neural networks and build a modular system estimating gaze from separately cropped eyes, taking advantage of existing well-functioning components for face detection (RetinaFace) and head pose estimation (6DRepNet). Our proposed method does not require any special hardware or infrared filters but uses a standard notebook-builtin RGB camera, as often approached with appearance-based methods. Using the MetaHuman tool, we also generated a large synthetic dataset of more than 57,000 human faces and made it publicly available. The inclusion of this dataset (with eye gaze and head pose information) on top of the standard Columbia Gaze dataset into training the model led to better accuracy with a mean average error below two degrees in eye pitch and yaw directions, which compares favourably to related methods. We also verified the feasibility of our model by its preliminary testing in real-world setting using the builtin 4K camera in NICO semi-humanoid robot's eye.", "url": "https://arxiv.org/abs/2311.14175"}, {"metadata": {"arXiv": "2311.14435", "Date": "Fri, 24 Nov 2023 12:22:00 ", "Title": "GCPV: Guided Concept Projection Vectors for the Explainable Inspection of CNN Feature Spaces", "Authors": ["Georgii Mikriukov", "Gesina Schwalbe", "Christian Hellert", "Korinna Bade"], "Categories": "cs.CV cs.AI"}, "abstract": "For debugging and verification of computer vision convolutional deep neural networks (CNNs) human inspection of the learned latent representations is imperative. Therefore, state-of-the-art eXplainable Artificial Intelligence (XAI) methods globally associate given natural language semantic concepts with representing vectors or regions in the CNN latent space supporting manual inspection. Yet, this approach comes with two major disadvantages: They are locally inaccurate when reconstructing a concept label and discard information about the distribution of concept instance representations. The latter, though, is of particular interest for debugging, like finding and understanding outliers, learned notions of sub-concepts, and concept confusion. Furthermore, current single-layer approaches neglect that information about a concept may be spread over the CNN depth. To overcome these shortcomings, we introduce the local-to-global Guided Concept Projection Vectors (GCPV) approach: It (1) generates local concept vectors that each precisely reconstruct a concept segmentation label, and then (2) generalizes these to global concept and even sub-concept vectors by means of hiearchical clustering. Our experiments on object detectors demonstrate improved performance compared to the state-of-the-art, the benefit of multi-layer concept vectors, and robustness against low-quality concept segmentation labels. Finally, we demonstrate that GCPVs can be applied to find root causes for confusion of concepts like bus and truck, and reveal interesting concept-level outliers. Thus, GCPVs pose a promising step towards interpretable model debugging and informed data improvement.", "url": "https://arxiv.org/abs/2311.14435"}, {"metadata": {"arXiv": "2311.14471", "Date": "Fri, 24 Nov 2023 13:25:29 ", "Title": "MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting", "Authors": ["Nathan Blake", "Hana Chockler", "David A. Kelly", "Santiago Calderon Pena", "Akchunya Chanchal"], "Categories": "cs.CV cs.AI"}, "abstract": "Existing tools for explaining the output of image classifiers can be divided into white-box, which rely on access to the model internals, and black-box, agnostic to the model. As the usage of AI in the medical domain grows, so too does the usage of explainability tools. Existing work on medical image explanations focuses on white-box tools, such as gradcam. However, there are clear advantages to switching to a black-box tool, including the ability to use it with any classifier and the wide selection of black-box tools available. On standard images, black-box tools are as precise as white-box. In this paper we compare the performance of several black-box methods against gradcam on a brain cancer MRI dataset. We demonstrate that most black-box tools are not suitable for explaining medical image classifications and present a detailed analysis of the reasons for their shortcomings. We also show that one black-box tool, a causal explainability-based rex, performs as well as \\gradcam.", "url": "https://arxiv.org/abs/2311.14471"}, {"metadata": {"arXiv": "2311.14544", "Date": "Fri, 24 Nov 2023 15:23:47 ", "Title": "Inferring Latent Class Statistics from Text for Robust Visual Few-Shot Learning", "Authors": ["Yassir Bendou", "Vincent Gripon", "Bastien Pasdeloup", "Giulia Lioi", "Lukas Mauch", "Fabien Cardinaux and Ghouthi Boukli Hacene"], "Categories": "cs.CV cs.AI", "Comments": ["R0-FoMo: Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models at NeurIPS 2023"]}, "abstract": "In the realm of few-shot learning, foundation models like CLIP have proven effective but exhibit limitations in cross-domain robustness especially in few-shot settings. Recent works add text as an extra modality to enhance the performance of these models. Most of these approaches treat text as an auxiliary modality without fully exploring its potential to elucidate the underlying class visual features distribution. In this paper, we present a novel approach that leverages text-derived statistics to predict the mean and covariance of the visual feature distribution for each class. This predictive framework enriches the latent space, yielding more robust and generalizable few-shot learning models. We demonstrate the efficacy of incorporating both mean and covariance statistics in improving few-shot classification performance across various datasets. Our method shows that we can use text to predict the mean and covariance of the distribution offering promising improvements in few-shot learning scenarios.", "url": "https://arxiv.org/abs/2311.14544"}, {"metadata": {"arXiv": "2311.14552", "Date": "Fri, 24 Nov 2023 15:35:07 ", "Title": "Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models", "Authors": ["Yufei Zhan", "Yousong Zhu", "Zhiyang Chen", "Fan Yang", "Ming Tang", "Jinqiao Wang"], "Categories": "cs.CV cs.AI", "Comments": ["Technical report. The codes and dataset will be released soon"]}, "abstract": "Replicating the innate human ability to detect all objects based on free-form texts at any granularity remains a formidable challenge for Vision-Language models. Current Large Vision Language Models (LVLMs) are predominantly constrained to grounding a single, pre-existing object, relying solely on data from Referring Expression Comprehension tasks. The limitation leads to a compromise in model design, necessitating the introduction of visual expert models or the integration of customized head structures. Beyond these constraints, our research delves into the untapped potential of LVLMs and uncover their inherent capability for basic object perception, allowing them to accurately identify and locate objects of interest. Building on this insight, we introduce a novel language-prompted localization dataset designed to fully unleash the capabilities of LVLMs in integrating fine-grained object perception with precise location awareness. More importantly, we present $\\textbf{Griffon}$, a purely LVLM-based baseline, which does not require the introduction of any special tokens, expert models, or additional detection modules. It simply maintains a consistent structure with popular LVLMs by unifying data formats across various localization-related scenarios and is trained end-to-end through a well-designed pipeline. Comprehensive experiments demonstrate that $\\textbf{Griffon}$ not only achieves state-of-the-art performance on the fine-grained RefCOCO series but also approaches the capabilities of the expert model Faster RCNN on the detection benchmark MSCOCO.", "url": "https://arxiv.org/abs/2311.14552"}, {"metadata": {"arXiv": "2311.14625", "Date": "Fri, 24 Nov 2023 17:40:31 ", "Title": "ARIA: On the interaction between Architectures, Aggregation methods and Initializations in federated visual classification", "Authors": ["Vasilis Siomos", "Sergio Naval-Marimont", "Jonathan Passerat-Palmbach", "Giacomo Tarroni"], "Categories": "cs.CV cs.AI cs.DC", "Comments": ["Under review at the 21st IEEE International Symposium on Biomedical Imaging"]}, "abstract": "Federated Learning (FL) is a collaborative training paradigm that allows for privacy-preserving learning of cross-institutional models by eliminating the exchange of sensitive data and instead relying on the exchange of model parameters between the clients and a server. Despite individual studies on how client models are aggregated, and, more recently, on the benefits of ImageNet pre-training, there is a lack of understanding of the effect the architecture chosen for the federation has, and of how the aforementioned elements interconnect. To this end, we conduct the first joint ARchitecture-Initialization-Aggregation study and benchmark ARIAs across a range of medical image classification tasks. We find that, contrary to current practices, ARIA elements have to be chosen together to achieve the best possible performance. Our results also shed light on good choices for each element depending on the task, the effect of normalisation layers, and the utility of SSL pre-training, pointing to potential directions for designing FL-specific architectures and training pipelines.", "url": "https://arxiv.org/abs/2311.14625"}, {"metadata": {"arXiv": "2311.14633", "Date": "Fri, 24 Nov 2023 18:02:14 ", "Title": "One Strike, You're Out: Detecting Markush Structures in Low Signal-to-Noise Ratio Images", "Authors": ["Thomas Jurriaans", "Kinga Szarkowska", "Eric Nalisnick", "Markus Schwoerer", "Camilo Thorne and Saber Akhondi"], "Categories": "cs.CV cs.AI", "Comments": ["15 pages", "9 tables", "16 figures"]}, "abstract": "Modern research increasingly relies on automated methods to assist researchers. An example of this is Optical Chemical Structure Recognition (OCSR), which aids chemists in retrieving information about chemicals from large amounts of documents. Markush structures are chemical structures that cannot be parsed correctly by OCSR and cause errors. The focus of this research was to propose and test a novel method for classifying Markush structures. Within this method, a comparison was made between fixed-feature extraction and end-to-end learning (CNN). The end-to-end method performed significantly better than the fixed-feature method, achieving 0.928 (0.035 SD) Macro F1 compared to the fixed-feature method's 0.701 (0.052 SD). Because of the nature of the experiment, these figures are a lower bound and can be improved further. These results suggest that Markush structures can be filtered out effectively and accurately using the proposed method. When implemented into OCSR pipelines, this method can improve their performance and use to other researchers.", "url": "https://arxiv.org/abs/2311.14633"}, {"metadata": {"arXiv": "2311.14656", "Date": "Fri, 24 Nov 2023 18:46:02 ", "Title": "Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs", "Authors": ["Jonathan Roberts", "Timo L\\\"uddecke", "Rehan Sheikh", "Kai Han", "Samuel Albanie"], "Categories": "cs.CV cs.AI"}, "abstract": "Multimodal large language models (MLLMs) have shown remarkable capabilities across a broad range of tasks but their knowledge and abilities in the geographic and geospatial domains are yet to be explored, despite potential wide-ranging benefits to navigation, environmental research, urban development, and disaster response. We conduct a series of experiments exploring various vision capabilities of MLLMs within these domains, particularly focusing on the frontier model GPT-4V, and benchmark its performance against open-source counterparts. Our methodology involves challenging these models with a small-scale geographic benchmark consisting of a suite of visual tasks, testing their abilities across a spectrum of complexity. The analysis uncovers not only where such models excel, including instances where they outperform humans, but also where they falter, providing a balanced view of their capabilities in the geographic domain. To enable the comparison and evaluation of future models, our benchmark will be publicly released.", "url": "https://arxiv.org/abs/2311.14656"}, {"metadata": {"arXiv": "2311.14651", "Date": "Fri, 24 Nov 2023 18:34:36 ", "Title": "History Filtering in Imperfect Information Games: Algorithms and Complexity", "Authors": ["Christopher Solinas", "Douglas Rebstock", "Nathan R. Sturtevant", "Michael Buro"], "Categories": "cs.GT cs.AI"}, "abstract": "Historically applied exclusively to perfect information games, depth-limited search with value functions has been key to recent advances in AI for imperfect information games. Most prominent approaches with strong theoretical guarantees require subgame decomposition - a process in which a subgame is computed from public information and player beliefs. However, subgame decomposition can itself require non-trivial computations, and its tractability depends on the existence of efficient algorithms for either full enumeration or generation of the histories that form the root of the subgame. Despite this, no formal analysis of the tractability of such computations has been established in prior work, and application domains have often consisted of games, such as poker, for which enumeration is trivial on modern hardware. Applying these ideas to more complex domains requires understanding their cost. In this work, we introduce and analyze the computational aspects and tractability of filtering histories for subgame decomposition. We show that constructing a single history from the root of the subgame is generally intractable, and then provide a necessary and sufficient condition for efficient enumeration. We also introduce a novel Markov Chain Monte Carlo-based generation algorithm for trick-taking card games - a domain where enumeration is often prohibitively expensive. Our experiments demonstrate its improved scalability in the trick-taking card game Oh Hell. These contributions clarify when and how depth-limited search via subgame decomposition can be an effective tool for sequential decision-making in imperfect information settings.", "url": "https://arxiv.org/abs/2311.14651"}, {"metadata": {"arXiv": "2311.14480", "Date": "Fri, 24 Nov 2023 13:42:55 ", "Title": "Evolutionary game theory: the mathematics of evolution and collective behaviours", "Authors": ["The Anh Han"], "Categories": "cs.MA cs.AI math-ph math.DS math.MP nlin.AO", "Comments": ["arXiv admin note: substantial text overlap with arXiv:2205.07369"]}, "abstract": "This brief discusses evolutionary game theory as a powerful and unified mathematical tool to study evolution of collective behaviours. It summarises some of my recent research directions using evolutionary game theory methods, which include i) the analysis of statistical properties of the number of (stable) equilibria in a random evolutionary game, and ii) the modelling of safety behaviours' evolution and the risk posed by advanced Artificial Intelligence technologies in a technology development race. Finally, it includes an outlook and some suggestions for future researchers.", "url": "https://arxiv.org/abs/2311.14480"}, {"metadata": {"arXiv": "2311.14125", "Date": "Thu, 23 Nov 2023 17:46:30 ", "Title": "Scalable AI Safety via Doubly-Efficient Debate", "Authors": ["Jonah Brown-Cohen", "Geoffrey Irving", "Georgios Piliouras"], "Categories": "cs.AI cs.LG"}, "abstract": "The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al. [2018] proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate deterministic AI systems for an exponential number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps.", "url": "https://arxiv.org/abs/2311.14125"}, {"metadata": {"arXiv": "2311.14305", "Date": "Fri, 24 Nov 2023 06:29:04 ", "Title": "New Epochs in AI Supervision: Design and Implementation of an Autonomous Radiology AI Monitoring System", "Authors": ["Vasantha Kumar Venugopal", "Abhishek Gupta", "Rohit Takhar", "Vidur Mahajan"], "Categories": "cs.AI cs.LG", "Comments": ["10 pages", "4 figures", "2 tables"]}, "abstract": "With the increasingly widespread adoption of AI in healthcare, maintaining the accuracy and reliability of AI models in clinical practice has become crucial. In this context, we introduce novel methods for monitoring the performance of radiology AI classification models in practice, addressing the challenges of obtaining real-time ground truth for performance monitoring. We propose two metrics - predictive divergence and temporal stability - to be used for preemptive alerts of AI performance changes. Predictive divergence, measured using Kullback-Leibler and Jensen-Shannon divergences, evaluates model accuracy by comparing predictions with those of two supplementary models. Temporal stability is assessed through a comparison of current predictions against historical moving averages, identifying potential model decay or data drift. This approach was retrospectively validated using chest X-ray data from a single-center imaging clinic, demonstrating its effectiveness in maintaining AI model reliability. By providing continuous, real-time insights into model performance, our system ensures the safe and effective use of AI in clinical decision-making, paving the way for more robust AI integration in healthcare", "url": "https://arxiv.org/abs/2311.14305"}, {"metadata": {"arXiv": "2311.14324", "Date": "Fri, 24 Nov 2023 07:53:48 ", "Title": "Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs", "Authors": ["Shengyin Sun", "Yuxiang Ren", "Chen Ma", "Xuecang Zhang"], "Categories": "cs.AI cs.CL cs.LG", "Comments": ["13 pages"]}, "abstract": "The latest advancements in large language models (LLMs) have revolutionized the field of natural language processing (NLP). Inspired by the success of LLMs in NLP tasks, some recent work has begun investigating the potential of applying LLMs in graph learning tasks. However, most of the existing work focuses on utilizing LLMs as powerful node feature augmenters, leaving employing LLMs to enhance graph topological structures an understudied problem. In this work, we explore how to leverage the information retrieval and text generation capabilities of LLMs to refine/enhance the topological structure of text-attributed graphs (TAGs) under the node classification setting. First, we propose using LLMs to help remove unreliable edges and add reliable ones in the TAG. Specifically, we first let the LLM output the semantic similarity between node attributes through delicate prompt designs, and then perform edge deletion and edge addition based on the similarity. Second, we propose using pseudo-labels generated by the LLM to improve graph topology, that is, we introduce the pseudo-label propagation as a regularization to guide the graph neural network (GNN) in learning proper edge weights. Finally, we incorporate the two aforementioned LLM-based methods for graph topological refinement into the process of GNN training, and perform extensive experiments on four real-world datasets. The experimental results demonstrate the effectiveness of LLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain on public benchmarks).", "url": "https://arxiv.org/abs/2311.14324"}, {"metadata": {"arXiv": "2311.14455", "Date": "Fri, 24 Nov 2023 13:09:34 ", "Title": "Universal Jailbreak Backdoors from Poisoned Human Feedback", "Authors": ["Javier Rando and Florian Tram\\`er"], "Categories": "cs.AI cs.CL cs.CR cs.LG"}, "abstract": "Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a \"jailbreak backdoor\" into the model. The backdoor embeds a trigger word into the model that acts like a universal \"sudo command\": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.", "url": "https://arxiv.org/abs/2311.14455"}, {"metadata": {"arXiv": "2311.13750", "Date": "Thu, 23 Nov 2023 00:53:11 ", "Title": "Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder", "Authors": ["Xiaohao Xu"], "Categories": "cs.CV cs.AI cs.LG cs.RO"}, "abstract": "This work proposes a unified self-supervised pre-training framework for transferable multi-modal perception representation learning via masked multi-modal reconstruction in Neural Radiance Field (NeRF), namely NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on certain view directions and locations, multi-modal embeddings extracted from corrupted multi-modal input signals, i.e., Lidar point clouds and images, are rendered into projected multi-modal feature maps via neural rendering. Then, original multi-modal signals serve as reconstruction targets for the rendered multi-modal feature maps to enable self-supervised representation learning. Extensive experiments show that the representation learned via NS-MAE shows promising transferability for diverse multi-modal and single-modal (camera-only and Lidar-only) perception models on diverse 3D perception downstream tasks (3D object detection and BEV map segmentation) with diverse amounts of fine-tuning labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of both the mechanism of masked autoencoder and neural radiance field. Our code shall be released upon acceptance.", "url": "https://arxiv.org/abs/2311.13750"}, {"metadata": {"arXiv": "2311.13628", "Date": "Wed, 22 Nov 2023 18:50:47 ", "Title": "Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models", "Authors": ["Thomas P. Zollo", "Todd Morrill", "Zhun Deng", "Jake C. Snell", "Toniann Pitassi", "Richard Zemel"], "Categories": "cs.LG cs.AI cs.CL", "Comments": ["33 pages", "10 figures", "and accepted to the Socially Responsible Language Modelling Research (SoLaR) workshop at NeurIPS 2023"]}, "abstract": "The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. While it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. To mitigate this prospect, we propose Prompt Risk Control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as open-ended chat, medical question summarization, and code generation highlight how such a framework can foster responsible deployment by reducing the risk of the worst outcomes.", "url": "https://arxiv.org/abs/2311.13628"}, {"metadata": {"arXiv": "2311.13664", "Date": "Wed, 22 Nov 2023 19:36:47 ", "Title": "Sample as You Infer: Predictive Coding With Langevin Dynamics", "Authors": ["Umais Zahid", "Qinghai Guo", "Zafeirios Fountas"], "Categories": "cs.LG cs.AI cs.CV cs.NE", "ACM-class": "I.2.0; I.2.6; I.2.10; I.4.0; I.4.8"}, "abstract": "We present a novel algorithm for parameter learning in generic deep generative models that builds upon the predictive coding (PC) framework of computational neuroscience. Our approach modifies the standard PC algorithm to bring performance on-par and exceeding that obtained from standard variational auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference procedure we re-envision it as an overdamped Langevin sampling, which facilitates optimisation with respect to a tight evidence lower bound (ELBO). We improve the resultant encoder-free training method by incorporating an encoder network to provide an amortised warm-start to our Langevin sampling and test three different objectives for doing so. Finally, to increase robustness to the sampling step size and reduce sensitivity to curvature, we validate a lightweight and easily computable form of preconditioning, inspired by Riemann Manifold Langevin and adaptive optimizers from the SGD literature. We compare against VAEs by training like-for-like generative models using our technique against those trained with standard reparameterisation-trick-based ELBOs. We observe our method out-performs or matches performance across a number of metrics, including sample quality, while converging in a fraction of the number of SGD training iterations.", "url": "https://arxiv.org/abs/2311.13664"}, {"metadata": {"arXiv": "2311.13693", "Date": "Wed, 22 Nov 2023 21:04:59 ", "Title": "Scalable CP Decomposition for Tensor Learning using GPU Tensor Cores", "Authors": ["Zeliang Zhang", "Zhuo Liu", "Susan Liang", "Zhiyuan Wang", "Yifan Zhu", "Chen Ding", "Chenliang Xu"], "Categories": "cs.LG cs.AI cs.DC"}, "abstract": "CP decomposition is a powerful tool for data science, especially gene analysis, deep learning, and quantum computation. However, the application of tensor decomposition is largely hindered by the exponential increment of the computational complexity and storage consumption with the size of tensors. While the data in our real world is usually presented as trillion- or even exascale-scale tensors, existing work can only support billion-scale scale tensors. In our work, we propose the Exascale-Tensor to mitigate the significant gap. Specifically, we propose a compression-based tensor decomposition framework, namely the exascale-tensor, to support exascale tensor decomposition. Then, we carefully analyze the inherent parallelism and propose a bag of strategies to improve computational efficiency. Last, we conduct experiments to decompose tensors ranging from million-scale to trillion-scale for evaluation. Compared to the baselines, the exascale-tensor supports 8,000x larger tensors and a speedup up to 6.95x. We also apply our method to two real-world applications, including gene analysis and tensor layer neural networks, of which the numeric results demonstrate the scalability and effectiveness of our method.", "url": "https://arxiv.org/abs/2311.13693"}, {"metadata": {"arXiv": "2311.13718", "Date": "Wed, 22 Nov 2023 22:23:34 ", "Title": "A Unified Approach to Count-Based Weakly-Supervised Learning", "Authors": ["Vinay Shukla", "Zhe Zeng", "Kareem Ahmed", "Guy Van den Broeck"], "Categories": "cs.LG cs.AI"}, "abstract": "High-quality labels are often very scarce, whereas unlabeled data with inferred weak labels occurs more naturally. In many cases, these weak labels dictate the frequency of each respective class over a set of instances. In this paper, we develop a unified approach to learning from such weakly-labeled data, which we call count-based weakly-supervised learning. At the heart of our approach is the ability to compute the probability of exactly k out of n outputs being set to true. This computation is differentiable, exact, and efficient. Building upon the previous computation, we derive a count loss penalizing the model for deviations in its distribution from an arithmetic constraint defined over label counts. We evaluate our approach on three common weakly-supervised learning paradigms and observe that our proposed approach achieves state-of-the-art or highly competitive results across all three of the paradigms.", "url": "https://arxiv.org/abs/2311.13718"}, {"metadata": {"arXiv": "2311.13816", "Date": "Thu, 23 Nov 2023 05:52:00 ", "Title": "Fairness-Aware Domain Generalization under Covariate and Dependence Shifts", "Authors": ["Chen Zhao", "Kai Jiang", "Xintao Wu", "Haoliang Wang", "Latifur Khan", "Christan Grant", "Feng Chen"], "Categories": "cs.LG cs.AI cs.CY"}, "abstract": "Achieving the generalization of an invariant classifier from source domains to shifted target domains while simultaneously considering model fairness is a substantial and complex challenge in machine learning. Existing domain generalization research typically attributes domain shifts to concept shift, which relates to alterations in class labels, and covariate shift, which pertains to variations in data styles. In this paper, by introducing another form of distribution shift, known as dependence shift, which involves variations in fair dependence patterns across domains, we propose a novel domain generalization approach that addresses domain shifts by considering both covariate and dependence shifts. We assert the existence of an underlying transformation model can transform data from one domain to another. By generating data in synthetic domains through the model, a fairness-aware invariant classifier is learned that enforces both model accuracy and fairness in unseen domains. Extensive empirical studies on four benchmark datasets demonstrate that our approach surpasses state-of-the-art methods.", "url": "https://arxiv.org/abs/2311.13816"}, {"metadata": {"arXiv": "2311.13821", "Date": "Thu, 23 Nov 2023 06:17:31 ", "Title": "HypUC: Hyperfine Uncertainty Calibration with Gradient-boosted Corrections for Reliable Regression on Imbalanced Electrocardiograms", "Authors": ["Uddeshya Upadhyay", "Sairam Bade", "Arjun Puranik", "Shahir Asfahan", "Melwin Babu", "Francisco Lopez-Jimenez", "Samuel J. Asirvatham", "Ashim Prasad", "Ajit Rajasekharan", "Samir Awasthi", "Rakesh Barve"], "Categories": "cs.LG cs.AI cs.CE stat.AP", "Comments": ["Published at TMLR"], "Journal-ref": "Transactions on Machine Learning Research (TMLR), 2023"}, "abstract": "The automated analysis of medical time series, such as the electrocardiogram (ECG), electroencephalogram (EEG), pulse oximetry, etc, has the potential to serve as a valuable tool for diagnostic decisions, allowing for remote monitoring of patients and more efficient use of expensive and time-consuming medical procedures. Deep neural networks (DNNs) have been demonstrated to process such signals effectively. However, previous research has primarily focused on classifying medical time series rather than attempting to regress the continuous-valued physiological parameters central to diagnosis. One significant challenge in this regard is the imbalanced nature of the dataset, as a low prevalence of abnormal conditions can lead to heavily skewed data that results in inaccurate predictions and a lack of certainty in such predictions when deployed. To address these challenges, we propose HypUC, a framework for imbalanced probabilistic regression in medical time series, making several contributions. (i) We introduce a simple kernel density-based technique to tackle the imbalanced regression problem with medical time series. (ii) Moreover, we employ a probabilistic regression framework that allows uncertainty estimation for the predicted continuous values. (iii) We also present a new approach to calibrate the predicted uncertainty further. (iv) Finally, we demonstrate a technique to use calibrated uncertainty estimates to improve the predicted continuous value and show the efficacy of the calibrated uncertainty estimates to flag unreliable predictions. HypUC is evaluated on a large, diverse, real-world dataset of ECGs collected from millions of patients, outperforming several conventional baselines on various diagnostic tasks, suggesting a potential use-case for the reliable clinical deployment of deep learning models.", "url": "https://arxiv.org/abs/2311.13821"}, {"metadata": {"arXiv": "2311.13843", "Date": "Thu, 23 Nov 2023 08:07:15 ", "Title": "Exact Combinatorial Optimization with Temporo-Attentional Graph Neural Networks", "Authors": ["Mehdi Seyfi", "Amin Banitalebi-Dehkordi", "Zirui Zhou", "and Yong Zhang"], "Categories": "cs.LG cs.AI cs.MS", "Comments": ["ECML PKDD 2023"], "Journal-ref": "ECML PKDD 2023"}, "abstract": "Combinatorial optimization finds an optimal solution within a discrete set of variables and constraints. The field has seen tremendous progress both in research and industry. With the success of deep learning in the past decade, a recent trend in combinatorial optimization has been to improve state-of-the-art combinatorial optimization solvers by replacing key heuristic components with machine learning (ML) models. In this paper, we investigate two essential aspects of machine learning algorithms for combinatorial optimization: temporal characteristics and attention. We argue that for the task of variable selection in the branch-and-bound (B&B) algorithm, incorporating the temporal information as well as the bipartite graph attention improves the solver's performance. We support our claims with intuitions and numerical results over several standard datasets used in the literature and competitions. Code is available at: https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=047c6cf2-8463-40d7-b92f-7b2ca998e935", "url": "https://arxiv.org/abs/2311.13843"}, {"metadata": {"arXiv": "2311.13845", "Date": "Thu, 23 Nov 2023 08:23:43 ", "Title": "Touring sampling with pushforward maps", "Authors": ["Vivien Cabannes", "Charles Arnal"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["5 pages"]}, "abstract": "The number of sampling methods could be daunting for a practitioner looking to cast powerful machine learning methods to their specific problem. This paper takes a theoretical stance to review and organize many sampling approaches in the ``generative modeling'' setting, where one wants to generate new data that are similar to some training examples. By revealing links between existing methods, it might prove useful to overcome some of the current challenges in sampling with diffusion models, such as long inference time due to diffusion simulation, or the lack of diversity in generated samples.", "url": "https://arxiv.org/abs/2311.13845"}, {"metadata": {"arXiv": "2311.13885", "Date": "Thu, 23 Nov 2023 10:15:09 ", "Title": "Can Physics Informed Neural Operators Self Improve?", "Authors": ["Ritam Majumdar", "Amey Varhade", "Shirish Karande", "Lovekesh Vig"], "Categories": "cs.LG cs.AI math.AP", "Comments": ["Paper accepted as a Spotlight talk at Symbiosis of Deep Learning and Differential Equations", "Neural Information Processing Systems 2023"]}, "abstract": "Self-training techniques have shown remarkable value across many deep learning models and tasks. However, such techniques remain largely unexplored when considered in the context of learning fast solvers for systems of partial differential equations (Eg: Neural Operators). In this work, we explore the use of self-training for Fourier Neural Operators (FNO). Neural Operators emerged as a data driven technique, however, data from experiments or traditional solvers is not always readily available. Physics Informed Neural Operators (PINO) overcome this constraint by utilizing a physics loss for the training, however the accuracy of PINO trained without data does not match the performance obtained by training with data. In this work we show that self-training can be used to close this gap in performance. We examine canonical examples, namely the 1D-Burgers and 2D-Darcy PDEs, to showcase the efficacy of self-training. Specifically, FNOs, when trained exclusively with physics loss through self-training, approach 1.07x for Burgers and 1.02x for Darcy, compared to FNOs trained with both data and physics loss. Furthermore, we discover that pseudo-labels can be used for self-training without necessarily training to convergence in each iteration. A consequence of this is that we are able to discover self-training schedules that improve upon the baseline performance of PINO in terms of accuracy as well as time.", "url": "https://arxiv.org/abs/2311.13885"}, {"metadata": {"arXiv": "2311.13953", "Date": "Thu, 23 Nov 2023 12:08:20 ", "Title": "Learning Uniform Clusters on Hypersphere for Deep Graph-level Clustering", "Authors": ["Mengling Hu", "Chaochao Chen", "Weiming Liu", "Xinyi Zhang", "Xinting Liao", "and Xiaolin Zheng"], "Categories": "cs.LG cs.AI"}, "abstract": "Graph clustering has been popularly studied in recent years. However, most existing graph clustering methods focus on node-level clustering, i.e., grouping nodes in a single graph into clusters. In contrast, graph-level clustering, i.e., grouping multiple graphs into clusters, remains largely unexplored. Graph-level clustering is critical in a variety of real-world applications, such as, properties prediction of molecules and community analysis in social networks. However, graph-level clustering is challenging due to the insufficient discriminability of graph-level representations, and the insufficient discriminability makes deep clustering be more likely to obtain degenerate solutions (cluster collapse). To address the issue, we propose a novel deep graph-level clustering method called Uniform Deep Graph Clustering (UDGC). UDGC assigns instances evenly to different clusters and then scatters those clusters on unit hypersphere, leading to a more uniform cluster-level distribution and a slighter cluster collapse. Specifically, we first propose Augmentation-Consensus Optimal Transport (ACOT) for generating uniformly distributed and reliable pseudo labels for partitioning clusters. Then we adopt contrastive learning to scatter those clusters. Besides, we propose Center Alignment Optimal Transport (CAOT) for guiding the model to learn better parameters, which further promotes the cluster performance. Our empirical study on eight well-known datasets demonstrates that UDGC significantly outperforms the state-of-the-art models.", "url": "https://arxiv.org/abs/2311.13953"}, {"metadata": {"arXiv": "2311.13983", "Date": "Thu, 23 Nov 2023 12:55:10 ", "Title": "Learning Dynamic Selection and Pricing of Out-of-Home Deliveries", "Authors": ["Fabian Akkerman", "Peter Dieter", "Martijn Mes"], "Categories": "cs.LG cs.AI"}, "abstract": "Home delivery failures, traffic congestion, and relatively large handling times have a negative impact on the profitability of last-mile logistics. These external factors contribute to up to $28\\%$ of the overall costs and $25\\%$ of emissions for the home delivery supply chain. A potential solution, showing annual growth rates up to $36\\%$, is the delivery to parcel lockers or parcel shops, denoted by out-of-home (OOH) delivery. In the academic literature, models of customer behavior with respect to OOH delivery were so far limited to deterministic settings, contrasting with the stochastic nature of actual customer choices. We model the sequential decision-making problem of which OOH location to offer against what incentive for each incoming customer, taking into account future customer arrivals and choices. We propose Dynamic Selection and Pricing of OOH (DSPO), an algorithmic pipeline that uses a novel spatial-temporal state encoding as input to a convolutional neural network. We demonstrate the performance of our method by benchmarking it against three state-of-the-art approaches. Our extensive numerical study, guided by real-world data, reveals that DSPO can save $20.8\\%$ in costs compared to a situation without OOH locations, $8.1\\%$ compared to a static selection and pricing policy, and $4.6\\%$ compared to a state-of-the-art demand management benchmark. We provide comprehensive insights into the complex interplay between OOH delivery dynamics and customer behavior influenced by pricing strategies. The implications of our findings suggest practitioners to adopt dynamic selection and pricing policies as OOH delivery gains a larger market share.", "url": "https://arxiv.org/abs/2311.13983"}, {"metadata": {"arXiv": "2311.14028", "Date": "Thu, 23 Nov 2023 14:33:03 ", "Title": "Continual Learning of Diffusion Models with Generative Distillation", "Authors": ["Sergi Masip", "Pau Rodriguez", "Tinne Tuytelaars", "Gido M. van de Ven"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Diffusion models are powerful generative models that achieve state-of-the-art performance in tasks such as image synthesis. However, training them demands substantial amounts of data and computational resources. Continual learning would allow for incrementally learning new tasks and accumulating knowledge, thus reusing already trained models would be possible. One potentially suitable approach is generative replay, where a copy of a generative model trained on previous tasks produces synthetic data that are interleaved with data from the current task. However, standard generative replay applied to diffusion models results in a catastrophic loss in denoising capabilities. In this paper, we propose generative distillation, an approach that distils the entire reverse process of a diffusion model. We demonstrate that our approach significantly improves the continual learning performance of generative replay with only a moderate increase in the computational costs.", "url": "https://arxiv.org/abs/2311.14028"}, {"metadata": {"arXiv": "2311.14110", "Date": "Thu, 23 Nov 2023 17:13:37 ", "Title": "When is Off-Policy Evaluation Useful? A Data-Centric Perspective", "Authors": ["Hao Sun", "Alex J. Chan", "Nabeel Seedat", "Alihan H\\\"uy\\\"uk", "Mihaela van der Schaar"], "Categories": "cs.LG cs.AI", "Comments": ["Off-Policy Evaluation", "Data-Centric AI", "Data-Centric Reinforcement Learning", "Reinforcement Learning"]}, "abstract": "Evaluating the value of a hypothetical target policy with only a logged dataset is important but challenging. On the one hand, it brings opportunities for safe policy improvement under high-stakes scenarios like clinical guidelines. On the other hand, such opportunities raise a need for precise off-policy evaluation (OPE). While previous work on OPE focused on improving the algorithm in value estimation, in this work, we emphasize the importance of the offline dataset, hence putting forward a data-centric framework for evaluating OPE problems. We propose DataCOPE, a data-centric framework for evaluating OPE, that answers the questions of whether and to what extent we can evaluate a target policy given a dataset. DataCOPE (1) forecasts the overall performance of OPE algorithms without access to the environment, which is especially useful before real-world deployment where evaluating OPE is impossible; (2) identifies the sub-group in the dataset where OPE can be inaccurate; (3) permits evaluations of datasets or data-collection strategies for OPE problems. Our empirical analysis of DataCOPE in the logged contextual bandit settings using healthcare datasets confirms its ability to evaluate both machine-learning and human expert policies like clinical guidelines.", "url": "https://arxiv.org/abs/2311.14110"}, {"metadata": {"arXiv": "2311.14115", "Date": "Thu, 23 Nov 2023 17:20:36 ", "Title": "A density estimation perspective on learning from pairwise human preferences", "Authors": ["Vincent Dumoulin", "Daniel D. Johnson", "Pablo Samuel Castro", "Hugo Larochelle", "Yann Dauphin"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on \"annotator misspecification\" -- failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models -- suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.", "url": "https://arxiv.org/abs/2311.14115"}, {"metadata": {"arXiv": "2311.14127", "Date": "Thu, 23 Nov 2023 17:50:30 ", "Title": "Byzantine Robustness and Partial Participation Can Be Achieved Simultaneously: Just Clip Gradient Differences", "Authors": ["Grigory Malinovsky", "Peter Richt\\'arik", "Samuel Horv\\'ath", "Eduard Gorbunov"], "Categories": "cs.LG cs.AI cs.DC math.OC", "Comments": ["50 pages; 1 figure"]}, "abstract": "Distributed learning has emerged as a leading paradigm for training large machine learning models. However, in real-world scenarios, participants may be unreliable or malicious, posing a significant challenge to the integrity and accuracy of the trained models. Byzantine fault tolerance mechanisms have been proposed to address these issues, but they often assume full participation from all clients, which is not always practical due to the unavailability of some clients or communication constraints. In our work, we propose the first distributed method with client sampling and provable tolerance to Byzantine workers. The key idea behind the developed method is the use of gradient clipping to control stochastic gradient differences in recursive variance reduction. This allows us to bound the potential harm caused by Byzantine workers, even during iterations when all sampled clients are Byzantine. Furthermore, we incorporate communication compression into the method to enhance communication efficiency. Under quite general assumptions, we prove convergence rates for the proposed method that match the existing state-of-the-art (SOTA) theoretical results.", "url": "https://arxiv.org/abs/2311.14127"}, {"metadata": {"arXiv": "2311.14139", "Date": "Thu, 23 Nov 2023 18:13:34 ", "Title": "Machine Learning For An Explainable Cost Prediction of Medical Insurance", "Authors": ["Ugochukwu Orji and Elochukwu Ukwandu"], "Categories": "cs.LG cs.AI", "Comments": ["42 pages", "16 figures and 9 tables"]}, "abstract": "Predictive modeling in healthcare continues to be an active actuarial research topic as more insurance companies aim to maximize the potential of Machine Learning approaches to increase their productivity and efficiency. In this paper, the authors deployed three regression-based ensemble ML models that combine variations of decision trees through Extreme Gradient Boosting, Gradient-boosting Machine, and Random Forest) methods in predicting medical insurance costs. Explainable Artificial Intelligence methods SHapley Additive exPlanations and Individual Conditional Expectation plots were deployed to discover and explain the key determinant factors that influence medical insurance premium prices in the dataset. The dataset used comprised 986 records and is publicly available in the KAGGLE repository. The models were evaluated using four performance evaluation metrics, including R-squared, Mean Absolute Error, Root Mean Squared Error, and Mean Absolute Percentage Error. The results show that all models produced impressive outcomes; however, the XGBoost model achieved a better overall performance although it also expanded more computational resources, while the RF model recorded a lesser prediction error and consumed far fewer computing resources than the XGBoost model. Furthermore, we compared the outcome of both XAi methods in identifying the key determinant features that influenced the PremiumPrices for each model and whereas both XAi methods produced similar outcomes, we found that the ICE plots showed in more detail the interactions between each variable than the SHAP analysis which seemed to be more high-level. It is the aim of the authors that the contributions of this study will help policymakers, insurers, and potential medical insurance buyers in their decision-making process for selecting the right policies that meet their specific needs.", "url": "https://arxiv.org/abs/2311.14139"}, {"metadata": {"arXiv": "2311.14156", "Date": "Thu, 23 Nov 2023 18:56:51 ", "Title": "Variational Annealing on Graphs for Combinatorial Optimization", "Authors": ["Sebastian Sanokowski", "Wilhelm Berghammer", "Sepp Hochreiter", "Sebastian Lehner"], "Categories": "cs.LG cs.AI cs.DM stat.ML", "Comments": ["Accepted at NeurIPS 2023"]}, "abstract": "Several recent unsupervised learning methods use probabilistic approaches to solve combinatorial optimization (CO) problems based on the assumption of statistically independent solution variables. We demonstrate that this assumption imposes performance limitations in particular on difficult problem instances. Our results corroborate that an autoregressive approach which captures statistical dependencies among solution variables yields superior performance on many popular CO problems. We introduce subgraph tokenization in which the configuration of a set of solution variables is represented by a single token. This tokenization technique alleviates the drawback of the long sequential sampling procedure which is inherent to autoregressive methods without sacrificing expressivity. Importantly, we theoretically motivate an annealed entropy regularization and show empirically that it is essential for efficient and stable learning.", "url": "https://arxiv.org/abs/2311.14156"}, {"metadata": {"arXiv": "2311.14335", "Date": "Fri, 24 Nov 2023 08:16:39 ", "Title": "Comparative Analysis of Transformers for Modeling Tabular Data: A Casestudy using Industry Scale Dataset", "Authors": ["Usneek Singh", "Piyush Arora", "Shamika Ganesan", "Mohit Kumar", "Siddhant Kulkarni", "Salil R. Joshi"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at 7th Joint International Conference on Data Science & Management of Data (11th ACMIKDD CODS and 29th COMAD)"]}, "abstract": "We perform a comparative analysis of transformer-based models designed for modeling tabular data, specifically on an industry-scale dataset. While earlier studies demonstrated promising outcomes on smaller public or synthetic datasets, the effectiveness did not extend to larger industry-scale datasets. The challenges identified include handling high-dimensional data, the necessity for efficient pre-processing of categorical and numerical features, and addressing substantial computational requirements. To overcome the identified challenges, the study conducts an extensive examination of various transformer-based models using both synthetic datasets and the default prediction Kaggle dataset (2022) from American Express. The paper presents crucial insights into optimal data pre-processing, compares pre-training and direct supervised learning methods, discusses strategies for managing categorical and numerical features, and highlights trade-offs between computational resources and performance. Focusing on temporal financial data modeling, the research aims to facilitate the systematic development and deployment of transformer-based models in real-world scenarios, emphasizing scalability.", "url": "https://arxiv.org/abs/2311.14335"}, {"metadata": {"arXiv": "2311.14390", "Date": "Fri, 24 Nov 2023 10:14:05 ", "Title": "Directly Attention Loss Adjusted Prioritized Experience Replay", "Authors": ["Zhuoying Chen", "Huiping Li", "Zhaoxu Wang"], "Categories": "cs.LG cs.AI"}, "abstract": "Prioritized Experience Replay (PER) enables the model to learn more about relatively important samples by artificially changing their accessed frequencies. However, this non-uniform sampling method shifts the state-action distribution that is originally used to estimate Q-value functions, which brings about the estimation deviation. In this article, an novel off policy reinforcement learning training framework called Directly Attention Loss Adjusted Prioritized Experience Replay (DALAP) is proposed, which can directly quantify the changed extent of the shifted distribution through Parallel Self-Attention network, so as to accurately compensate the error. In addition, a Priority-Encouragement mechanism is designed simultaneously to optimize the sample screening criterion, and further improve the training efficiency. In order to verify the effectiveness and generality of DALAP, we integrate it with the value-function based, the policy-gradient based and multi-agent reinforcement learning algorithm, respectively. The multiple groups of comparative experiments show that DALAP has the significant advantages of both improving the convergence rate and reducing the training variance.", "url": "https://arxiv.org/abs/2311.14390"}, {"metadata": {"arXiv": "2311.14407", "Date": "Fri, 24 Nov 2023 10:59:12 ", "Title": "LLamol: A Dynamic Multi-Conditional Generative Transformer for De Novo Molecular Design", "Authors": ["Niklas Dobberstein", "Astrid Maass", "Jan Hamaekers"], "Categories": "cs.LG cs.AI physics.chem-ph"}, "abstract": "Generative models have demonstrated substantial promise in Natural Language Processing (NLP) and have found application in designing molecules, as seen in General Pretrained Transformer (GPT) models. In our efforts to develop such a tool for exploring the organic chemical space in search of potentially electro-active compounds, we present \"LLamol\", a single novel generative transformer model based on the LLama 2 architecture, which was trained on a 13M superset of organic compounds drawn from diverse public sources. To allow for a maximum flexibility in usage and robustness in view of potentially incomplete data, we introduce \"Stochastic Context Learning\" as a new training procedure. We demonstrate that the resulting model adeptly handles single- and multi-conditional organic molecule generation with up to four conditions, yet more are possible. The model generates valid molecular structures in SMILES notation while flexibly incorporating three numerical and/or one token sequence into the generative process, just as requested. The generated compounds are very satisfactory in all scenarios tested. In detail, we showcase the model's capability to utilize token sequences for conditioning, either individually or in combination with numerical properties, making LLamol a potent tool for de novo molecule design, easily expandable with new properties.", "url": "https://arxiv.org/abs/2311.14407"}, {"metadata": {"arXiv": "2311.14495", "Date": "Fri, 24 Nov 2023 14:08:31 ", "Title": "StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization", "Authors": ["Shida Wang", "Qianxiao Li"], "Categories": "cs.LG cs.AI cs.CL math.DS"}, "abstract": "In this paper, we investigate the long-term memory learning capabilities of state-space models (SSMs) from the perspective of parameterization. We prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory. Our analysis identifies this \"curse of memory\" as a result of the recurrent weights converging to a stability boundary, suggesting that a reparameterization technique can be effective. To this end, we introduce a class of reparameterization techniques for SSMs that effectively lift its memory limitations. Besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability. We validate our findings using synthetic datasets and language models.", "url": "https://arxiv.org/abs/2311.14495"}, {"metadata": {"arXiv": "2311.14153", "Date": "Thu, 23 Nov 2023 18:54:25 ", "Title": "Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC using Tube-Guided Data Augmentation and NeRFs", "Authors": ["Andrea Tagliabue", "Jonathan P. How"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["Video: https://youtu.be/_W5z33ZK1m4. Evolved paper from our previous work: arXiv:2210.10127"]}, "abstract": "Imitation learning (IL) can train computationally-efficient sensorimotor policies from a resource-intensive Model Predictive Controller (MPC), but it often requires many samples, leading to long training times or limited robustness. To address these issues, we combine IL with a variant of robust MPC that accounts for process and sensing uncertainties, and we design a data augmentation (DA) strategy that enables efficient learning of vision-based policies. The proposed DA method, named Tube-NeRF, leverages Neural Radiance Fields (NeRFs) to generate novel synthetic images, and uses properties of the robust MPC (the tube) to select relevant views and to efficiently compute the corresponding actions. We tailor our approach to the task of localization and trajectory tracking on a multirotor, by learning a visuomotor policy that generates control actions using images from the onboard camera as only source of horizontal position. Our evaluations numerically demonstrate learning of a robust visuomotor policy with an 80-fold increase in demonstration efficiency and a 50% reduction in training time over current IL methods. Additionally, our policies successfully transfer to a real multirotor, achieving accurate localization and low tracking errors despite large disturbances, with an onboard inference time of only 1.5 ms.", "url": "https://arxiv.org/abs/2311.14153"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
