<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2311.17093", "Date": "Tue, 28 Nov 2023 06:12:28 ", "Title": "Improved Prototypical Semi-Supervised Learning with Foundation Models: Prototype Selection, Parametric vMF-SNE Pretraining and Multi-view Pseudolabelling", "Authors": ["Evelyn Mannix and Howard Bondell"], "Categories": "cs.CV cs.LG"}, "abstract": "In this paper we present an improved approach to prototypical semi-supervised learning for computer vision, in the context of leveraging a frozen foundation model as the backbone of our neural network. As a general tool, we propose parametric von-Mises Fisher Stochastic Neighbour Embedding (vMF-SNE) to create mappings with neural networks between high-dimensional latent spaces that preserve local structure. This enables us to pretrain the projection head of our network using the high-quality embeddings of the foundation model with vMF-SNE. We also propose soft multi-view pseudolabels, where predictions across multiple views are combined to provide a more reliable supervision signal compared to a consistency or swapped assignment approach. We demonstrate that these ideas improve upon P}redicting View-Assignments with Support Samples (PAWS), a current state-of-the-art semi-supervised learning method, as well as Robust PAWS (RoPAWS), over a range of benchmarking datasets. We also introduce simple $k$-means prototype selection, a technique that provides superior performance to other unsupervised label selection approaches in this context. These changes improve upon PAWS by an average of +2.9% for CIFAR-10 and +5.7% for CIFAR-100 with four labels per class, and by +15.2% for DeepWeeds, a particularly challenging dataset for semi-supervised learning. We also achieve new state-of-the-art results in semi-supervised learning in this small label regime for CIFAR-10 - 95.8% (+0.7%) and CIFAR-100 - 76.6% (+12.0%).", "url": "https://arxiv.org/abs/2311.17093"}, {"metadata": {"arXiv": "2311.17121", "Date": "Tue, 28 Nov 2023 13:44:33 ", "Title": "Generative Data Augmentation Improves Scribble-supervised Semantic Segmentation", "Authors": ["Jacob Schnell", "Jieke Wang", "Lu Qi", "Vincent Tao Hu", "Meng Tang"], "Categories": "cs.CV cs.LG"}, "abstract": "Recent advances in generative models, such as diffusion models, have made generating high-quality synthetic images widely accessible. Prior works have shown that training on synthetic images improves many perception tasks, such as image classification, object detection, and semantic segmentation. We are the first to explore generative data augmentations for scribble-supervised semantic segmentation. We propose a generative data augmentation method that leverages a ControlNet diffusion model conditioned on semantic scribbles to produce high-quality training data. However, naive implementations of generative data augmentations may inadvertently harm the performance of the downstream segmentor rather than improve it. We leverage classifier-free diffusion guidance to enforce class consistency and introduce encode ratios to trade off data diversity for data realism. Using the guidance scale and encode ratio, we are able to generate a spectrum of high-quality training images. We propose multiple augmentation schemes and find that these schemes significantly impact model performance, especially in the low-data regime. Our framework further reduces the gap between the performance of scribble-supervised segmentation and that of fully-supervised segmentation. We also show that our framework significantly improves segmentation performance on small datasets, even surpassing fully-supervised segmentation.", "url": "https://arxiv.org/abs/2311.17121"}, {"metadata": {"arXiv": "2311.17129", "Date": "Tue, 28 Nov 2023 16:09:09 ", "Title": "Feedback RoI Features Improve Aerial Object Detection", "Authors": ["Botao Ren", "Botian Xu", "Tengyu Liu", "Jingyi Wang", "Zhidong Deng"], "Categories": "cs.CV cs.LG"}, "abstract": "Neuroscience studies have shown that the human visual system utilizes high-level feedback information to guide lower-level perception, enabling adaptation to signals of different characteristics. In light of this, we propose Feedback multi-Level feature Extractor (Flex) to incorporate a similar mechanism for object detection. Flex refines feature selection based on image-wise and instance-level feedback information in response to image quality variation and classification uncertainty. Experimental results show that Flex offers consistent improvement to a range of existing SOTA methods on the challenging aerial object detection datasets including DOTA-v1.0, DOTA-v1.5, and HRSC2016. Although the design originates in aerial image detection, further experiments on MS COCO also reveal our module's efficacy in general detection models. Quantitative and qualitative analyses indicate that the improvements are closely related to image qualities, which match our motivation.", "url": "https://arxiv.org/abs/2311.17129"}, {"metadata": {"arXiv": "2311.17218", "Date": "Tue, 28 Nov 2023 20:42:30 ", "Title": "BIM: Block-Wise Self-Supervised Learning with Masked Image Modeling", "Authors": ["Yixuan Luo", "Mengye Ren", "Sai Qian Zhang"], "Categories": "cs.CV cs.LG"}, "abstract": "Like masked language modeling (MLM) in natural language processing, masked image modeling (MIM) aims to extract valuable insights from image patches to enhance the feature extraction capabilities of the underlying deep neural network (DNN). Contrasted with other training paradigms like supervised learning and unsupervised contrastive learning, masked image modeling (MIM) pretraining typically demands significant computational resources in order to manage large training data batches (e.g., 4096). The significant memory and computation requirements pose a considerable challenge to its broad adoption. To mitigate this, we introduce a novel learning framework, termed~\\textit{Block-Wise Masked Image Modeling} (BIM). This framework involves decomposing the MIM tasks into several sub-tasks with independent computation patterns, resulting in block-wise back-propagation operations instead of the traditional end-to-end approach. Our proposed BIM maintains superior performance compared to conventional MIM while greatly reducing peak memory consumption. Moreover, BIM naturally enables the concurrent training of numerous DNN backbones of varying depths. This leads to the creation of multiple trained DNN backbones, each tailored to different hardware platforms with distinct computing capabilities. This approach significantly reduces computational costs in comparison with training each DNN backbone individually. Our framework offers a promising solution for resource constrained training of MIM.", "url": "https://arxiv.org/abs/2311.17218"}, {"metadata": {"arXiv": "2311.17434", "Date": "Wed, 29 Nov 2023 08:26:18 ", "Title": "Group-wise Sparse and Explainable Adversarial Attacks", "Authors": ["Shpresim Sadiku", "Moritz Wagner", "Sebastian Pokutta"], "Categories": "cs.CV cs.CR cs.LG math.OC"}, "abstract": "Sparse adversarial attacks fool deep neural networks (DNNs) through minimal pixel perturbations, typically regularized by the $\\ell_0$ norm. Recent efforts have replaced this norm with a structural sparsity regularizer, such as the nuclear group norm, to craft group-wise sparse adversarial attacks. The resulting perturbations are thus explainable and hold significant practical relevance, shedding light on an even greater vulnerability of DNNs than previously anticipated. However, crafting such attacks poses an optimization challenge, as it involves computing norms for groups of pixels within a non-convex objective. In this paper, we tackle this challenge by presenting an algorithm that simultaneously generates group-wise sparse attacks within semantically meaningful areas of an image. In each iteration, the core operation of our algorithm involves the optimization of a quasinorm adversarial loss. This optimization is achieved by employing the $1/2$-quasinorm proximal operator for some iterations, a method tailored for nonconvex programming. Subsequently, the algorithm transitions to a projected Nesterov's accelerated gradient descent with $2$-norm regularization applied to perturbation magnitudes. We rigorously evaluate the efficacy of our novel attack in both targeted and non-targeted attack scenarios, on CIFAR-10 and ImageNet datasets. When compared to state-of-the-art methods, our attack consistently results in a remarkable increase in group-wise sparsity, e.g., an increase of $48.12\\%$ on CIFAR-10 and $40.78\\%$ on ImageNet (average case, targeted attack), all while maintaining lower perturbation magnitudes. Notably, this performance is complemented by a significantly faster computation time and a $100\\%$ attack success rate.", "url": "https://arxiv.org/abs/2311.17434"}, {"metadata": {"arXiv": "2311.17552", "Date": "Wed, 29 Nov 2023 11:35:54 ", "Title": "An Efficient Illumination Invariant Tiger Detection Framework for Wildlife Surveillance", "Authors": ["Gaurav Pendharkar", "A.Ancy Micheal", "Jason Misquitta", "Ranjeesh Kaippada"], "Categories": "cs.CV cs.LG", "Comments": ["accepted at ICCIS 2023"]}, "abstract": "Tiger conservation necessitates the strategic deployment of multifaceted initiatives encompassing the preservation of ecological habitats, anti-poaching measures, and community involvement for sustainable growth in the tiger population. With the advent of artificial intelligence, tiger surveillance can be automated using object detection. In this paper, an accurate illumination invariant framework is proposed based on EnlightenGAN and YOLOv8 for tiger detection. The fine-tuned YOLOv8 model achieves a mAP score of 61% without illumination enhancement. The illumination enhancement improves the mAP by 0.7%. The approaches elevate the state-of-the-art performance on the ATRW dataset by approximately 6% to 7%.", "url": "https://arxiv.org/abs/2311.17552"}, {"metadata": {"arXiv": "2311.17607", "Date": "Wed, 29 Nov 2023 13:05:06 ", "Title": "Topology-Preserving Adversarial Training", "Authors": ["Xiaoyue Mi", "Fan Tang", "Yepeng Weng", "Danding Wang", "Juan Cao", "Sheng Tang", "Peng Li", "Yang Liu"], "Categories": "cs.CV cs.LG"}, "abstract": "Despite the effectiveness in improving the robustness of neural networks, adversarial training has suffered from the natural accuracy degradation problem, i.e., accuracy on natural samples has reduced significantly. In this study, we reveal that natural accuracy degradation is highly related to the disruption of the natural sample topology in the representation space by quantitative and qualitative experiments. Based on this observation, we propose Topology-pReserving Adversarial traINing (TRAIN) to alleviate the problem by preserving the topology structure of natural samples from a standard model trained only on natural samples during adversarial training. As an additional regularization, our method can easily be combined with various popular adversarial training algorithms in a plug-and-play manner, taking advantage of both sides. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet show that our proposed method achieves consistent and significant improvements over various strong baselines in most cases. Specifically, without additional data, our proposed method achieves up to 8.78% improvement in natural accuracy and 4.50% improvement in robust accuracy.", "url": "https://arxiv.org/abs/2311.17607"}, {"metadata": {"arXiv": "2311.17608", "Date": "Wed, 29 Nov 2023 13:05:20 ", "Title": "Adversarial Robust Memory-Based Continual Learner", "Authors": ["Xiaoyue Mi", "Fan Tang", "Zonghan Yang", "Danding Wang", "Juan Cao", "Peng Li", "Yang Liu"], "Categories": "cs.CV cs.LG"}, "abstract": "Despite the remarkable advances that have been made in continual learning, the adversarial vulnerability of such methods has not been fully discussed. We delve into the adversarial robustness of memory-based continual learning algorithms and observe limited robustness improvement by directly applying adversarial training techniques. Preliminary studies reveal the twin challenges for building adversarial robust continual learners: accelerated forgetting in continual learning and gradient obfuscation in adversarial robustness. In this study, we put forward a novel adversarial robust memory-based continual learner that adjusts data logits to mitigate the forgetting of pasts caused by adversarial samples. Furthermore, we devise a gradient-based data selection mechanism to overcome the gradient obfuscation caused by limited stored data. The proposed approach can widely integrate with existing memory-based continual learning as well as adversarial training algorithms in a plug-and-play way. Extensive experiments on Split-CIFAR10/100 and Split-Tiny-ImageNet demonstrate the effectiveness of our approach, achieving up to 8.13% higher accuracy for adversarial data.", "url": "https://arxiv.org/abs/2311.17608"}, {"metadata": {"arXiv": "2311.17609", "Date": "Wed, 29 Nov 2023 13:06:48 ", "Title": "AnyLens: A Generative Diffusion Model with Any Rendering Lens", "Authors": ["Andrey Voynov", "Amir Hertz", "Moab Arar", "Shlomi Fruchter", "Daniel Cohen-Or"], "Categories": "cs.CV cs.GR cs.LG"}, "abstract": "State-of-the-art diffusion models can generate highly realistic images based on various conditioning like text, segmentation, and depth. However, an essential aspect often overlooked is the specific camera geometry used during image capture. The influence of different optical systems on the final scene appearance is frequently overlooked. This study introduces a framework that intimately integrates a text-to-image diffusion model with the particular lens geometry used in image rendering. Our method is based on a per-pixel coordinate conditioning method, enabling the control over the rendering geometry. Notably, we demonstrate the manipulation of curvature properties, achieving diverse visual effects, such as fish-eye, panoramic views, and spherical texturing using a single diffusion model.", "url": "https://arxiv.org/abs/2311.17609"}, {"metadata": {"arXiv": "2311.17717", "Date": "Wed, 29 Nov 2023 15:19:49 ", "Title": "Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers", "Authors": ["Chi-Pin Huang", "Kai-Po Chang", "Chung-Ting Tsai", "Yung-Hsuan Lai", "Yu-Chiang Frank Wang"], "Categories": "cs.CV cs.LG"}, "abstract": "Concept erasure in text-to-image diffusion models aims to disable pre-trained diffusion models from generating images related to a target concept. To perform reliable concept erasure, the properties of robustness and locality are desirable. The former refrains the model from producing images associated with the target concept for any paraphrased or learned prompts, while the latter preserves the model ability in generating images for non-target concepts. In this paper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler), which learns a lightweight Eraser to perform concept erasing and enhances locality and robustness with the proposed concept-localized regularization and adversarial prompt learning, respectively. Comprehensive quantitative and qualitative experiments with various concept prompts verify the superiority of Receler over the previous erasing methods on the above two desirable properties.", "url": "https://arxiv.org/abs/2311.17717"}, {"metadata": {"arXiv": "2311.17898", "Date": "Wed, 29 Nov 2023 18:51:46 ", "Title": "Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis", "Authors": ["Jinqi Luo", "Kwan Ho Ryan Chan", "Dimitris Dimos", "Ren\\'e Vidal"], "Categories": "cs.CV cs.CL cs.LG"}, "abstract": "Hallucinations and unfaithful synthesis due to inaccurate prompts with insufficient semantic details are widely observed in multimodal generative models. A prevalent strategy to align multiple modalities is to fine-tune the generator with a large number of annotated text-image pairs. However, such a procedure is labor-consuming and resource-draining. The key question we ask is: can we enhance the quality and faithfulness of text-driven generative models beyond extensive text-image pair annotations? To address this question, we propose Knowledge Pursuit Prompting (KPP), a zero-shot framework that iteratively incorporates external knowledge to help generators produce reliable visual content. Instead of training generators to handle generic prompts, KPP employs a recursive knowledge query process to gather informative external facts from the knowledge base, instructs a language model to compress the acquired knowledge for prompt refinement, and utilizes text-driven generators for visual synthesis. The entire process is zero-shot, without accessing the architectures and parameters of generative models. We evaluate the framework across multiple text-driven generative tasks (image, 3D rendering, and video) on datasets of different domains. We further demonstrate the extensibility and adaptability of KPP through varying foundation model bases and instructions. Our results show that KPP is capable of generating faithful and semantically rich content across diverse visual domains, offering a promising solution to improve multimodal generative models.", "url": "https://arxiv.org/abs/2311.17898"}, {"metadata": {"arXiv": "2311.17410", "Date": "Wed, 29 Nov 2023 07:30:32 ", "Title": "GNNFlow: A Distributed Framework for Continuous Temporal GNN Learning on Dynamic Graphs", "Authors": ["Yuchen Zhong", "Guangming Sheng", "Tianzuo Qin", "Minjie Wang", "Quan Gan", "and Chuan Wu"], "Categories": "cs.DC cs.LG"}, "abstract": "Graph Neural Networks (GNNs) play a crucial role in various fields. However, most existing deep graph learning frameworks assume pre-stored static graphs and do not support training on graph streams. In contrast, many real-world graphs are dynamic and contain time domain information. We introduce GNNFlow, a distributed framework that enables efficient continuous temporal graph representation learning on dynamic graphs on multi-GPU machines. GNNFlow introduces an adaptive time-indexed block-based data structure that effectively balances memory usage with graph update and sampling operation efficiency. It features a hybrid GPU-CPU graph data placement for rapid GPU-based temporal neighborhood sampling and kernel optimizations for enhanced sampling processes. A dynamic GPU cache for node and edge features is developed to maximize cache hit rates through reuse and restoration strategies. GNNFlow supports distributed training across multiple machines with static scheduling to ensure load balance. We implement GNNFlow based on DGL and PyTorch. Our experimental results show that GNNFlow provides up to 21.1x faster continuous learning than existing systems.", "url": "https://arxiv.org/abs/2311.17410"}, {"metadata": {"arXiv": "2311.17073", "Date": "Mon, 27 Nov 2023 19:02:43 ", "Title": "Practical Layout-Aware Analog/Mixed-Signal Design Automation with Bayesian Neural Networks", "Authors": ["Ahmet F. Budak", "Keren Zhu", "and David Z. Pan"], "Categories": "cs.LG cs.CE cs.SY eess.SY math.OC", "Comments": ["Accepted to the 42nd International Conference on Computer-Aided Design (ICCAD 2023); 8 pages", "8 figures"]}, "abstract": "The high simulation cost has been a bottleneck of practical analog/mixed-signal design automation. Many learning-based algorithms require thousands of simulated data points, which is impractical for expensive to simulate circuits. We propose a learning-based algorithm that can be trained using a small amount of data and, therefore, scalable to tasks with expensive simulations. Our efficient algorithm solves the post-layout performance optimization problem where simulations are known to be expensive. Our comprehensive study also solves the schematic-level sizing problem. For efficient optimization, we utilize Bayesian Neural Networks as a regression model to approximate circuit performance. For layout-aware optimization, we handle the problem as a multi-fidelity optimization problem and improve efficiency by exploiting the correlations from cheaper evaluations. We present three test cases to demonstrate the efficiency of our algorithms. Our tests prove that the proposed approach is more efficient than conventional baselines and state-of-the-art algorithms.", "url": "https://arxiv.org/abs/2311.17073"}, {"metadata": {"arXiv": "2311.17094", "Date": "Tue, 28 Nov 2023 06:17:49 ", "Title": "In Search of a Data Transformation That Accelerates Neural Field Training", "Authors": ["Junwon Seo", "Sangyoon Lee", "Kwang In Kim", "Jaeho Lee"], "Categories": "cs.LG cs.CV", "Comments": ["17 pages", "preprint"]}, "abstract": "Neural field is an emerging paradigm in data representation that trains a neural network to approximate the given signal. A key obstacle that prevents its widespread adoption is the encoding speed-generating neural fields requires an overfitting of a neural network, which can take a significant number of SGD steps to reach the desired fidelity level. In this paper, we delve into the impacts of data transformations on the speed of neural field training, specifically focusing on how permuting pixel locations affect the convergence speed of SGD. Counterintuitively, we find that randomly permuting the pixel locations can considerably accelerate the training. To explain this phenomenon, we examine the neural field training through the lens of PSNR curves, loss landscapes, and error patterns. Our analyses suggest that the random pixel permutations remove the easy-to-fit patterns, which facilitate easy optimization in the early stage but hinder capturing fine details of the signal.", "url": "https://arxiv.org/abs/2311.17094"}, {"metadata": {"arXiv": "2311.17134", "Date": "Tue, 28 Nov 2023 18:51:19 ", "Title": "\\texttt{GlycoNMR}: Dataset and benchmarks for NMR chemical shift prediction of carbohydrates with graph neural networks", "Authors": ["Zizhang Chen", "Ryan Paul Badman", "Lachele Foley", "Robert Woods", "Pengyu Hong"], "Categories": "cs.LG q-bio.QM"}, "abstract": "Molecular representation learning (MRL) is a powerful tool for bridging the gap between machine learning and chemical sciences, as it converts molecules into numerical representations while preserving their chemical features. These encoded representations serve as a foundation for various downstream biochemical studies, including property prediction and drug design. MRL has had great success with proteins and general biomolecule datasets. Yet, in the growing sub-field of glycoscience (the study of carbohydrates, where longer carbohydrates are also called glycans), MRL methods have been barely explored. This under-exploration can be primarily attributed to the limited availability of comprehensive and well-curated carbohydrate-specific datasets and a lack of Machine learning (ML) pipelines specifically tailored to meet the unique problems presented by carbohydrate data. Since interpreting and annotating carbohydrate-specific data is generally more complicated than protein data, domain experts are usually required to get involved. The existing MRL methods, predominately optimized for proteins and small biomolecules, also cannot be directly used in carbohydrate applications without special modifications. To address this challenge, accelerate progress in glycoscience, and enrich the data resources of the MRL community, we introduce GlycoNMR. GlycoNMR contains two laboriously curated datasets with 2,609 carbohydrate structures and 211,543 annotated nuclear magnetic resonance (NMR) chemical shifts for precise atomic-level prediction. We tailored carbohydrate-specific features and adapted existing MRL models to tackle this problem effectively. For illustration, we benchmark four modified MRL models on our new datasets.", "url": "https://arxiv.org/abs/2311.17134"}, {"metadata": {"arXiv": "2311.17173", "Date": "Tue, 28 Nov 2023 19:07:30 ", "Title": "A personalized Uncertainty Quantification framework for patient survival models: estimating individual uncertainty of patients with metastatic brain tumors in the absence of ground truth", "Authors": ["Yuqi Wang", "Aarzu Gupta", "David Carpenter", "Trey Mullikin", "Zachary J. Reitman", "Scott Floyd", "John Kirkpatrick", "Joseph K. Salama", "Paul W. Sperduto", "Jian-Guo Liu", "Mustafa R. Bashir", "Kyle J. Lafata"], "Categories": "cs.LG q-bio.QM stat.AP"}, "abstract": "TodevelopanovelUncertaintyQuantification (UQ) framework to estimate the uncertainty of patient survival models in the absence of ground truth, we developed and evaluated our approach based on a dataset of 1383 patients treated with stereotactic radiosurgery (SRS) for brain metastases between January 2015 and December 2020. Our motivating hypothesis is that a time-to-event prediction of a test patient on inference is more certain given a higher feature-space-similarity to patients in the training set. Therefore, the uncertainty for a particular patient-of-interest is represented by the concordance index between a patient similarity rank and a prediction similarity rank. Model uncertainty was defined as the increased percentage of the max uncertainty-constrained-AUC compared to the model AUC. We evaluated our method on multiple clinically-relevant endpoints, including time to intracranial progression (ICP), progression-free survival (PFS) after SRS, overall survival (OS), and time to ICP and/or death (ICPD), on a variety of both statistical and non-statistical models, including CoxPH, conditional survival forest (CSF), and neural multi-task linear regression (NMTLR). Our results show that all models had the lowest uncertainty on ICP (2.21%) and the highest uncertainty (17.28%) on ICPD. OS models demonstrated high variation in uncertainty performance, where NMTLR had the lowest uncertainty(1.96%)and CSF had the highest uncertainty (14.29%). In conclusion, our method can estimate the uncertainty of individual patient survival modeling results. As expected, our data empirically demonstrate that as model uncertainty measured via our technique increases, the similarity between a feature-space and its predicted outcome decreases.", "url": "https://arxiv.org/abs/2311.17173"}, {"metadata": {"arXiv": "2311.17204", "Date": "Tue, 28 Nov 2023 20:18:42 ", "Title": "Optimal EEG Electrode Set for Emotion Recognition From Brain Signals: An Empirical Quest", "Authors": ["Rumman Ahmed Prodhan", "Sumya Akter", "Tanmoy Sarkar Pias", "Md. Akhtaruzzaman Adnan"], "Categories": "cs.LG eess.SP"}, "abstract": "The human brain is a complex organ, still completely undiscovered, that controls almost all the parts of the body. Apart from survival, the human brain stimulates emotions. Recent research indicates that brain signals can be very effective for emotion recognition. However, which parts of the brain exhibit most of the emotions is still under-explored. In this study, we empirically analyze the contribution of each part of the brain in exhibiting emotions. We use the DEAP dataset to find the most optimal electrode set which eventually leads to the effective brain part associated with emotions. We use Fast Fourier Transformation for effective feature extraction and a 1D-CNN with residual connection for classification. Though 32 electrodes from the DEAP dataset got an accuracy of 97.34%, only 12 electrodes (F7, P8, O1, F8, C4, T7, PO3, Fp1, Fp2, O2, P3, and Fz) achieve 95.81% accuracy. This study also shows that adding more than 10 electrodes does not improve performance significantly. Moreover, the frontal lobe is the most important for recognizing emotion.", "url": "https://arxiv.org/abs/2311.17204"}, {"metadata": {"arXiv": "2311.17225", "Date": "Tue, 28 Nov 2023 20:57:10 ", "Title": "Invariance assumptions for class distribution estimation", "Authors": ["Dirk Tasche"], "Categories": "cs.LG", "Comments": ["16 pages", "presented at workshop Learning to Quantify: Methods and Applications (LQ 2023)", "Torino", "September 18", "2023"], "MSC-class": "68P99, 62B05", "ACM-class": "G.3; I.5.1"}, "abstract": "We study the problem of class distribution estimation under dataset shift. On the training dataset, both features and class labels are observed while on the test dataset only the features can be observed. The task then is the estimation of the distribution of the class labels, i.e. the estimation of the class prior probabilities, in the test dataset. Assumptions of invariance between the training joint distribution of features and labels and the test distribution can considerably facilitate this task. We discuss the assumptions of covariate shift, factorizable joint shift, and sparse joint shift and their implications for class distribution estimation.", "url": "https://arxiv.org/abs/2311.17225"}, {"metadata": {"arXiv": "2311.17250", "Date": "Tue, 28 Nov 2023 22:11:15 ", "Title": "Fourier Neural Differential Equations for learning Quantum Field Theories", "Authors": ["Isaac Brant", "Alexander Norcliffe and Pietro Li\\`o"], "Categories": "cs.LG hep-ph quant-ph", "Comments": ["9 pages", "6 figures"]}, "abstract": "A Quantum Field Theory is defined by its interaction Hamiltonian, and linked to experimental data by the scattering matrix. The scattering matrix is calculated as a perturbative series, and represented succinctly as a first order differential equation in time. Neural Differential Equations (NDEs) learn the time derivative of a residual network's hidden state, and have proven efficacy in learning differential equations with physical constraints. Hence using an NDE to learn particle scattering matrices presents a possible experiment-theory phenomenological connection. In this paper, NDE models are used to learn $\\phi^4$ theory, Scalar-Yukawa theory and Scalar Quantum Electrodynamics. A new NDE architecture is also introduced, the Fourier Neural Differential Equation (FNDE), which combines NDE integration and Fourier network convolution. The FNDE model demonstrates better generalisability than the non-integrated equivalent FNO model. It is also shown that by training on scattering data, the interaction Hamiltonian of a theory can be extracted from network parameters.", "url": "https://arxiv.org/abs/2311.17250"}, {"metadata": {"arXiv": "2311.17259", "Date": "Tue, 28 Nov 2023 22:48:00 ", "Title": "SoUnD Framework: Analyzing (So)cial Representation in (Un)structured (D)ata", "Authors": ["Mark D\\'iaz", "Sunipa Dev", "Emily Reif", "Remi Denton", "Vinodkumar Prabhakaran"], "Categories": "cs.LG cs.CY"}, "abstract": "The unstructured nature of data used in foundation model development is a challenge to systematic analyses for making data use and documentation decisions. From a Responsible AI perspective, these decisions often rely upon understanding how people are represented in data. We propose a framework designed to guide analysis of human representation in unstructured data and identify downstream risks. We apply the framework in two toy examples using the Common Crawl web text corpus (C4) and LAION-400M. We also propose a set of hypothetical action steps in service of dataset use, development, and documentation.", "url": "https://arxiv.org/abs/2311.17259"}, {"metadata": {"arXiv": "2311.17277", "Date": "Tue, 28 Nov 2023 23:33:16 ", "Title": "An Online Optimization-Based Decision Support Tool for Small Farmers in India: Learning in Non-stationary Environments", "Authors": ["Tuxun Lu", "Aviva Prins"], "Categories": "cs.LG cs.CY"}, "abstract": "Crop management decision support systems are specialized tools for farmers that reduce the riskiness of revenue streams, especially valuable for use under the current climate changes that impact agricultural productivity. Unfortunately, small farmers in India, who could greatly benefit from these tools, do not have access to them. In this paper, we model an individual greenhouse as a Markov Decision Process (MDP) and adapt Li and Li (2019)'s Follow the Weighted Leader (FWL) online learning algorithm to offer crop planning advice. We successfully produce utility-preserving cropping pattern suggestions in simulations. When we compare against an offline planning algorithm, we achieve the same cumulative revenue with greatly reduced runtime.", "url": "https://arxiv.org/abs/2311.17277"}, {"metadata": {"arXiv": "2311.17279", "Date": "Tue, 28 Nov 2023 23:38:42 ", "Title": "LiveTune: Dynamic Parameter Tuning for Training Deep Neural Networks", "Authors": ["Soheil Zibakhsh Shabgahi", "Nojan Sheybani", "Aiden Tabrizi", "Farinaz Koushanfar"], "Categories": "cs.LG cs.PF"}, "abstract": "Traditional machine learning training is a static process that lacks real-time adaptability of hyperparameters. Popular tuning solutions during runtime involve checkpoints and schedulers. Adjusting hyper-parameters usually require the program to be restarted, wasting utilization and time, while placing unnecessary strain on memory and processors. We present LiveTune, a new framework allowing real-time parameter tuning during training through LiveVariables. Live Variables allow for a continuous training session by storing parameters on designated ports on the system, allowing them to be dynamically adjusted. Extensive evaluations of our framework show saving up to 60 seconds and 5.4 Kilojoules of energy per hyperparameter change.", "url": "https://arxiv.org/abs/2311.17279"}, {"metadata": {"arXiv": "2311.17287", "Date": "Wed, 29 Nov 2023 00:14:30 ", "Title": "Utilizing Model Residuals to Identify Rental Properties of Interest: The Price Anomaly Score (PAS) and Its Application to Real-time Data in Manhattan", "Authors": ["Youssef Sultan", "Jackson C. Rafter", "Huyen T. Nguyen"], "Categories": "cs.LG stat.AP", "Comments": ["8 pages", "8 figures", "dataset is available with DOI"]}, "abstract": "Understanding whether a property is priced fairly hinders buyers and sellers since they usually do not have an objective viewpoint of the price distribution for the overall market of their interest. Drawing from data collected of all possible available properties for rent in Manhattan as of September 2023, this paper aims to strengthen our understanding of model residuals; specifically on machine learning models which generalize for a majority of the distribution of a well-proportioned dataset. Most models generally perceive deviations from predicted values as mere inaccuracies, however this paper proposes a different vantage point: when generalizing to at least 75\\% of the data-set, the remaining deviations reveal significant insights. To harness these insights, we introduce the Price Anomaly Score (PAS), a metric capable of capturing boundaries between irregularly predicted prices. By combining relative pricing discrepancies with statistical significance, the Price Anomaly Score (PAS) offers a multifaceted view of rental valuations. This metric allows experts to identify overpriced or underpriced properties within a dataset by aggregating PAS values, then fine-tuning upper and lower boundaries to any threshold to set indicators of choice.", "url": "https://arxiv.org/abs/2311.17287"}, {"metadata": {"arXiv": "2311.17299", "Date": "Wed, 29 Nov 2023 01:10:39 ", "Title": "Federated Fine-Tuning of Foundation Models via Probabilistic Masking", "Authors": ["Vasileios Tsouvalas", "Yuki Asano", "Aaqib Saeed"], "Categories": "cs.LG cs.CV cs.DC", "Comments": ["19 pages", "9 figures"]}, "abstract": "Foundation Models (FMs) have revolutionized machine learning with their adaptability and high performance across tasks; yet, their integration into Federated Learning (FL) is challenging due to substantial communication overhead from their extensive parameterization. Current communication-efficient FL strategies, such as gradient compression, reduce bitrates to around $1$ bit-per-parameter (bpp). However, these approaches fail to harness the characteristics of FMs, with their large number of parameters still posing a challenge to communication efficiency, even at these bitrate regimes. In this work, we present DeltaMask, a novel method that efficiently fine-tunes FMs in FL at an ultra-low bitrate, well below 1 bpp. DeltaMask employs stochastic masking to detect highly effective subnetworks within FMs and leverage stochasticity and sparsity in client masks to compress updates into a compact grayscale image using probabilistic filters, deviating from traditional weight training approaches. Our comprehensive evaluations across various datasets and architectures demonstrate DeltaMask efficiently achieves bitrates as low as 0.09 bpp, enhancing communication efficiency while maintaining FMs performance, as measured on 8 datasets and 5 pre-trained models of various network architectures.", "url": "https://arxiv.org/abs/2311.17299"}, {"metadata": {"arXiv": "2311.17326", "Date": "Wed, 29 Nov 2023 02:53:32 ", "Title": "Mostly Beneficial Clustering: Aggregating Data for Operational Decision Making", "Authors": ["Chengzhang Li", "Zhenkang Peng", "and Ying Rong"], "Categories": "cs.LG stat.AP"}, "abstract": "With increasingly volatile market conditions and rapid product innovations, operational decision-making for large-scale systems entails solving thousands of problems with limited data. Data aggregation is proposed to combine the data across problems to improve the decisions obtained by solving those problems individually. We propose a novel cluster-based shrunken-SAA approach that can exploit the cluster structure among problems when implementing the data aggregation approaches. We prove that, as the number of problems grows, leveraging the known cluster structure among problems yields additional benefits over the data aggregation approaches that neglect such structure. When the cluster structure is unknown, we show that unveiling the cluster structure, even at the cost of a few data points, can be beneficial, especially when the distance between clusters of problems is substantial. Our proposed approach can be extended to general cost functions under mild conditions. When the number of problems gets large, the optimality gap of our proposed approach decreases exponentially in the distance between the clusters. We explore the performance of the proposed approach through the application of managing newsvendor systems via numerical experiments. We investigate the impacts of distance metrics between problem instances on the performance of the cluster-based Shrunken-SAA approach with synthetic data. We further validate our proposed approach with real data and highlight the advantages of cluster-based data aggregation, especially in the small-data large-scale regime, compared to the existing approaches.", "url": "https://arxiv.org/abs/2311.17326"}, {"metadata": {"arXiv": "2311.17327", "Date": "Wed, 29 Nov 2023 02:58:30 ", "Title": "Improving Self-supervised Molecular Representation Learning using Persistent Homology", "Authors": ["Yuankai Luo", "Lei Shi", "Veronika Thost"], "Categories": "cs.LG", "Comments": ["NeurIPS 2023"]}, "abstract": "Self-supervised learning (SSL) has great potential for molecular representation learning given the complexity of molecular graphs, the large amounts of unlabelled data available, the considerable cost of obtaining labels experimentally, and the hence often only small training datasets. The importance of the topic is reflected in the variety of paradigms and architectures that have been investigated recently. Yet the differences in performance seem often minor and are barely understood to date. In this paper, we study SSL based on persistent homology (PH), a mathematical tool for modeling topological features of data that persist across multiple scales. It has several unique features which particularly suit SSL, naturally offering: different views of the data, stability in terms of distance preservation, and the opportunity to flexibly incorporate domain knowledge. We (1) investigate an autoencoder, which shows the general representational power of PH, and (2) propose a contrastive loss that complements existing approaches. We rigorously evaluate our approach for molecular property prediction and demonstrate its particular features in improving the embedding space: after SSL, the representations are better and offer considerably more predictive power than the baselines over different probing tasks; our loss increases baseline performance, sometimes largely; and we often obtain substantial improvements over very small datasets, a common scenario in practice.", "url": "https://arxiv.org/abs/2311.17327"}, {"metadata": {"arXiv": "2311.17352", "Date": "Wed, 29 Nov 2023 04:31:35 ", "Title": "Efficient Stitchable Task Adaptation", "Authors": ["Haoyu He", "Zizheng Pan", "Jing Liu", "Jianfei Cai", "Bohan Zhuang"], "Categories": "cs.LG cs.CL cs.CV", "Comments": ["Source code will be released at https://github.com/ziplab/Stitched_LLaMA"]}, "abstract": "The paradigm of pre-training and fine-tuning has laid the foundation for deploying deep learning models. However, most fine-tuning methods are designed to meet a specific resource budget. Recently, considering diverse deployment scenarios with various resource budgets, stitchable neural network (SN-Net) is introduced to quickly obtain numerous new networks (stitches) from the pre-trained models (anchors) in a model family via model stitching. Although promising, SN-Net confronts new challenges when adapting it to new target domains, including huge memory and storage requirements and a long and sub-optimal multistage adaptation process. In this work, we present a novel framework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce a palette of fine-tuned models that adhere to diverse resource constraints. Specifically, we first tailor parameter-efficient fine-tuning to share low-rank updates among the stitches while maintaining independent bias terms. In this way, we largely reduce fine-tuning memory burdens and mitigate the interference among stitches that arises in task adaptation. Furthermore, we streamline a simple yet effective one-stage deployment pipeline, which estimates the important stitches to deploy with training-time gradient statistics. By assigning higher sampling probabilities to important stitches, we also get a boosted Pareto frontier. Extensive experiments on 25 downstream visual recognition tasks demonstrate that our ESTA is capable of generating stitches with smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net adaptation by remarkable margins with significantly lower training time and fewer trainable parameters. Furthermore, we demonstrate the flexibility and scalability of our ESTA framework by stitching LLMs from LLaMA family, obtaining chatbot stitches of assorted sizes.", "url": "https://arxiv.org/abs/2311.17352"}, {"metadata": {"arXiv": "2311.17373", "Date": "Wed, 29 Nov 2023 05:54:58 ", "Title": "The Devil is in the Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation", "Authors": ["Yuchang Zhu", "Jintang Li", "Liang Chen", "Zibin Zheng"], "Categories": "cs.LG cs.CY cs.SI", "Comments": ["Accepted by WSDM 2024"]}, "abstract": "Graph neural networks (GNNs) are being increasingly used in many high-stakes tasks, and as a result, there is growing attention on their fairness recently. GNNs have been shown to be unfair as they tend to make discriminatory decisions toward certain demographic groups, divided by sensitive attributes such as gender and race. While recent works have been devoted to improving their fairness performance, they often require accessible demographic information. This greatly limits their applicability in real-world scenarios due to legal restrictions. To address this problem, we present a demographic-agnostic method to learn fair GNNs via knowledge distillation, namely FairGKD. Our work is motivated by the empirical observation that training GNNs on partial data (i.e., only node attributes or topology data) can improve their fairness, albeit at the cost of utility. To make a balanced trade-off between fairness and utility performance, we employ a set of fairness experts (i.e., GNNs trained on different partial data) to construct the synthetic teacher, which distills fairer and informative knowledge to guide the learning of the GNN student. Experiments on several benchmark datasets demonstrate that FairGKD, which does not require access to demographic information, significantly improves the fairness of GNNs by a large margin while maintaining their utility.", "url": "https://arxiv.org/abs/2311.17373"}, {"metadata": {"arXiv": "2311.17508", "Date": "Wed, 29 Nov 2023 10:32:40 ", "Title": "Model Performance Prediction for Hyperparameter Optimization of Deep Learning Models Using High Performance Computing and Quantum Annealing", "Authors": ["Juan Pablo Garc\\'ia Amboage", "Eric Wulff", "Maria Girone", "Tom\\'as F. Pena"], "Categories": "cs.LG physics.data-an"}, "abstract": "Hyperparameter Optimization (HPO) of Deep Learning-based models tends to be a compute resource intensive process as it usually requires to train the target model with many different hyperparameter configurations. We show that integrating model performance prediction with early stopping methods holds great potential to speed up the HPO process of deep learning models. Moreover, we propose a novel algorithm called Swift-Hyperband that can use either classical or quantum support vector regression for performance prediction and benefit from distributed High Performance Computing environments. This algorithm is tested not only for the Machine-Learned Particle Flow model used in High Energy Physics, but also for a wider range of target models from domains such as computer vision and natural language processing. Swift-Hyperband is shown to find comparable (or better) hyperparameters as well as using less computational resources in all test cases.", "url": "https://arxiv.org/abs/2311.17508"}, {"metadata": {"arXiv": "2311.17539", "Date": "Wed, 29 Nov 2023 11:19:50 ", "Title": "The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis", "Authors": ["Sungbin Shin", "Dongyeop Lee", "Maksym Andriushchenko", "Namhoon Lee"], "Categories": "cs.LG math.OC stat.ML"}, "abstract": "Training an overparameterized neural network can yield minimizers of the same level of training loss and yet different generalization capabilities. With evidence that indicates a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions. This sharpness-aware minimization (SAM) strategy, however, has not been studied much yet as to how overparameterization can actually affect its behavior. In this work, we analyze SAM under varying degrees of overparameterization and present both empirical and theoretical results that suggest a critical influence of overparameterization on SAM. Specifically, we first use standard techniques in optimization to prove that SAM can achieve a linear convergence rate under overparameterization in a stochastic setting. We also show that the linearly stable minima found by SAM are indeed flatter and have more uniformly distributed Hessian moments compared to those of SGD. These results are corroborated with our experiments that reveal a consistent trend that the generalization improvement made by SAM continues to increase as the model becomes more overparameterized. We further present that sparsity can open up an avenue for effective overparameterization in practice.", "url": "https://arxiv.org/abs/2311.17539"}, {"metadata": {"arXiv": "2311.17560", "Date": "Wed, 29 Nov 2023 11:48:16 ", "Title": "Interpreting Differentiable Latent States for Healthcare Time-series Data", "Authors": ["Yu Chen", "Nivedita Bijlani", "Samaneh Kouchaki", "Payam Barnaghi"], "Categories": "cs.LG", "Journal-ref": "Workshop on Interpretable ML in Healthcare at International Con- ference on Machine Learning (ICML), Honolulu, Hawaii, USA. 2023"}, "abstract": "Machine learning enables extracting clinical insights from large temporal datasets. The applications of such machine learning models include identifying disease patterns and predicting patient outcomes. However, limited interpretability poses challenges for deploying advanced machine learning in digital healthcare. Understanding the meaning of latent states is crucial for interpreting machine learning models, assuming they capture underlying patterns. In this paper, we present a concise algorithm that allows for i) interpreting latent states using highly related input features; ii) interpreting predictions using subsets of input features via latent states; and iii) interpreting changes in latent states over time. The proposed algorithm is feasible for any model that is differentiable. We demonstrate that this approach enables the identification of a daytime behavioral pattern for predicting nocturnal behavior in a real-world healthcare dataset.", "url": "https://arxiv.org/abs/2311.17560"}, {"metadata": {"arXiv": "2311.17582", "Date": "Wed, 29 Nov 2023 12:18:46 ", "Title": "LoCoMotif: Discovering time-warped motifs in time series", "Authors": ["Daan Van Wesenbeeck", "Aras Yurtman", "Wannes Meert", "Hendrik Blockeel"], "Categories": "cs.LG", "Comments": ["26 pages", "15 figures. Submitted to the journal track of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD) 2024 in partnership with the Data Mining and Knowledge Discovery journal. Source code of the method is available at http://github.com/ML-KULeuven/locomotif"]}, "abstract": "Time Series Motif Discovery (TSMD) refers to the task of identifying patterns that occur multiple times (possibly with minor variations) in a time series. All existing methods for TSMD have one or more of the following limitations: they only look for the two most similar occurrences of a pattern; they only look for patterns of a pre-specified, fixed length; they cannot handle variability along the time axis; and they only handle univariate time series. In this paper, we present a new method, LoCoMotif, that has none of these limitations. The method is motivated by a concrete use case from physiotherapy. We demonstrate the value of the proposed method on this use case. We also introduce a new quantitative evaluation metric for motif discovery, and benchmark data for comparing TSMD methods. LoCoMotif substantially outperforms the existing methods, on top of being more broadly applicable.", "url": "https://arxiv.org/abs/2311.17582"}, {"metadata": {"arXiv": "2311.17586", "Date": "Wed, 29 Nov 2023 12:29:54 ", "Title": "Federated Online and Bandit Convex Optimization", "Authors": ["Kumar Kshitij Patel", "Lingxiao Wang", "Aadirupa Saha", "Nati Sebro"], "Categories": "cs.LG math.OC stat.ML"}, "abstract": "We study the problems of distributed online and bandit convex optimization against an adaptive adversary. We aim to minimize the average regret on $M$ machines working in parallel over $T$ rounds with $R$ intermittent communications. Assuming the underlying cost functions are convex and can be generated adaptively, our results show that collaboration is not beneficial when the machines have access to the first-order gradient information at the queried points. This is in contrast to the case for stochastic functions, where each machine samples the cost functions from a fixed distribution. Furthermore, we delve into the more challenging setting of federated online optimization with bandit (zeroth-order) feedback, where the machines can only access values of the cost functions at the queried points. The key finding here is identifying the high-dimensional regime where collaboration is beneficial and may even lead to a linear speedup in the number of machines. We further illustrate our findings through federated adversarial linear bandits by developing novel distributed single and two-point feedback algorithms. Our work is the first attempt towards a systematic understanding of federated online optimization with limited feedback, and it attains tight regret bounds in the intermittent communication setting for both first and zeroth-order feedback. Our results thus bridge the gap between stochastic and adaptive settings in federated online optimization.", "url": "https://arxiv.org/abs/2311.17586"}, {"metadata": {"arXiv": "2311.17795", "Date": "Wed, 29 Nov 2023 16:45:43 ", "Title": "Marginal Laplacian Score", "Authors": ["Guy Hay and Ohad Volk"], "Categories": "cs.LG stat.ML", "Comments": ["10 pages"], "ACM-class": "I.5.0"}, "abstract": "High-dimensional imbalanced data poses a machine learning challenge. In the absence of sufficient or high-quality labels, unsupervised feature selection methods are crucial for the success of subsequent algorithms. Therefore, there is a growing need for unsupervised feature selection algorithms focused on imbalanced data. Thus, we propose a Marginal Laplacian Score (MLS) a modification of the well-known Laplacian Score (LS) to be better suited for imbalance data. We introduce an assumption that the minority class or anomalous appear more frequently in the margin of the features. Consequently, MLS aims to preserve the local structure of the data set's margin. As MLS is better suited for handling imbalanced data, we propose its integration into modern feature selection methods that utilize the Laplacian score. We integrate the MLS algorithm into the Differentiable Unsupervised Feature Selection (DUFS), resulting in DUFS-MLS. The proposed methods demonstrate robust and improved performance on synthetic and public data sets.", "url": "https://arxiv.org/abs/2311.17795"}, {"metadata": {"arXiv": "2311.17797", "Date": "Wed, 29 Nov 2023 16:46:24 ", "Title": "Learning to Simulate: Generative Metamodeling via Quantile Regression", "Authors": ["L. Jeff Hong and Yanxi Hou and Qingkai Zhang and Xiaowei Zhang"], "Categories": "cs.LG stat.ME", "Comments": ["Main body: 36 pages", "7 figures; supplemental material: 12 pages"]}, "abstract": "Stochastic simulation models, while effective in capturing the dynamics of complex systems, are often too slow to run for real-time decision-making. Metamodeling techniques are widely used to learn the relationship between a summary statistic of the outputs (e.g., the mean or quantile) and the inputs of the simulator, so that it can be used in real time. However, this methodology requires the knowledge of an appropriate summary statistic in advance, making it inflexible for many practical situations. In this paper, we propose a new metamodeling concept, called generative metamodeling, which aims to construct a \"fast simulator of the simulator\". This technique can generate random outputs substantially faster than the original simulation model, while retaining an approximately equal conditional distribution given the same inputs. Once constructed, a generative metamodel can instantaneously generate a large amount of random outputs as soon as the inputs are specified, thereby facilitating the immediate computation of any summary statistic for real-time decision-making. Furthermore, we propose a new algorithm -- quantile-regression-based generative metamodeling (QRGMM) -- and study its convergence and rate of convergence. Extensive numerical experiments are conducted to investigate the empirical performance of QRGMM, compare it with other state-of-the-art generative algorithms, and demonstrate its usefulness in practical real-time decision-making.", "url": "https://arxiv.org/abs/2311.17797"}, {"metadata": {"arXiv": "2311.17853", "Date": "Wed, 29 Nov 2023 17:59:18 ", "Title": "On the Adversarial Robustness of Graph Contrastive Learning Methods", "Authors": ["Filippo Guerranti", "Zinuo Yi", "Anna Starovoit", "Rafiq Kamel", "Simon Geisler", "Stephan G\\\"unnemann"], "Categories": "cs.LG", "Comments": ["Accepted at NeurIPS 2023 New Frontiers in Graph Learning Workshop (NeurIPS GLFrontiers 2023)"]}, "abstract": "Contrastive learning (CL) has emerged as a powerful framework for learning representations of images and text in a self-supervised manner while enhancing model robustness against adversarial attacks. More recently, researchers have extended the principles of contrastive learning to graph-structured data, giving birth to the field of graph contrastive learning (GCL). However, whether GCL methods can deliver the same advantages in adversarial robustness as their counterparts in the image and text domains remains an open question. In this paper, we introduce a comprehensive robustness evaluation protocol tailored to assess the robustness of GCL models. We subject these models to adaptive adversarial attacks targeting the graph structure, specifically in the evasion scenario. We evaluate node and graph classification tasks using diverse real-world datasets and attack strategies. With our work, we aim to offer insights into the robustness of GCL methods and hope to open avenues for potential future research directions.", "url": "https://arxiv.org/abs/2311.17853"}, {"metadata": {"arXiv": "2311.17856", "Date": "Wed, 29 Nov 2023 18:02:29 ", "Title": "Leveraging Graph Diffusion Models for Network Refinement Tasks", "Authors": ["Puja Trivedi", "Ryan Rossi", "David Arbour", "Tong Yu", "Franck Dernoncourt", "Sungchul Kim", "Nedim Lipka", "Namyong Park", "Nesreen K. Ahmed", "Danai Koutra"], "Categories": "cs.LG cs.SI", "Comments": ["Work in Progress. 21 pages", "7 figures"]}, "abstract": "Most real-world networks are noisy and incomplete samples from an unknown target distribution. Refining them by correcting corruptions or inferring unobserved regions typically improves downstream performance. Inspired by the impressive generative capabilities that have been used to correct corruptions in images, and the similarities between \"in-painting\" and filling in missing nodes and edges conditioned on the observed graph, we propose a novel graph generative framework, SGDM, which is based on subgraph diffusion. Our framework not only improves the scalability and fidelity of graph diffusion models, but also leverages the reverse process to perform novel, conditional generation tasks. In particular, through extensive empirical analysis and a set of novel metrics, we demonstrate that our proposed model effectively supports the following refinement tasks for partially observable networks: T1: denoising extraneous subgraphs, T2: expanding existing subgraphs and T3: performing \"style\" transfer by regenerating a particular subgraph to match the characteristics of a different node or subgraph.", "url": "https://arxiv.org/abs/2311.17856"}, {"metadata": {"arXiv": "2311.17869", "Date": "Wed, 29 Nov 2023 18:17:35 ", "Title": "SAIBench: A Structural Interpretation of AI for Science Through Benchmarks", "Authors": ["Yatao Li", "Jianfeng Zhan"], "Categories": "cs.LG"}, "abstract": "Artificial Intelligence for Science (AI4S) is an emerging research field that utilizes machine learning advancements to tackle complex scientific computational issues, aiming to enhance computational efficiency and accuracy. However, the data-driven nature of AI4S lacks the correctness or accuracy assurances of conventional scientific computing, posing challenges when deploying AI4S models in real-world applications. To mitigate these, more comprehensive benchmarking procedures are needed to better understand AI4S models. This paper introduces a novel benchmarking approach, known as structural interpretation, which addresses two key requirements: identifying the trusted operating range in the problem space and tracing errors back to their computational components. This method partitions both the problem and metric spaces, facilitating a structural exploration of these spaces. The practical utility and effectiveness of structural interpretation are illustrated through its application to three distinct AI4S workloads: machine-learning force fields (MLFF), jet tagging, and precipitation nowcasting. The benchmarks effectively model the trusted operating range, trace errors, and reveal novel perspectives for refining the model, training process, and data sampling strategy. This work is part of the SAIBench project, an AI4S benchmarking suite.", "url": "https://arxiv.org/abs/2311.17869"}, {"metadata": {"arXiv": "2311.17693", "Date": "Wed, 29 Nov 2023 15:00:06 ", "Title": "Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning", "Authors": ["Amr Gomaa and Bilal Mahdy and Niko Kleer and Antonio Kr\\\"uger"], "Categories": "cs.RO cs.CV cs.HC cs.LG"}, "abstract": "Robotic-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems lack the ability to accommodate the unique preferences and requirements of individual surgeons. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are not suitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose a simulation-based image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon's skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach utilizes a simulated environment to train reinforcement and imitation learning agents guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon's actions and preferences into the training process with the surgeon-in-the-loop, our approach enables the robot to implicitly learn and adapt to the individual surgeon's unique approach through demonstrations. This results in a more intuitive and personalized surgical experience for the surgeon. Simultaneously, it ensures consistent performance for the autonomous robotic apprentice. We define and evaluate the effectiveness of our approach using our proposed metrics; and highlight the trade-off between a generic agent and a surgeon-centered adapted agent. Moreover, our approach has the potential to extend to other ophthalmic surgical procedures, opening the door to a new generation of surgeon-in-the-loop autonomous surgical robots. We provide an open-source simulation framework for future development and reproducibility.", "url": "https://arxiv.org/abs/2311.17693"}, {"metadata": {"arXiv": "2311.17631", "Date": "Wed, 29 Nov 2023 13:45:07 ", "Title": "Q-learning Based Optimal False Data Injection Attack on Probabilistic Boolean Control Networks", "Authors": ["Xianlun Peng", "Yang Tang", "Fangfei Li and Yang Liu"], "Categories": "eess.SY cs.CR cs.LG cs.SY"}, "abstract": "In this paper, we present a reinforcement learning (RL) method for solving optimal false data injection attack problems in probabilistic Boolean control networks (PBCNs) where the attacker lacks knowledge of the system model. Specifically, we employ a Q-learning (QL) algorithm to address this problem. We then propose an improved QL algorithm that not only enhances learning efficiency but also obtains optimal attack strategies for large-scale PBCNs that the standard QL algorithm cannot handle. Finally, we verify the effectiveness of our proposed approach by considering two attacked PBCNs, including a 10-node network and a 28-node network.", "url": "https://arxiv.org/abs/2311.17631"}, {"metadata": {"arXiv": "2311.17227", "Date": "Tue, 28 Nov 2023 20:59:49 ", "Title": "War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars", "Authors": ["Wenyue Hua", "Lizhou Fan", "Lingyao Li", "Kai Mei", "Jianchao Ji", "Yingqiang Ge", "Libby Hemphill", "Yongfeng Zhang"], "Categories": "cs.AI cs.CL cs.CY", "Comments": ["40 pages", "7 figures"]}, "abstract": "Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at \\url{https://github.com/agiresearch/WarAgent}.", "url": "https://arxiv.org/abs/2311.17227"}, {"metadata": {"arXiv": "2311.17305", "Date": "Wed, 29 Nov 2023 01:31:21 ", "Title": "Two-Step Reinforcement Learning for Multistage Strategy Card Game", "Authors": ["Konrad Godlewski", "Bartosz Sawicki"], "Categories": "cs.AI cs.GT"}, "abstract": "In the realm of artificial intelligence and card games, this study introduces a two-step reinforcement learning (RL) strategy tailored for \"The Lord of the Rings: The Card Game (LOTRCG),\" a complex multistage strategy card game. This research diverges from conventional RL methods by adopting a phased learning approach, beginning with a foundational learning stage in a simplified version of the game and subsequently progressing to the complete, intricate game environment. This methodology notably enhances the AI agent's adaptability and performance in the face of LOTRCG's unpredictable and challenging nature. The paper also explores a multi-agent system, where distinct RL agents are employed for various decision-making aspects of the game. This approach has demonstrated a remarkable improvement in game outcomes, with the RL agents achieving a winrate of 78.5% across a set of 10,000 random games.", "url": "https://arxiv.org/abs/2311.17305"}, {"metadata": {"arXiv": "2311.17351", "Date": "Wed, 29 Nov 2023 04:25:15 ", "Title": "Exploring Large Language Models for Human Mobility Prediction under Public Events", "Authors": ["Yuebing Liang", "Yichao Liu", "Xiaohan Wang", "Zhan Zhao"], "Categories": "cs.AI cs.CL"}, "abstract": "Public events, such as concerts and sports games, can be major attractors for large crowds, leading to irregular surges in travel demand. Accurate human mobility prediction for public events is thus crucial for event planning as well as traffic or crowd management. While rich textual descriptions about public events are commonly available from online sources, it is challenging to encode such information in statistical or machine learning models. Existing methods are generally limited in incorporating textual information, handling data sparsity, or providing rationales for their predictions. To address these challenges, we introduce a framework for human mobility prediction under public events (LLM-MPE) based on Large Language Models (LLMs), leveraging their unprecedented ability to process textual data, learn from minimal examples, and generate human-readable explanations. Specifically, LLM-MPE first transforms raw, unstructured event descriptions from online sources into a standardized format, and then segments historical mobility data into regular and event-related components. A prompting strategy is designed to direct LLMs in making and rationalizing demand predictions considering historical mobility and event features. A case study is conducted for Barclays Center in New York City, based on publicly available event information and taxi trip data. Results show that LLM-MPE surpasses traditional models, particularly on event days, with textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers interpretable insights into its predictions. Despite the great potential of LLMs, we also identify key challenges including misinformation and high costs that remain barriers to their broader adoption in large-scale human mobility analysis.", "url": "https://arxiv.org/abs/2311.17351"}, {"metadata": {"arXiv": "2311.17393", "Date": "Wed, 29 Nov 2023 06:45:07 ", "Title": "Comparison of metaheuristics for the firebreak placement problem: a simulation-based optimization approach", "Authors": ["David Palacios-Meneses", "Jaime Carrasco", "Sebasti\\'an D\\'avila", "Maximiliano Mart\\'inez", "Rodrigo Mahaluf", "and Andr\\'es Weintraub"], "Categories": "cs.AI"}, "abstract": "The problem of firebreak placement is crucial for fire prevention, and its effectiveness at landscape scale will depend on their ability to impede the progress of future wildfires. To provide an adequate response, it is therefore necessary to consider the stochastic nature of fires, which are highly unpredictable from ignition to extinction. Thus, the placement of firebreaks can be considered a stochastic optimization problem where: (1) the objective function is to minimize the expected cells burnt of the landscape; (2) the decision variables being the location of firebreaks; and (3) the random variable being the spatial propagation/behavior of fires. In this paper, we propose a solution approach for the problem from the perspective of simulation-based optimization (SbO), where the objective function is not available (a black-box function), but can be computed (and/or approximated) by wildfire simulations. For this purpose, Genetic Algorithm and GRASP are implemented. The final implementation yielded favorable results for the Genetic Algorithm, demonstrating strong performance in scenarios with medium to high operational capacity, as well as medium levels of stochasticity", "url": "https://arxiv.org/abs/2311.17393"}, {"metadata": {"arXiv": "2311.17453", "Date": "Wed, 29 Nov 2023 08:51:40 ", "Title": "Privacy Measurement in Tabular Synthetic Data: State of the Art and Future Research Directions", "Authors": ["Alexander Boudewijn", "Andrea Filippo Ferraris", "Daniele Panfilo", "Vanessa Cocca", "Sabrina Zinutti", "Karel De Schepper", "Carlo Rossi Chauvenet"], "Categories": "cs.AI cs.CR cs.DB stat.ML", "Comments": ["20 pages", "4 tables", "8 figures; NeurIPS 2023 Workshop on Synthetic Data Generation with Generative AI"], "Journal-ref": "NeurIPS 2023 Workshop on Synthetic Data Generation with Generative AI"}, "abstract": "Synthetic data (SD) have garnered attention as a privacy enhancing technology. Unfortunately, there is no standard for quantifying their degree of privacy protection. In this paper, we discuss proposed quantification approaches. This contributes to the development of SD privacy standards; stimulates multi-disciplinary discussion; and helps SD researchers make informed modeling and evaluation decisions.", "url": "https://arxiv.org/abs/2311.17453"}, {"metadata": {"arXiv": "2311.17471", "Date": "Wed, 29 Nov 2023 09:28:33 ", "Title": "Distributed AI in Zero-touch Provisioning for Edge Networks: Challenges and Research Directions", "Authors": ["Abhishek Hazra", "Andrea Morichetta", "Ilir Murturi", "Lauri Lov\\'en", "Chinmaya Kumar Dehury", "Victor Casamayor Pujol", "Praveen Kumar Donta", "Schahram Dustdar"], "Categories": "cs.AI cs.NI"}, "abstract": "Zero-touch network is anticipated to inaugurate the generation of intelligent and highly flexible resource provisioning strategies where multiple service providers collaboratively offer computation and storage resources. This transformation presents substantial challenges to network administration and service providers regarding sustainability and scalability. This article combines Distributed Artificial Intelligence (DAI) with Zero-touch Provisioning (ZTP) for edge networks. This combination helps to manage network devices seamlessly and intelligently by minimizing human intervention. In addition, several advantages are also highlighted that come with incorporating Distributed AI into ZTP in the context of edge networks. Further, we draw potential research directions to foster novel studies in this field and overcome the current limitations.", "url": "https://arxiv.org/abs/2311.17471"}, {"metadata": {"arXiv": "2311.17541", "Date": "Wed, 29 Nov 2023 11:23:42 ", "Title": "TaskWeaver: A Code-First Agent Framework", "Authors": ["Bo Qiao", "Liqun Li", "Xu Zhang", "Shilin He", "Yu Kang", "Chaoyun Zhang", "Fangkai Yang", "Hang Dong", "Jue Zhang", "Lu Wang", "Minghua Ma", "Pu Zhao", "Si Qin", "Xiaoting Qin", "Chao Du", "Yong Xu", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "Categories": "cs.AI"}, "abstract": "Language Language Models (LLMs) have shown impressive abilities in natural language understanding and generation, leading to their use in applications such as chatbots and virtual assistants. However, existing LLM frameworks face limitations in handling domain-specific data analytics tasks with rich data structures. Moreover, they struggle with flexibility to meet diverse user requirements. To address these issues, TaskWeaver is proposed as a code-first framework for building LLM-powered autonomous agents. It converts user requests into executable code and treats user-defined plugins as callable functions. TaskWeaver provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, and leverages LLM coding capabilities for complex logic. It also incorporates domain-specific knowledge through examples and ensures the secure execution of generated code. TaskWeaver offers a powerful and flexible framework for creating intelligent conversational agents that can handle complex tasks and adapt to domain-specific scenarios. The code is open-sourced at https://github.com/microsoft/TaskWeaver/.", "url": "https://arxiv.org/abs/2311.17541"}, {"metadata": {"arXiv": "2311.17766", "Date": "Wed, 29 Nov 2023 16:06:17 ", "Title": "Robustness Approaches for the Examination Timetabling Problem under Data Uncertainty", "Authors": ["Bernd Bassimir", "Rolf Wanka"], "Categories": "cs.AI", "Comments": ["original paper: 15 pages", "published at the Multidisciplinary International Scheduling Conference 2019 (MISTA 2019)"]}, "abstract": "In the literature the examination timetabling problem (ETTP) is often considered a post-enrollment problem (PE-ETTP). In the real world, universities often schedule their exams before students register using information from previous terms. A direct consequence of this approach is the uncertainty present in the resulting models. In this work we discuss several approaches available in the robust optimization literature. We consider the implications of each approach in respect to the examination timetabling problem and present how the most favorable approaches can be applied to the ETTP. Afterwards we analyze the impact of some possible implementations of the given robustness approaches on two real world instances and several random instances generated by our instance generation framework which we introduce in this work.", "url": "https://arxiv.org/abs/2311.17766"}, {"metadata": {"arXiv": "2311.17822", "Date": "Wed, 29 Nov 2023 17:22:28 ", "Title": "Anomalous Behavior Detection in Trajectory Data of Older Drivers", "Authors": ["Seyedeh Gol Ara Ghoreishi", "Sonia Moshfeghi", "Muhammad Tanveer Jan", "Joshua Conniff", "KwangSoo Yang", "Jinwoo Jang", "Borko Furht", "Ruth Tappen", "David Newman", "Monica Rosselli", "Jiannan Zhai"], "Categories": "cs.AI", "Comments": ["IEEE HONET 2023"]}, "abstract": "Given a road network and a set of trajectory data, the anomalous behavior detection (ABD) problem is to identify drivers that show significant directional deviations, hardbrakings, and accelerations in their trips. The ABD problem is important in many societal applications, including Mild Cognitive Impairment (MCI) detection and safe route recommendations for older drivers. The ABD problem is computationally challenging due to the large size of temporally-detailed trajectories dataset. In this paper, we propose an Edge-Attributed Matrix that can represent the key properties of temporally-detailed trajectory datasets and identify abnormal driving behaviors. Experiments using real-world datasets demonstrated that our approach identifies abnormal driving behaviors.", "url": "https://arxiv.org/abs/2311.17822"}, {"metadata": {"arXiv": "2311.17080", "Date": "Tue, 28 Nov 2023 00:00:34 ", "Title": "Combating the \"Sameness\" in AI Art: Reflections on the Interactive AI Installation Fencing Hallucination", "Authors": ["Weihao Qiu", "George Legrady"], "Categories": "cs.CV cs.AI", "Comments": ["Paper for NeurIPS 2023 Workshop", "Machine Learning for Creativity and Design"]}, "abstract": "The article summarizes three types of \"sameness\" issues in Artificial Intelligence(AI) art, each occurring at different stages of development in AI image creation tools. Through the Fencing Hallucination project, the article reflects on the design of AI art production in alleviating the sense of uniformity, maintaining the uniqueness of images from an AI image synthesizer, and enhancing the connection between the artworks and the audience. This paper endeavors to stimulate the creation of distinctive AI art by recounting the efforts and insights derived from the Fencing Hallucination project, all dedicated to addressing the issue of \"sameness\".", "url": "https://arxiv.org/abs/2311.17080"}, {"metadata": {"arXiv": "2311.17095", "Date": "Tue, 28 Nov 2023 06:42:58 ", "Title": "Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models", "Authors": ["Luo Jiayun", "Siddhesh Khandelwal", "Leonid Sigal", "Boyang Li"], "Categories": "cs.CV cs.AI"}, "abstract": "From an enormous amount of image-text pairs, large-scale vision-language models (VLMs) learn to implicitly associate image regions with words, which is vital for tasks such as image captioning and visual question answering. However, leveraging such pre-trained models for open-vocabulary semantic segmentation remains a challenge. In this paper, we propose a simple, yet extremely effective, training-free technique, Plug-and-Play Open-Vocabulary Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with direct text-to-image cross-attention and an image-text matching loss to produce semantic segmentation. However, cross-attention alone tends to over-segment, whereas cross-attention plus GradCAM tend to under-segment. To alleviate this issue, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask. Compared to existing techniques, the proposed method does not require any neural network training and performs hyperparameter tuning without the need for any segmentation annotations, even for a validation set. PnP-OVSS demonstrates substantial improvements over a comparable baseline (+29.4% mIoU on Pascal VOC, +13.2% mIoU on Pascal Context, +14.0% mIoU on MS COCO, +2.4% mIoU on COCO Stuff) and even outperforms most baselines that conduct additional network training on top of pretrained VLMs.", "url": "https://arxiv.org/abs/2311.17095"}, {"metadata": {"arXiv": "2311.17098", "Date": "Tue, 28 Nov 2023 07:52:41 ", "Title": "DyRA: Dynamic Resolution Adjustment for Scale-robust Object Detection", "Authors": ["Daeun Seo", "Hoeseok Yang", "Hyungshin Kim"], "Categories": "cs.CV cs.AI"}, "abstract": "In object detection, achieving constant accuracy is challenging due to the variability of object sizes. One possible solution to this problem is to optimize the input resolution, known as a multi-resolution strategy. Previous approaches for optimizing resolution are often based on pre-defined resolutions or a dynamic neural network, but there is a lack of study for run-time resolution optimization for existing architecture. In this paper, we propose an adaptive resolution scaling network called DyRA, which comprises convolutions and transformer encoder blocks, for existing detectors. Our DyRA returns a scale factor from an input image, which enables instance-specific scaling. This network is jointly trained with detectors with specially designed loss functions, namely ParetoScaleLoss and BalanceLoss. The ParetoScaleLoss produces an adaptive scale factor from the image, while the BalanceLoss optimizes the scale factor according to localization power for the dataset. The loss function is designed to minimize accuracy drop about the contrasting objective of small and large objects. Our experiments on COCO, RetinaNet, Faster-RCNN, FCOS, and Mask-RCNN achieved 1.3%, 1.1%, 1.3%, and 0.8% accuracy improvement than a multi-resolution baseline with solely resolution adjustment. The code is available at https://github.com/DaEunFullGrace/DyRA.git.", "url": "https://arxiv.org/abs/2311.17098"}, {"metadata": {"arXiv": "2311.17099", "Date": "Tue, 28 Nov 2023 07:53:51 ", "Title": "StreamFlow: Streamlined Multi-Frame Optical Flow Estimation for Video Sequences", "Authors": ["Shangkun Sun", "Jiaming Liu", "Thomas H. Li", "Huaxia Li", "Guoqing Liu", "Wei Gao"], "Categories": "cs.CV cs.AI"}, "abstract": "Occlusions between consecutive frames have long posed a significant challenge in optical flow estimation. The inherent ambiguity introduced by occlusions directly violates the brightness constancy constraint and considerably hinders pixel-to-pixel matching. To address this issue, multi-frame optical flow methods leverage adjacent frames to mitigate the local ambiguity. Nevertheless, prior multi-frame methods predominantly adopt recursive flow estimation, resulting in a considerable computational overlap. In contrast, we propose a streamlined in-batch framework that eliminates the need for extensive redundant recursive computations while concurrently developing effective spatio-temporal modeling approaches under in-batch estimation constraints. Specifically, we present a Streamlined In-batch Multi-frame (SIM) pipeline tailored to video input, attaining a similar level of time efficiency to two-frame networks. Furthermore, we introduce an efficient Integrative Spatio-temporal Coherence (ISC) modeling method for effective spatio-temporal modeling during the encoding phase, which introduces no additional parameter overhead. Additionally, we devise a Global Temporal Regressor (GTR) that effectively explores temporal relations during decoding. Benefiting from the efficient SIM pipeline and effective modules, StreamFlow not only excels in terms of performance on the challenging KITTI and Sintel datasets, with particular improvement in occluded areas but also attains a remarkable $63.82\\%$ enhancement in speed compared with previous multi-frame methods. The code will be available soon at https://github.com/littlespray/StreamFlow.", "url": "https://arxiv.org/abs/2311.17099"}, {"metadata": {"arXiv": "2311.17123", "Date": "Tue, 28 Nov 2023 13:55:53 ", "Title": "ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis", "Authors": ["Xiangjun Gao", "Xiaoyu Li", "Chaopeng Zhang", "Qi Zhang", "Yanpei Cao", "Ying Shan", "Long Quan"], "Categories": "cs.CV cs.AI", "Comments": ["see project page: https://gaoxiangjun.github.io/contex_human/"]}, "abstract": "In this work, we propose a method to address the challenge of rendering a 3D human from a single image in a free-view manner. Some existing approaches could achieve this by using generalizable pixel-aligned implicit fields to reconstruct a textured mesh of a human or by employing a 2D diffusion model as guidance with the Score Distillation Sampling (SDS) method, to lift the 2D image into 3D space. However, a generalizable implicit field often results in an over-smooth texture field, while the SDS method tends to lead to a texture-inconsistent novel view with the input image. In this paper, we introduce a texture-consistent back view synthesis module that could transfer the reference image content to the back view through depth and text-guided attention injection. Moreover, to alleviate the color distortion that occurs in the side region, we propose a visibility-aware patch consistency regularization for texture mapping and refinement combined with the synthesized back view texture. With the above techniques, we could achieve high-fidelity and texture-consistent human rendering from a single image. Experiments conducted on both real and synthetic data demonstrate the effectiveness of our method and show that our approach outperforms previous baseline methods.", "url": "https://arxiv.org/abs/2311.17123"}, {"metadata": {"arXiv": "2311.17128", "Date": "Tue, 28 Nov 2023 15:22:23 ", "Title": "Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks", "Authors": ["Lucas Beerens and Desmond J. Higham"], "Categories": "cs.CV cs.AI", "MSC-class": "65F35", "ACM-class": "I.2.10; G.1.3"}, "abstract": "Recent advancements in Optical Character Recognition (OCR) have been driven by transformer-based models. OCR systems are critical in numerous high-stakes domains, yet their vulnerability to adversarial attack remains largely uncharted territory, raising concerns about security and compliance with emerging AI regulations. In this work we present a novel framework to assess the resilience of Transformer-based OCR (TrOCR) models. We develop and assess algorithms for both targeted and untargeted attacks. For the untargeted case, we measure the Character Error Rate (CER), while for the targeted case we use the success ratio. We find that TrOCR is highly vulnerable to untargeted attacks and somewhat less vulnerable to targeted attacks. On a benchmark handwriting data set, untargeted attacks can cause a CER of more than 1 without being noticeable to the eye. With a similar perturbation size, targeted attacks can lead to success rates of around $25\\%$ -- here we attacked single tokens, requiring TrOCR to output the tenth most likely token from a large vocabulary.", "url": "https://arxiv.org/abs/2311.17128"}, {"metadata": {"arXiv": "2311.17132", "Date": "Tue, 28 Nov 2023 18:03:27 ", "Title": "TransNeXt: Robust Foveal Visual Perception for Vision Transformers", "Authors": ["Dai Shi"], "Categories": "cs.CV cs.AI", "Comments": ["Code will be released at https://github.com/DaiShiResearch/TransNeXt"]}, "abstract": "Due to the depth degradation effect in residual connections, many efficient Vision Transformers models that rely on stacking layers for information exchange often fail to form sufficient information mixing, leading to unnatural visual perception. To address this issue, in this paper, we propose Aggregated Attention, a biomimetic design-based token mixer that simulates biological foveal vision and continuous eye movement while enabling each token on the feature map to have a global perception. Furthermore, we incorporate learnable tokens that interact with conventional queries and keys, which further diversifies the generation of affinity matrices beyond merely relying on the similarity between queries and keys. Our approach does not rely on stacking for information exchange, thus effectively avoiding depth degradation and achieving natural visual perception. Additionally, we propose Convolutional GLU, a channel mixer that bridges the gap between GLU and SE mechanism, which empowers each token to have channel attention based on its nearest neighbor image features, enhancing local modeling capability and model robustness. We combine aggregated attention and convolutional GLU to create a new visual backbone called TransNeXt. Extensive experiments demonstrate that our TransNeXt achieves state-of-the-art performance across multiple model sizes. At a resolution of $224^2$, TransNeXt-Tiny attains an ImageNet accuracy of 84.0%, surpassing ConvNeXt-B with 69% fewer parameters. Our TransNeXt-Base achieves an ImageNet accuracy of 86.2% and an ImageNet-A accuracy of 61.6% at a resolution of $384^2$, a COCO object detection mAP of 57.1, and an ADE20K semantic segmentation mIoU of 54.7.", "url": "https://arxiv.org/abs/2311.17132"}, {"metadata": {"arXiv": "2311.17136", "Date": "Tue, 28 Nov 2023 18:55:52 ", "Title": "UniIR: Training and Benchmarking Universal Multimodal Information Retrievers", "Authors": ["Cong Wei", "Yang Chen", "Haonan Chen", "Hexiang Hu", "Ge Zhang", "Jie Fu", "Alan Ritter", "Wenhu Chen"], "Categories": "cs.CV cs.AI cs.CL cs.IR", "Comments": ["Our code and dataset are available on this project page: https://tiger-ai-lab.github.io/UniIR/"]}, "abstract": "Existing information retrieval (IR) models often assume a homogeneous format, limiting their applicability to diverse user needs, such as searching for images with text descriptions, searching for a news article with a headline image, or finding a similar photo with a query image. To approach such different information-seeking demands, we introduce UniIR, a unified instruction-guided multimodal retriever capable of handling eight distinct retrieval tasks across modalities. UniIR, a single retrieval system jointly trained on ten diverse multimodal-IR datasets, interprets user instructions to execute various retrieval tasks, demonstrating robust performance across existing datasets and zero-shot generalization to new tasks. Our experiments highlight that multi-task training and instruction tuning are keys to UniIR's generalization ability. Additionally, we construct the M-BEIR, a multimodal retrieval benchmark with comprehensive results, to standardize the evaluation of universal multimodal information retrieval.", "url": "https://arxiv.org/abs/2311.17136"}, {"metadata": {"arXiv": "2311.17232", "Date": "Tue, 28 Nov 2023 21:14:05 ", "Title": "ReWaRD: Retinal Waves for Pre-Training Artificial Neural Networks Mimicking Real Prenatal Development", "Authors": ["Benjamin Cappell and Andreas Stoll and Williams Chukwudi Umah and Bernhard Egger"], "Categories": "cs.CV cs.AI", "Comments": ["https://github.com/BennyCa/ReWaRD/ IN: Proceedings of the first edition of the Workshop on Unifying Representations in Neural Models (UniReps 2023) @ NeurIPS 2023"]}, "abstract": "Computational models trained on a large amount of natural images are the state-of-the-art to study human vision - usually adult vision. Computational models of infant vision and its further development are gaining more and more attention in the community. In this work we aim at the very beginning of our visual experience - pre- and post-natal retinal waves which suggest to be a pre-training mechanism for the primate visual system at a very early stage of development. We see this approach as an instance of biologically plausible data driven inductive bias through pre-training. We built a computational model that mimics this development mechanism by pre-training different artificial convolutional neural networks with simulated retinal wave images. The resulting features of this biologically plausible pre-training closely match the V1 features of the primate visual system. We show that the performance gain by pre-training with retinal waves is similar to a state-of-the art pre-training pipeline. Our framework contains the retinal wave generator, as well as a training strategy, which can be a first step in a curriculum learning based training diet for various models of development. We release code, data and trained networks to build the basis for future work on visual development and based on a curriculum learning approach including prenatal development to support studies of innate vs. learned properties of the primate visual system. An additional benefit of our pre-trained networks for neuroscience or computer vision applications is the absence of biases inherited from datasets like ImageNet.", "url": "https://arxiv.org/abs/2311.17232"}, {"metadata": {"arXiv": "2311.17338", "Date": "Wed, 29 Nov 2023 03:36:07 ", "Title": "VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model", "Authors": ["Haoyu Zhao", "Tianyi Lu", "Jiaxi Gu", "Xing Zhang", "Zuxuan Wu", "Hang Xu", "Yu-Gang Jiang"], "Categories": "cs.CV cs.AI"}, "abstract": "Identity-consistent video generation seeks to synthesize videos that are guided by both textual prompts and reference images of entities. Current approaches typically utilize cross-attention layers to integrate the appearance of the entity, which predominantly captures semantic attributes, resulting in compromised fidelity of entities. Moreover, these methods necessitate iterative fine-tuning for each new entity encountered, thereby limiting their applicability. To address these challenges, we introduce VideoAssembler, a novel end-to-end framework for identity-consistent video generation that can conduct inference directly when encountering new entities. VideoAssembler is adept at producing videos that are not only flexible with respect to the input reference entities but also responsive to textual conditions. Additionally, by modulating the quantity of input images for the entity, VideoAssembler enables the execution of tasks ranging from image-to-video generation to sophisticated video editing. VideoAssembler comprises two principal components: the Reference Entity Pyramid (REP) encoder and the Entity-Prompt Attention Fusion (EPAF) module. The REP encoder is designed to infuse comprehensive appearance details into the denoising stages of the stable diffusion model. Concurrently, the EPAF module is utilized to integrate text-aligned features effectively. Furthermore, to mitigate the challenge of scarce data, we present a methodology for the preprocessing of training data. Our evaluation of the VideoAssembler framework on the UCF-101, MSR-VTT, and DAVIS datasets indicates that it achieves good performances in both quantitative and qualitative analyses (346.84 in FVD and 48.01 in IS on UCF-101). Our project page is at https://videoassembler.github.io/videoassembler.", "url": "https://arxiv.org/abs/2311.17338"}, {"metadata": {"arXiv": "2311.17368", "Date": "Wed, 29 Nov 2023 05:42:25 ", "Title": "Two Scalable Approaches for Burned-Area Mapping Using U-Net and Landsat Imagery", "Authors": ["Ian Mancilla-Wulff", "Jaime Carrasco", "Cristobal Pais", "Alejandro Miranda", "Andres Weintraub"], "Categories": "cs.CV cs.AI"}, "abstract": "Monitoring wildfires is an essential step in minimizing their impact on the planet, understanding the many negative environmental, economic, and social consequences. Recent advances in remote sensing technology combined with the increasing application of artificial intelligence methods have improved real-time, high-resolution fire monitoring. This study explores two proposed approaches based on the U-Net model for automating and optimizing the burned-area mapping process. Denoted 128 and AllSizes (AS), they are trained on datasets with a different class balance by cropping input images to different sizes. They are then applied to Landsat imagery and time-series data from two fire-prone regions in Chile. The results obtained after enhancement of model performance by hyperparameter optimization demonstrate the effectiveness of both approaches. Tests based on 195 representative images of the study area show that increasing dataset balance using the AS model yields better performance. More specifically, AS exhibited a Dice Coefficient (DC) of 0.93, an Omission Error (OE) of 0.086, and a Commission Error (CE) of 0.045, while the 128 model achieved a DC of 0.86, an OE of 0.12, and a CE of 0.12. These findings should provide a basis for further development of scalable automatic burned-area mapping tools.", "url": "https://arxiv.org/abs/2311.17368"}, {"metadata": {"arXiv": "2311.17404", "Date": "Wed, 29 Nov 2023 07:15:34 ", "Title": "VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models", "Authors": ["Shicheng Li", "Lei Li", "Shuhuai Ren", "Yuanxin Liu", "Yi Liu", "Rundong Gao", "Xu Sun", "Lu Hou"], "Categories": "cs.CV cs.AI cs.CL", "Comments": ["23 pages", "6 figures", "18 tables", "data is available at https://github.com/lscpku/VITATECS"]}, "abstract": "The ability to perceive how objects change over time is a crucial ingredient in human intelligence. However, current benchmarks cannot faithfully reflect the temporal understanding abilities of video-language models (VidLMs) due to the existence of static visual shortcuts. To remedy this issue, we present VITATECS, a diagnostic VIdeo-Text dAtaset for the evaluation of TEmporal Concept underStanding. Specifically, we first introduce a fine-grained taxonomy of temporal concepts in natural language in order to diagnose the capability of VidLMs to comprehend different temporal aspects. Furthermore, to disentangle the correlation between static and temporal information, we generate counterfactual video descriptions that differ from the original one only in the specified temporal aspect. We employ a semi-automatic data collection framework using large language models and human-in-the-loop annotation to obtain high-quality counterfactual descriptions efficiently. Evaluation of representative video-language understanding models confirms their deficiency in temporal understanding, revealing the need for greater emphasis on the temporal elements in video-language research.", "url": "https://arxiv.org/abs/2311.17404"}, {"metadata": {"arXiv": "2311.17428", "Date": "Wed, 29 Nov 2023 08:09:01 ", "Title": "SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action Segmentation", "Authors": ["Qi Liu", "Xinchen Liu", "Kun Liu", "Xiaoyan Gu", "Wu Liu"], "Categories": "cs.CV cs.AI"}, "abstract": "Multi-modal human action segmentation is a critical and challenging task with a wide range of applications. Nowadays, the majority of approaches concentrate on the fusion of dense signals (i.e., RGB, optical flow, and depth maps). However, the potential contributions of sparse IoT sensor signals, which can be crucial for achieving accurate recognition, have not been fully explored. To make up for this, we introduce a Sparse signalguided Transformer (SigFormer) to combine both dense and sparse signals. We employ mask attention to fuse localized features by constraining cross-attention within the regions where sparse signals are valid. However, since sparse signals are discrete, they lack sufficient information about the temporal action boundaries. Therefore, in SigFormer, we propose to emphasize the boundary information at two stages to alleviate this problem. In the first feature extraction stage, we introduce an intermediate bottleneck module to jointly learn both category and boundary features of each dense modality through the inner loss functions. After the fusion of dense modalities and sparse signals, we then devise a two-branch architecture that explicitly models the interrelationship between action category and temporal boundary. Experimental results demonstrate that SigFormer outperforms the state-of-the-art approaches on a multi-modal action segmentation dataset from real industrial environments, reaching an outstanding F1 score of 0.958. The codes and pre-trained models have been available at https://github.com/LIUQI-creat/SigFormer.", "url": "https://arxiv.org/abs/2311.17428"}, {"metadata": {"arXiv": "2311.17435", "Date": "Wed, 29 Nov 2023 08:27:00 ", "Title": "MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning", "Authors": ["Chaoyi Zhang", "Kevin Lin", "Zhengyuan Yang", "Jianfeng Wang", "Linjie Li", "Chung-Ching Lin", "Zicheng Liu", "Lijuan Wang"], "Categories": "cs.CV cs.AI", "Comments": ["Project page at https://mm-narrator.github.io/"]}, "abstract": "We present MM-Narrator, a novel system leveraging GPT-4 with multimodal in-context learning for the generation of audio descriptions (AD). Unlike previous methods that primarily focused on downstream fine-tuning with short video clips, MM-Narrator excels in generating precise audio descriptions for videos of extensive lengths, even beyond hours, in an autoregressive manner. This capability is made possible by the proposed memory-augmented generation process, which effectively utilizes both the short-term textual context and long-term visual memory through an efficient register-and-recall mechanism. These contextual memories compile pertinent past information, including storylines and character identities, ensuring an accurate tracking and depicting of story-coherent and character-centric audio descriptions. Maintaining the training-free design of MM-Narrator, we further propose a complexity-based demonstration selection strategy to largely enhance its multi-step reasoning capability via few-shot multimodal in-context learning (MM-ICL). Experimental results on MAD-eval dataset demonstrate that MM-Narrator consistently outperforms both the existing fine-tuning-based approaches and LLM-based approaches in most scenarios, as measured by standard evaluation metrics. Additionally, we introduce the first segment-based evaluator for recurrent text generation. Empowered by GPT-4, this evaluator comprehensively reasons and marks AD generation performance in various extendable dimensions.", "url": "https://arxiv.org/abs/2311.17435"}, {"metadata": {"arXiv": "2311.17647", "Date": "Wed, 29 Nov 2023 14:08:53 ", "Title": "VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following", "Authors": ["Yujie Lu", "Xiujun Li", "William Yang Wang", "Yejin Choi"], "Categories": "cs.CV cs.AI cs.CL", "Comments": ["20 pages", "8 figures", "20 tables"]}, "abstract": "We introduce VISUAL EMBEDDED INSTRUCTION (VIM), a new framework designed to evaluate the visual instruction following capability of Multimodal Large Language Models (MLLMs). As illustrated in Figure 2, VIM challenges the MLLMs by embedding the instructions into the visual scenes, demanding strong visual interpretative skills for instruction following. We adapt VIM to various benchmarks, including VQAv2, MME, MM-Vet, and RefCOCO series, compose a VIM bench, and probe diverse MLLMs across three distinct in-context learning settings: Zero Shot, One Shot, and Pair Shot. We observe that there is a significant performance disparity between the open-source MLLMs and GPT-4V, implying that their proficiency in visual instruction comprehension is not up to par. Our results highlight a promising direction for the enhancement of MLLMs capabilities on instruction following. We aim VIM to serve as a useful norm for advancing the state of the art and driving further progress in the field.", "url": "https://arxiv.org/abs/2311.17647"}, {"metadata": {"arXiv": "2311.17655", "Date": "Wed, 29 Nov 2023 14:18:04 ", "Title": "Vulnerability of Automatic Identity Recognition to Audio-Visual Deepfakes", "Authors": ["Pavel Korshunov", "Haolin Chen", "Philip N. Garner", "and Sebastien Marcel"], "Categories": "cs.CV cs.AI cs.MM cs.SD eess.AS", "Comments": ["10 pages", "3 figures", "3 tables"], "ACM-class": "I.4.3; I.2.10; H.5.1"}, "abstract": "The task of deepfakes detection is far from being solved by speech or vision researchers. Several publicly available databases of fake synthetic video and speech were built to aid the development of detection methods. However, existing databases typically focus on visual or voice modalities and provide no proof that their deepfakes can in fact impersonate any real person. In this paper, we present the first realistic audio-visual database of deepfakes SWAN-DF, where lips and speech are well synchronized and video have high visual and audio qualities. We took the publicly available SWAN dataset of real videos with different identities to create audio-visual deepfakes using several models from DeepFaceLab and blending techniques for face swapping and HiFiVC, DiffVC, YourTTS, and FreeVC models for voice conversion. From the publicly available speech dataset LibriTTS, we also created a separate database of only audio deepfakes LibriTTS-DF using several latest text to speech methods: YourTTS, Adaspeech, and TorToiSe. We demonstrate the vulnerability of a state of the art speaker recognition system, such as ECAPA-TDNN-based model from SpeechBrain, to the synthetic voices. Similarly, we tested face recognition system based on the MobileFaceNet architecture to several variants of our visual deepfakes. The vulnerability assessment show that by tuning the existing pretrained deepfake models to specific identities, one can successfully spoof the face and speaker recognition systems in more than 90% of the time and achieve a very realistic looking and sounding fake video of a given person.", "url": "https://arxiv.org/abs/2311.17655"}, {"metadata": {"arXiv": "2311.17907", "Date": "Wed, 29 Nov 2023 18:55:38 ", "Title": "CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting", "Authors": ["Alexander Vilesov", "Pradyumna Chari", "Achuta Kadambi"], "Categories": "cs.CV cs.AI"}, "abstract": "With the onset of diffusion-based generative models and their ability to generate text-conditioned images, content generation has received a massive invigoration. Recently, these models have been shown to provide useful guidance for the generation of 3D graphics assets. However, existing work in text-conditioned 3D generation faces fundamental constraints: (i) inability to generate detailed, multi-object scenes, (ii) inability to textually control multi-object configurations, and (iii) physically realistic scene composition. In this work, we propose CG3D, a method for compositionally generating scalable 3D assets that resolves these constraints. We find that explicit Gaussian radiance fields, parameterized to allow for compositions of objects, possess the capability to enable semantically and physically consistent scenes. By utilizing a guidance framework built around this explicit representation, we show state of the art results, capable of even exceeding the guiding diffusion model in terms of object combinations and physics accuracy.", "url": "https://arxiv.org/abs/2311.17907"}, {"metadata": {"arXiv": "2311.17406", "Date": "Wed, 29 Nov 2023 07:23:22 ", "Title": "LLM-State: Expandable State Representation for Long-horizon Task Planning in the Open World", "Authors": ["Siwei Chen", "Anxing Xiao", "David Hsu"], "Categories": "cs.RO cs.AI", "Comments": ["Submitted to ICRA 2024"]}, "abstract": "This work addresses the problem of long-horizon task planning with the Large Language Model (LLM) in an open-world household environment. Existing works fail to explicitly track key objects and attributes, leading to erroneous decisions in long-horizon tasks, or rely on highly engineered state features and feedback, which is not generalizable. We propose a novel, expandable state representation that provides continuous expansion and updating of object attributes from the LLM's inherent capabilities for context understanding and historical action reasoning. Our proposed representation maintains a comprehensive record of an object's attributes and changes, enabling robust retrospective summary of the sequence of actions leading to the current state. This allows enhanced context understanding for decision-making in task planning. We validate our model through experiments across simulated and real-world task planning scenarios, demonstrating significant improvements over baseline methods in a variety of tasks requiring long-horizon state tracking and reasoning.", "url": "https://arxiv.org/abs/2311.17406"}, {"metadata": {"arXiv": "2311.17165", "Date": "Tue, 28 Nov 2023 19:01:09 ", "Title": "(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions", "Authors": ["Olivia Macmillan-Scott and Mirco Musolesi"], "Categories": "cs.AI cs.CY cs.HC cs.LG cs.MA"}, "abstract": "The concept of rationality is central to the field of artificial intelligence. Whether we are seeking to simulate human reasoning, or the goal is to achieve bounded optimality, we generally seek to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in artificial intelligence, and sets out the open questions in this area. The understanding of rationality in other fields has influenced its conception within artificial intelligence, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we consider irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area remains limited. Methods that have up to now been developed for other purposes, namely adversarial scenarios, may be adapted to suit interactions with artificial agents. We further discuss the interplay between human and artificial agents, and the role that rationality plays within this interaction; many questions remain in this area, relating to potentially irrational behaviour of both humans and artificial agents.", "url": "https://arxiv.org/abs/2311.17165"}, {"metadata": {"arXiv": "2311.17072", "Date": "Mon, 27 Nov 2023 19:00:06 ", "Title": "IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers", "Authors": ["Chenglin Yang", "Siyuan Qiao", "Yuan Cao", "Yu Zhang", "Tao Zhu", "Alan Yuille", "Jiahui Yu"], "Categories": "cs.CV cs.AI cs.LG cs.MM"}, "abstract": "Generative training has been demonstrated to be powerful for building visual-language models. However, on zero-shot discriminative benchmarks, there is still a performance gap between models trained with generative and discriminative objectives. In this paper, we aim to narrow this gap by improving the efficacy of generative training on classification tasks, without any finetuning processes or additional modules. Specifically, we focus on narrowing the gap between the generative captioner and the CLIP classifier. We begin by analysing the predictions made by the captioner and classifier and observe that the caption generation inherits the distribution bias from the language model trained with pure text modality, making it less grounded on the visual signal. To tackle this problem, we redesign the scoring objective for the captioner to alleviate the distributional bias and focus on measuring the gain of information brought by the visual inputs. We further design a generative training objective to match the evaluation objective. We name our model trained and evaluated from the novel procedures as Information Gain (IG) captioner. We pretrain the models on the public Laion-5B dataset and perform a series of discriminative evaluations. For the zero-shot classification on ImageNet, IG captioner achieves $> 18\\%$ improvements over the standard captioner, achieving comparable performances with the CLIP classifier. IG captioner also demonstrated strong performance on zero-shot image-text retrieval tasks on MSCOCO and Flickr30K. We hope this paper inspires further research towards unifying generative and discriminative training procedures for visual-language models.", "url": "https://arxiv.org/abs/2311.17072"}, {"metadata": {"arXiv": "2311.17076", "Date": "Mon, 27 Nov 2023 22:23:27 ", "Title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models", "Authors": ["Chancharik Mitra", "Brandon Huang", "Trevor Darrell", "Roei Herzig"], "Categories": "cs.CV cs.AI cs.CL cs.LG"}, "abstract": "The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations in order to extract compositional knowledge from an LMM. Specifically, we first generate an SG using the LMM, and then use that SG in the prompt to produce a response. Through extensive experiments, we find that the proposed CCoT approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs.", "url": "https://arxiv.org/abs/2311.17076"}, {"metadata": {"arXiv": "2311.17137", "Date": "Tue, 28 Nov 2023 18:59:02 ", "Title": "Generative Models: What do they know? Do they know things? Let's find out!", "Authors": ["Xiaodan Du", "Nicholas Kolkin", "Greg Shakhnarovich", "Anand Bhattad"], "Categories": "cs.CV cs.AI cs.GR cs.LG", "Comments": ["https://intrinsic-lora.github.io/"]}, "abstract": "Generative models have been shown to be capable of synthesizing highly detailed and realistic images. It is natural to suspect that they implicitly learn to model some image intrinsics such as surface normals, depth, or shadows. In this paper, we present compelling evidence that generative models indeed internally produce high-quality scene intrinsic maps. We introduce Intrinsic LoRA (I LoRA), a universal, plug-and-play approach that transforms any generative model into a scene intrinsic predictor, capable of extracting intrinsic scene maps directly from the original generator network without needing additional decoders or fully fine-tuning the original network. Our method employs a Low-Rank Adaptation (LoRA) of key feature maps, with newly learned parameters that make up less than 0.6% of the total parameters in the generative model. Optimized with a small set of labeled images, our model-agnostic approach adapts to various generative architectures, including Diffusion models, GANs, and Autoregressive models. We show that the scene intrinsic maps produced by our method compare well with, and in some cases surpass those generated by leading supervised techniques.", "url": "https://arxiv.org/abs/2311.17137"}, {"metadata": {"arXiv": "2311.17138", "Date": "Tue, 28 Nov 2023 18:59:06 ", "Title": "Shadows Don't Lie and Lines Can't Bend! Generative Models don't know Projective Geometry...for now", "Authors": ["Ayush Sarkar", "Hanlin Mai", "Amitabh Mahapatra", "Svetlana Lazebnik", "D.A. Forsyth", "Anand Bhattad"], "Categories": "cs.CV cs.AI cs.GR cs.LG", "Comments": ["Project Page: https://projective-geometry.github.io | First three authors contributed equally"]}, "abstract": "Generative models can produce impressively realistic images. This paper demonstrates that generated images have geometric features different from those of real images. We build a set of collections of generated images, prequalified to fool simple, signal-based classifiers into believing they are real. We then show that prequalified generated images can be identified reliably by classifiers that only look at geometric properties. We use three such classifiers. All three classifiers are denied access to image pixels, and look only at derived geometric features. The first classifier looks at the perspective field of the image, the second looks at lines detected in the image, and the third looks at relations between detected objects and shadows. Our procedure detects generated images more reliably than SOTA local signal based detectors, for images from a number of distinct generators. Saliency maps suggest that the classifiers can identify geometric problems reliably. We conclude that current generators cannot reliably reproduce geometric properties of real images.", "url": "https://arxiv.org/abs/2311.17138"}, {"metadata": {"arXiv": "2311.17179", "Date": "Tue, 28 Nov 2023 19:14:40 ", "Title": "SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery", "Authors": ["Konstantin Klemmer", "Esther Rolf", "Caleb Robinson", "Lester Mackey", "Marc Ru{\\ss}wurm"], "Categories": "cs.CV cs.AI cs.CY cs.LG"}, "abstract": "Geographic location is essential for modeling tasks in fields ranging from ecology to epidemiology to the Earth system sciences. However, extracting relevant and meaningful characteristics of a location can be challenging, often entailing expensive data fusion or data distillation from global imagery datasets. To address this challenge, we introduce Satellite Contrastive Location-Image Pretraining (SatCLIP), a global, general-purpose geographic location encoder that learns an implicit representation of locations from openly available satellite imagery. Trained location encoders provide vector embeddings summarizing the characteristics of any given location for convenient usage in diverse downstream tasks. We show that SatCLIP embeddings, pretrained on globally sampled multi-spectral Sentinel-2 satellite data, can be used in various predictive tasks that depend on location information but not necessarily satellite imagery, including temperature prediction, animal recognition in imagery, and population density estimation. Across tasks, SatCLIP embeddings consistently outperform embeddings from existing pretrained location encoders, ranging from models trained on natural images to models trained on semantic context. SatCLIP embeddings also help to improve geographic generalization. This demonstrates the potential of general-purpose location encoders and opens the door to learning meaningful representations of our planet from the vast, varied, and largely untapped modalities of geospatial data.", "url": "https://arxiv.org/abs/2311.17179"}, {"metadata": {"arXiv": "2311.17466", "Date": "Wed, 29 Nov 2023 09:18:39 ", "Title": "Slot-Mixup with Subsampling: A Simple Regularization for WSI Classification", "Authors": ["Seongho Keum", "Sanghyun Kim", "Soojeong Lee", "Juho Lee"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Whole slide image (WSI) classification requires repetitive zoom-in and out for pathologists, as only small portions of the slide may be relevant to detecting cancer. Due to the lack of patch-level labels, multiple instance learning (MIL) is a common practice for training a WSI classifier. One of the challenges in MIL for WSIs is the weak supervision coming only from the slide-level labels, often resulting in severe overfitting. In response, researchers have considered adopting patch-level augmentation or applying mixup augmentation, but their applicability remains unverified. Our approach augments the training dataset by sampling a subset of patches in the WSI without significantly altering the underlying semantics of the original slides. Additionally, we introduce an efficient model (Slot-MIL) that organizes patches into a fixed number of slots, the abstract representation of patches, using an attention mechanism. We empirically demonstrate that the subsampling augmentation helps to make more informative slots by restricting the over-concentration of attention and to improve interpretability. Finally, we illustrate that combining our attention-based aggregation model with subsampling and mixup, which has shown limited compatibility in existing MIL methods, can enhance both generalization and calibration. Our proposed methods achieve the state-of-the-art performance across various benchmark datasets including class imbalance and distribution shifts.", "url": "https://arxiv.org/abs/2311.17466"}, {"metadata": {"arXiv": "2311.17518", "Date": "Wed, 29 Nov 2023 10:40:52 ", "Title": "The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding", "Authors": ["Lorenzo Bianchi", "Fabio Carrara", "Nicola Messina", "Claudio Gennaro and Fabrizio Falchi"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Recent advancements in large vision-language models enabled visual object detection in open-vocabulary scenarios, where object classes are defined in free-text formats during inference. In this paper, we aim to probe the state-of-the-art methods for open-vocabulary object detection to determine to what extent they understand fine-grained properties of objects and their parts. To this end, we introduce an evaluation protocol based on dynamic vocabulary generation to test whether models detect, discern, and assign the correct fine-grained description to objects in the presence of hard-negative classes. We contribute with a benchmark suite of increasing difficulty and probing different properties like color, pattern, and material. We further enhance our investigation by evaluating several state-of-the-art open-vocabulary object detectors using the proposed protocol and find that most existing solutions, which shine in standard open-vocabulary benchmarks, struggle to accurately capture and distinguish finer object details. We conclude the paper by highlighting the limitations of current methodologies and exploring promising research directions to overcome the discovered drawbacks. Data and code are available at https://github.com/lorebianchi98/FG-OVD.", "url": "https://arxiv.org/abs/2311.17518"}, {"metadata": {"arXiv": "2311.17695", "Date": "Wed, 29 Nov 2023 15:02:01 ", "Title": "Fair Text-to-Image Diffusion via Fair Mapping", "Authors": ["Jia Li", "Lijie Hu", "Jingfeng Zhang", "Tianhang Zheng", "Hua Zhang", "Di Wang"], "Categories": "cs.CV cs.AI cs.CY cs.LG"}, "abstract": "In this paper, we address the limitations of existing text-to-image diffusion models in generating demographically fair results when given human-related descriptions. These models often struggle to disentangle the target language context from sociocultural biases, resulting in biased image generation. To overcome this challenge, we propose Fair Mapping, a general, model-agnostic, and lightweight approach that modifies a pre-trained text-to-image model by controlling the prompt to achieve fair image generation. One key advantage of our approach is its high efficiency. The training process only requires updating a small number of parameters in an additional linear mapping network. This not only reduces the computational cost but also accelerates the optimization process. We first demonstrate the issue of bias in generated results caused by language biases in text-guided diffusion models. By developing a mapping network that projects language embeddings into an unbiased space, we enable the generation of relatively balanced demographic results based on a keyword specified in the prompt. With comprehensive experiments on face image generation, we show that our method significantly improves image generation performance when prompted with descriptions related to human faces. By effectively addressing the issue of bias, we produce more fair and diverse image outputs. This work contributes to the field of text-to-image generation by enhancing the ability to generate images that accurately reflect the intended demographic characteristics specified in the text.", "url": "https://arxiv.org/abs/2311.17695"}, {"metadata": {"arXiv": "2311.17833", "Date": "Wed, 29 Nov 2023 17:35:29 ", "Title": "Analyzing and Explaining Image Classifiers via Diffusion Guidance", "Authors": ["Maximilian Augustin", "Yannic Neuhaus", "Matthias Hein"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "While deep learning has led to huge progress in complex image classification tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call into question how reliably these classifiers work in the wild. Furthermore, for safety-critical tasks the black-box nature of their decisions is problematic, and explanations or at least methods which make decisions plausible are needed urgently. In this paper, we address these problems by generating images that optimize a classifier-derived objective using a framework for guided image generation. We analyze the behavior and decisions of image classifiers by visual counterfactual explanations (VCEs), detection of systematic mistakes by analyzing images where classifiers maximally disagree, and visualization of neurons to verify potential spurious features. In this way, we validate existing observations, e.g. the shape bias of adversarially robust models, as well as novel failure modes, e.g. systematic errors of zero-shot CLIP classifiers, or identify harmful spurious features. Moreover, our VCEs outperform previous work while being more versatile.", "url": "https://arxiv.org/abs/2311.17833"}, {"metadata": {"arXiv": "2311.17901", "Date": "Wed, 29 Nov 2023 18:53:34 ", "Title": "SODA: Bottleneck Diffusion Models for Representation Learning", "Authors": ["Drew A. Hudson", "Daniel Zoran", "Mateusz Malinowski", "Andrew K. Lampinen", "Andrew Jaegle", "James L. McClelland", "Loic Matthey", "Felix Hill", "Alexander Lerchner"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "We introduce SODA, a self-supervised diffusion model, designed for representation learning. The model incorporates an image encoder, which distills a source view into a compact representation, that, in turn, guides the generation of related novel views. We show that by imposing a tight bottleneck between the encoder and a denoising decoder, and leveraging novel view synthesis as a self-supervised objective, we can turn diffusion models into strong representation learners, capable of capturing visual semantics in an unsupervised manner. To the best of our knowledge, SODA is the first diffusion model to succeed at ImageNet linear-probe classification, and, at the same time, it accomplishes reconstruction, editing and synthesis tasks across a wide range of datasets. Further investigation reveals the disentangled nature of its emergent latent space, that serves as an effective interface to control and manipulate the model's produced images. All in all, we aim to shed light on the exciting and promising potential of diffusion models, not only for image generation, but also for learning rich and robust representations.", "url": "https://arxiv.org/abs/2311.17901"}, {"metadata": {"arXiv": "2311.17097", "Date": "Tue, 28 Nov 2023 07:23:15 ", "Title": "Anonymous Jamming Detection in 5G with Bayesian Network Model Based Inference Analysis", "Authors": ["Ying Wang", "Shashank Jere", "Soumya Banerjee", "Lingjia Liu", "Sachin Shetty", "and Shehadi Dayekh"], "Categories": "cs.LG cs.AI cs.CR cs.NI", "Comments": ["6 pages", "9 figures", "Published in HPSR22. arXiv admin note: text overlap with arXiv:2304.13660"], "DOI": "10.1109/HPSR54439.2022.9831286"}, "abstract": "Jamming and intrusion detection are critical in 5G research, aiming to maintain reliability, prevent user experience degradation, and avoid infrastructure failure. This paper introduces an anonymous jamming detection model for 5G based on signal parameters from the protocol stacks. The system uses supervised and unsupervised learning for real-time, high-accuracy detection of jamming, including unknown types. Supervised models reach an AUC of 0.964 to 1, compared to LSTM models with an AUC of 0.923 to 1. However, the need for data annotation limits the supervised approach. To address this, an unsupervised auto-encoder-based anomaly detection is presented with an AUC of 0.987. The approach is resistant to adversarial training samples. For transparency and domain knowledge injection, a Bayesian network-based causation analysis is introduced.", "url": "https://arxiv.org/abs/2311.17097"}, {"metadata": {"arXiv": "2311.17104", "Date": "Tue, 28 Nov 2023 09:14:55 ", "Title": "Single-Cell Clustering via Dual-Graph Alignment", "Authors": ["Dayu Hu", "Ke Liang", "Xinwang Liu"], "Categories": "cs.LG cs.AI q-bio.MN"}, "abstract": "In recent years, the field of single-cell RNA sequencing has seen a surge in the development of clustering methods. These methods enable the identification of cell subpopulations, thereby facilitating the understanding of tumor microenvironments. Despite their utility, most existing clustering algorithms primarily focus on the attribute information provided by the cell matrix or the network structure between cells, often neglecting the network between genes. This oversight could lead to loss of information and clustering results that lack clinical significance. To address this limitation, we develop an advanced single-cell clustering model incorporating dual-graph alignment, which integrates gene network information into the clustering process based on self-supervised and unsupervised optimization. Specifically, we designed a graph-based autoencoder enhanced by an attention mechanism to effectively capture relationships between cells. Moreover, we performed the node2vec method on Protein-Protein Interaction (PPI) networks to derive the gene network structure and maintained this structure throughout the clustering process. Our proposed method has been demonstrated to be effective through experimental results, showcasing its ability to optimize clustering outcomes while preserving the original associations between cells and genes. This research contributes to obtaining accurate cell subpopulations and generates clustering results that more closely resemble real-world biological scenarios. It provides better insights into the characteristics and distribution of diseased cells, ultimately building a foundation for early disease diagnosis and treatment.", "url": "https://arxiv.org/abs/2311.17104"}, {"metadata": {"arXiv": "2311.17107", "Date": "Tue, 28 Nov 2023 10:26:57 ", "Title": "ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?", "Authors": ["Romain Lacombe", "Kerrie Wu", "Eddie Dilworth"], "Categories": "cs.LG cs.AI cs.CL cs.CY cs.IR", "Comments": ["Tackling Climate Change with Machine Learning workshop at NeurIPS 2023"]}, "abstract": "Evaluating the accuracy of outputs generated by Large Language Models (LLMs) is especially important in the climate science and policy domain. We introduce the Expert Confidence in Climate Statements (ClimateX) dataset, a novel, curated, expert-labeled dataset consisting of 8094 climate statements collected from the latest Intergovernmental Panel on Climate Change (IPCC) reports, labeled with their associated confidence levels. Using this dataset, we show that recent LLMs can classify human expert confidence in climate-related statements, especially in a few-shot learning setting, but with limited (up to 47%) accuracy. Overall, models exhibit consistent and significant over-confidence on low and medium confidence statements. We highlight implications of our results for climate communication, LLMs evaluation strategies, and the use of LLMs in information retrieval systems.", "url": "https://arxiv.org/abs/2311.17107"}, {"metadata": {"arXiv": "2311.17110", "Date": "Tue, 28 Nov 2023 10:59:18 ", "Title": "XAI for time-series classification leveraging image highlight methods", "Authors": ["Georgios Makridis", "Georgios Fatouros", "Vasileios Koukos", "Dimitrios Kotios", "Dimosthenis Kyriazis", "Ioannis Soldatos"], "Categories": "cs.LG cs.AI"}, "abstract": "Although much work has been done on explainability in the computer vision and natural language processing (NLP) fields, there is still much work to be done to explain methods applied to time series as time series by nature can not be understood at first sight. In this paper, we present a Deep Neural Network (DNN) in a teacher-student architecture (distillation model) that offers interpretability in time-series classification tasks. The explainability of our approach is based on transforming the time series to 2D plots and applying image highlight methods (such as LIME and GradCam), making the predictions interpretable. At the same time, the proposed approach offers increased accuracy competing with the baseline model with the trade-off of increasing the training time.", "url": "https://arxiv.org/abs/2311.17110"}, {"metadata": {"arXiv": "2311.17124", "Date": "Tue, 28 Nov 2023 14:31:38 ", "Title": "A knowledge-driven AutoML architecture", "Authors": ["Corneliu Cofaru and Johan Loeckx"], "Categories": "cs.LG cs.AI cs.SE"}, "abstract": "This paper proposes a knowledge-driven AutoML architecture for pipeline and deep feature synthesis. The main goal is to render the AutoML process explainable and to leverage domain knowledge in the synthesis of pipelines and features. The architecture explores several novel ideas: first, the construction of pipelines and deep features is approached in an unified way. Next, synthesis is driven by a shared knowledge system, interactively queried as to what pipeline operations to use or features to compute. Lastly, the synthesis processes takes decisions at runtime using partial solutions and results of their application on data. Two experiments are conducted to demonstrate the functionality of a na\\\"{\\i}ve implementation of the proposed architecture and to discuss its advantages, trade-offs as well as future potential for AutoML.", "url": "https://arxiv.org/abs/2311.17124"}, {"metadata": {"arXiv": "2311.17133", "Date": "Tue, 28 Nov 2023 18:15:53 ", "Title": "Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond", "Authors": ["Jacob R. Epifano", "Stephen Glass", "Ravi P. Ramachandran", "Sharad Patel", "Aaron J. Masino", "Ghulam Rasool"], "Categories": "cs.LG cs.AI"}, "abstract": "This study investigated the performance, explainability, and robustness of deployed artificial intelligence (AI) models in predicting mortality during the COVID-19 pandemic and beyond. The first study of its kind, we found that Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our models to maintain performance amidst significant data shifts. Our results emphasize the importance of developing robust AI models capable of matching or surpassing clinician predictions, even under challenging conditions. Our exploration of model explainability revealed that stochastic models generate more diverse and personalized explanations thereby highlighting the need for AI models that provide detailed and individualized insights in real-world clinical settings. Furthermore, we underscored the importance of quantifying uncertainty in AI models which enables clinicians to make better-informed decisions based on reliable predictions. Our study advocates for prioritizing implementation science in AI research for healthcare and ensuring that AI solutions are practical, beneficial, and sustainable in real-world clinical environments. By addressing unique challenges and complexities in healthcare settings, researchers can develop AI models that effectively improve clinical practice and patient outcomes.", "url": "https://arxiv.org/abs/2311.17133"}, {"metadata": {"arXiv": "2311.17190", "Date": "Tue, 28 Nov 2023 19:34:40 ", "Title": "Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play", "Authors": ["Daniel Bairamian", "Philippe Marcotte", "Joshua Romoff", "Gabriel Robert", "Derek Nowrouzezahrai"], "Categories": "cs.LG cs.AI cs.MA"}, "abstract": "Recent advances in Competitive Self-Play (CSP) have achieved, or even surpassed, human level performance in complex game environments such as Dota 2 and StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL). One core component of these methods relies on creating a pool of learning agents -- consisting of the Main Agent, past versions of this agent, and Exploiter Agents -- where Exploiter Agents learn counter-strategies to the Main Agents. A key drawback of these approaches is the large computational cost and physical time that is required to train the system, making them impractical to deploy in highly iterative real-life settings such as video game productions. In this paper, we propose the Minimax Exploiter, a game theoretic approach to exploiting Main Agents that leverages knowledge of its opponents, leading to significant increases in data efficiency. We validate our approach in a diversity of settings, including simple turn based games, the arcade learning environment, and For Honor, a modern video game. The Minimax Exploiter consistently outperforms strong baselines, demonstrating improved stability and data efficiency, leading to a robust CSP-MARL method that is both flexible and easy to deploy.", "url": "https://arxiv.org/abs/2311.17190"}, {"metadata": {"arXiv": "2311.17303", "Date": "Wed, 29 Nov 2023 01:25:00 ", "Title": "Enhancing the Performance of Neural Networks Through Causal Discovery and Integration of Domain Knowledge", "Authors": ["Xiaoge Zhang", "Xiao-Lin Wang", "Fenglei Fan", "Yiu-Ming Cheung", "Indranil Bose"], "Categories": "cs.LG cs.AI stat.ME"}, "abstract": "In this paper, we develop a generic methodology to encode hierarchical causality structure among observed variables into a neural network in order to improve its predictive performance. The proposed methodology, called causality-informed neural network (CINN), leverages three coherent steps to systematically map the structural causal knowledge into the layer-to-layer design of neural network while strictly preserving the orientation of every causal relationship. In the first step, CINN discovers causal relationships from observational data via directed acyclic graph (DAG) learning, where causal discovery is recast as a continuous optimization problem to avoid the combinatorial nature. In the second step, the discovered hierarchical causality structure among observed variables is systematically encoded into neural network through a dedicated architecture and customized loss function. By categorizing variables in the causal DAG as root, intermediate, and leaf nodes, the hierarchical causal DAG is translated into CINN with a one-to-one correspondence between nodes in the causal DAG and units in the CINN while maintaining the relative order among these nodes. Regarding the loss function, both intermediate and leaf nodes in the DAG graph are treated as target outputs during CINN training so as to drive co-learning of causal relationships among different types of nodes. As multiple loss components emerge in CINN, we leverage the projection of conflicting gradients to mitigate gradient interference among the multiple learning tasks. Computational experiments across a broad spectrum of UCI data sets demonstrate substantial advantages of CINN in predictive performance over other state-of-the-art methods. In addition, an ablation study underscores the value of integrating structural and quantitative causal knowledge in enhancing the neural network's predictive performance incrementally.", "url": "https://arxiv.org/abs/2311.17303"}, {"metadata": {"arXiv": "2311.17401", "Date": "Wed, 29 Nov 2023 07:09:25 ", "Title": "Gene-MOE: A Sparsely-gated Framework for Pan-Cancer Genomic Analysis", "Authors": ["Xiangyu Meng", "Tao Song", "Qing Yang", "Huanhuan Dai", "Lian Qiao", "Hongzhen Ding", "Long Hao and Xun Wang"], "Categories": "cs.LG cs.AI", "Comments": ["submit to bioinformatics"]}, "abstract": "Analyzing the genomic information from the Pan-Cancer database can help us understand cancer-related factors and contribute to the cancer diagnosis and prognosis. However, existing computational methods and deep learning methods can not effectively find the deep correlations between tens of thousands of genes, which leads to precision loss. In this paper, we proposed a novel pretrained model called Gene-MOE to learn the general feature representations of the Pan-Cancer dataset and transfer the pretrained weights to the downstream tasks. The Gene-MOE fully exploits the mixture of expert (MOE) layers to learn rich feature representations of high-dimensional genes. At the same time, we build a mixture of attention expert (MOAE) model to learn the deep semantic relationships within genetic features. Finally, we proposed a new self-supervised pretraining strategy including loss function design, data enhancement, and optimization strategy to train the Gene-MOE and further improve the performance for the downstream analysis. We carried out cancer classification and survival analysis experiments based on the Gene-MOE. According to the survival analysis results on 14 cancer types, using Gene-MOE outperformed state-of-the-art models on 12 cancer types. According to the classification results, the total accuracy of the classification model for 33 cancer classifications reached 95.2\\%. Through detailed feature analysis, we found the Gene-MOE model can learn rich feature representations of high-dimensional genes.", "url": "https://arxiv.org/abs/2311.17401"}, {"metadata": {"arXiv": "2311.17431", "Date": "Wed, 29 Nov 2023 08:21:42 ", "Title": "Grounding Foundation Models through Federated Transfer Learning: A General Framework", "Authors": ["Yan Kang", "Tao Fan", "Hanlin Gu", "Lixin Fan", "Qiang Yang"], "Categories": "cs.LG cs.AI", "Comments": ["in progress"]}, "abstract": "Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. Grounding FMs by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of FMs. However, grounding FMs faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. Federated Transfer Learning (FTL), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. In recent years, the need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in both academia and industry. Motivated by the strong growth in FTL-FM research and the potential impact of FTL-FM on industrial applications, we propose an FTL-FM framework that formulates problems of grounding FMs in the federated learning setting, construct a detailed taxonomy based on the FTL-FM framework to categorize state-of-the-art FTL-FM works, and comprehensively overview FTL-FM works based on the proposed taxonomy. We also establish correspondences between FTL-FM and conventional phases of adapting FM so that FM practitioners can align their research works with FTL-FM. In addition, we overview advanced efficiency-improving and privacy-preserving techniques because efficiency and privacy are critical concerns in FTL-FM. Last, we discuss opportunities and future research directions of FTL-FM.", "url": "https://arxiv.org/abs/2311.17431"}, {"metadata": {"arXiv": "2311.17446", "Date": "Wed, 29 Nov 2023 08:40:46 ", "Title": "Uncertainty in Additive Feature Attribution methods", "Authors": ["Abhishek Madaan", "Tanya Chowdhury", "Neha Rana", "James Allan", "Tanmoy Chakraborty"], "Categories": "cs.LG cs.AI", "Comments": ["14"], "ACM-class": "I.2.6"}, "abstract": "In this work, we explore various topics that fall under the umbrella of Uncertainty in post-hoc Explainable AI (XAI) methods. We in particular focus on the class of additive feature attribution explanation methods. We first describe our specifications of uncertainty and compare various statistical and recent methods to quantify the same. Next, for a particular instance, we study the relationship between a feature's attribution and its uncertainty and observe little correlation. As a result, we propose a modification in the distribution from which perturbations are sampled in LIME-based algorithms such that the important features have minimal uncertainty without an increase in computational cost. Next, while studying how the uncertainty in explanations varies across the feature space of a classifier, we observe that a fraction of instances show near-zero uncertainty. We coin the term \"stable instances\" for such instances and diagnose factors that make an instance stable. Next, we study how an XAI algorithm's uncertainty varies with the size and complexity of the underlying model. We observe that the more complex the model, the more inherent uncertainty is exhibited by it. As a result, we propose a measure to quantify the relative complexity of a blackbox classifier. This could be incorporated, for example, in LIME-based algorithms' sampling densities, to help different explanation algorithms achieve tighter confidence levels. Together, the above measures would have a strong impact on making XAI models relatively trustworthy for the end-user as well as aiding scientific discovery.", "url": "https://arxiv.org/abs/2311.17446"}, {"metadata": {"arXiv": "2311.17565", "Date": "Wed, 29 Nov 2023 11:59:03 ", "Title": "Bias Resilient Multi-Step Off-Policy Goal-Conditioned Reinforcement Learning", "Authors": ["Lisheng Wu and Ke Chen"], "Categories": "cs.LG cs.AI", "Comments": ["26 pages", "7 figures"]}, "abstract": "In goal-conditioned reinforcement learning (GCRL), sparse rewards present significant challenges, often obstructing efficient learning. Although multi-step GCRL can boost this efficiency, it can also lead to off-policy biases in target values. This paper dives deep into these biases, categorizing them into two distinct categories: \"shooting\" and \"shifting\". Recognizing that certain behavior policies can hasten policy refinement, we present solutions designed to capitalize on the positive aspects of these biases while minimizing their drawbacks, enabling the use of larger step sizes to speed up GCRL. An empirical study demonstrates that our approach ensures a resilient and robust improvement, even in ten-step learning scenarios, leading to superior learning efficiency and performance that generally surpass the baseline and several state-of-the-art multi-step GCRL benchmarks.", "url": "https://arxiv.org/abs/2311.17565"}, {"metadata": {"arXiv": "2311.17593", "Date": "Wed, 29 Nov 2023 12:41:55 ", "Title": "LanGWM: Language Grounded World Model", "Authors": ["Rudra P.K. Poudel", "Harit Pandya", "Chao Zhang", "Roberto Cipolla"], "Categories": "cs.LG cs.AI cs.CL cs.CV cs.RO"}, "abstract": "Recent advances in deep reinforcement learning have showcased its potential in tackling complex tasks. However, experiments on visual control tasks have revealed that state-of-the-art reinforcement learning models struggle with out-of-distribution generalization. Conversely, expressing higher-level concepts and global contexts is relatively easy using language. Building upon recent success of the large language models, our main objective is to improve the state abstraction technique in reinforcement learning by leveraging language for robust action selection. Specifically, we focus on learning language-grounded visual features to enhance the world model learning, a model-based reinforcement learning technique. To enforce our hypothesis explicitly, we mask out the bounding boxes of a few objects in the image observation and provide the text prompt as descriptions for these masked objects. Subsequently, we predict the masked objects along with the surrounding regions as pixel reconstruction, similar to the transformer-based masked autoencoder approach. Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art performance in out-of-distribution test at the 100K interaction steps benchmarks of iGibson point navigation tasks. Furthermore, our proposed technique of explicit language-grounded visual representation learning has the potential to improve models for human-robot interaction because our extracted visual features are language grounded.", "url": "https://arxiv.org/abs/2311.17593"}, {"metadata": {"arXiv": "2311.17598", "Date": "Wed, 29 Nov 2023 12:48:33 ", "Title": "Improving embedding of graphs with missing data by soft manifolds", "Authors": ["Andrea Marinoni", "Pietro Lio'", "Alessandro Barp", "Christian Jutten", "Mark Girolami"], "Categories": "cs.LG cs.AI cs.CG"}, "abstract": "Embedding graphs in continous spaces is a key factor in designing and developing algorithms for automatic information extraction to be applied in diverse tasks (e.g., learning, inferring, predicting). The reliability of graph embeddings directly depends on how much the geometry of the continuous space matches the graph structure. Manifolds are mathematical structure that can enable to incorporate in their topological spaces the graph characteristics, and in particular nodes distances. State-of-the-art of manifold-based graph embedding algorithms take advantage of the assumption that the projection on a tangential space of each point in the manifold (corresponding to a node in the graph) would locally resemble a Euclidean space. Although this condition helps in achieving efficient analytical solutions to the embedding problem, it does not represent an adequate set-up to work with modern real life graphs, that are characterized by weighted connections across nodes often computed over sparse datasets with missing records. In this work, we introduce a new class of manifold, named soft manifold, that can solve this situation. In particular, soft manifolds are mathematical structures with spherical symmetry where the tangent spaces to each point are hypocycloids whose shape is defined according to the velocity of information propagation across the data points. Using soft manifolds for graph embedding, we can provide continuous spaces to pursue any task in data analysis over complex datasets. Experimental results on reconstruction tasks on synthetic and real datasets show how the proposed approach enable more accurate and reliable characterization of graphs in continuous spaces with respect to the state-of-the-art.", "url": "https://arxiv.org/abs/2311.17598"}, {"metadata": {"arXiv": "2311.17601", "Date": "Wed, 29 Nov 2023 12:53:32 ", "Title": "Continual Learning with Low Rank Adaptation", "Authors": ["Martin Wistuba", "Prabhu Teja Sivaprasad", "Lukas Balles", "Giovanni Zappella"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at Workshop on Distribution Shifts (DistShift)", "NeurIPS 2023"]}, "abstract": "Recent work using pretrained transformers has shown impressive performance when fine-tuned with data from the downstream problem of interest. However, they struggle to retain that performance when the data characteristics changes. In this paper, we focus on continual learning, where a pre-trained transformer is updated to perform well on new data, while retaining its performance on data it was previously trained on. Earlier works have tackled this primarily through methods inspired from prompt tuning. We question this choice, and investigate the applicability of Low Rank Adaptation (LoRA) to continual learning. On a range of domain-incremental learning benchmarks, our LoRA-based solution, CoLoR, yields state-of-the-art performance, while still being as parameter efficient as the prompt tuning based methods.", "url": "https://arxiv.org/abs/2311.17601"}, {"metadata": {"arXiv": "2311.17750", "Date": "Wed, 29 Nov 2023 15:54:15 ", "Title": "Addressing Membership Inference Attack in Federated Learning with Model Compression", "Authors": ["Gergely D\\'aniel N\\'emeth", "Miguel \\'Angel Lozano", "Novi Quadrianto", "Nuria Oliver"], "Categories": "cs.LG cs.AI cs.CR"}, "abstract": "Federated Learning (FL) has been proposed as a privacy-preserving solution for machine learning. However, recent works have shown that Federated Learning can leak private client data through membership attacks. In this paper, we show that the effectiveness of these attacks on the clients negatively correlates with the size of the client datasets and model complexity. Based on this finding, we propose model-agnostic Federated Learning as a privacy-enhancing solution because it enables the use of models of varying complexity in the clients. To this end, we present $\\texttt{MaPP-FL}$, a novel privacy-aware FL approach that leverages model compression on the clients while keeping a full model on the server. We compare the performance of $\\texttt{MaPP-FL}$ against state-of-the-art model-agnostic FL methods on the CIFAR-10, CIFAR-100, and FEMNIST vision datasets. Our experiments show the effectiveness of $\\texttt{MaPP-FL}$ in preserving the clients' and the server's privacy while achieving competitive classification accuracies.", "url": "https://arxiv.org/abs/2311.17750"}, {"metadata": {"arXiv": "2311.17781", "Date": "Wed, 29 Nov 2023 16:26:24 ", "Title": "Propagate & Distill: Towards Effective Graph Learners Using Propagation-Embracing MLPs", "Authors": ["Yong-Min Shin", "Won-Yong Shin"], "Categories": "cs.LG cs.AI cs.IT cs.NE cs.SI math.IT", "Comments": ["17 pages", "2 figures", "8 tables; 2nd Learning on Graphs Conference (LoG 2023) (Please cite our conference version.). arXiv admin note: substantial text overlap with arXiv:2311.11759"]}, "abstract": "Recent studies attempted to utilize multilayer perceptrons (MLPs) to solve semisupervised node classification on graphs, by training a student MLP by knowledge distillation from a teacher graph neural network (GNN). While previous studies have focused mostly on training the student MLP by matching the output probability distributions between the teacher and student models during distillation, it has not been systematically studied how to inject the structural information in an explicit and interpretable manner. Inspired by GNNs that separate feature transformation $T$ and propagation $\\Pi$, we re-frame the distillation process as making the student MLP learn both $T$ and $\\Pi$. Although this can be achieved by applying the inverse propagation $\\Pi^{-1}$ before distillation from the teacher, it still comes with a high computational cost from large matrix multiplications during training. To solve this problem, we propose Propagate & Distill (P&D), which propagates the output of the teacher before distillation, which can be interpreted as an approximate process of the inverse propagation. We demonstrate that P&D can readily improve the performance of the student MLP.", "url": "https://arxiv.org/abs/2311.17781"}, {"metadata": {"arXiv": "2311.17855", "Date": "Wed, 29 Nov 2023 18:00:41 ", "Title": "Maximum Entropy Model Correction in Reinforcement Learning", "Authors": ["Amin Rakhsha", "Mete Kemertas", "Mohammad Ghavamzadeh", "Amir-massoud Farahmand"], "Categories": "cs.LG cs.AI cs.SY eess.SY math.OC stat.ML"}, "abstract": "We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model's next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna's convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function.", "url": "https://arxiv.org/abs/2311.17855"}, {"metadata": {"arXiv": "2311.17842", "Date": "Wed, 29 Nov 2023 17:46:25 ", "Title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning", "Authors": ["Yingdong Hu", "Fanqi Lin", "Tong Zhang", "Li Yi", "Yang Gao"], "Categories": "cs.RO cs.AI cs.CL cs.CV cs.LG"}, "abstract": "In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.", "url": "https://arxiv.org/abs/2311.17842"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
