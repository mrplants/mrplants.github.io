<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2307.13698", "Date": "Fri, 07 Jul 2023 18:33:52 ", "Title": "Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance", "Authors": ["Shantanu Ghosh", "Kayhan Batmanghelich"], "Categories": "cs.CV cs.LG"}, "abstract": "Discovering a high-performing sparse network within a massive neural network is advantageous for deploying them on devices with limited storage, such as mobile phones. Additionally, model explainability is essential to fostering trust in AI. The Lottery Ticket Hypothesis (LTH) finds a network within a deep network with comparable or superior performance to the original model. However, limited study has been conducted on the success or failure of LTH in terms of explainability. In this work, we examine why the performance of the pruned networks gradually increases or decreases. Using Grad-CAM and Post-hoc concept bottleneck models (PCBMs), respectively, we investigate the explainability of pruned networks in terms of pixels and high-level concepts. We perform extensive experiments across vision and medical imaging datasets. As more weights are pruned, the performance of the network degrades. The discovered concepts and pixels from the pruned networks are inconsistent with the original network -- a possible reason for the drop in performance.", "url": "https://arxiv.org/abs/2307.13698"}, {"metadata": {"arXiv": "2307.13851", "Date": "Tue, 25 Jul 2023 22:54:47 ", "Title": "SplitFed resilience to packet loss: Where to split, that is the question", "Authors": ["Chamani Shiranthika", "Zahra Hafezi Kafshgari", "Parvaneh Saeedi", "Ivan V. Baji\\'c"], "Categories": "cs.CV cs.LG", "Comments": ["10 pages", "4 figures", "MICCAI 2023 Workshop on Distributed", "Collaborative and Federated Learning"]}, "abstract": "Decentralized machine learning has broadened its scope recently with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy. This paper investigates the robustness of SFL against packet loss on communication links. The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model. Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.", "url": "https://arxiv.org/abs/2307.13851"}, {"metadata": {"arXiv": "2307.13855", "Date": "Tue, 25 Jul 2023 23:02:35 ", "Title": "Exploring the Sharpened Cosine Similarity", "Authors": ["Skyler Wu", "Fred Lu", "Edward Raff", "James Holt"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted to I Can't Believe It's Not Better Workshop (ICBINB) at NeurIPS 2022"]}, "abstract": "Convolutional layers have long served as the primary workhorse for image classification. Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector. While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new layers. In our work, we explore SCS's parameter behavior and potential as a drop-in replacement for convolutions in multiple CNN architectures benchmarked on CIFAR-10. We find that while SCS may not yield significant increases in accuracy, it may learn more interpretable representations. We also find that, in some circumstances, SCS may confer a slight increase in adversarial robustness.", "url": "https://arxiv.org/abs/2307.13855"}, {"metadata": {"arXiv": "2307.13856", "Date": "Tue, 25 Jul 2023 23:09:05 ", "Title": "On the unreasonable vulnerability of transformers for image restoration -- and an easy fix", "Authors": ["Shashank Agnihotri", "Kanchana Vaishnavi Gandikota", "Julia Grabinski", "Paramanand Chandramouli", "Margret Keuper"], "Categories": "cs.CV cs.LG eess.IV", "Comments": ["Tags: Robustness", "adversarial attacks", "image deblurring", "image restoration", "NAFNet", "Baseline", "Restormer", "adversarial training"]}, "abstract": "Following their success in visual recognition tasks, Vision Transformers(ViTs) are being increasingly employed for image restoration. As a few recent works claim that ViTs for image classification also have better robustness properties, we investigate whether the improved adversarial robustness of ViTs extends to image restoration. We consider the recently proposed Restormer model, as well as NAFNet and the \"Baseline network\" which are both simplified versions of a Restormer. We use Projected Gradient Descent (PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise prediction tasks for our robustness evaluation. Our experiments are performed on real-world images from the GoPro dataset for image deblurring. Our analysis indicates that contrary to as advocated by ViTs in image classification works, these models are highly susceptible to adversarial attacks. We attempt to improve their robustness through adversarial training. While this yields a significant increase in robustness for Restormer, results on other networks are less promising. Interestingly, the design choices in NAFNet and Baselines, which were based on iid performance, and not on robust generalization, seem to be at odds with the model robustness. Thus, we investigate this further and find a fix.", "url": "https://arxiv.org/abs/2307.13856"}, {"metadata": {"arXiv": "2307.13865", "Date": "Tue, 25 Jul 2023 23:46:48 ", "Title": "Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT", "Authors": ["Taha Emre", "Marzieh Oghbaie", "Arunava Chakravarty", "Antoine Rivail", "Sophie Riedl", "Julia Mai", "Hendrik P.N. Scholl", "Sobha Sivaprasad", "Daniel Rueckert", "Andrew Lotery", "Ursula Schmidt-Erfurth", "and Hrvoje Bogunovi\\'c"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted at OMIA-X MICCAI'23 Workshop"]}, "abstract": "In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression. However, the size of these models presents significant challenges, both in terms of computational resources and data requirements. Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging. To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models. Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements. In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers. In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further. We demonstrate the effectiveness of architectures and associated pretraining on a task of predicting progression to wet age-related macular degeneration (AMD) within a six-month period on two large longitudinal OCT datasets.", "url": "https://arxiv.org/abs/2307.13865"}, {"metadata": {"arXiv": "2307.13924", "Date": "Wed, 26 Jul 2023 02:45:59 ", "Title": "trajdata: A Unified Interface to Multiple Human Trajectory Datasets", "Authors": ["Boris Ivanovic", "Guanyu Song", "Igor Gilitschenski", "Marco Pavone"], "Categories": "cs.CV cs.LG cs.RO", "Comments": ["15 pages", "15 figures", "3 tables"]}, "abstract": "The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata", "url": "https://arxiv.org/abs/2307.13924"}, {"metadata": {"arXiv": "2307.13938", "Date": "Wed, 26 Jul 2023 03:30:28 ", "Title": "Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network", "Authors": ["Zhibo Tain", "Xiaolin Zhang", "Peng Zhang", "Kun Zhan"], "Categories": "cs.CV cs.LG", "Comments": ["ACM MM 2023 accpeted"], "Journal-ref": "ACM MM 2023"}, "abstract": "Semi-supervised semantic segmentation (SSS) is an important task that utilizes both labeled and unlabeled data to reduce expenses on labeling training examples. However, the effectiveness of SSS algorithms is limited by the difficulty of fully exploiting the potential of unlabeled data. To address this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise contrastive learning. By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space, the proposed DSSN is designed to maximize the utilization of available unlabeled data. Additionally, we introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, which addresses the limitations of most existing methods that do not perform selection or apply a predefined threshold for all classes. Specifically, our strategy selects the top high-confidence prediction of the weak view for each class to generate pseudo labels that supervise the strong augmented views. This strategy is capable of taking into account the class imbalance and improving the performance of long-tailed classes. Our proposed method achieves state-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes, outperforming other SSS algorithms by a significant margin.", "url": "https://arxiv.org/abs/2307.13938"}, {"metadata": {"arXiv": "2307.14066", "Date": "Wed, 26 Jul 2023 09:33:24 ", "Title": "Pre-Training with Diffusion models for Dental Radiography segmentation", "Authors": ["J\\'er\\'emy Rousseau", "Christian Alaka", "Emma Covili", "Hippolyte Mayard", "Laura Misrachi", "Willy Au"], "Categories": "cs.CV cs.LG", "Comments": ["13 pages", "6 figures", "Deep Generative Models workshop @ MICCAI 2023"]}, "abstract": "Medical radiography segmentation, and specifically dental radiography, is highly limited by the cost of labeling which requires specific expertise and labor-intensive annotations. In this work, we propose a straightforward pre-training method for semantic segmentation leveraging Denoising Diffusion Probabilistic Models (DDPM), which have shown impressive results for generative modeling. Our straightforward approach achieves remarkable performance in terms of label efficiency and does not require architectural modifications between pre-training and downstream tasks. We propose to first pre-train a Unet by exploiting the DDPM training objective, and then fine-tune the resulting model on a segmentation task. Our experimental results on the segmentation of dental radiographs demonstrate that the proposed method is competitive with state-of-the-art pre-training methods.", "url": "https://arxiv.org/abs/2307.14066"}, {"metadata": {"arXiv": "2307.14243", "Date": "Wed, 26 Jul 2023 15:14:10 ", "Title": "Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy", "Authors": ["Luca Clissa", "Antonio Macaluso", "Roberto Morelli", "Alessandra Occhinegro", "Emiliana Piscitiello", "Ludovico Taddei", "Marco Luppi", "Roberto Amici", "Matteo Cerri", "Timna Hitrec", "Lorenzo Rinaldi", "Antonio Zoccoli"], "Categories": "cs.CV cs.LG physics.app-ph", "Comments": ["11 pages; 5 figures; 2 tables"]}, "abstract": "Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning. This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics. Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting. The contribution is two-fold. First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas. Second, by enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal Cells v2 will catalyze breakthroughs in fluorescence microscopy analysis and promote cutting-edge discoveries in life sciences. The data are available at: https://amsacta.unibo.it/id/eprint/7347", "url": "https://arxiv.org/abs/2307.14243"}, {"metadata": {"arXiv": "2307.13744", "Date": "Tue, 25 Jul 2023 18:03:29 ", "Title": "mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization", "Authors": ["Yue Niu", "Zalan Fabian", "Sunwoo Lee", "Mahdi Soltanolkotabi", "Salman Avestimehr"], "Categories": "cs.LG math.OC", "Comments": ["Accepted to TMLR 2023 (21 pages", "8 figures)"]}, "abstract": "Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block-wise Hessian, thus enabling distributing compute and memory costs across all computing nodes. We provide a supporting convergence analysis for mL-BFGS in stochastic settings. To investigate mL-BFGS potential in large-scale DNN training, we train benchmark neural models using mL-BFGS and compare performance with baselines (SGD, Adam, and other quasi-Newton methods). Results show that mL-BFGS achieves both noticeable iteration-wise and wall-clock speedup.", "url": "https://arxiv.org/abs/2307.13744"}, {"metadata": {"arXiv": "2307.13757", "Date": "Tue, 25 Jul 2023 18:30:41 ", "Title": "UPREVE: An End-to-End Causal Discovery Benchmarking System", "Authors": ["Suraj Jyothi Unni", "Paras Sheth", "Kaize Ding", "Huan Liu", "and K. Selcuk Candan"], "Categories": "cs.LG cs.HC stat.ME", "Comments": ["8 pages", "Accepted to SBP-BRiMS 2023"]}, "abstract": "Discovering causal relationships in complex socio-behavioral systems is challenging but essential for informed decision-making. We present Upload, PREprocess, Visualize, and Evaluate (UPREVE), a user-friendly web-based graphical user interface (GUI) designed to simplify the process of causal discovery. UPREVE allows users to run multiple algorithms simultaneously, visualize causal relationships, and evaluate the accuracy of learned causal graphs. With its accessible interface and customizable features, UPREVE empowers researchers and practitioners in social computing and behavioral-cultural modeling (among others) to explore and understand causal relationships effectively. Our proposed solution aims to make causal discovery more accessible and user-friendly, enabling users to gain valuable insights for better decision-making.", "url": "https://arxiv.org/abs/2307.13757"}, {"metadata": {"arXiv": "2307.13771", "Date": "Tue, 25 Jul 2023 19:07:03 ", "Title": "Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach", "Authors": ["Mohammad Hoseinpour", "Milad Hoseinpour", "Ali Aghagolzadeh"], "Categories": "cs.LG cs.CR"}, "abstract": "Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.", "url": "https://arxiv.org/abs/2307.13771"}, {"metadata": {"arXiv": "2307.13787", "Date": "Tue, 25 Jul 2023 19:45:46 ", "Title": "The GANfather: Controllable generation of malicious activity to improve defence systems", "Authors": ["Ricardo Ribeiro Pereira", "Jacopo Bono", "Jo\\~ao Tiago Ascens\\~ao", "David Apar\\'icio", "Pedro Ribeiro", "Pedro Bizarro"], "Categories": "cs.LG cs.CR"}, "abstract": "Machine learning methods to aid defence systems in detecting malicious activity typically rely on labelled data. In some domains, such labelled data is unavailable or incomplete. In practice this can lead to low detection rates and high false positive rates, which characterise for example anti-money laundering systems. In fact, it is estimated that 1.7--4 trillion euros are laundered annually and go undetected. We propose The GANfather, a method to generate samples with properties of malicious activity, without label requirements. We propose to reward the generation of malicious samples by introducing an extra objective to the typical Generative Adversarial Networks (GANs) loss. Ultimately, our goal is to enhance the detection of illicit activity using the discriminator network as a novel and robust defence system. Optionally, we may encourage the generator to bypass pre-existing detection systems. This setup then reveals defensive weaknesses for the discriminator to correct. We evaluate our method in two real-world use cases, money laundering and recommendation systems. In the former, our method moves cumulative amounts close to 350 thousand dollars through a network of accounts without being detected by an existing system. In the latter, we recommend the target item to a broad user base with as few as 30 synthetic attackers. In both cases, we train a new defence system to capture the synthetic attacks.", "url": "https://arxiv.org/abs/2307.13787"}, {"metadata": {"arXiv": "2307.13818", "Date": "Tue, 25 Jul 2023 21:09:55 ", "Title": "Gradient-Based Spectral Embeddings of Random Dot Product Graphs", "Authors": ["Marcelo Fiori", "Bernardo Marenco", "Federico Larroca", "Paola Bermolen", "Gonzalo Mateos"], "Categories": "cs.LG math.OC"}, "abstract": "The Random Dot Product Graph (RDPG) is a generative model for relational data, where nodes are represented via latent vectors in low-dimensional Euclidean space. RDPGs crucially postulate that edge formation probabilities are given by the dot product of the corresponding latent positions. Accordingly, the embedding task of estimating these vectors from an observed graph is typically posed as a low-rank matrix factorization problem. The workhorse Adjacency Spectral Embedding (ASE) enjoys solid statistical properties, but it is formally solving a surrogate problem and can be computationally intensive. In this paper, we bring to bear recent advances in non-convex optimization and demonstrate their impact to RDPG inference. We advocate first-order gradient descent methods to better solve the embedding problem, and to organically accommodate broader network embedding applications of practical relevance. Notably, we argue that RDPG embeddings of directed graphs loose interpretability unless the factor matrices are constrained to have orthogonal columns. We thus develop a novel feasible optimization method in the resulting manifold. The effectiveness of the graph representation learning framework is demonstrated on reproducible experiments with both synthetic and real network data. Our open-source algorithm implementations are scalable, and unlike the ASE they are robust to missing edge data and can track slowly-varying latent positions from streaming graphs.", "url": "https://arxiv.org/abs/2307.13818"}, {"metadata": {"arXiv": "2307.13831", "Date": "Tue, 25 Jul 2023 21:59:17 ", "Title": "Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search", "Authors": ["Yuki Tsukada", "Hideaki Iiduka"], "Categories": "cs.LG math.OC"}, "abstract": "Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic first-order oracle (SFO) complexity, which is the stochastic gradient computation cost, is a convex function of the batch size; that is, there exists a critical batch size that minimizes the SFO complexity. Finally, we provide numerical results that support our theoretical results. The numerical results indicate that the number of steps needed for training deep neural networks decreases as the batch size increases and that there exist the critical batch sizes that can be estimated from the theoretical results.", "url": "https://arxiv.org/abs/2307.13831"}, {"metadata": {"arXiv": "2307.13861", "Date": "Tue, 25 Jul 2023 23:25:05 ", "Title": "Learning to Design Analog Circuits to Meet Threshold Specifications", "Authors": ["Dmitrii Krylov", "Pooya Khajeh", "Junhan Ouyang", "Thomas Reeves", "Tongkai Liu", "Hiba Ajmal", "Hamidreza Aghasi", "Roy Fox"], "Categories": "cs.LG eess.SP", "Comments": ["in proceedings of ICML 23"]}, "abstract": "Automated design of analog and radio-frequency circuits using supervised or reinforcement learning from simulation data has recently been studied as an alternative to manual expert design. It is straightforward for a design agent to learn an inverse function from desired performance metrics to circuit parameters. However, it is more common for a user to have threshold performance criteria rather than an exact target vector of feasible performance measures. In this work, we propose a method for generating from simulation data a dataset on which a system can be trained via supervised learning to design circuits to meet threshold specifications. We moreover perform the to-date most extensive evaluation of automated analog circuit design, including experimenting in a significantly more diverse set of circuits than in prior work, covering linear, nonlinear, and autonomous circuit configurations, and show that our method consistently reaches success rate better than 90% at 5% error margin, while also improving data efficiency by upward of an order of magnitude. A demo of this system is available at circuits.streamlit.app", "url": "https://arxiv.org/abs/2307.13861"}, {"metadata": {"arXiv": "2307.13869", "Date": "Wed, 26 Jul 2023 00:01:21 ", "Title": "Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory", "Authors": ["Takashi Matsubara", "Takaharu Yaguchi"], "Categories": "cs.LG cs.NA math.NA"}, "abstract": "Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 times fewer collocation points (resulting in lower computational cost) than uniformly random sampling or Latin hypercube sampling, while achieving competitive performance.", "url": "https://arxiv.org/abs/2307.13869"}, {"metadata": {"arXiv": "2307.13883", "Date": "Wed, 26 Jul 2023 01:07:52 ", "Title": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis", "Authors": ["Kensen Shi", "Joey Hong", "Manzil Zaheer", "Pengcheng Yin", "Charles Sutton"], "Categories": "cs.LG cs.PL", "Comments": ["arXiv admin note: text overlap with arXiv:2204.03758"]}, "abstract": "When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.", "url": "https://arxiv.org/abs/2307.13883"}, {"metadata": {"arXiv": "2307.13885", "Date": "Wed, 26 Jul 2023 01:10:29 ", "Title": "Efficient Estimation of the Local Robustness of Machine Learning Models", "Authors": ["Tessa Han", "Suraj Srinivas", "Himabindu Lakkaraju"], "Categories": "cs.LG"}, "abstract": "Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\\\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estimators' usefulness for various tasks involving local robustness, such as measuring robustness bias and identifying examples that are vulnerable to noise perturbation in a dataset. By developing these analytical estimators, this work not only advances conceptual understanding of local robustness, but also makes its computation practical, enabling the use of local robustness in critical downstream applications.", "url": "https://arxiv.org/abs/2307.13885"}, {"metadata": {"arXiv": "2307.13903", "Date": "Wed, 26 Jul 2023 02:02:19 ", "Title": "Corruption-Robust Lipschitz Contextual Search", "Authors": ["Shiliang Zuo"], "Categories": "cs.LG stat.ML"}, "abstract": "I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\\log T)$ with $d = 1$ and $O_d(C\\log T + T^{(d-1)/d})$ with $d > 1$; for the pricing loss the learner achieves regret $\\widetilde{O} (T^{d/(d+1)} + C\\cdot T^{1/(d+1)})$.", "url": "https://arxiv.org/abs/2307.13903"}, {"metadata": {"arXiv": "2307.13909", "Date": "Wed, 26 Jul 2023 02:18:04 ", "Title": "Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength", "Authors": ["Tongya Zheng", "Tianli Zhang", "Qingzheng Guan", "Wenjie Huang", "Zunlei Feng", "Mingli Song", "Chun Chen"], "Categories": "cs.LG cs.CE"}, "abstract": "Graph Neural Networks have emerged as an effective machine learning tool for multi-disciplinary tasks such as pharmaceutical molecule classification and chemical reaction prediction, because they can model non-euclidean relationships between different entities. Particle crushing, as a significant field of civil engineering, describes the breakage of granular materials caused by the breakage of particle fragment bonds under the modeling of numerical simulations, which motivates us to characterize the mechanical behaviors of particle crushing through the connectivity of particle fragments with Graph Neural Networks (GNNs). However, there lacks an open-source large-scale particle crushing dataset for research due to the expensive costs of laboratory tests or numerical simulations. Therefore, we firstly generate a dataset with 45,000 numerical simulations and 900 particle types to facilitate the research progress of machine learning for particle crushing. Secondly, we devise a hybrid framework based on GNNs to predict particle crushing strength in a particle fragment view with the advances of state of the art GNNs. Finally, we compare our hybrid framework against traditional machine learning methods and the plain MLP to verify its effectiveness. The usefulness of different features is further discussed through the gradient attribution explanation w.r.t the predictions. Our data and code are released at https://github.com/doujiang-zheng/GNN-For-Particle-Crushing.", "url": "https://arxiv.org/abs/2307.13909"}, {"metadata": {"arXiv": "2307.13917", "Date": "Wed, 26 Jul 2023 02:34:13 ", "Title": "BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery", "Authors": ["Yashas Annadani", "Nick Pawlowski", "Joel Jennings", "Stefan Bauer", "Cheng Zhang", "Wenbo Gong"], "Categories": "cs.LG stat.ME"}, "abstract": "Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.", "url": "https://arxiv.org/abs/2307.13917"}, {"metadata": {"arXiv": "2307.13943", "Date": "Wed, 26 Jul 2023 03:48:37 ", "Title": "Topology-aware Robust Optimization for Out-of-distribution Generalization", "Authors": ["Fengchun Qiao", "Xi Peng"], "Categories": "cs.LG", "Comments": ["In ICLR 2023 (17 pages including appendix). The source code and pre-trained models are publicly available at: https://github.com/joffery/TRO"]}, "abstract": "Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.", "url": "https://arxiv.org/abs/2307.13943"}, {"metadata": {"arXiv": "2307.13995", "Date": "Wed, 26 Jul 2023 07:07:27 ", "Title": "Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space", "Authors": ["Guogang Zhu", "Xuefeng Liu", "Shaojie Tang", "Jianwei Niu", "Xinghao Wu", "Jiaxing Shen"], "Categories": "cs.LG cs.DC", "Comments": ["13 pages", "13 figures"]}, "abstract": "Personalized federated learning (PFL) is a popular framework that allows clients to have different models to address application scenarios where clients' data are in different domains. The typical model of a client in PFL features a global encoder trained by all clients to extract universal features from the raw data and personalized layers (e.g., a classifier) trained using the client's local data. Nonetheless, due to the differences between the data distributions of different clients (aka, domain gaps), the universal features produced by the global encoder largely encompass numerous components irrelevant to a certain client's local task. Some recent PFL methods address the above problem by personalizing specific parameters within the encoder. However, these methods encounter substantial challenges attributed to the high dimensionality and non-linearity of neural network parameter space. In contrast, the feature space exhibits a lower dimensionality, providing greater intuitiveness and interpretability as compared to the parameter space. To this end, we propose a novel PFL framework named FedPick. FedPick achieves PFL in the low-dimensional feature space by selecting task-relevant features adaptively for each client from the features generated by the global encoder based on its local data distribution. It presents a more accessible and interpretable implementation of PFL compared to those methods working in the parameter space. Extensive experimental results show that FedPick could effectively select task-relevant features for each client and improve model performance in cross-domain FL.", "url": "https://arxiv.org/abs/2307.13995"}, {"metadata": {"arXiv": "2307.14023", "Date": "Wed, 26 Jul 2023 08:07:37 ", "Title": "Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?", "Authors": ["Tokio Kajitsuka and Issei Sato"], "Categories": "cs.LG", "MSC-class": "68T07", "ACM-class": "I.2.0"}, "abstract": "Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.", "url": "https://arxiv.org/abs/2307.14023"}, {"metadata": {"arXiv": "2307.14025", "Date": "Wed, 26 Jul 2023 08:14:18 ", "Title": "Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification", "Authors": ["Salome Kazeminia", "Ario Sadafi", "Asya Makhro", "Anna Bogdanova", "Carsten Marr", "Bastian Rieck"], "Categories": "cs.LG cs.CV eess.IV q-bio.QM stat.ML"}, "abstract": "Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.", "url": "https://arxiv.org/abs/2307.14025"}, {"metadata": {"arXiv": "2307.14067", "Date": "Wed, 26 Jul 2023 09:34:34 ", "Title": "Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions", "Authors": ["Mrinmoy Roy", "Sarwar J. Minar", "Porarthi Dhar", "A T M Omor Faruq"], "Categories": "cs.LG", "Journal-ref": "BJMHR, 10(6), 24-54 (2023)"}, "abstract": "Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system. Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems. As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals. This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references. We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection. Dividing these groups into subgroups, we provided relevant references with description in tabular form for quick access. Our objective is to inform people about ML applicability in healthcare industry, reduce the knowledge gap of clinicians about the ML applications and motivate healthcare professionals towards more machine learning based healthcare system.", "url": "https://arxiv.org/abs/2307.14067"}, {"metadata": {"arXiv": "2307.14068", "Date": "Wed, 26 Jul 2023 09:40:19 ", "Title": "Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation", "Authors": ["Long Liu", "Bo Zhou", "Zhipeng Zhao", "Zening Liu"], "Categories": "cs.LG"}, "abstract": "Multi-source unsupervised domain adaptation (MUDA) aims to transfer knowledge from related source domains to an unlabeled target domain. While recent MUDA methods have shown promising results, most focus on aligning the overall feature distributions across source domains, which can lead to negative effects due to redundant features within each domain. Moreover, there is a significant performance gap between MUDA and supervised methods. To address these challenges, we propose a novel approach called Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation (D3AAMDA). Firstly, we establish a multi-source dynamic modulation mechanism during the training process based on the degree of distribution differences between source and target domains. This mechanism controls the alignment level of features between each source domain and the target domain, effectively leveraging the local advantageous feature information within the source domains. Additionally, we propose a Multi-source Active Boundary Sample Selection (MABS) strategy, which utilizes a guided dynamic boundary loss to design an efficient query function for selecting important samples. This strategy achieves improved generalization to the target domain with minimal sampling costs. We extensively evaluate our proposed method on commonly used domain adaptation datasets, comparing it against existing UDA and ADA methods. The experimental results unequivocally demonstrate the superiority of our approach.", "url": "https://arxiv.org/abs/2307.14068"}, {"metadata": {"arXiv": "2307.14151", "Date": "Wed, 26 Jul 2023 12:29:58 ", "Title": "Learning Disentangled Discrete Representations", "Authors": ["David Friede", "Christian Reimers", "Heiner Stuckenschmidt and Mathias Niepert"], "Categories": "cs.LG stat.ML"}, "abstract": "Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear. We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder. We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations. We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations. Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.", "url": "https://arxiv.org/abs/2307.14151"}, {"metadata": {"arXiv": "2307.14185", "Date": "Wed, 26 Jul 2023 13:24:01 ", "Title": "A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia", "Authors": ["Diana McSpadden and Steven Goldenberg and Binata Roy and Malachi Schram and Jonathan L. Goodall and Heather Richter"], "Categories": "cs.LG", "Comments": ["10 pages", "8 figures"]}, "abstract": "Low-lying coastal cities, exemplified by Norfolk, Virginia, face the challenge of street flooding caused by rainfall and tides, which strain transportation and sewer systems and can lead to property damage. While high-fidelity, physics-based simulations provide accurate predictions of urban pluvial flooding, their computational complexity renders them unsuitable for real-time applications. Using data from Norfolk rainfall events between 2016 and 2018, this study compares the performance of a previous surrogate model based on a random forest algorithm with two deep learning models: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). This investigation underscores the importance of using a model architecture that supports the communication of prediction uncertainty and the effective integration of relevant, multi-modal features.", "url": "https://arxiv.org/abs/2307.14185"}, {"metadata": {"arXiv": "2307.14193", "Date": "Wed, 26 Jul 2023 13:47:52 ", "Title": "Efficient Learning of Discrete-Continuous Computation Graphs", "Authors": ["David Friede and Mathias Niepert"], "Categories": "cs.LG", "Journal-ref": "NeurIPS 34 (2021) 6720-6732"}, "abstract": "Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.", "url": "https://arxiv.org/abs/2307.14193"}, {"metadata": {"arXiv": "2307.14199", "Date": "Wed, 26 Jul 2023 13:52:53 ", "Title": "Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling", "Authors": ["Masoume Kazemi", "Davood Moradkhani", "Alireza Abbas Alipour"], "Categories": "cs.LG"}, "abstract": "The hydrometallurgical method of zinc production involves leaching zinc from ore and then separating the solid residue from the liquid solution by pressure filtration. This separation process is very important since the solid residue contains some moisture that can reduce the amount of zinc recovered. This study modeled the pressure filtration process through Random Forest (RF) and Support Vector Machine (SVM). The models take continuous variables (extracted features) from the lab samples as inputs. Thus, regression models namely Random Forest Regression (RFR) and Support Vector Regression (SVR) were chosen. A total dataset was obtained during the pressure filtration process in two conditions: 1) Polypropylene (S1) and 2) Polyester fabrics (S2). To predict the cake moisture, solids concentration (0.2 and 0.38), temperature (35 and 65 centigrade), pH (2, 3.5, and 5), pressure, cake thickness (14, 20, 26, and 34 mm), air-blow time (2, 10 and 15 min) and filtration time were applied as input variables. The models' predictive accuracy was evaluated by the coefficient of determination (R2) parameter. The results revealed that the RFR model is superior to the SVR model for cake moisture prediction.", "url": "https://arxiv.org/abs/2307.14199"}, {"metadata": {"arXiv": "2307.14208", "Date": "Wed, 26 Jul 2023 14:14:38 ", "Title": "Online Modeling and Monitoring of Dependent Processes under Resource Constraints", "Authors": ["Tanapol Kosolwattana", "Huazheng Wang", "Ying Lin"], "Categories": "cs.LG"}, "abstract": "Monitoring a population of dependent processes under limited resources is critical for abnormal events detection. A novel online collaborative learning method is proposed to adaptively allocate the resources for exploitation of high-risk processes and exploration of dependent dynamics. Efficiency of the proposed method is proved through theoretical analysis and experiments.", "url": "https://arxiv.org/abs/2307.14208"}, {"metadata": {"arXiv": "2307.14338", "Date": "Wed, 26 Jul 2023 17:58:07 ", "Title": "TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning", "Authors": ["Yury Gorishniy", "Ivan Rubachev", "Nikolay Kartashev", "Daniil Shlenskii", "Akim Kotelnikov", "Artem Babenko"], "Categories": "cs.LG", "Comments": ["Code: https://github.com/yandex-research/tabular-dl-tabr"]}, "abstract": "Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution. Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed. For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction. However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines. Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL. In this work, we give a strong positive answer to this question. We start by incrementally augmenting a simple feed-forward architecture with an attention-like retrieval component similar to those of many (tabular) retrieval-based models. Then, we highlight several details of the attention mechanism that turn out to have a massive impact on the performance on tabular data problems, but that were not explored in prior work. As a result, we design TabR -- a simple retrieval-based tabular DL model which, on a set of public benchmarks, demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed ``GBDT-friendly'' benchmark (see the first figure).", "url": "https://arxiv.org/abs/2307.14338"}, {"metadata": {"arXiv": "2307.13991", "Date": "Wed, 26 Jul 2023 06:58:19 ", "Title": "METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation", "Authors": ["Junwon Seo", "Taekyung Kim", "Seongyong Ahn", "Kiho Kwak"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["Our video can be found at https://youtu.be/4rIAMM1ZKMo"]}, "abstract": "Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is performed to rapidly adapt the network to the local environment by exploiting recent interaction experiences. To conduct a comprehensive evaluation, we collect driving data from various terrains and demonstrate that our method can obtain a global model that minimizes uncertainty. Moreover, by integrating our model with a model predictive controller, we demonstrate that the reduced uncertainty results in safe and stable navigation in unstructured and unknown terrains.", "url": "https://arxiv.org/abs/2307.13991"}, {"metadata": {"arXiv": "2307.14237", "Date": "Wed, 26 Jul 2023 15:05:17 ", "Title": "Evolving Multi-Objective Neural Network Controllers for Robot Swarms", "Authors": ["Karl Mason", "Sabine Hauert"], "Categories": "cs.RO cs.LG cs.NE", "Comments": ["This paper was presented at the 2023 Autonomous Robots and Multirobot Systems (ARMS) Workshop", "at The 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023)"]}, "abstract": "Many swarm robotics tasks consist of multiple conflicting objectives. This research proposes a multi-objective evolutionary neural network approach to developing controllers for swarms of robots. The swarm robot controllers are trained in a low-fidelity Python simulator and then tested in a high-fidelity simulated environment using Webots. Simulations are then conducted to test the scalability of the evolved multi-objective robot controllers to environments with a larger number of robots. The results presented demonstrate that the proposed approach can effectively control each of the robots. The robot swarm exhibits different behaviours as the weighting for each objective is adjusted. The results also confirm that multi-objective neural network controllers evolved in a low-fidelity simulator can be transferred to high-fidelity simulated environments and that the controllers can scale to environments with a larger number of robots without further retraining needed.", "url": "https://arxiv.org/abs/2307.14237"}, {"metadata": {"arXiv": "2307.14304", "Date": "Wed, 26 Jul 2023 17:12:04 ", "Title": "A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch", "Authors": ["Shengren Hou and Edgar Mauricio Salazar Duque and Peter Palensky and Pedro P. Vergara"], "Categories": "eess.SY cs.LG cs.SY math.OC", "Comments": ["This paper has been submitted to a publication in a journal. This corresponds to the submitted version. After acceptance", "it may be removed depending on the journal's requirements for copyright"]}, "abstract": "The optimal dispatch of energy storage systems (ESSs) presents formidable challenges due to the uncertainty introduced by fluctuations in dynamic prices, demand consumption, and renewable-based energy generation. By exploiting the generalization capabilities of deep neural networks (DNNs), deep reinforcement learning (DRL) algorithms can learn good-quality control models that adaptively respond to distribution networks' stochastic nature. However, current DRL algorithms lack the capabilities to enforce operational constraints strictly, often even providing unfeasible control actions. To address this issue, we propose a DRL framework that effectively handles continuous action spaces while strictly enforcing the environments and action space operational constraints during online operation. Firstly, the proposed framework trains an action-value function modeled using DNNs. Subsequently, this action-value function is formulated as a mixed-integer programming (MIP) formulation enabling the consideration of the environment's operational constraints. Comprehensive numerical simulations show the superior performance of the proposed MIP-DRL framework, effectively enforcing all constraints while delivering high-quality dispatch decisions when compared with state-of-the-art DRL algorithms and the optimal solution obtained with a perfect forecast of the stochastic variables.", "url": "https://arxiv.org/abs/2307.14304"}, {"metadata": {"arXiv": "2307.13815", "Date": "Tue, 25 Jul 2023 20:53:31 ", "Title": "ForestMonkey: Toolkit for Reasoning with AI-based Defect Detection and Classification Models", "Authors": ["Jiajun Zhang", "Georgina Cosma", "Sarah Bugby", "Jason Watkins"], "Categories": "cs.AI", "Comments": ["6 pages", "5 figures", "submitted to 2023 IEEE symposium series on computational intelligence (SSCI)"]}, "abstract": "Artificial intelligence (AI) reasoning and explainable AI (XAI) tasks have gained popularity recently, enabling users to explain the predictions or decision processes of AI models. This paper introduces Forest Monkey (FM), a toolkit designed to reason the outputs of any AI-based defect detection and/or classification model with data explainability. Implemented as a Python package, FM takes input in the form of dataset folder paths (including original images, ground truth labels, and predicted labels) and provides a set of charts and a text file to illustrate the reasoning results and suggest possible improvements. The FM toolkit consists of processes such as feature extraction from predictions to reasoning targets, feature extraction from images to defect characteristics, and a decision tree-based AI-Reasoner. Additionally, this paper investigates the time performance of the FM toolkit when applied to four AI models with different datasets. Lastly, a tutorial is provided to guide users in performing reasoning tasks using the FM toolkit.", "url": "https://arxiv.org/abs/2307.13815"}, {"metadata": {"arXiv": "2307.13837", "Date": "Tue, 25 Jul 2023 22:21:07 ", "Title": "Scaling Integer Arithmetic in Probabilistic Programs", "Authors": ["William X. Cao", "Poorva Garg", "Ryan Tjoa", "Steven Holtzen", "Todd Millstein", "Guy Van den Broeck"], "Categories": "cs.AI cs.PL", "Comments": ["Accepted to UAI 2023"]}, "abstract": "Distributions on integers are ubiquitous in probabilistic modeling but remain challenging for many of today's probabilistic programming languages (PPLs). The core challenge comes from discrete structure: many of today's PPL inference strategies rely on enumeration, sampling, or differentiation in order to scale, which fail for high-dimensional complex discrete distributions involving integers. Our insight is that there is structure in arithmetic that these approaches are not using. We present a binary encoding strategy for discrete distributions that exploits the rich logical structure of integer operations like summation and comparison. We leverage this structured encoding with knowledge compilation to perform exact probabilistic inference, and show that this approach scales to much larger integer distributions with arithmetic.", "url": "https://arxiv.org/abs/2307.13837"}, {"metadata": {"arXiv": "2307.14239", "Date": "Wed, 26 Jul 2023 15:07:40 ", "Title": "Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)", "Authors": ["Barnaby Crook", "Maximilian Schl\\\"uter", "Timo Speith"], "Categories": "cs.AI cs.SE", "Comments": ["Accepted at the Third International Workshop on Requirements Engineering for Explainable Systems (RE4ES) co-located with the 31st IEEE International Requirements Engineering Conference (RE'23)"]}, "abstract": "Within the field of Requirements Engineering (RE), the increasing significance of Explainable Artificial Intelligence (XAI) in aligning AI-supported systems with user needs, societal expectations, and regulatory standards has garnered recognition. In general, explainability has emerged as an important non-functional requirement that impacts system quality. However, the supposed trade-off between explainability and performance challenges the presumed positive influence of explainability. If meeting the requirement of explainability entails a reduction in system performance, then careful consideration must be given to which of these quality aspects takes precedence and how to compromise between them. In this paper, we critically examine the alleged trade-off. We argue that it is best approached in a nuanced way that incorporates resource availability, domain characteristics, and considerations of risk. By providing a foundation for future research and best practices, this work aims to advance the field of RE for AI.", "url": "https://arxiv.org/abs/2307.14239"}, {"metadata": {"arXiv": "2307.14246", "Date": "Wed, 26 Jul 2023 15:15:44 ", "Title": "A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)", "Authors": ["Timo Speith", "Markus Langer"], "Categories": "cs.AI cs.SE", "Comments": ["Accepted at the Third International Workshop on Requirements Engineering for Explainable Systems (RE4ES) co-located with the 31st IEEE International Requirements Engineering Conference (RE'23)"]}, "abstract": "Within the field of Requirements Engineering (RE), the increasing significance of Explainable Artificial Intelligence (XAI) in aligning AI-supported systems with user needs, societal expectations, and regulatory standards has garnered recognition. In general, explainability has emerged as an important non-functional requirement that impacts system quality. However, the supposed trade-off between explainability and performance challenges the presumed positive influence of explainability. If meeting the requirement of explainability entails a reduction in system performance, then careful consideration must be given to which of these quality aspects takes precedence and how to compromise between them. In this paper, we critically examine the alleged trade-off. We argue that it is best approached in a nuanced way that incorporates resource availability, domain characteristics, and considerations of risk. By providing a foundation for future research and best practices, this work aims to advance the field of RE for AI.", "url": "https://arxiv.org/abs/2307.14246"}, {"metadata": {"arXiv": "2307.13720", "Date": "Tue, 25 Jul 2023 17:58:43 ", "Title": "Composite Diffusion | whole >= \\Sigma parts", "Authors": ["Vikram Jamwal and Ramaneswaran S"], "Categories": "cs.CV cs.AI cs.GR cs.HC", "Comments": ["44 pages"], "ACM-class": "I.3.3; I.4.6; I.4.9; J.5"}, "abstract": "For an artist or a graphic designer, the spatial layout of a scene is a critical design choice. However, existing text-to-image diffusion models provide limited support for incorporating spatial information. This paper introduces Composite Diffusion as a means for artists to generate high-quality images by composing from the sub-scenes. The artists can specify the arrangement of these sub-scenes through a flexible free-form segment layout. They can describe the content of each sub-scene primarily using natural text and additionally by utilizing reference images or control inputs such as line art, scribbles, human pose, canny edges, and more. We provide a comprehensive and modular method for Composite Diffusion that enables alternative ways of generating, composing, and harmonizing sub-scenes. Further, we wish to evaluate the composite image for effectiveness in both image quality and achieving the artist's intent. We argue that existing image quality metrics lack a holistic evaluation of image composites. To address this, we propose novel quality criteria especially relevant to composite generation. We believe that our approach provides an intuitive method of art creation. Through extensive user surveys, quantitative and qualitative analysis, we show how it achieves greater spatial, semantic, and creative control over image generation. In addition, our methods do not need to retrain or modify the architecture of the base diffusion models and can work in a plug-and-play manner with the fine-tuned models.", "url": "https://arxiv.org/abs/2307.13720"}, {"metadata": {"arXiv": "2307.13721", "Date": "Tue, 25 Jul 2023 17:59:18 ", "Title": "Foundational Models Defining a New Era in Vision: A Survey and Outlook", "Authors": ["Muhammad Awais", "Muzammal Naseer", "Salman Khan", "Rao Muhammad Anwer", "Hisham Cholakkal", "Mubarak Shah", "Ming-Hsuan Yang", "Fahad Shahbaz Khan"], "Categories": "cs.CV cs.AI", "Comments": ["Project page: https://github.com/awaisrauf/Awesome-CV-Foundational-Models"]}, "abstract": "Vision systems to see and reason about the compositional nature of visual scenes are fundamental to understanding our world. The complex relations between objects and their locations, ambiguities, and variations in the real-world environment can be better described in human language, naturally governed by grammatical rules and other modalities such as audio and depth. The models learned to bridge the gap between such modalities coupled with large-scale training data facilitate contextual reasoning, generalization, and prompt capabilities at test time. These models are referred to as foundational models. The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box, having interactive dialogues by asking questions about an image or video scene or manipulating the robot's behavior through language instructions. In this survey, we provide a comprehensive review of such emerging foundational models, including typical architecture designs to combine different modalities (vision, text, audio, etc), training objectives (contrastive, generative), pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous. We discuss the open challenges and research directions for foundational models in computer vision, including difficulties in their evaluations and benchmarking, gaps in their real-world understanding, limitations of their contextual understanding, biases, vulnerability to adversarial attacks, and interpretability issues. We review recent developments in this field, covering a wide range of applications of foundation models systematically and comprehensively. A comprehensive list of foundational models studied in this work is available at \\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.", "url": "https://arxiv.org/abs/2307.13721"}, {"metadata": {"arXiv": "2307.13755", "Date": "Tue, 25 Jul 2023 18:26:22 ", "Title": "TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection", "Authors": ["Seyed Mojtaba Marvasti-Zadeh", "Nilanjan Ray", "Nadir Erbilgin"], "Categories": "cs.CV cs.AI"}, "abstract": "Semi-supervised object detection (SSOD) can incorporate limited labeled data and large amounts of unlabeled data to improve the performance and generalization of existing object detectors. Despite many advances, recent SSOD methods are still challenged by noisy/misleading pseudo-labels, classical exponential moving average (EMA) strategy, and the consensus of Teacher-Student models in the latter stages of training. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model's weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore complementary representations. In addition, we use cascade regression to generate more reliable pseudo-labels for supervising the student model. Extensive experiments demonstrate the superior performance of our approach over state-of-the-art SSOD methods. Specifically, the proposed approach outperforms the Unbiased-Teacher method by an average mAP margin of 4.6% and 5.3% when using partially-labeled and fully-labeled data on the MS-COCO dataset, respectively.", "url": "https://arxiv.org/abs/2307.13755"}, {"metadata": {"arXiv": "2307.13770", "Date": "Tue, 25 Jul 2023 19:03:21 ", "Title": "E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning", "Authors": ["Cheng Han", "Qifan Wang", "Yiming Cui", "Zhiwen Cao", "Wenguan Wang", "Siyuan Qi", "Dongfang Liu"], "Categories": "cs.CV cs.AI", "Comments": ["12 pages", "4 figures"]}, "abstract": "As the size of transformer-based models continues to grow, fine-tuning these large-scale pretrained vision models for new tasks has become increasingly parameter-intensive. Parameter-efficient learning has been developed to reduce the number of tunable parameters during fine-tuning. Although these methods show promising results, there is still a significant performance gap compared to full fine-tuning. To address this challenge, we propose an Effective and Efficient Visual Prompt Tuning (E^2VPT) approach for large-scale transformer-based model adaptation. Specifically, we introduce a set of learnable key-value prompts and visual prompts into self-attention and input layers, respectively, to improve the effectiveness of model fine-tuning. Moreover, we design a prompt pruning procedure to systematically prune low importance prompts while preserving model performance, which largely enhances the model's efficiency. Empirical results demonstrate that our approach outperforms several state-of-the-art baselines on two benchmarks, with considerably low parameter usage (e.g., 0.32% of model parameters on VTAB-1k). Our code is available at https://github.com/ChengHan111/E2VPT.", "url": "https://arxiv.org/abs/2307.13770"}, {"metadata": {"arXiv": "2307.14010", "Date": "Wed, 26 Jul 2023 07:45:14 ", "Title": "ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution", "Authors": ["Mingjin Zhang", "Chi Zhang", "Qiming Zhang", "Jie Guo", "Xinbo Gao", "Jing Zhang"], "Categories": "cs.CV cs.AI", "Comments": ["16 pages", "18 figures"]}, "abstract": "Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a high-resolution hyperspectral image from a low-resolution observation. However, the prevailing CNN-based approaches have shown limitations in building long-range dependencies and capturing interaction information between spectral features. This results in inadequate utilization of spectral information and artifacts after upsampling. To address this issue, we propose ESSAformer, an ESSA attention-embedded Transformer network for single-HSI-SR with an iterative refining structure. Specifically, we first introduce a robust and spectral-friendly similarity metric, \\ie, the spectral correlation coefficient of the spectrum (SCC), to replace the original attention matrix and incorporates inductive biases into the model to facilitate training. Built upon it, we further utilize the kernelizable attention technique with theoretical support to form a novel efficient SCC-kernel-based self-attention (ESSA) and reduce attention computation to linear complexity. ESSA enlarges the receptive field for features after upsampling without bringing much computation and allows the model to effectively utilize spatial-spectral information from different scales, resulting in the generation of more natural high-resolution images. Without the need for pretraining on large-scale datasets, our experiments demonstrate ESSA's effectiveness in both visual quality and quantitative results.", "url": "https://arxiv.org/abs/2307.14010"}, {"metadata": {"arXiv": "2307.14019", "Date": "Wed, 26 Jul 2023 08:04:01 ", "Title": "One-Nearest Neighborhood Guides Inlier Estimation for Unsupervised Point Cloud Registration", "Authors": ["Yongzhe Yuan", "Yue Wu", "Maoguo Gong", "Qiguang Miao and A. K. Qin"], "Categories": "cs.CV cs.AI"}, "abstract": "The precision of unsupervised point cloud registration methods is typically limited by the lack of reliable inlier estimation and self-supervised signal, especially in partially overlapping scenarios. In this paper, we propose an effective inlier estimation method for unsupervised point cloud registration by capturing geometric structure consistency between the source point cloud and its corresponding reference point cloud copy. Specifically, to obtain a high quality reference point cloud copy, an One-Nearest Neighborhood (1-NN) point cloud is generated by input point cloud. This facilitates matching map construction and allows for integrating dual neighborhood matching scores of 1-NN point cloud and input point cloud to improve matching confidence. Benefiting from the high quality reference copy, we argue that the neighborhood graph formed by inlier and its neighborhood should have consistency between source point cloud and its corresponding reference copy. Based on this observation, we construct transformation-invariant geometric structure representations and capture geometric structure consistency to score the inlier confidence for estimated correspondences between source point cloud and its reference copy. This strategy can simultaneously provide the reliable self-supervised signal for model optimization. Finally, we further calculate transformation estimation by the weighted SVD algorithm with the estimated correspondences and corresponding inlier confidence. We train the proposed model in an unsupervised manner, and extensive experiments on synthetic and real-world datasets illustrate the effectiveness of the proposed method.", "url": "https://arxiv.org/abs/2307.14019"}, {"metadata": {"arXiv": "2307.14119", "Date": "Wed, 26 Jul 2023 11:38:45 ", "Title": "A semantics-driven methodology for high-quality image annotation", "Authors": ["Fausto Giunchiglia", "Mayukh Bagchi and Xiaolei Diao"], "Categories": "cs.CV cs.AI cs.MM", "Comments": ["Accepted @ 26th European Conference on Artificial Intelligence (ECAI) 2023", "Krak\\'ow", "Poland"], "Report-no": "KDECAI23"}, "abstract": "Recent work in Machine Learning and Computer Vision has highlighted the presence of various types of systematic flaws inside ground truth object recognition benchmark datasets. Our basic tenet is that these flaws are rooted in the many-to-many mappings which exist between the visual information encoded in images and the intended semantics of the labels annotating them. The net consequence is that the current annotation process is largely under-specified, thus leaving too much freedom to the subjective judgment of annotators. In this paper, we propose vTelos, an integrated Natural Language Processing, Knowledge Representation, and Computer Vision methodology whose main goal is to make explicit the (otherwise implicit) intended annotation semantics, thus minimizing the number and role of subjective choices. A key element of vTelos is the exploitation of the WordNet lexico-semantic hierarchy as the main means for providing the meaning of natural language labels and, as a consequence, for driving the annotation of images based on the objects and the visual properties they depict. The methodology is validated on images populating a subset of the ImageNet hierarchy.", "url": "https://arxiv.org/abs/2307.14119"}, {"metadata": {"arXiv": "2307.14142", "Date": "Wed, 26 Jul 2023 12:13:00 ", "Title": "LOIS: Looking Out of Instance Semantics for Visual Question Answering", "Authors": ["Siyu Zhang", "Yeming Chen", "Yaoru Sun", "Fang Wang", "Haibo Shi", "Haoran Wang"], "Categories": "cs.CV cs.AI cs.CL"}, "abstract": "Visual question answering (VQA) has been intensively studied as a multimodal task that requires effort in bridging vision and language to infer answers correctly. Recent attempts have developed various attention-based modules for solving VQA tasks. However, the performance of model inference is largely bottlenecked by visual processing for semantics understanding. Most existing detection methods rely on bounding boxes, remaining a serious challenge for VQA models to understand the causal nexus of object semantics in images and correctly infer contextual information. To this end, we propose a finer model framework without bounding boxes in this work, termed Looking Out of Instance Semantics (LOIS) to tackle this important issue. LOIS enables more fine-grained feature descriptions to produce visual facts. Furthermore, to overcome the label ambiguity caused by instance masks, two types of relation attention modules: 1) intra-modality and 2) inter-modality, are devised to infer the correct answers from the different multi-view features. Specifically, we implement a mutual relation attention module to model sophisticated and deeper visual semantic relations between instance objects and background information. In addition, our proposed attention model can further analyze salient image regions by focusing on important word-related questions. Experimental results on four benchmark VQA datasets prove that our proposed method has favorable performance in improving visual reasoning capability.", "url": "https://arxiv.org/abs/2307.14142"}, {"metadata": {"arXiv": "2307.14332", "Date": "Wed, 26 Jul 2023 17:50:17 ", "Title": "Event-based Vision for Early Prediction of Manipulation Actions", "Authors": ["Daniel Deniz and Cornelia Fermuller and Eduardo Ros and Manuel Rodriguez-Alvarez and Francisco Barranco"], "Categories": "cs.CV cs.AI", "Comments": ["15 pages", "9 figures"]}, "abstract": "Neuromorphic visual sensors are artificial retinas that output sequences of asynchronous events when brightness changes occur in the scene. These sensors offer many advantages including very high temporal resolution, no motion blur and smart data compression ideal for real-time processing. In this study, we introduce an event-based dataset on fine-grained manipulation actions and perform an experimental study on the use of transformers for action prediction with events. There is enormous interest in the fields of cognitive robotics and human-robot interaction on understanding and predicting human actions as early as possible. Early prediction allows anticipating complex stages for planning, enabling effective and real-time interaction. Our Transformer network uses events to predict manipulation actions as they occur, using online inference. The model succeeds at predicting actions early on, building up confidence over time and achieving state-of-the-art classification. Moreover, the attention-based transformer architecture allows us to study the role of the spatio-temporal patterns selected by the model. Our experiments show that the Transformer network captures action dynamic features outperforming video-based approaches and succeeding with scenarios where the differences between actions lie in very subtle cues. Finally, we release the new event dataset, which is the first in the literature for manipulation action recognition. Code will be available at https://github.com/DaniDeniz/EventVisionTransformer.", "url": "https://arxiv.org/abs/2307.14332"}, {"metadata": {"arXiv": "2307.13922", "Date": "Wed, 26 Jul 2023 02:45:02 ", "Title": "Stability of Multi-Agent Learning: Convergence in Network Games with Many Players", "Authors": ["Aamal Hussain", "Dan Leonte", "Francesco Belardinelli and Georgios Piliouras"], "Categories": "cs.GT cs.AI cs.MA math.DS", "Comments": ["Presented at the Workshop on New Frontiers in Learning", "Control", "and Dynamical Systems at the International Conference on Machine Learning (ICML)", "Honolulu", "Hawaii", "USA", "2023"], "MSC-class": "93A16, 91A26, 91A68, 58K35", "ACM-class": "G.3; J.4; F.2.2"}, "abstract": "The behaviour of multi-agent learning in many player games has been shown to display complex dynamics outside of restrictive examples such as network zero-sum games. In addition, it has been shown that convergent behaviour is less likely to occur as the number of players increase. To make progress in resolving this problem, we study Q-Learning dynamics and determine a sufficient condition for the dynamics to converge to a unique equilibrium in any network game. We find that this condition depends on the nature of pairwise interactions and on the network structure, but is explicitly independent of the total number of agents in the game. We evaluate this result on a number of representative network games and show that, under suitable network conditions, stable learning dynamics can be achieved with an arbitrary number of agents.", "url": "https://arxiv.org/abs/2307.13922"}, {"metadata": {"arXiv": "2307.13945", "Date": "Wed, 26 Jul 2023 03:56:24 ", "Title": "Learning-based Control for PMSM Using Distributed Gaussian Processes with Optimal Aggregation Strategy", "Authors": ["Zhenxiao Yin", "Xiaobing Dai", "Zewen Yang", "Yang Shen", "Georges Hattab", "Hang Zhao"], "Categories": "eess.SY cs.AI cs.SY"}, "abstract": "The growing demand for accurate control in varying and unknown environments has sparked a corresponding increase in the requirements for power supply components, including permanent magnet synchronous motors (PMSMs). To infer the unknown part of the system, machine learning techniques are widely employed, especially Gaussian process regression (GPR) due to its flexibility of continuous system modeling and its guaranteed performance. For practical implementation, distributed GPR is adopted to alleviate the high computational complexity. However, the study of distributed GPR from a control perspective remains an open problem. In this paper, a control-aware optimal aggregation strategy of distributed GPR for PMSMs is proposed based on the Lyapunov stability theory. This strategy exclusively leverages the posterior mean, thereby obviating the need for computationally intensive calculations associated with posterior variance in alternative approaches. Moreover, the straightforward calculation process of our proposed strategy lends itself to seamless implementation in high-frequency PMSM control. The effectiveness of the proposed strategy is demonstrated in the simulations.", "url": "https://arxiv.org/abs/2307.13945"}, {"metadata": {"arXiv": "2307.13700", "Date": "Fri, 14 Jul 2023 15:12:10 ", "Title": "CAMP: A Context-Aware Cricket Players Performance Metric", "Authors": ["Muhammad Sohaib Ayub", "Naimat Ullah", "Sarwan Ali", "Imdad Ullah Khan", "Mian Muhammad Awais", "Muhammad Asad Khan and Safiullah Faizullah"], "Categories": "cs.AI cs.CY cs.LG", "Journal-ref": "Journal of the Operational Research Society (2023) 1-27", "DOI": "10.1080/01605682.2023.2237530"}, "abstract": "Cricket is the second most popular sport after soccer in terms of viewership. However, the assessment of individual player performance, a fundamental task in team sports, is currently primarily based on aggregate performance statistics, including average runs and wickets taken. We propose Context-Aware Metric of player Performance, CAMP, to quantify individual players' contributions toward a cricket match outcome. CAMP employs data mining methods and enables effective data-driven decision-making for selection and drafting, coaching and training, team line-ups, and strategy development. CAMP incorporates the exact context of performance, such as opponents' strengths and specific circumstances of games, such as pressure situations. We empirically evaluate CAMP on data of limited-over cricket matches between 2001 and 2019. In every match, a committee of experts declares one player as the best player, called Man of the M}atch (MoM). The top two rated players by CAMP match with MoM in 83\\% of the 961 games. Thus, the CAMP rating of the best player closely matches that of the domain experts. By this measure, CAMP significantly outperforms the current best-known players' contribution measure based on the Duckworth-Lewis-Stern (DLS) method.", "url": "https://arxiv.org/abs/2307.13700"}, {"metadata": {"arXiv": "2307.13701", "Date": "Sat, 15 Jul 2023 13:18:20 ", "Title": "$\\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation", "Authors": ["Hang Yin", "Zihao Wang", "Weizhi Fei", "Yangqiu Song"], "Categories": "cs.AI cs.DB cs.LG cs.LO"}, "abstract": "To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge is required due to the open-world assumption. Learning-based methods are essential because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables ($\\text{EFO}_{k}$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset, $\\text{EFO}_{k}$-CQA, with 741 types of query for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systematically biased that hinders the appropriate development of query-answering methods, highlighting the importance of our work. Our code and data are provided in~\\url{https://github.com/HKUST-KnowComp/EFOK-CQA}.", "url": "https://arxiv.org/abs/2307.13701"}, {"metadata": {"arXiv": "2307.13702", "Date": "Mon, 17 Jul 2023 01:08:39 ", "Title": "Measuring Faithfulness in Chain-of-Thought Reasoning", "Authors": ["Tamera Lanham", "Anna Chen", "Ansh Radhakrishnan", "Benoit Steiner", "Carson Denison", "Danny Hernandez", "Dustin Li", "Esin Durmus", "Evan Hubinger", "Jackson Kernion", "Kamil\\.e Luko\\v{s}i\\=ut\\.e", "Karina Nguyen", "Newton Cheng", "Nicholas Joseph", "Nicholas Schiefer", "Oliver Rausch", "Robin Larson", "Sam McCandlish", "Sandipan Kundu", "Saurav Kadavath", "Shannon Yang", "Thomas Henighan", "Timothy Maxwell", "Timothy Telleen-Lawton", "Tristan Hume", "Zac Hatfield-Dodds", "Jared Kaplan", "Jan Brauner", "Samuel R. Bowman", "Ethan Perez"], "Categories": "cs.AI cs.CL cs.LG"}, "abstract": "Large language models (LLMs) perform better when they produce step-by-step, \"Chain-of-Thought\" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.", "url": "https://arxiv.org/abs/2307.13702"}, {"metadata": {"arXiv": "2307.13704", "Date": "Fri, 21 Jul 2023 18:06:43 ", "Title": "eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review", "Authors": ["Alena Kalyakulina and Igor Yusipov"], "Categories": "cs.AI cs.LG eess.IV", "ACM-class": "I.2.1; J.3"}, "abstract": "eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.", "url": "https://arxiv.org/abs/2307.13704"}, {"metadata": {"arXiv": "2307.13854", "Date": "Tue, 25 Jul 2023 22:59:32 ", "Title": "WebArena: A Realistic Web Environment for Building Autonomous Agents", "Authors": ["Shuyan Zhou", "Frank F. Xu", "Hao Zhu", "Xuhui Zhou", "Robert Lo", "Abishek Sridhar", "Xianyi Cheng", "Yonatan Bisk", "Daniel Fried", "Uri Alon", "Graham Neubig"], "Categories": "cs.AI cs.CL cs.LG", "Comments": ["Work in progress"]}, "abstract": "With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet. We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%. These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/.", "url": "https://arxiv.org/abs/2307.13854"}, {"metadata": {"arXiv": "2307.14283", "Date": "Wed, 26 Jul 2023 16:35:48 ", "Title": "General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications", "Authors": ["Isaac Triguero", "Daniel Molina", "Javier Poyatos", "Javier Del Ser", "Francisco Herrera"], "Categories": "cs.AI cs.LG"}, "abstract": "Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research. This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and ability based on several factors such as adaptation to new tasks, competence in domains not intentionally trained for, ability to learn from few data, or proactive acknowledgment of their own limitations. We then propose a taxonomy of approaches to realise GPAIS, describing research trends such as the use of AI techniques to improve another AI or foundation models. As a prime example, we delve into generative AI, aligning them with the terms and concepts presented in the taxonomy. Through the proposed definition and taxonomy, our aim is to facilitate research collaboration across different areas that are tackling general-purpose tasks, as they share many common aspects. Finally, we discuss the current state of GPAIS, its challenges and prospects, implications for our society, and the need for responsible and trustworthy AI systems and regulation, with the goal of providing a holistic view of GPAIS.", "url": "https://arxiv.org/abs/2307.14283"}, {"metadata": {"arXiv": "2307.13705", "Date": "Mon, 24 Jul 2023 10:16:11 ", "Title": "Control and Monitoring of Artificial Intelligence Algorithms", "Authors": ["Carlos Mario Braga Ortu\\~no", "Blanza Martinez Donoso and Bel\\'en Mu\\~niz Villanueva"], "Categories": "cs.LG cs.AI", "Comments": ["10 pages", "33 equations"], "MSC-class": "62-01", "ACM-class": "G.3"}, "abstract": "This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.", "url": "https://arxiv.org/abs/2307.13705"}, {"metadata": {"arXiv": "2307.13709", "Date": "Mon, 24 Jul 2023 20:56:42 ", "Title": "Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items", "Authors": ["Satoru Fujii"], "Categories": "cs.LG cs.AI", "MSC-class": "68T99"}, "abstract": "Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.", "url": "https://arxiv.org/abs/2307.13709"}, {"metadata": {"arXiv": "2307.13715", "Date": "Tue, 25 Jul 2023 16:10:45 ", "Title": "Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions", "Authors": ["Shih-Hong Chen", "Pin-Hsuan Chou", "Yong-Fu Liu and Chien-An Han"], "Categories": "cs.LG cs.AI", "Comments": ["4 pages", "4 figures"]}, "abstract": "In this paper, our objective is to improve the performance of the existing framework ShuttleNet in predicting badminton shot types and locations by leveraging past strokes. We participated in the CoachAI Badminton Challenge at IJCAI 2023 and achieved significantly better results compared to the baseline. Ultimately, our team achieved the first position in the competition and we made our code available.", "url": "https://arxiv.org/abs/2307.13715"}, {"metadata": {"arXiv": "2307.13716", "Date": "Tue, 25 Jul 2023 17:24:32 ", "Title": "FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning", "Authors": ["Leiming Chen", "Cihao Dong", "Sibo Qiao", "Ziling Huang", "Kai Wang", "Yuming Nie", "Zhaoxiang Hou", "Cheewei Tan"], "Categories": "cs.LG cs.AI"}, "abstract": "Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and aggregates the optimal global model. We also define five model fusion scenarios and compare our method with two baseline algorithms in those scenarios. The experimental results show that our algorithm has higher reliability than other algorithms while maintaining accuracy.", "url": "https://arxiv.org/abs/2307.13716"}, {"metadata": {"arXiv": "2307.13824", "Date": "Tue, 25 Jul 2023 21:38:08 ", "Title": "Offline Reinforcement Learning with On-Policy Q-Function Regularization", "Authors": ["Laixi Shi", "Robert Dadashi", "Yuejie Chi", "Pablo Samuel Castro", "Matthieu Geist"], "Categories": "cs.LG cs.AI", "Comments": ["Published at European Conference on Machine Learning (ECML)", "2023"]}, "abstract": "The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy. A large portion of prior work tackles this challenge by implicitly/explicitly regularizing the learning policy towards the behavior policy, which is hard to estimate reliably in practice. In this work, we propose to regularize towards the Q-function of the behavior policy instead of the behavior policy itself, under the premise that the Q-function can be estimated more reliably and easily by a SARSA-style estimate and handles the extrapolation error more straightforwardly. We propose two algorithms taking advantage of the estimated Q-function through regularizations, and demonstrate they exhibit strong performance on the D4RL benchmarks.", "url": "https://arxiv.org/abs/2307.13824"}, {"metadata": {"arXiv": "2307.13850", "Date": "Tue, 25 Jul 2023 22:51:36 ", "Title": "MAEA: Multimodal Attribution for Embodied AI", "Authors": ["Vidhi Jain", "Jayant Sravan Tamarapalli", "Sahiti Yerramilli", "Yonatan Bisk"], "Categories": "cs.LG cs.AI cs.CV cs.RO"}, "abstract": "Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.", "url": "https://arxiv.org/abs/2307.13850"}, {"metadata": {"arXiv": "2307.13899", "Date": "Wed, 26 Jul 2023 01:47:49 ", "Title": "Regularizing Neural Networks with Meta-Learning Generative Models", "Authors": ["Shin'ya Yamaguchi", "Daiki Chijiwa", "Sekitoshi Kanai", "Atsutoshi Kumagai", "Hisashi Kashima"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Accepted to Data-centric Machine Learning Research (DMLR) Workshop at ICML 2023"]}, "abstract": "This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR can avoid the performance degradation of na\\\"ive generative data augmentation and boost the baselines. Experiments on six datasets showed that MGR is effective particularly when datasets are smaller and stably outperforms baselines.", "url": "https://arxiv.org/abs/2307.13899"}, {"metadata": {"arXiv": "2307.13907", "Date": "Wed, 26 Jul 2023 02:15:11 ", "Title": "Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input", "Authors": ["Neelanjana Pal", "Diego Manzanas Lopez", "and Taylor T Johnson"], "Categories": "cs.LG cs.AI cs.CY cs.NE", "Comments": ["Under Review", "26 Pages", "14 figures", "2 tables"]}, "abstract": "Data-driven, neural network (NN) based anomaly detection and predictive maintenance are emerging research areas. NN-based analytics of time-series data offer valuable insights into past behaviors and estimates of critical parameters like remaining useful life (RUL) of equipment and state-of-charge (SOC) of batteries. However, input time series data can be exposed to intentional or unintentional noise when passing through sensors, necessitating robust validation and verification of these NNs. This paper presents a case study of the robustness verification approach for time series regression NNs (TSRegNN) using set-based formal methods. It focuses on utilizing variable-length input data to streamline input manipulation and enhance network architecture generalizability. The method is applied to two data sets in the Prognostics and Health Management (PHM) application areas: (1) SOC estimation of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs' robustness is checked using star-based reachability analysis, and several performance measures evaluate the effect of bounded perturbations in the input on network outputs, i.e., future outcomes. Overall, the paper offers a comprehensive case study for validating and verifying NN-based analytics of time-series data in real-world applications, emphasizing the importance of robustness testing for accurate and reliable predictions, especially considering the impact of noise on future outcomes.", "url": "https://arxiv.org/abs/2307.13907"}, {"metadata": {"arXiv": "2307.13944", "Date": "Wed, 26 Jul 2023 03:55:08 ", "Title": "Entropy Neural Estimation for Graph Contrastive Learning", "Authors": ["Yixuan Ma", "Xiaolin Zhang", "Peng Zhang", "Kun Zhan"], "Categories": "cs.LG cs.AI", "Comments": ["ACM MM 2023 accepted"], "Journal-ref": "ACM MM 2023"}, "abstract": "Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \\ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous works and we present a novel strategy to enhance the representation ability of the graph encoder by selecting nodes based on cross-view similarities. We enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively. We also introduce a cross-view consistency constraint on the representations generated from the different views. This objective guarantees the learned representations are consistent across views from the perspective of the entire graph. We conduct extensive experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods. The source code will be publicly released once this paper is accepted.", "url": "https://arxiv.org/abs/2307.13944"}, {"metadata": {"arXiv": "2307.13962", "Date": "Wed, 26 Jul 2023 05:29:29 ", "Title": "Understanding Deep Neural Networks via Linear Separability of Hidden Layers", "Authors": ["Chao Zhang", "Xinyu Chen", "Wensheng Li", "Lixue Liu", "Wei Wu", "Dacheng Tao"], "Categories": "cs.LG cs.AI"}, "abstract": "In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision transformer (ViT) and GoogLeNet.", "url": "https://arxiv.org/abs/2307.13962"}, {"metadata": {"arXiv": "2307.13978", "Date": "Wed, 26 Jul 2023 06:34:24 ", "Title": "Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation", "Authors": ["Mahyar Abbasian", "Taha Rajabzadeh", "Ahmadreza Moradipari", "Seyed Amir Hossein Aqajari", "Hongsheng Lu", "Amir Rahmani"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["7 pages", "7 figures", "2 tables", "conference paper"]}, "abstract": "Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to generate realistic outputs based on training datasets. However, the challenge of exerting control over the generation process of GANs remains a significant hurdle. In this paper, we propose a novel methodology to address this issue by integrating a reinforcement learning (RL) agent with a latent-space GAN (l-GAN), thereby facilitating the generation of desired outputs. More specifically, we have developed an actor-critic RL agent with a meticulously designed reward policy, enabling it to acquire proficiency in navigating the latent space of the l-GAN and generating outputs based on specified tasks. To substantiate the efficacy of our approach, we have conducted a series of experiments employing the MNIST dataset, including arithmetic addition as an illustrative task. The outcomes of these experiments serve to validate our methodology. Our pioneering integration of an RL agent with a GAN model represents a novel advancement, holding great potential for enhancing generative networks in the future.", "url": "https://arxiv.org/abs/2307.13978"}, {"metadata": {"arXiv": "2307.14085", "Date": "Wed, 26 Jul 2023 10:24:17 ", "Title": "Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks", "Authors": ["Siyu Chen", "Mengdi Wang", "Zhuoran Yang"], "Categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "Comments": ["129 pages", "1 figure"]}, "abstract": "We study reinforcement learning (RL) for learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure. In specific, at the outset of the game, the leader announces her policy to the follower and commits to it. The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy. The goal of the leader is to find her optimal policy, which yields the optimal expected total return, by interacting with the follower and learning from data. A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies. We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation. Our algorithms are based on (i) learning the quantal response model via maximum likelihood estimation and (ii) model-free or model-based RL for solving the leader's decision making problem, and we show that they achieve sublinear regret upper bounds. Moreover, we quantify the uncertainty of these estimators and leverage the uncertainty to implement optimistic and pessimistic algorithms for online and offline settings. Besides, when specialized to the linear and myopic setting, our algorithms are also computationally efficient. Our theoretical analysis features a novel performance-difference lemma which incorporates the error of quantal response model, which might be of independent interest.", "url": "https://arxiv.org/abs/2307.14085"}, {"metadata": {"arXiv": "2307.14109", "Date": "Wed, 26 Jul 2023 11:12:55 ", "Title": "GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs", "Authors": ["Taniya Das", "Mark Koch", "Maya Ravichandran", "Nikhil Khatri"], "Categories": "cs.LG cs.AI cs.SI"}, "abstract": "GraphRNN is a deep learning-based architecture proposed by You et al. for learning generative models for graphs. We replicate the results of You et al. using a reproduced implementation of the GraphRNN architecture and evaluate this against baseline models using new metrics. Through an ablation study, we find that the BFS traversal suggested by You et al. to collapse representations of isomorphic graphs contributes significantly to model performance. Additionally, we extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. We demonstrate that this method improves significantly over a directed-multiclass variant of GraphRNN on a real-world dataset.", "url": "https://arxiv.org/abs/2307.14109"}, {"metadata": {"arXiv": "2307.14138", "Date": "Wed, 26 Jul 2023 12:06:13 ", "Title": "Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards", "Authors": ["Behzad Nourani-Koliji", "Steven Bilaj", "Amir Rezaei Balef", "Setareh Maghsudi"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards. In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process. In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly. The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms. The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We assume the agent relies on an adaptive approach to overcome the challenge. More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio (GLR) test. Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments. Finally, our algorithm integrates a mechanism to trace the variations of the underlying graph structure, which captures the causal relationships between the rewards in the bandit setting. Theoretically, we establish a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance. The outcome of our numerical experiments in real-world scenarios exhibits applicability and superior performance of our proposal compared to the state-of-the-art benchmarks.", "url": "https://arxiv.org/abs/2307.14138"}, {"metadata": {"arXiv": "2307.14294", "Date": "Wed, 26 Jul 2023 16:51:18 ", "Title": "Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis", "Authors": ["Diego Botache", "Kristina Dingel", "Rico Huhnstock", "Arno Ehresmann", "Bernhard Sick"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Splitting of sequential data, such as videos and time series, is an essential step in various data analysis tasks, including object tracking and anomaly detection. However, splitting sequential data presents a variety of challenges that can impact the accuracy and reliability of subsequent analyses. This concept article examines the challenges associated with splitting sequential data, including data acquisition, data representation, split ratio selection, setting up quality criteria, and choosing suitable selection strategies. We explore these challenges through two real-world examples: motor test benches and particle tracking in liquids.", "url": "https://arxiv.org/abs/2307.14294"}, {"metadata": {"arXiv": "2307.14316", "Date": "Wed, 26 Jul 2023 17:26:21 ", "Title": "Reinforcement Learning by Guided Safe Exploration", "Authors": ["Qisong Yang", "Thiago D. Sim\\~ao", "Nils Jansen", "Simon H. Tindemans", "Matthijs T. J. Spaan"], "Categories": "cs.LG cs.AI", "Comments": ["Accecpted at ECAI 2023"]}, "abstract": "Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.", "url": "https://arxiv.org/abs/2307.14316"}, {"metadata": {"arXiv": "2307.14326", "Date": "Wed, 26 Jul 2023 17:45:55 ", "Title": "Waypoint-Based Imitation Learning for Robotic Manipulation", "Authors": ["Lucy Xiaoyang Shi", "Archit Sharma", "Tony Z. Zhao", "Chelsea Finn"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["The first two authors contributed equally"]}, "abstract": "While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/", "url": "https://arxiv.org/abs/2307.14326"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
