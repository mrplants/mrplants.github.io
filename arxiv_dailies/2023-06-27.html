<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2306.13674", "Date": "Mon, 19 Jun 2023 09:47:33 ", "Title": "MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities", "Authors": ["Hymalai Bello", "Sungho Suh", "Bo Zhou and Paul Lukowicz"], "Categories": "cs.CV cs.LG eess.IV eess.SP", "Comments": ["Submitted to the International Symposium on Wearable Computers (ISWC) 2023"]}, "abstract": "We present MeciFace, a low-power (0.55 Watts), privacy-conscious, real-time on-the-edge (RTE) wearable solution with a tiny memory footprint (11-19 KB), designed to monitor facial expressions and eating activities. We employ lightweight convolutional neural networks as the backbone models for both facial and eating scenarios. The system yielded an F1-score of 86% for the RTE evaluation in the facial expression case. In addition, we obtained an F1-score of 90% for eating/drinking monitoring for the RTE of an unseen user.", "url": "https://arxiv.org/abs/2306.13674"}, {"metadata": {"arXiv": "2306.13735", "Date": "Fri, 23 Jun 2023 18:51:22 ", "Title": "Combining Public Human Activity Recognition Datasets to Mitigate Labeled Data Scarcity", "Authors": ["Riccardo Presotto", "Sannara Ek", "Gabriele Civitarese", "Fran\\c{c}ois Portet", "Philippe Lalanda", "Claudio Bettini"], "Categories": "cs.CV cs.LG", "Comments": ["IEEE SMARTCOMP 2023"]}, "abstract": "The use of supervised learning for Human Activity Recognition (HAR) on mobile devices leads to strong classification performances. Such an approach, however, requires large amounts of labeled data, both for the initial training of the models and for their customization on specific clients (whose data often differ greatly from the training data). This is actually impractical to obtain due to the costs, intrusiveness, and time-consuming nature of data annotation. Moreover, even with the help of a significant amount of labeled data, model deployment on heterogeneous clients faces difficulties in generalizing well on unseen data. Other domains, like Computer Vision or Natural Language Processing, have proposed the notion of pre-trained models, leveraging large corpora, to reduce the need for annotated data and better manage heterogeneity. This promising approach has not been implemented in the HAR domain so far because of the lack of public datasets of sufficient size. In this paper, we propose a novel strategy to combine publicly available datasets with the goal of learning a generalized HAR model that can be fine-tuned using a limited amount of labeled data on an unseen target domain. Our experimental evaluation, which includes experimenting with different state-of-the-art neural network architectures, shows that combining public datasets can significantly reduce the number of labeled samples required to achieve satisfactory performance on an unseen target domain.", "url": "https://arxiv.org/abs/2306.13735"}, {"metadata": {"arXiv": "2306.13776", "Date": "Fri, 23 Jun 2023 20:19:58 ", "Title": "Swin-Free: Achieving Better Cross-Window Attention and Efficiency with Size-varying Window", "Authors": ["Jinkyu Koo", "John Yang", "Le An", "Gwenaelle Cunha Sergio", "Su Inn Park"], "Categories": "cs.CV cs.LG", "Comments": ["8 pages", "3 figures"]}, "abstract": "Transformer models have shown great potential in computer vision, following their success in language tasks. Swin Transformer is one of them that outperforms convolution-based architectures in terms of accuracy, while improving efficiency when compared to Vision Transformer (ViT) and its variants, which have quadratic complexity with respect to the input size. Swin Transformer features shifting windows that allows cross-window connection while limiting self-attention computation to non-overlapping local windows. However, shifting windows introduces memory copy operations, which account for a significant portion of its runtime. To mitigate this issue, we propose Swin-Free in which we apply size-varying windows across stages, instead of shifting windows, to achieve cross-connection among local windows. With this simple design change, Swin-Free runs faster than the Swin Transformer at inference with better accuracy. Furthermore, we also propose a few of Swin-Free variants that are faster than their Swin Transformer counterparts.", "url": "https://arxiv.org/abs/2306.13776"}, {"metadata": {"arXiv": "2306.13988", "Date": "Sat, 24 Jun 2023 14:46:49 ", "Title": "SAM++: Enhancing Anatomic Matching using Semantic Information and Structural Inference", "Authors": ["Xiaoyu Bai", "Yong Xia"], "Categories": "cs.CV cs.LG"}, "abstract": "Medical images like CT and MRI provide detailed information about the internal structure of the body, and identifying key anatomical structures from these images plays a crucial role in clinical workflows. Current methods treat it as a registration or key-point regression task, which has limitations in accurate matching and can only handle predefined landmarks. Recently, some methods have been introduced to address these limitations. One such method, called SAM, proposes using a dense self-supervised approach to learn a distinct embedding for each point on the CT image and achieving promising results. Nonetheless, SAM may still face difficulties when dealing with structures that have similar appearances but different semantic meanings or similar semantic meanings but different appearances. To overcome these limitations, we propose SAM++, a framework that simultaneously learns appearance and semantic embeddings with a novel fixed-points matching mechanism. We tested the SAM++ framework on two challenging tasks, demonstrating a significant improvement over the performance of SAM and outperforming other existing methods.", "url": "https://arxiv.org/abs/2306.13988"}, {"metadata": {"arXiv": "2306.14060", "Date": "Sat, 24 Jun 2023 21:05:02 ", "Title": "DesCo: Learning Object Recognition with Rich Language Descriptions", "Authors": ["Liunian Harold Li", "Zi-Yi Dou", "Nanyun Peng", "Kai-Wei Chang"], "Categories": "cs.CV cs.CL cs.LG"}, "abstract": "Recent development in vision-language approaches has instigated a paradigm shift in learning visual recognition models from language supervision. These approaches align objects with language queries (e.g. \"a photo of a cat\") and improve the models' adaptability to identify novel objects and domains. Recently, several studies have attempted to query these models with complex language expressions that include specifications of fine-grained semantic details, such as attributes, shapes, textures, and relations. However, simply incorporating language descriptions as queries does not guarantee accurate interpretation by the models. In fact, our experiments show that GLIP, the state-of-the-art vision-language model for object detection, often disregards contextual information in the language descriptions and instead relies heavily on detecting objects solely by their names. To tackle the challenges, we propose a new description-conditioned (DesCo) paradigm of learning object recognition models with rich language descriptions consisting of two major innovations: 1) we employ a large language model as a commonsense knowledge engine to generate rich language descriptions of objects based on object names and the raw image-text caption; 2) we design context-sensitive queries to improve the model's ability in deciphering intricate nuances embedded within descriptions and enforce the model to focus on context rather than object names alone. On two novel object detection benchmarks, LVIS and OminiLabel, under the zero-shot detection setting, our approach achieves 34.8 APr minival (+9.1) and 29.3 AP (+3.6), respectively, surpassing the prior state-of-the-art models, GLIP and FIBER, by a large margin.", "url": "https://arxiv.org/abs/2306.14060"}, {"metadata": {"arXiv": "2306.14217", "Date": "Sun, 25 Jun 2023 11:45:08 ", "Title": "On Evaluating the Adversarial Robustness of Semantic Segmentation Models", "Authors": ["Levente Halmosi and Mark Jelasity"], "Categories": "cs.CV cs.LG"}, "abstract": "Achieving robustness against adversarial input perturbation is an important and intriguing problem in machine learning. In the area of semantic image segmentation, a number of adversarial training approaches have been proposed as a defense against adversarial perturbation, but the methodology of evaluating the robustness of the models is still lacking, compared to image classification. Here, we demonstrate that, just like in image classification, it is important to evaluate the models over several different and hard attacks. We propose a set of gradient based iterative attacks and show that it is essential to perform a large number of iterations. We include attacks against the internal representations of the models as well. We apply two types of attacks: maximizing the error with a bounded perturbation, and minimizing the perturbation for a given level of error. Using this set of attacks, we show for the first time that a number of models in previous work that are claimed to be robust are in fact not robust at all. We then evaluate simple adversarial training algorithms that produce reasonably robust models even under our set of strong attacks. Our results indicate that a key design decision to achieve any robustness is to use only adversarial examples during training. However, this introduces a trade-off between robustness and accuracy.", "url": "https://arxiv.org/abs/2306.14217"}, {"metadata": {"arXiv": "2306.14260", "Date": "Sun, 25 Jun 2023 14:40:26 ", "Title": "HOKEM: Human and Object Keypoint-based Extension Module for Human-Object Interaction Detection", "Authors": ["Yoshiki Ito"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted to IEEE ICIP 2023"]}, "abstract": "Human-object interaction (HOI) detection for capturing relationships between humans and objects is an important task in the semantic understanding of images. When processing human and object keypoints extracted from an image using a graph convolutional network (GCN) to detect HOI, it is crucial to extract appropriate object keypoints regardless of the object type and to design a GCN that accurately captures the spatial relationships between keypoints. This paper presents the human and object keypoint-based extension module (HOKEM) as an easy-to-use extension module to improve the accuracy of the conventional detection models. The proposed object keypoint extraction method is simple yet accurately represents the shapes of various objects. Moreover, the proposed human-object adaptive GCN (HO-AGCN), which introduces adaptive graph optimization and attention mechanism, accurately captures the spatial relationships between keypoints. Experiments using the HOI dataset, V-COCO, showed that HOKEM boosted the accuracy of an appearance-based model by a large margin.", "url": "https://arxiv.org/abs/2306.14260"}, {"metadata": {"arXiv": "2306.14264", "Date": "Sun, 25 Jun 2023 15:09:21 ", "Title": "Visual Question Answering in Remote Sensing with Cross-Attention and Multimodal Information Bottleneck", "Authors": ["Jayesh Songara", "Shivam Pande", "Shabnam Choudhury", "Biplab Banerjee and Rajbabu Velmurugan"], "Categories": "cs.CV cs.CL cs.LG"}, "abstract": "In this research, we deal with the problem of visual question answering (VQA) in remote sensing. While remotely sensed images contain information significant for the task of identification and object detection, they pose a great challenge in their processing because of high dimensionality, volume and redundancy. Furthermore, processing image information jointly with language features adds additional constraints, such as mapping the corresponding image and language features. To handle this problem, we propose a cross attention based approach combined with information maximization. The CNN-LSTM based cross-attention highlights the information in the image and language modalities and establishes a connection between the two, while information maximization learns a low dimensional bottleneck layer, that has all the relevant information required to carry out the VQA task. We evaluate our method on two VQA remote sensing datasets of different resolutions. For the high resolution dataset, we achieve an overall accuracy of 79.11% and 73.87% for the two test sets while for the low resolution dataset, we achieve an overall accuracy of 85.98%.", "url": "https://arxiv.org/abs/2306.14264"}, {"metadata": {"arXiv": "2306.14291", "Date": "Sun, 25 Jun 2023 16:45:20 ", "Title": "Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection", "Authors": ["Thang Doan", "Xin Li", "Sima Behpour", "Wenbin He", "Liang Gou", "Liu Ren"], "Categories": "cs.CV cs.LG", "Comments": ["keywords: Open World Object Detection", "Hyperbolic Distance", "Unknown Detection", "Deformable Transformers"]}, "abstract": "Open World Object Detection (OWOD) is a challenging and realistic task that extends beyond the scope of standard Object Detection task. It involves detecting both known and unknown objects while integrating learned knowledge for future tasks. However, the level of 'unknownness' varies significantly depending on the context. For example, a tree is typically considered part of the background in a self-driving scene, but it may be significant in a household context. We argue that this external or contextual information should already be embedded within the known classes. In other words, there should be a semantic or latent structure relationship between the known and unknown items to be discovered. Motivated by this observation, we propose Hyp-OW, a method that learns and models hierarchical representation of known items through a SuperClass Regularizer. Leveraging this learned representation allows us to effectively detect unknown objects using a Similarity Distance-based Relabeling module. Extensive experiments on benchmark datasets demonstrate the effectiveness of Hyp-OW achieving improvement in both known and unknown detection (up to 6 points). These findings are particularly pronounced in our newly designed benchmark, where a strong hierarchical structure exists between known and unknown objects.", "url": "https://arxiv.org/abs/2306.14291"}, {"metadata": {"arXiv": "2306.14313", "Date": "Sun, 25 Jun 2023 18:59:52 ", "Title": "A Closer Look at Geometric Temporal Dynamics for Face Anti-Spoofing", "Authors": ["Chih-Jung Chang", "Yaw-Chern Lee", "Shih-Hsuan Yao", "Min-Hung Chen", "Chien-Yi Wang", "Shang-Hong Lai", "Trista Pei-Chun Chen"], "Categories": "cs.CV cs.LG", "Comments": ["2023 CVPR Biometrics Workshop", "Best Paper Award"]}, "abstract": "Face anti-spoofing (FAS) is indispensable for a face recognition system. Many texture-driven countermeasures were developed against presentation attacks (PAs), but the performance against unseen domains or unseen spoofing types is still unsatisfactory. Instead of exhaustively collecting all the spoofing variations and making binary decisions of live/spoof, we offer a new perspective on the FAS task to distinguish between normal and abnormal movements of live and spoof presentations. We propose Geometry-Aware Interaction Network (GAIN), which exploits dense facial landmarks with spatio-temporal graph convolutional network (ST-GCN) to establish a more interpretable and modularized FAS model. Additionally, with our cross-attention feature interaction mechanism, GAIN can be easily integrated with other existing methods to significantly boost performance. Our approach achieves state-of-the-art performance in the standard intra- and cross-dataset evaluations. Moreover, our model outperforms state-of-the-art methods by a large margin in the cross-dataset cross-type protocol on CASIA-SURF 3DMask (+10.26% higher AUC score), exhibiting strong robustness against domain shifts and unseen spoofing types.", "url": "https://arxiv.org/abs/2306.14313"}, {"metadata": {"arXiv": "2306.14435", "Date": "Mon, 26 Jun 2023 06:04:09 ", "Title": "DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing", "Authors": ["Yujun Shi", "Chuhui Xue", "Jiachun Pan", "Wenqing Zhang", "Vincent Y. F. Tan", "Song Bai"], "Categories": "cs.CV cs.LG", "Comments": ["Preliminary version. Work in Progress"]}, "abstract": "Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabling DragDiffusion to complete high-quality editing efficiently. Extensive experiments across a wide range of challenging cases (e.g., multi-objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of DragDiffusion.", "url": "https://arxiv.org/abs/2306.14435"}, {"metadata": {"arXiv": "2306.14610", "Date": "Mon, 26 Jun 2023 11:35:22 ", "Title": "SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality", "Authors": ["Cheng-Yu Hsieh", "Jieyu Zhang", "Zixian Ma", "Aniruddha Kembhavi", "Ranjay Krishna"], "Categories": "cs.CV cs.CL cs.LG"}, "abstract": "In the last year alone, a surge of new benchmarks to measure compositional understanding of vision-language models have permeated the machine learning ecosystem. Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors. Surprisingly, we find significant biases in all these benchmarks rendering them hackable. This hackability is so dire that blind models with no access to the image outperform state-of-the-art vision-language models. To remedy this rampant vulnerability, we introduce SugarCrepe, a new benchmark for vision-language compositionality evaluation. We employ large language models, instead of rule-based templates used in previous benchmarks, to generate fluent and sensical hard negatives, and utilize an adversarial refinement mechanism to maximally reduce biases. We re-evaluate state-of-the-art models and recently proposed compositionality inducing strategies, and find that their improvements were hugely overestimated, suggesting that more innovation is needed in this important direction. We release SugarCrepe and the code for evaluation at: https://github.com/RAIVNLab/sugar-crepe.", "url": "https://arxiv.org/abs/2306.14610"}, {"metadata": {"arXiv": "2306.14662", "Date": "Mon, 26 Jun 2023 12:54:28 ", "Title": "Cross Architecture Distillation for Face Recognition", "Authors": ["Weisong Zhao", "Xiangyu Zhu", "Zhixiang He", "Xiao-Yu Zhang", "Zhen Lei"], "Categories": "cs.CV cs.LG"}, "abstract": "Transformers have emerged as the superior choice for face recognition tasks, but their insufficient platform acceleration hinders their application on mobile devices. In contrast, Convolutional Neural Networks (CNNs) capitalize on hardware-compatible acceleration libraries. Consequently, it has become indispensable to preserve the distillation efficacy when transferring knowledge from a Transformer-based teacher model to a CNN-based student model, known as Cross-Architecture Knowledge Distillation (CAKD). Despite its potential, the deployment of CAKD in face recognition encounters two challenges: 1) the teacher and student share disparate spatial information for each pixel, obstructing the alignment of feature space, and 2) the teacher network is not trained in the role of a teacher, lacking proficiency in handling distillation-specific knowledge. To surmount these two constraints, 1) we first introduce a Unified Receptive Fields Mapping module (URFM) that maps pixel features of the teacher and student into local features with unified receptive fields, thereby synchronizing the pixel-wise spatial information of teacher and student. Subsequently, 2) we develop an Adaptable Prompting Teacher network (APT) that integrates prompts into the teacher, enabling it to manage distillation-specific knowledge while preserving the model's discriminative capacity. Extensive experiments on popular face benchmarks and two large-scale verification sets demonstrate the superiority of our method.", "url": "https://arxiv.org/abs/2306.14662"}, {"metadata": {"arXiv": "2306.14709", "Date": "Mon, 26 Jun 2023 13:57:05 ", "Title": "Self-supervised novel 2D view synthesis of large-scale scenes with efficient multi-scale voxel carving", "Authors": ["Alexandra Budisteanu", "Dragos Costea", "Alina Marcu and Marius Leordeanu"], "Categories": "cs.CV cs.LG cs.RO", "Comments": ["11 pages", "3 figures"]}, "abstract": "The task of generating novel views of real scenes is increasingly important nowadays when AI models become able to create realistic new worlds. In many practical applications, it is important for novel view synthesis methods to stay grounded in the physical world as much as possible, while also being able to imagine it from previously unseen views. While most current methods are developed and tested in virtual environments with small scenes and no errors in pose and depth information, we push the boundaries to the real-world domain of large scales in the new context of UAVs. Our algorithmic contributions are two folds. First, we manage to stay anchored in the real 3D world, by introducing an efficient multi-scale voxel carving method, which is able to accommodate significant noises in pose, depth, and illumination variations, while being able to reconstruct the view of the world from drastically different poses at test time. Second, our final high-resolution output is efficiently self-trained on data automatically generated by the voxel carving module, which gives it the flexibility to adapt efficiently to any scene. We demonstrated the effectiveness of our method on highly complex and large-scale scenes in real environments while outperforming the current state-of-the-art. Our code is publicly available: https://github.com/onorabil/MSVC.", "url": "https://arxiv.org/abs/2306.14709"}, {"metadata": {"arXiv": "2306.14810", "Date": "Mon, 26 Jun 2023 16:12:35 ", "Title": "Robust Wind Turbine Blade Segmentation from RGB Images in the Wild", "Authors": ["Ra\\\"ul P\\'erez-Gonzalo", "Andreas Espersen", "Antonio Agudo"], "Categories": "cs.CV cs.LG eess.IV", "Comments": ["Accepted to ICIP 2023"]}, "abstract": "With the relentless growth of the wind industry, there is an imperious need to design automatic data-driven solutions for wind turbine maintenance. As structural health monitoring mainly relies on visual inspections, the first stage in any automatic solution is to identify the blade region on the image. Thus, we propose a novel segmentation algorithm that strengthens the U-Net results by a tailored loss, which pools the focal loss with a contiguity regularization term. To attain top performing results, a set of additional steps are proposed to ensure a reliable, generic, robust and efficient algorithm. First, we leverage our prior knowledge on the images by filling the holes enclosed by temporarily-classified blade pixels and by the image boundaries. Subsequently, the mislead classified pixels are successfully amended by training an on-the-fly random forest. Our algorithm demonstrates its effectiveness reaching a non-trivial 97.39% of accuracy.", "url": "https://arxiv.org/abs/2306.14810"}, {"metadata": {"arXiv": "2306.14891", "Date": "Mon, 26 Jun 2023 17:58:00 ", "Title": "Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction", "Authors": ["Majed El Helou"], "Categories": "cs.CV cs.LG eess.IV"}, "abstract": "Image diffusion has recently shown remarkable performance in image synthesis and implicitly as an image prior. Such a prior has been used with conditioning to solve the inpainting problem, but only supporting binary user-based conditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion priors can be exploited with controllable strength. Our fuzzy conditioning can be applied pixel-wise, enabling the modification of different image components to varying degrees. Additionally, we propose an application to facial image correction, where we combine our fuzzy-conditioned diffusion with diffusion-derived attention maps. Our map estimates the degree of anomaly, and we obtain it by projecting on the diffusion space. We show how our approach also leads to interpretable and autonomous facial image correction.", "url": "https://arxiv.org/abs/2306.14891"}, {"metadata": {"arXiv": "2306.13673", "Date": "Mon, 19 Jun 2023 03:03:44 ", "Title": "Taming the Exponential Action Set: Sublinear Regret and Fast Convergence to Nash Equilibrium in Online Congestion Games", "Authors": ["Jing Dong", "Jingyu Wu", "Siwei Wang", "Baoxiang Wang", "Wei Chen"], "Categories": "cs.GT cs.LG stat.ML"}, "abstract": "The congestion game is a powerful model that encompasses a range of engineering systems such as traffic networks and resource allocation. It describes the behavior of a group of agents who share a common set of $F$ facilities and take actions as subsets with $k$ facilities. In this work, we study the online formulation of congestion games, where agents participate in the game repeatedly and observe feedback with randomness. We propose CongestEXP, a decentralized algorithm that applies the classic exponential weights method. By maintaining weights on the facility level, the regret bound of CongestEXP avoids the exponential dependence on the size of possible facility sets, i.e., $\\binom{F}{k} \\approx F^k$, and scales only linearly with $F$. Specifically, we show that CongestEXP attains a regret upper bound of $O(kF\\sqrt{T})$ for every individual player, where $T$ is the time horizon. On the other hand, exploiting the exponential growth of weights enables CongestEXP to achieve a fast convergence rate. If a strict Nash equilibrium exists, we show that CongestEXP can converge to the strict Nash policy almost exponentially fast in $O(F\\exp(-t^{1-\\alpha}))$, where $t$ is the number of iterations and $\\alpha \\in (1/2, 1)$.", "url": "https://arxiv.org/abs/2306.13673"}, {"metadata": {"arXiv": "2306.14670", "Date": "Mon, 26 Jun 2023 13:06:34 ", "Title": "Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition", "Authors": ["Meena Jagadeesan", "Michael I. Jordan", "Jacob Steinhardt", "Nika Haghtalab"], "Categories": "cs.GT cs.CY cs.LG stat.ML"}, "abstract": "As the scale of machine learning models increases, trends such as scaling laws anticipate consistent downstream improvements in predictive accuracy. However, these trends take the perspective of a single model-provider in isolation, while in reality providers often compete with each other for users. In this work, we demonstrate that competition can fundamentally alter the behavior of these scaling trends, even causing overall predictive accuracy across users to be non-monotonic or decreasing with scale. We define a model of competition for classification tasks, and use data representations as a lens for studying the impact of increases in scale. We find many settings where improving data representation quality (as measured by Bayes risk) decreases the overall predictive accuracy across users (i.e., social welfare) for a marketplace of competing model-providers. Our examples range from closed-form formulas in simple settings to simulations with pretrained representations on CIFAR-10. At a conceptual level, our work suggests that favorable scaling trends for individual model-providers need not translate to downstream improvements in social welfare in marketplaces with multiple model providers.", "url": "https://arxiv.org/abs/2306.14670"}, {"metadata": {"arXiv": "2306.14858", "Date": "Mon, 26 Jun 2023 17:10:10 ", "Title": "Proportional Aggregation of Preferences for Sequential Decision Making", "Authors": ["Nikhil Chandak", "Shashwat Goel", "Dominik Peters"], "Categories": "cs.GT cs.LG", "Comments": ["35 pages"]}, "abstract": "We study the problem of fair sequential decision making given voter preferences. In each round, a decision rule must choose a decision from a set of alternatives where each voter reports which of these alternatives they approve. Instead of going with the most popular choice in each round, we aim for proportional representation. We formalize this aim using axioms based on Proportional Justified Representation (PJR), which were proposed in the literature on multi-winner voting and were recently adapted to multi-issue decision making. The axioms require that every group of $\\alpha\\%$ of the voters, if it agrees in every round (i.e., approves a common alternative), then those voters must approve at least $\\alpha\\%$ of the decisions. A stronger version of the axioms requires that every group of $\\alpha\\%$ of the voters that agrees in a $\\beta$ fraction of rounds must approve $\\beta\\cdot\\alpha\\%$ of the decisions. We show that three attractive voting rules satisfy axioms of this style. One of them (Sequential Phragm\\'en) makes its decisions online, and the other two satisfy strengthened versions of the axioms but make decisions semi-online (Method of Equal Shares) or fully offline (Proportional Approval Voting). The first two are polynomial-time computable, and the latter is based on an NP-hard optimization, but it admits a polynomial-time local search algorithm that satisfies the same axiomatic properties. We present empirical results about the performance of these rules based on synthetic data and U.S. political elections. We also run experiments where votes are cast by preference models trained on user responses from the moral machine dataset about ethical dilemmas.", "url": "https://arxiv.org/abs/2306.14858"}, {"metadata": {"arXiv": "2306.13690", "Date": "Thu, 22 Jun 2023 19:59:54 ", "Title": "Prediction of Deep Ice Layer Thickness Using Adaptive Recurrent Graph Neural Networks", "Authors": ["Benjamin Zalatan", "Maryam Rahnemoonfar"], "Categories": "cs.LG eess.SP", "Comments": ["Accepted to ICIP 2023. 5 pages", "1 figure", "1 table. arXiv admin note: substantial text overlap with arXiv:2302.00817. text overlap with arXiv:2306.13181"]}, "abstract": "As we deal with the effects of climate change and the increase of global atmospheric temperatures, the accurate tracking and prediction of ice layers within polar ice sheets grows in importance. Studying these ice layers reveals climate trends, how snowfall has changed over time, and the trajectory of future climate and precipitation. In this paper, we propose a machine learning model that uses adaptive, recurrent graph convolutional networks to, when given the amount of snow accumulation in recent years gathered through airborne radar data, predict historic snow accumulation by way of the thickness of deep ice layers. We found that our model performs better and with greater consistency than our previous model as well as equivalent non-temporal, non-geometric, and non-adaptive models.", "url": "https://arxiv.org/abs/2306.13690"}, {"metadata": {"arXiv": "2306.13724", "Date": "Fri, 23 Jun 2023 18:13:15 ", "Title": "Review of compressed embedding layers and their applications for recommender systems", "Authors": ["Tamas Hajgato"], "Categories": "cs.LG", "Comments": ["9 pages", "1 figure"], "ACM-class": "I.2.6"}, "abstract": "We review the literature on trainable, compressed embedding layers and discuss their applicability for compressing gigantic neural recommender systems. We also report the results we measured with our compressed embedding layers.", "url": "https://arxiv.org/abs/2306.13724"}, {"metadata": {"arXiv": "2306.13738", "Date": "Fri, 23 Jun 2023 18:57:14 ", "Title": "Multi-Target Multiplicity: Flexibility and Fairness in Target Specification under Resource Constraints", "Authors": ["Jamelle Watson-Daniels", "Solon Barocas", "Jake M. Hofman", "Alexandra Chouldechova"], "Categories": "cs.LG cs.CY", "Comments": ["Conference on Fairness", "Accountability", "and Transparency (FAccT '23)", "June 12-15", "2023", "Chicago", "IL", "USA"], "Journal-ref": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, June 2023, Pages 297-311", "DOI": "10.1145/3593013.3593998"}, "abstract": "Prediction models have been widely adopted as the basis for decision-making in domains as diverse as employment, education, lending, and health. Yet, few real world problems readily present themselves as precisely formulated prediction tasks. In particular, there are often many reasonable target variable options. Prior work has argued that this is an important and sometimes underappreciated choice, and has also shown that target choice can have a significant impact on the fairness of the resulting model. However, the existing literature does not offer a formal framework for characterizing the extent to which target choice matters in a particular task. Our work fills this gap by drawing connections between the problem of target choice and recent work on predictive multiplicity. Specifically, we introduce a conceptual and computational framework for assessing how the choice of target affects individuals' outcomes and selection rate disparities across groups. We call this multi-target multiplicity. Along the way, we refine the study of single-target multiplicity by introducing notions of multiplicity that respect resource constraints -- a feature of many real-world tasks that is not captured by existing notions of predictive multiplicity. We apply our methods on a healthcare dataset, and show that the level of multiplicity that stems from target variable choice can be greater than that stemming from nearly-optimal models of a single target.", "url": "https://arxiv.org/abs/2306.13738"}, {"metadata": {"arXiv": "2306.13750", "Date": "Fri, 23 Jun 2023 19:15:43 ", "Title": "Analyzing scRNA-seq data by CCP-assisted UMAP and t-SNE", "Authors": ["Yuta Hozumi", "Gu-Wei Wei"], "Categories": "cs.LG"}, "abstract": "Single-cell RNA sequencing (scRNA-seq) is widely used to reveal heterogeneity in cells, which has given us insights into cell-cell communication, cell differentiation, and differential gene expression. However, analyzing scRNA-seq data is a challenge due to sparsity and the large number of genes involved. Therefore, dimensionality reduction and feature selection are important for removing spurious signals and enhancing downstream analysis. Correlated clustering and projection (CCP) was recently introduced as an effective method for preprocessing scRNA-seq data. CCP utilizes gene-gene correlations to partition the genes and, based on the partition, employs cell-cell interactions to obtain super-genes. Because CCP is a data-domain approach that does not require matrix diagonalization, it can be used in many downstream machine learning tasks. In this work, we utilize CCP as an initialization tool for uniform manifold approximation and projection (UMAP) and t-distributed stochastic neighbor embedding (t-SNE). By using eight publicly available datasets, we have found that CCP significantly improves UMAP and t-SNE visualization and dramatically improve their accuracy.", "url": "https://arxiv.org/abs/2306.13750"}, {"metadata": {"arXiv": "2306.13753", "Date": "Fri, 23 Jun 2023 19:21:13 ", "Title": "Four Axiomatic Characterizations of the Integrated Gradients Attribution Method", "Authors": ["Daniel Lundstrom", "Meisam Razaviyayn"], "Categories": "cs.LG"}, "abstract": "Deep neural networks have produced significant progress among machine learning models in terms of accuracy and functionality, but their inner workings are still largely unknown. Attribution methods seek to shine a light on these \"black box\" models by indicating how much each input contributed to a model's outputs. The Integrated Gradients (IG) method is a state of the art baseline attribution method in the axiomatic vein, meaning it is designed to conform to particular principles of attributions. We present four axiomatic characterizations of IG, establishing IG as the unique method to satisfy different sets of axioms among a class of attribution methods.", "url": "https://arxiv.org/abs/2306.13753"}, {"metadata": {"arXiv": "2306.13759", "Date": "Fri, 23 Jun 2023 19:46:02 ", "Title": "Incremental Profit per Conversion: a Response Transformation for Uplift Modeling in E-Commerce Promotions", "Authors": ["Hugo Manuel Proen\\c{c}a", "Felipe Moraes"], "Categories": "cs.LG"}, "abstract": "Promotions play a crucial role in e-commerce platforms, and various cost structures are employed to drive user engagement. This paper focuses on promotions with response-dependent costs, where expenses are incurred only when a purchase is made. Such promotions include discounts and coupons. While existing uplift model approaches aim to address this challenge, these approaches often necessitate training multiple models, like meta-learners, or encounter complications when estimating profit due to zero-inflated values stemming from non-converted individuals with zero cost and profit. To address these challenges, we introduce Incremental Profit per Conversion (IPC), a novel uplift measure of promotional campaigns' efficiency in unit economics. Through a proposed response transformation, we demonstrate that IPC requires only converted data, its propensity, and a single model to be estimated. As a result, IPC resolves the issues mentioned above while mitigating the noise typically associated with the class imbalance in conversion datasets and biases arising from the many-to-one mapping between search and purchase data. Lastly, we validate the efficacy of our approach by presenting results obtained from a synthetic simulation of a discount coupon campaign.", "url": "https://arxiv.org/abs/2306.13759"}, {"metadata": {"arXiv": "2306.13773", "Date": "Fri, 23 Jun 2023 20:09:01 ", "Title": "Nearest Neighbour with Bandit Feedback", "Authors": ["Stephen Pasteris", "Chris Hicks", "Vasilios Mavroudis"], "Categories": "cs.LG"}, "abstract": "In this paper we adapt the nearest neighbour rule to the contextual bandit problem. Our algorithm handles the fully adversarial setting in which no assumptions at all are made about the data-generation process. When combined with a sufficiently fast data-structure for (perhaps approximate) adaptive nearest neighbour search, such as a navigating net, our algorithm is extremely efficient - having a per trial running time polylogarithmic in both the number of trials and actions, and taking only quasi-linear space.", "url": "https://arxiv.org/abs/2306.13773"}, {"metadata": {"arXiv": "2306.13793", "Date": "Fri, 23 Jun 2023 21:40:24 ", "Title": "QNNRepair: Quantized Neural Network Repair", "Authors": ["Xidan Song", "Youcheng Sun", "Mustafa A. Mustafa", "and Lucas C. Cordeiro"], "Categories": "cs.LG cs.NE"}, "abstract": "We present QNNRepair, the first method in the literature for repairing quantized neural networks (QNNs). QNNRepair aims to improve the accuracy of a neural network model after quantization. It accepts the full-precision and weight-quantized neural networks and a repair dataset of passing and failing tests. At first, QNNRepair applies a software fault localization method to identify the neurons that cause performance degradation during neural network quantization. Then, it formulates the repair problem into a linear programming problem of solving neuron weights parameters, which corrects the QNN's performance on failing tests while not compromising its performance on passing tests. We evaluate QNNRepair with widely used neural network architectures such as MobileNetV2, ResNet, and VGGNet on popular datasets, including high-resolution images. We also compare QNNRepair with the state-of-the-art data-free quantization method SQuant. According to the experiment results, we conclude that QNNRepair is effective in improving the quantized model's performance in most cases. Its repaired models have 24% higher accuracy than SQuant's in the independent validation set, especially for the ImageNet dataset.", "url": "https://arxiv.org/abs/2306.13793"}, {"metadata": {"arXiv": "2306.13796", "Date": "Fri, 23 Jun 2023 22:05:08 ", "Title": "On Learning Latent Models with Multi-Instance Weak Supervision", "Authors": ["Kaifu Wang", "Efi Tsamoura", "Dan Roth"], "Categories": "cs.LG stat.ML"}, "abstract": "We consider a weakly supervised learning scenario where the supervision signal is generated by a transition function $\\sigma$ of labels associated with multiple input instances. We formulate this problem as \\emph{multi-instance Partial Label Learning (multi-instance PLL)}, which is an extension to the standard PLL problem. Our problem is met in different fields, including latent structural learning and neuro-symbolic integration. Despite the existence of many learning techniques, limited theoretical analysis has been dedicated to this problem. In this paper, we provide the first theoretical study of multi-instance PLL with possibly an unknown transition $\\sigma$. Our main contributions are as follows. Firstly, we propose a necessary and sufficient condition for the learnability of the problem. This condition non-trivially generalizes and relaxes the existing small ambiguity degree in the PLL literature, since we allow the transition to be deterministic. Secondly, we derive Rademacher-style error bounds based on a top-$k$ surrogate loss that is widely used in the neuro-symbolic literature. Furthermore, we conclude with empirical experiments for learning under unknown transitions. The empirical results align with our theoretical findings; however, they also expose the issue of scalability in the weak supervision literature.", "url": "https://arxiv.org/abs/2306.13796"}, {"metadata": {"arXiv": "2306.13800", "Date": "Fri, 23 Jun 2023 22:22:33 ", "Title": "A First Order Meta Stackelberg Method for Robust Federated Learning", "Authors": ["Yunian Pan", "Tao Li", "Henger Li", "Tianyi Xu", "Zizhan Zheng", "and Quanyan Zhu"], "Categories": "cs.LG cs.GT", "Comments": ["Accepted to ICML 2023 Workshop on The 2nd New Frontiers In Adversarial Machine Learning. arXiv admin note: substantial text overlap with arXiv:2306.13273"]}, "abstract": "Previous research has shown that federated learning (FL) systems are exposed to an array of security risks. Despite the proposal of several defensive strategies, they tend to be non-adaptive and specific to certain types of attacks, rendering them ineffective against unpredictable or adaptive threats. This work models adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) to capture the defender's incomplete information of various attack types. We propose meta-Stackelberg learning (meta-SL), a provably efficient meta-learning algorithm, to solve the equilibrium strategy in BSMG, leading to an adaptable FL defense. We demonstrate that meta-SL converges to the first-order $\\varepsilon$-equilibrium point in $O(\\varepsilon^{-2})$ gradient iterations, with $O(\\varepsilon^{-4})$ samples needed per iteration, matching the state of the art. Empirical evidence indicates that our meta-Stackelberg framework performs exceptionally well against potent model poisoning and backdoor attacks of an uncertain nature.", "url": "https://arxiv.org/abs/2306.13800"}, {"metadata": {"arXiv": "2306.13812", "Date": "Fri, 23 Jun 2023 23:19:21 ", "Title": "Maintaining Plasticity in Deep Continual Learning", "Authors": ["Shibhansh Dohare", "J. Fernando Hernandez-Garcia", "Parash Rahman", "Richard S. Sutton", "A. Rupam Mahmood"], "Categories": "cs.LG"}, "abstract": "Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail catastrophically to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to adapt to new data, a phenomenon called \\textit{loss of plasticity}. We show loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89% correct on an early task down to 77%, or to about the level of a linear network, on the 2000th task. Such loss of plasticity occurred with a wide range of deep network architectures, optimizers, and activation functions, and was not eased by batch normalization or dropout. In our experiments, loss of plasticity was correlated with the proliferation of dead units, with very large weights, and more generally with a loss of unit diversity. Loss of plasticity was substantially eased by $L^2$-regularization, particularly when combined with weight perturbation (Shrink and Perturb). We show that plasticity can be fully maintained by a new algorithm -- called $\\textit{continual backpropagation}$ -- which is just like conventional backpropagation except that a small fraction of less-used units are reinitialized after each example.", "url": "https://arxiv.org/abs/2306.13812"}, {"metadata": {"arXiv": "2306.13814", "Date": "Fri, 23 Jun 2023 23:25:34 ", "Title": "BatchGNN: Efficient CPU-Based Distributed GNN Training on Very Large Graphs", "Authors": ["Loc Hoang", "Rita Brugarolas Brufau", "Ke Ding", "Bo Wu"], "Categories": "cs.LG cs.DC", "Comments": ["Edited preprint of a conference submission"]}, "abstract": "We present BatchGNN, a distributed CPU system that showcases techniques that can be used to efficiently train GNNs on terabyte-sized graphs. It reduces communication overhead with macrobatching in which multiple minibatches' subgraph sampling and feature fetching are batched into one communication relay to reduce redundant feature fetches when input features are static. BatchGNN provides integrated graph partitioning and native GNN layer implementations to improve runtime, and it can cache aggregated input features to further reduce sampling overhead. BatchGNN achieves an average $3\\times$ speedup over DistDGL on three GNN models trained on OGBN graphs, outperforms the runtimes reported by distributed GPU systems $P^3$ and DistDGLv2, and scales to a terabyte-sized graph.", "url": "https://arxiv.org/abs/2306.13814"}, {"metadata": {"arXiv": "2306.13815", "Date": "Fri, 23 Jun 2023 23:29:05 ", "Title": "Upscaling Global Hourly GPP with Temporal Fusion Transformer (TFT)", "Authors": ["Rumi Nakagawa", "Mary Chau", "John Calzaretta", "Trevor Keenan", "Puya Vahabi", "Alberto Todeschini", "Maoya Bassiouni", "Yanghui Kang"], "Categories": "cs.LG", "Comments": ["Accepted Oral Presentation at CVPR 2023 MultiEarth Workshop"]}, "abstract": "Reliable estimates of Gross Primary Productivity (GPP), crucial for evaluating climate change initiatives, are currently only available from sparsely distributed eddy covariance tower sites. This limitation hampers access to reliable GPP quantification at regional to global scales. Prior machine learning studies on upscaling \\textit{in situ} GPP to global wall-to-wall maps at sub-daily time steps faced limitations such as lack of input features at higher temporal resolutions and significant missing values. This research explored a novel upscaling solution using Temporal Fusion Transformer (TFT) without relying on past GPP time series. Model development was supplemented by Random Forest Regressor (RFR) and XGBoost, followed by the hybrid model of TFT and tree algorithms. The best preforming model yielded to model performance of 0.704 NSE and 3.54 RMSE. Another contribution of the study was the breakdown analysis of encoder feature importance based on time and flux tower sites. Such analysis enhanced the interpretability of the multi-head attention layer as well as the visual understanding of temporal dynamics of influential features.", "url": "https://arxiv.org/abs/2306.13815"}, {"metadata": {"arXiv": "2306.13826", "Date": "Sat, 24 Jun 2023 00:39:12 ", "Title": "Generalised $f$-Mean Aggregation for Graph Neural Networks", "Authors": ["Ryan Kortvelesy", "Steven Morad", "Amanda Prorok"], "Categories": "cs.LG"}, "abstract": "Graph Neural Network (GNN) architectures are defined by their implementations of update and aggregation modules. While many works focus on new ways to parametrise the update modules, the aggregation modules receive comparatively little attention. Because it is difficult to parametrise aggregation functions, currently most methods select a \"standard aggregator\" such as $\\mathrm{mean}$, $\\mathrm{sum}$, or $\\mathrm{max}$. While this selection is often made without any reasoning, it has been shown that the choice in aggregator has a significant impact on performance, and the best choice in aggregator is problem-dependent. Since aggregation is a lossy operation, it is crucial to select the most appropriate aggregator in order to minimise information loss. In this paper, we present GenAgg, a generalised aggregation operator, which parametrises a function space that includes all standard aggregators. In our experiments, we show that GenAgg is able to represent the standard aggregators with much higher accuracy than baseline methods. We also show that using GenAgg as a drop-in replacement for an existing aggregator in a GNN often leads to a significant boost in performance across various tasks.", "url": "https://arxiv.org/abs/2306.13826"}, {"metadata": {"arXiv": "2306.13830", "Date": "Sat, 24 Jun 2023 01:14:48 ", "Title": "Aircraft Environmental Impact Segmentation via Metric Learning", "Authors": ["Zhenyu Gao", "Dimitri N. Mavris"], "Categories": "cs.LG stat.AP", "Comments": ["22 pages", "7 figures"]}, "abstract": "Metric learning is the process of learning a tailored distance metric for a particular task. This advanced subfield of machine learning is useful to any machine learning or data mining task that relies on the computation of distances or similarities over objects. In recently years, machine learning techniques have been extensively used in aviation and aerospace engineering to make predictions, extract patterns, discover knowledge, etc. Nevertheless, metric learning, an element that can advance the performance of complex machine learning tasks, has so far been hardly utilized in relevant literature. In this study, we apply classic metric learning formulations with novel components on aviation environmental impact modeling. Through a weakly-supervised metric learning task, we achieve significant improvement in the newly emerged problem of aircraft characterization and segmentation for environmental impacts. The result will enable the more efficient and accurate modeling of aircraft environmental impacts, a focal topic in sustainable aviation. This work is also a demonstration that shows the potential and value of metric learning in a wide variety of similar studies in the transportation domain.", "url": "https://arxiv.org/abs/2306.13830"}, {"metadata": {"arXiv": "2306.13831", "Date": "Sat, 24 Jun 2023 01:16:07 ", "Title": "Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks", "Authors": ["Maxime Chevalier-Boisvert", "Bolun Dai", "Mark Towers", "Rodrigo de Lazcano", "Lucas Willems", "Salem Lahlou", "Suman Pal", "Pablo Samuel Castro", "Jordan Terry"], "Categories": "cs.LG"}, "abstract": "We present the Minigrid and Miniworld libraries which provide a suite of goal-oriented 2D and 3D environments. The libraries were explicitly created with a minimalistic design paradigm to allow users to rapidly develop new environments for a wide range of research-specific needs. As a result, both have received widescale adoption by the RL community, facilitating research in a wide range of areas. In this paper, we outline the design philosophy, environment details, and their world generation API. We also showcase the additional capabilities brought by the unified API between Minigrid and Miniworld through case studies on transfer learning (for both RL agents and humans) between the different observation spaces. The source code of Minigrid and Miniworld can be found at https://github.com/Farama-Foundation/{Minigrid, Miniworld} along with their documentation at https://{minigrid, miniworld}.farama.org/.", "url": "https://arxiv.org/abs/2306.13831"}, {"metadata": {"arXiv": "2306.13853", "Date": "Sat, 24 Jun 2023 03:57:26 ", "Title": "A Unified Approach to Controlling Implicit Regularization via Mirror Descent", "Authors": ["Haoyuan Sun", "Khashayar Gatmiry", "Kwangjun Ahn", "Navid Azizan"], "Categories": "cs.LG", "Comments": ["arXiv admin note: substantial text overlap with arXiv:2205.12808"]}, "abstract": "Inspired by the remarkable success of deep neural networks, there has been significant interest in understanding the generalization performance of overparameterized models. Substantial efforts have been invested in characterizing how optimization algorithms impact generalization through their \"preferred\" solutions, a phenomenon commonly referred to as implicit regularization. In particular, it has been argued that gradient descent (GD) induces an implicit $\\ell_2$-norm regularization in regression and classification problems. However, the implicit regularization of different algorithms are confined to either a specific geometry or a particular class of learning problems, indicating a gap in a general approach for controlling the implicit regularization. To address this, we present a unified approach using mirror descent (MD), a notable generalization of GD, to control implicit regularization in both regression and classification settings. More specifically, we show that MD with the general class of homogeneous potential functions converges in direction to a generalized maximum-margin solution for linear classification problems, thereby answering a long-standing question in the classification setting. Further, we show that MD can be implemented efficiently and under suitable conditions, enjoys fast convergence. Through comprehensive experiments, we demonstrate that MD is a versatile method to produce learned models with different regularizers, which in turn have different generalization performances.", "url": "https://arxiv.org/abs/2306.13853"}, {"metadata": {"arXiv": "2306.13866", "Date": "Sat, 24 Jun 2023 05:10:43 ", "Title": "MIRACLE: Multi-task Learning based Interpretable Regulation of Autoimmune Diseases through Common Latent Epigenetics", "Authors": ["Pengcheng Xu", "Jinpu Cai", "Yulin Gao", "Ziqi Rong", "Hongyi Xin"], "Categories": "cs.LG"}, "abstract": "DNA methylation is a crucial regulator of gene transcription and has been linked to various diseases, including autoimmune diseases and cancers. However, diagnostics based on DNA methylation face challenges due to large feature sets and small sample sizes, resulting in overfitting and suboptimal performance. To address these issues, we propose MIRACLE, a novel interpretable neural network that leverages autoencoder-based multi-task learning to integrate multiple datasets and jointly identify common patterns in DNA methylation. MIRACLE's architecture reflects the relationships between methylation sites, genes, and pathways, ensuring biological interpretability and meaningfulness. The network comprises an encoder and a decoder, with a bottleneck layer representing pathway information as the basic unit of heredity. Customized defined MaskedLinear Layer is constrained by site-gene-pathway graph adjacency matrix information, which provides explainability and expresses the site-gene-pathway hierarchical structure explicitly. And from the embedding, there are different multi-task classifiers to predict diseases. Tested on six datasets, including rheumatoid arthritis, systemic lupus erythematosus, multiple sclerosis, inflammatory bowel disease, psoriasis, and type 1 diabetes, MIRACLE demonstrates robust performance in identifying common functions of DNA methylation across different phenotypes, with higher accuracy in prediction dieseases than baseline methods. By incorporating biological prior knowledge, MIRACLE offers a meaningful and interpretable framework for DNA methylation data analysis in the context of autoimmune diseases.", "url": "https://arxiv.org/abs/2306.13866"}, {"metadata": {"arXiv": "2306.13879", "Date": "Sat, 24 Jun 2023 07:06:14 ", "Title": "Action Q-Transformer: Visual Explanation in Deep Reinforcement Learning with Encoder-Decoder Model using Action Query", "Authors": ["Hidenori Itaya", "Tsubasa Hirakawa", "Takayoshi Yamashita", "Hironobu Fujiyoshi", "Komei Sugiura"], "Categories": "cs.LG", "Comments": ["16 pages", "8 figures", "3 tables"]}, "abstract": "The excellent performance of Transformer in supervised learning has led to growing interest in its potential application to deep reinforcement learning (DRL) to achieve high performance on a wide variety of problems. However, the decision making of a DRL agent is a black box, which greatly hinders the application of the agent to real-world problems. To address this problem, we propose the Action Q-Transformer (AQT), which introduces a transformer encoder-decoder structure to Q-learning based DRL methods. In AQT, the encoder calculates the state value function and the decoder calculates the advantage function to promote the acquisition of different attentions indicating the agent's decision-making. The decoder in AQT utilizes action queries, which represent the information of each action, as queries. This enables us to obtain the attentions for the state value and for each action. By acquiring and visualizing these attentions that detail the agent's decision-making, we achieve a DRL model with high interpretability. In this paper, we show that visualization of attention in Atari 2600 games enables detailed analysis of agents' decision-making in various game tasks. Further, experimental results demonstrate that our method can achieve higher performance than the baseline in some games.", "url": "https://arxiv.org/abs/2306.13879"}, {"metadata": {"arXiv": "2306.13924", "Date": "Sat, 24 Jun 2023 10:07:52 ", "Title": "Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning", "Authors": ["Sharut Gupta", "Joshua Robinson", "Derek Lim", "Soledad Villar", "Stefanie Jegelka"], "Categories": "cs.LG cs.CV", "Comments": ["22 pages"]}, "abstract": "Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objective and theoretically prove that its minima forces augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, CARE: Contrastive Augmentation-induced Rotational Equivariance, leads to improved performance on downstream tasks, and ensures sensitivity in embedding space to important variations in data (e.g., color) that standard contrastive methods do not achieve. Code is available at https://github.com/Sharut/CARE.", "url": "https://arxiv.org/abs/2306.13924"}, {"metadata": {"arXiv": "2306.13926", "Date": "Sat, 24 Jun 2023 10:21:11 ", "Title": "Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective", "Authors": ["Wei Huang", "Yuan Cao", "Haonan Wang", "Xin Cao", "Taiji Suzuki"], "Categories": "cs.LG", "Comments": ["33 pages", "3 figures"]}, "abstract": "Graph neural networks (GNNs) have pioneered advancements in graph representation learning, exhibiting superior feature learning and performance over multilayer perceptrons (MLPs) when handling graph inputs. However, understanding the feature learning aspect of GNNs is still in its initial stage. This study aims to bridge this gap by investigating the role of graph convolution within the context of feature learning theory in neural networks using gradient descent training. We provide a distinct characterization of signal learning and noise memorization in two-layer graph convolutional networks (GCNs), contrasting them with two-layer convolutional neural networks (CNNs). Our findings reveal that graph convolution significantly augments the benign overfitting regime over the counterpart CNNs, where signal learning surpasses noise memorization, by approximately factor $\\sqrt{D}^{q-2}$, with $D$ denoting a node's expected degree and $q$ being the power of the ReLU activation function where $q > 2$. These findings highlight a substantial discrepancy between GNNs and MLPs in terms of feature learning and generalization capacity after gradient descent training, a conclusion further substantiated by our empirical simulations.", "url": "https://arxiv.org/abs/2306.13926"}, {"metadata": {"arXiv": "2306.13932", "Date": "Sat, 24 Jun 2023 10:39:44 ", "Title": "Tuning structure learning algorithms with out-of-sample and resampling strategies", "Authors": ["Kiattikun Chobtham", "Anthony C. Constantinou"], "Categories": "cs.LG stat.ML"}, "abstract": "One of the challenges practitioners face when applying structure learning algorithms to their data involves determining a set of hyperparameters; otherwise, a set of hyperparameter defaults is assumed. The optimal hyperparameter configuration often depends on multiple factors, including the size and density of the usually unknown underlying true graph, the sample size of the input data, and the structure learning algorithm. We propose a novel hyperparameter tuning method, called the Out-of-sample Tuning for Structure Learning (OTSL), that employs out-of-sample and resampling strategies to estimate the optimal hyperparameter configuration for structure learning, given the input data set and structure learning algorithm. Synthetic experiments show that employing OTSL as a means to tune the hyperparameters of hybrid and score-based structure learning algorithms leads to improvements in graphical accuracy compared to the state-of-the-art. We also illustrate the applicability of this approach to real datasets from different disciplines.", "url": "https://arxiv.org/abs/2306.13932"}, {"metadata": {"arXiv": "2306.13957", "Date": "Sat, 24 Jun 2023 13:08:55 ", "Title": "DiffDTM: A conditional structure-free framework for bioactive molecules generation targeted for dual proteins", "Authors": ["Lei Huang", "Zheng Yuan", "Huihui Yan", "Rong Sheng", "Linjing Liu", "Fuzhou Wang", "Weidun Xie", "Nanjun Chen", "Fei Huang", "Songfang Huang", "Ka-Chun Wong", "Yaoyun Zhang"], "Categories": "cs.LG q-bio.BM"}, "abstract": "Advances in deep generative models shed light on de novo molecule generation with desired properties. However, molecule generation targeted for dual protein targets still faces formidable challenges including protein 3D structure data requisition for model training, auto-regressive sampling, and model generalization for unseen targets. Here, we proposed DiffDTM, a novel conditional structure-free deep generative model based on a diffusion model for dual targets based molecule generation to address the above issues. Specifically, DiffDTM receives protein sequences and molecular graphs as inputs instead of protein and molecular conformations and incorporates an information fusion module to achieve conditional generation in a one-shot manner. We have conducted comprehensive multi-view experiments to demonstrate that DiffDTM can generate drug-like, synthesis-accessible, novel, and high-binding affinity molecules targeting specific dual proteins, outperforming the state-of-the-art (SOTA) models in terms of multiple evaluation metrics. Furthermore, we utilized DiffDTM to generate molecules towards dopamine receptor D2 and 5-hydroxytryptamine receptor 1A as new antipsychotics. The experimental results indicate that DiffDTM can be easily plugged into unseen dual targets to generate bioactive molecules, addressing the issues of requiring insufficient active molecule data for training as well as the need to retrain when encountering new targets.", "url": "https://arxiv.org/abs/2306.13957"}, {"metadata": {"arXiv": "2306.13982", "Date": "Sat, 24 Jun 2023 14:22:53 ", "Title": "Mobile-Cloud Inference for Collaborative Intelligence", "Authors": ["Mateen Ulhaq"], "Categories": "cs.LG cs.CV eess.IV", "Comments": ["56 pages", "20 figures", "Bachelor's Thesis", "defended in 2020"]}, "abstract": "As AI applications for mobile devices become more prevalent, there is an increasing need for faster execution and lower energy consumption for deep learning model inference. Historically, the models run on mobile devices have been smaller and simpler in comparison to large state-of-the-art research models, which can only run on the cloud. However, cloud-only inference has drawbacks such as increased network bandwidth consumption and higher latency. In addition, cloud-only inference requires the input data (images, audio) to be fully transferred to the cloud, creating concerns about potential privacy breaches. There is an alternative approach: shared mobile-cloud inference. Partial inference is performed on the mobile in order to reduce the dimensionality of the input data and arrive at a compact feature tensor, which is a latent space representation of the input signal. The feature tensor is then transmitted to the server for further inference. This strategy can reduce inference latency, energy consumption, and network bandwidth usage, as well as provide privacy protection, because the original signal never leaves the mobile. Further performance gain can be achieved by compressing the feature tensor before its transmission.", "url": "https://arxiv.org/abs/2306.13982"}, {"metadata": {"arXiv": "2306.13990", "Date": "Sat, 24 Jun 2023 14:50:20 ", "Title": "Cross-Validation Is All You Need: A Statistical Approach To Label Noise Estimation", "Authors": ["Jianan Chen and Anne Martel"], "Categories": "cs.LG cs.CV"}, "abstract": "Label noise is prevalent in machine learning datasets. It is crucial to identify and remove label noise because models trained on noisy data can have substantially reduced accuracy and generalizability. Most existing label noise detection approaches are designed for classification tasks, and data cleaning for outcome prediction analysis is relatively unexplored. Inspired by the fluctuations in performance across different folds in cross-validation, we propose Repeated Cross-Validations for label noise estimation (ReCoV) to address this gap. ReCoV constructs a noise histogram that ranks the noise level of samples based on a large number of cross-validations by recording sample IDs in each worst-performing fold. We further propose three approaches for identifying noisy samples based on noise histograms to address increasingly complex noise distributions. We show that ReCoV outperforms state-of-the-art algorithms for label cleaning in a classification task benchmark. More importantly, we show that removing ReCoV-identified noisy samples in two medical imaging outcome prediction datasets significantly improves model performance on test sets. As a statistical approach that does not rely on hyperparameters, noise distributions, or model structures, ReCoV is compatible with any machine learning analysis.", "url": "https://arxiv.org/abs/2306.13990"}, {"metadata": {"arXiv": "2306.13991", "Date": "Sat, 24 Jun 2023 14:52:44 ", "Title": "Kernel Support Vector Machine Classifiers with the $\\ell_0$-Norm Hinge Loss", "Authors": ["Rongrong Lin", "Yingjia Yao", "Yulan Liu"], "Categories": "cs.LG math.OC"}, "abstract": "Support Vector Machine (SVM) has been one of the most successful machine learning techniques for binary classification problems. The key idea is to maximize the margin from the data to the hyperplane subject to correct classification on training samples. The commonly used hinge loss and its variations are sensitive to label noise, and unstable for resampling due to its unboundedness. This paper is concentrated on the kernel SVM with the $\\ell_0$-norm hinge loss (referred as $\\ell_0$-KSVM), which is a composite function of hinge loss and $\\ell_0$-norm and then could overcome the difficulties mentioned above. In consideration of the nonconvexity and nonsmoothness of $\\ell_0$-norm hinge loss, we first characterize the limiting subdifferential of the $\\ell_0$-norm hinge loss and then derive the equivalent relationship among the proximal stationary point, the Karush-Kuhn-Tucker point, and the local optimal solution of $\\ell_0$-KSVM. Secondly, we develop an ADMM algorithm for $\\ell_0$-KSVM, and obtain that any limit point of the sequence generated by the proposed algorithm is a locally optimal solution. Lastly, some experiments on the synthetic and real datasets are illuminated to show that $\\ell_0$-KSVM can achieve comparable accuracy compared with the standard KSVM while the former generally enjoys fewer support vectors.", "url": "https://arxiv.org/abs/2306.13991"}, {"metadata": {"arXiv": "2306.14009", "Date": "Sat, 24 Jun 2023 15:53:38 ", "Title": "Boosting Multitask Learning on Graphs through Higher-Order Task Affinities", "Authors": ["Dongyue Li", "Haotian Ju", "Aneesh Sharma", "and Hongyang R. Zhang"], "Categories": "cs.LG cs.SI", "Comments": ["15 pages", "6 figures", "7 tables"], "DOI": "10.1145/3580305.3599265"}, "abstract": "Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss of one task in the presence of another task and a random subset of other tasks. Then, we use spectral clustering on the affinity score matrix to identify task grouping. We design several speedup techniques to compute the higher-order affinity scores efficiently and show that they can predict negative transfers more accurately than pairwise task affinities. We validate our procedure using various community detection and molecular graph prediction data sets, showing favorable results compared with existing methods. Lastly, we provide a theoretical analysis to show that under a planted block model of tasks on graphs, our affinity scores can provably separate tasks into groups.", "url": "https://arxiv.org/abs/2306.14009"}, {"metadata": {"arXiv": "2306.14016", "Date": "Sat, 24 Jun 2023 16:23:54 ", "Title": "Interpreting Forecasted Vital Signs Using N-BEATS in Sepsis Patients", "Authors": ["Anubhav Bhatti", "Naveen Thangavelu", "Marium Hassan", "Choongmin Kim", "San Lee", "Yonghwan Kim", "Jang Yong Kim"], "Categories": "cs.LG cs.CY", "Comments": ["Accepted in 1st World Conference on eXplainable Artificial Intelligence - Late-breaking work", "Demos and Doctoral Consortium", "2023"], "MSC-class": "68T01"}, "abstract": "Detecting and predicting septic shock early is crucial for the best possible outcome for patients. Accurately forecasting the vital signs of patients with sepsis provides valuable insights to clinicians for timely interventions, such as administering stabilizing drugs or optimizing infusion strategies. Our research examines N-BEATS, an interpretable deep-learning forecasting model that can forecast 3 hours of vital signs for sepsis patients in intensive care units (ICUs). In this work, we use the N-BEATS interpretable configuration to forecast the vital sign trends and compare them with the actual trend to understand better the patient's changing condition and the effects of infused drugs on their vital signs. We evaluate our approach using the publicly available eICU Collaborative Research Database dataset and rigorously evaluate the vital sign forecasts using out-of-sample evaluation criteria. We present the performance of our model using error metrics, including mean squared error (MSE), mean average percentage error (MAPE), and dynamic time warping (DTW), where the best scores achieved are 18.52e-4, 7.60, and 17.63e-3, respectively. We analyze the samples where the forecasted trend does not match the actual trend and study the impact of infused drugs on changing the actual vital signs compared to the forecasted trend. Additionally, we examined the mortality rates of patients where the actual trend and the forecasted trend did not match. We observed that the mortality rate was higher (92%) when the actual and forecasted trends closely matched, compared to when they were not similar (84%).", "url": "https://arxiv.org/abs/2306.14016"}, {"metadata": {"arXiv": "2306.14020", "Date": "Sat, 24 Jun 2023 17:01:51 ", "Title": "Individualized Dosing Dynamics via Neural Eigen Decomposition", "Authors": ["Stav Belogolovsky", "Ido Greenberg", "Danny Eytan", "Shie Mannor"], "Categories": "cs.LG", "Comments": ["arXiv admin note: text overlap with arXiv:2202.00117"]}, "abstract": "Dosing models often use differential equations to model biological dynamics. Neural differential equations in particular can learn to predict the derivative of a process, which permits predictions at irregular points of time. However, this temporal flexibility often comes with a high sensitivity to noise, whereas medical problems often present high noise and limited data. Moreover, medical dosing models must generalize reliably over individual patients and changing treatment policies. To address these challenges, we introduce the Neural Eigen Stochastic Differential Equation algorithm (NESDE). NESDE provides individualized modeling (using a hypernetwork over patient-level parameters); generalization to new treatment policies (using decoupled control); tunable expressiveness according to the noise level (using piecewise linearity); and fast, continuous, closed-form prediction (using spectral representation). We demonstrate the robustness of NESDE in both synthetic and real medical problems, and use the learned dynamics to publish simulated medical gym environments.", "url": "https://arxiv.org/abs/2306.14020"}, {"metadata": {"arXiv": "2306.14026", "Date": "Sat, 24 Jun 2023 17:40:40 ", "Title": "Information criteria for structured parameter selection in high dimensional tree and graph models", "Authors": ["Maarten Jansen"], "Categories": "cs.LG math.ST stat.TH"}, "abstract": "Parameter selection in high-dimensional models is typically finetuned in a way that keeps the (relative) number of false positives under control. This is because otherwise the few true positives may be dominated by the many possible false positives. This happens, for instance, when the selection follows from a naive optimisation of an information criterion, such as AIC or Mallows's Cp. It can be argued that the overestimation of the selection comes from the optimisation process itself changing the statistics of the selected variables, in a way that the information criterion no longer reflects the true divergence between the selection and the data generating process. In lasso, the overestimation can also be linked to the shrinkage estimator, which makes the selection too tolerant of false positive selections. For these reasons, this paper works on refined information criteria, carefully balancing false positives and false negatives, for use with estimators without shrinkage. In particular, the paper develops corrected Mallows's Cp criteria for structured selection in trees and graphical models.", "url": "https://arxiv.org/abs/2306.14026"}, {"metadata": {"arXiv": "2306.14048", "Date": "Sat, 24 Jun 2023 20:11:14 ", "Title": "H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models", "Authors": ["Zhenyu Zhang", "Ying Sheng", "Tianyi Zhou", "Tianlong Chen", "Lianmin Zheng", "Ruisi Cai", "Zhao Song", "Yuandong Tian", "Christopher R\\'e", "Clark Barrett", "Zhangyang Wang", "Beidi Chen"], "Categories": "cs.LG"}, "abstract": "Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O.", "url": "https://arxiv.org/abs/2306.14048"}, {"metadata": {"arXiv": "2306.14052", "Date": "Sat, 24 Jun 2023 20:20:45 ", "Title": "A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware", "Authors": ["Shichang Zhang", "Atefeh Sohrabizadeh", "Cheng Wan", "Zijie Huang", "Ziniu Hu", "Yewen Wang", "Yingyan (Celine) Lin", "Jason Cong", "Yizhou Sun"], "Categories": "cs.LG cs.AR cs.DC"}, "abstract": "Graph neural networks (GNNs) are emerging for machine learning research on graph-structured data. GNNs achieve state-of-the-art performance on many tasks, but they face scalability challenges when it comes to real-world applications that have numerous data and strict latency requirements. Many studies have been conducted on how to accelerate GNNs in an effort to address these challenges. These acceleration techniques touch on various aspects of the GNN pipeline, from smart training and inference algorithms to efficient systems and customized hardware. As the amount of research on GNN acceleration has grown rapidly, there lacks a systematic treatment to provide a unified view and address the complexity of relevant works. In this survey, we provide a taxonomy of GNN acceleration, review the existing approaches, and suggest future research directions. Our taxonomic treatment of GNN acceleration connects the existing works and sets the stage for further development in this area.", "url": "https://arxiv.org/abs/2306.14052"}, {"metadata": {"arXiv": "2306.14054", "Date": "Sat, 24 Jun 2023 20:46:33 ", "Title": "Towards Understanding Gradient Approximation in Equality Constrained Deep Declarative Networks", "Authors": ["Stephen Gould", "Ming Xu", "Zhiwei Xu", "Yanbin Liu"], "Categories": "cs.LG", "Comments": ["10 pages", "4 figures", "ICML 2023 workshop on Differentiable Almost Everything"]}, "abstract": "We explore conditions for when the gradient of a deep declarative node can be approximated by ignoring constraint terms and still result in a descent direction for the global loss function. This has important practical application when training deep learning models since the approximation is often computationally much more efficient than the true gradient calculation. We provide theoretical analysis for problems with linear equality constraints and normalization constraints, and show examples where the approximation works well in practice as well as some cautionary tales for when it fails.", "url": "https://arxiv.org/abs/2306.14054"}, {"metadata": {"arXiv": "2306.14064", "Date": "Sat, 24 Jun 2023 21:50:53 ", "Title": "Modeling Graphs Beyond Hyperbolic: Graph Neural Networks in Symmetric Positive Definite Matrices", "Authors": ["Wei Zhao", "Federico Lopez", "J. Maxwell Riestenberg", "Michael Strube", "Diaaeldin Taha", "Steve Trettel"], "Categories": "cs.LG", "Comments": ["ECML2023 Camera Ready"]}, "abstract": "Recent research has shown that alignment between the structure of graph data and the geometry of an embedding space is crucial for learning high-quality representations of the data. The uniform geometry of Euclidean and hyperbolic spaces allows for representing graphs with uniform geometric and topological features, such as grids and hierarchies, with minimal distortion. However, real-world graph data is characterized by multiple types of geometric and topological features, necessitating more sophisticated geometric embedding spaces. In this work, we utilize the Riemannian symmetric space of symmetric positive definite matrices (SPD) to construct graph neural networks that can robustly handle complex graphs. To do this, we develop an innovative library that leverages the SPD gyrocalculus tools \\cite{lopez2021gyroSPD} to implement the building blocks of five popular graph neural networks in SPD. Experimental results demonstrate that our graph neural networks in SPD substantially outperform their counterparts in Euclidean and hyperbolic spaces, as well as the Cartesian product thereof, on complex graphs for node and graph classification tasks. We release the library and datasets at \\url{https://github.com/andyweizhao/SPD4GNNs}.", "url": "https://arxiv.org/abs/2306.14064"}, {"metadata": {"arXiv": "2306.14066", "Date": "Sat, 24 Jun 2023 22:00:06 ", "Title": "SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models", "Authors": ["Lizao Li", "Rob Carver", "Ignacio Lopez-Gomez", "Fei Sha", "John Anderson"], "Categories": "cs.LG physics.ao-ph"}, "abstract": "Probabilistic forecasting is crucial to decision-making under uncertainty about future weather. The dominant approach is to use an ensemble of forecasts to represent and quantify uncertainty in operational numerical weather prediction. However, generating ensembles is computationally costly. In this paper, we propose to generate ensemble forecasts at scale by leveraging recent advances in generative artificial intelligence. Our approach learns a data-driven probabilistic diffusion model from the 5-member ensemble GEFS reforecast dataset. The model can then be sampled efficiently to produce realistic weather forecasts, conditioned on a few members of the operational GEFS forecasting system. The generated ensembles have similar predictive skill as the full GEFS 31-member ensemble, evaluated against ERA5 reanalysis, and emulate well the statistics of large physics-based ensembles. We also apply the same methodology to developing a diffusion model for generative post-processing: the model directly learns to correct biases present in the emulated forecasting system by leveraging reanalysis data as labels during training. Ensembles from this generative post-processing model show greater reliability and accuracy, particularly in extreme event classification. In general, they are more reliable and forecast the probability of extreme weather more accurately than the GEFS operational ensemble. Our models achieve these results at less than 1/10th of the computational cost incurred by the operational GEFS system.", "url": "https://arxiv.org/abs/2306.14066"}, {"metadata": {"arXiv": "2306.14069", "Date": "Sat, 24 Jun 2023 22:25:29 ", "Title": "Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets", "Authors": ["Anirudhan Badrinath and Yannis Flet-Berliac and Allen Nie and Emma Brunskill"], "Categories": "cs.LG"}, "abstract": "Despite the recent advancements in offline reinforcement learning via supervised learning (RvS) and the success of the decision transformer (DT) architecture in various domains, DTs have fallen short in several challenging benchmarks. The root cause of this underperformance lies in their inability to seamlessly connect segments of suboptimal trajectories. To overcome this limitation, we present a novel approach to enhance RvS methods by integrating intermediate targets. We introduce the Waypoint Transformer (WT), using an architecture that builds upon the DT framework and conditioned on automatically-generated waypoints. The results show a significant increase in the final return compared to existing RvS methods, with performance on par or greater than existing state-of-the-art temporal difference learning-based methods. Additionally, the performance and stability improvements are largest in the most challenging environments and data configurations, including AntMaze Large Play/Diverse and Kitchen Mixed/Partial.", "url": "https://arxiv.org/abs/2306.14069"}, {"metadata": {"arXiv": "2306.14087", "Date": "Sun, 25 Jun 2023 01:30:37 ", "Title": "A Circuit Complexity Formulation of Algorithmic Information Theory", "Authors": ["Cole Wyeth and Carl Sturtivant"], "Categories": "cs.LG cs.CC", "Comments": ["8 pages", "no figures"]}, "abstract": "Inspired by Solomonoffs theory of inductive inference, we propose a prior based on circuit complexity. There are several advantages to this approach. First, it relies on a complexity measure that does not depend on the choice of UTM. There is one universal definition for Boolean circuits involving an universal operation such as nand with simple conversions to alternative definitions such as and, or, and not. Second, there is no analogue of the halting problem. The output value of a circuit can be calculated recursively by computer in time proportional to the number of gates, while a short program may run for a very long time. Our prior assumes that a Boolean function, or equivalently, Boolean string of fixed length, is generated by some Bayesian mixture of circuits. This model is appropriate for learning Boolean functions from partial information, a problem often encountered within machine learning as \"binary classification.\" We argue that an inductive bias towards simple explanations as measured by circuit complexity is appropriate for this problem.", "url": "https://arxiv.org/abs/2306.14087"}, {"metadata": {"arXiv": "2306.14088", "Date": "Sun, 25 Jun 2023 01:31:54 ", "Title": "Private Aggregation in Wireless Federated Learning with Heterogeneous Clusters", "Authors": ["Maximilian Egger", "Christoph Hofmeister", "Antonia Wachter-Zeh and Rawad Bitar"], "Categories": "cs.LG cs.CR cs.DC stat.ML"}, "abstract": "Federated learning collaboratively trains a neural network on privately owned data held by several participating clients. The gradient descent algorithm, a well-known and popular iterative optimization procedure, is run to train the neural network. Every client uses its local data to compute partial gradients and sends it to the federator which aggregates the results. Privacy of the clients' data is a major concern. In fact, observing the partial gradients can be enough to reveal the clients' data. Private aggregation schemes have been investigated to tackle the privacy problem in federated learning where all the users are connected to each other and to the federator. In this paper, we consider a wireless system architecture where clients are only connected to the federator via base stations. We derive fundamental limits on the communication cost when information-theoretic privacy is required, and introduce and analyze a private aggregation scheme tailored for this setting.", "url": "https://arxiv.org/abs/2306.14088"}, {"metadata": {"arXiv": "2306.14094", "Date": "Sun, 25 Jun 2023 02:05:34 ", "Title": "Locally Differentially Private Distributed Online Learning with Guaranteed Optimality", "Authors": ["Ziqin Chen and Yongqiang Wang"], "Categories": "cs.LG cs.CR cs.MA", "Comments": ["20 pages", "7 figures"]}, "abstract": "Distributed online learning is gaining increased traction due to its unique ability to process large-scale datasets and streaming data. To address the growing public awareness and concern on privacy protection, plenty of private distributed online learning algorithms have been proposed, mostly based on differential privacy which has emerged as the ``gold standard\" for privacy protection. However, these algorithms often face the dilemma of trading learning accuracy for privacy. By exploiting the unique characteristics of online learning, this paper proposes an approach that tackles the dilemma and ensures both differential privacy and learning accuracy in distributed online learning. More specifically, while ensuring a diminishing expected instantaneous regret, the approach can simultaneously ensure a finite cumulative privacy budget, even on the infinite time horizon. To cater for the fully distributed setting, we adopt the local differential-privacy framework which avoids the reliance on a trusted data curator, and hence, provides stronger protection than the classic ``centralized\" (global) differential privacy. To the best of our knowledge, this is the first algorithm that successfully ensures both rigorous local differential privacy and learning accuracy. The effectiveness of the proposed algorithm is evaluated using machine learning tasks, including logistic regression on the ``Mushrooms\" and ``Covtype\" datasets and CNN based image classification on the ``MNIST\" and ``CIFAR-10\" datasets.", "url": "https://arxiv.org/abs/2306.14094"}, {"metadata": {"arXiv": "2306.14118", "Date": "Sun, 25 Jun 2023 03:58:15 ", "Title": "Machine Learning and Consumer Data", "Authors": ["Hannah H. Chang", "Anirban Mukherjee"], "Categories": "cs.LG"}, "abstract": "The digital revolution has led to the digitization of human behavior, creating unprecedented opportunities to understand observable actions on an unmatched scale. Emerging phenomena such as crowdfunding and crowdsourcing have further illuminated consumer behavior while also introducing new behavioral patterns. However, the sheer volume and complexity of this data present significant challenges for marketing researchers and practitioners. Traditional methods used to analyze consumer data fall short in handling the breadth, precision, and scale of emerging data sources. To address this, computational methods have been developed to manage the \"big data\" associated with consumer behavior, which typically includes structured data, textual data, audial data, and visual data. These methods, particularly machine learning, allow for effective parsing and processing of multi-faceted data. Given these recent developments, this review article seeks to familiarize researchers and practitioners with new data sources and analysis techniques for studying consumer behavior at scale. It serves as an introduction to the application of computational social science in understanding and leveraging publicly available consumer data.", "url": "https://arxiv.org/abs/2306.14118"}, {"metadata": {"arXiv": "2306.14126", "Date": "Sun, 25 Jun 2023 04:53:29 ", "Title": "Robust Spatiotemporal Traffic Forecasting with Reinforced Dynamic Adversarial Training", "Authors": ["Fan Liu and Weijia Zhang and Hao Liu"], "Categories": "cs.LG eess.SP", "Comments": ["Accepted by KDD 2023"], "DOI": "10.1145/3580305.3599492"}, "abstract": "Machine learning-based forecasting models are commonly used in Intelligent Transportation Systems (ITS) to predict traffic patterns and provide city-wide services. However, most of the existing models are susceptible to adversarial attacks, which can lead to inaccurate predictions and negative consequences such as congestion and delays. Therefore, improving the adversarial robustness of these models is crucial for ITS. In this paper, we propose a novel framework for incorporating adversarial training into spatiotemporal traffic forecasting tasks. We demonstrate that traditional adversarial training methods designated for static domains cannot be directly applied to traffic forecasting tasks, as they fail to effectively defend against dynamic adversarial attacks. Then, we propose a reinforcement learning-based method to learn the optimal node selection strategy for adversarial examples, which simultaneously strengthens the dynamic attack defense capability and reduces the model overfitting. Additionally, we introduce a self-knowledge distillation regularization module to overcome the \"forgetting issue\" caused by continuously changing adversarial nodes during training. We evaluate our approach on two real-world traffic datasets and demonstrate its superiority over other baselines. Our method effectively enhances the adversarial robustness of spatiotemporal traffic forecasting models. The source code for our framework is available at https://github.com/usail-hkust/RDAT.", "url": "https://arxiv.org/abs/2306.14126"}, {"metadata": {"arXiv": "2306.14131", "Date": "Sun, 25 Jun 2023 05:15:25 ", "Title": "Safety-Critical Scenario Generation Via Reinforcement Learning Based Editing", "Authors": ["Haolan Liu", "Liangjun Zhang", "Siva Kumar Sastry Hari", "Jishen Zhao"], "Categories": "cs.LG cs.RO"}, "abstract": "Generating safety-critical scenarios is essential for testing and verifying the safety of autonomous vehicles. Traditional optimization techniques suffer from the curse of dimensionality and limit the search space to fixed parameter spaces. To address these challenges, we propose a deep reinforcement learning approach that generates scenarios by sequential editing, such as adding new agents or modifying the trajectories of the existing agents. Our framework employs a reward function consisting of both risk and plausibility objectives. The plausibility objective leverages generative models, such as a variational autoencoder, to learn the likelihood of the generated parameters from the training datasets; It penalizes the generation of unlikely scenarios. Our approach overcomes the dimensionality challenge and explores a wide range of safety-critical scenarios. Our evaluation demonstrates that the proposed method generates safety-critical scenarios of higher quality compared with previous approaches.", "url": "https://arxiv.org/abs/2306.14131"}, {"metadata": {"arXiv": "2306.14194", "Date": "Sun, 25 Jun 2023 10:19:00 ", "Title": "Autoencoders for a manifold learning problem with a Jacobian rank constraint", "Authors": ["Rustem Takhanov", "Y. Sultan Abylkairov", "Maxat Tezekbayev"], "Categories": "cs.LG"}, "abstract": "We formulate the manifold learning problem as the problem of finding an operator that maps any point to a close neighbor that lies on a ``hidden'' $k$-dimensional manifold. We call this operator the correcting function. Under this formulation, autoencoders can be viewed as a tool to approximate the correcting function. Given an autoencoder whose Jacobian has rank $k$, we deduce from the classical Constant Rank Theorem that its range has a structure of a $k$-dimensional manifold. A $k$-dimensionality of the range can be forced by the architecture of an autoencoder (by fixing the dimension of the code space), or alternatively, by an additional constraint that the rank of the autoencoder mapping is not greater than $k$. This constraint is included in the objective function as a new term, namely a squared Ky-Fan $k$-antinorm of the Jacobian function. We claim that this constraint is a factor that effectively reduces the dimension of the range of an autoencoder, additionally to the reduction defined by the architecture. We also add a new curvature term into the objective. To conclude, we experimentally compare our approach with the CAE+H method on synthetic and real-world datasets.", "url": "https://arxiv.org/abs/2306.14194"}, {"metadata": {"arXiv": "2306.14245", "Date": "Sun, 25 Jun 2023 13:38:51 ", "Title": "FedSampling: A Better Sampling Strategy for Federated Learning", "Authors": ["Tao Qi", "Fangzhao Wu", "Lingjuan Lyu", "Yongfeng Huang", "and Xing Xie"], "Categories": "cs.LG", "Comments": ["IJCAI 2023"]}, "abstract": "Federated learning (FL) is an important technique for learning models from decentralized data in a privacy-preserving way. Existing FL methods usually uniformly sample clients for local model learning in each round. However, different clients may have significantly different data sizes, and the clients with more data cannot have more opportunities to contribute to model training, which may lead to inferior performance. In this paper, instead of client uniform sampling, we propose a novel data uniform sampling strategy for federated learning (FedSampling), which can effectively improve the performance of federated learning especially when client data size distribution is highly imbalanced across clients. In each federated learning round, local data on each client is randomly sampled for local model learning according to a probability based on the server desired sample size and the total sample size on all available clients. Since the data size on each client is privacy-sensitive, we propose a privacy-preserving way to estimate the total sample size with a differential privacy guarantee. Experiments on four benchmark datasets show that FedSampling can effectively improve the performance of federated learning.", "url": "https://arxiv.org/abs/2306.14245"}, {"metadata": {"arXiv": "2306.14257", "Date": "Sun, 25 Jun 2023 14:30:31 ", "Title": "A Self-Encoder for Learning Nearest Neighbors", "Authors": ["Armand Boschin and Thomas Bonald and Marc Jeanmougin"], "Categories": "cs.LG"}, "abstract": "We present the self-encoder, a neural network trained to guess the identity of each data sample. Despite its simplicity, it learns a very useful representation of data, in a self-supervised way. Specifically, the self-encoder learns to distribute the data samples in the embedding space so that they are linearly separable from one another. This induces a geometry where two samples are close in the embedding space when they are not easy to differentiate. The self-encoder can then be combined with a nearest-neighbor classifier or regressor for any subsequent supervised task. Unlike regular nearest neighbors, the predictions resulting from this encoding of data are invariant to any scaling of features, making any preprocessing like min-max scaling not necessary. The experiments show the efficiency of the approach, especially on heterogeneous data mixing numerical features and categorical features.", "url": "https://arxiv.org/abs/2306.14257"}, {"metadata": {"arXiv": "2306.14258", "Date": "Sun, 25 Jun 2023 14:30:33 ", "Title": "A Neural RDE approach for continuous-time non-Markovian stochastic control problems", "Authors": ["Melker Hoglund", "Emilio Ferrucci", "Camilo Hernandez", "Aitor Muguruza Gonzalez", "Cristopher Salvi", "Leandro Sanchez-Betancourt", "Yufei Zhang"], "Categories": "cs.LG math.OC", "Comments": ["Accepted at ICML 2023", "Workshop on New Frontiers in Learning", "Control", "and Dynamical Systems"]}, "abstract": "We propose a novel framework for solving continuous-time non-Markovian stochastic control problems by means of neural rough differential equations (Neural RDEs) introduced in Morrill et al. (2021). Non-Markovianity naturally arises in control problems due to the time delay effects in the system coefficients or the driving noises, which leads to optimal control strategies depending explicitly on the historical trajectories of the system state. By modelling the control process as the solution of a Neural RDE driven by the state process, we show that the control-state joint dynamics are governed by an uncontrolled, augmented Neural RDE, allowing for fast Monte-Carlo estimation of the value function via trajectories simulation and memory-efficient backpropagation. We provide theoretical underpinnings for the proposed algorithmic framework by demonstrating that Neural RDEs serve as universal approximators for functions of random rough paths. Exhaustive numerical experiments on non-Markovian stochastic control problems are presented, which reveal that the proposed framework is time-resolution-invariant and achieves higher accuracy and better stability in irregular sampling compared to existing RNN-based approaches.", "url": "https://arxiv.org/abs/2306.14258"}, {"metadata": {"arXiv": "2306.14306", "Date": "Sun, 25 Jun 2023 18:29:29 ", "Title": "Adaptive Sharpness-Aware Pruning for Robust Sparse Networks", "Authors": ["Anna Bair", "Hongxu Yin", "Maying Shen", "Pavlo Molchanov", "Jose Alvarez"], "Categories": "cs.LG cs.CV"}, "abstract": "Robustness and compactness are two essential components of deep learning models that are deployed in the real world. The seemingly conflicting aims of (i) generalization across domains as in robustness, and (ii) specificity to one domain as in compression, are why the overall design goal of achieving robust compact models, despite being highly important, is still a challenging open problem. We introduce Adaptive Sharpness-Aware Pruning, or AdaSAP, a method that yields robust sparse networks. The central tenet of our approach is to optimize the loss landscape so that the model is primed for pruning via adaptive weight perturbation, and is also consistently regularized toward flatter regions for improved robustness. This unifies both goals through the lens of network sharpness. AdaSAP achieves strong performance in a comprehensive set of experiments. For classification on ImageNet and object detection on Pascal VOC datasets, AdaSAP improves the robust accuracy of pruned models by +6% on ImageNet C, +4% on ImageNet V2, and +4% on corrupted VOC datasets, over a wide range of compression ratios, saliency criteria, and network architectures, outperforming recent pruning art by large margins.", "url": "https://arxiv.org/abs/2306.14306"}, {"metadata": {"arXiv": "2306.14326", "Date": "Sun, 25 Jun 2023 19:41:14 ", "Title": "Computational Asymmetries in Robust Classification", "Authors": ["Samuele Marro", "Michele Lombardi"], "Categories": "cs.LG cs.CR", "MSC-class": "68T07", "Journal-ref": "40th International Conference on Machine Learning (ICML 2023)"}, "abstract": "In the context of adversarial robustness, we make three strongly related contributions. First, we prove that while attacking ReLU classifiers is $\\mathit{NP}$-hard, ensuring their robustness at training time is $\\Sigma^2_P$-hard (even on a single example). This asymmetry provides a rationale for the fact that robust classifications approaches are frequently fooled in the literature. Second, we show that inference-time robustness certificates are not affected by this asymmetry, by introducing a proof-of-concept approach named Counter-Attack (CA). Indeed, CA displays a reversed asymmetry: running the defense is $\\mathit{NP}$-hard, while attacking it is $\\Sigma_2^P$-hard. Finally, motivated by our previous result, we argue that adversarial attacks can be used in the context of robustness certification, and provide an empirical evaluation of their effectiveness. As a byproduct of this process, we also release UG100, a benchmark dataset for adversarial attacks.", "url": "https://arxiv.org/abs/2306.14326"}, {"metadata": {"arXiv": "2306.14346", "Date": "Sun, 25 Jun 2023 21:22:21 ", "Title": "Evolution of $K$-means solution landscapes with the addition of dataset outliers and a robust clustering comparison measure for their analysis", "Authors": ["Luke Dicks and David J. Wales"], "Categories": "cs.LG", "Comments": ["27 pages", "9 figures"]}, "abstract": "The $K$-means algorithm remains one of the most widely-used clustering methods due to its simplicity and general utility. The performance of $K$-means depends upon location of minima low in cost function, amongst a potentially vast number of solutions. Here, we use the energy landscape approach to map the change in $K$-means solution space as a result of increasing dataset outliers and show that the cost function surface becomes more funnelled. Kinetic analysis reveals that in all cases the overall funnel is composed of shallow locally-funnelled regions, each of which are separated by areas that do not support any clustering solutions. These shallow regions correspond to different types of clustering solution and their increasing number with outliers leads to longer pathways within the funnel and a reduced correlation between accuracy and cost function. Finally, we propose that the rates obtained from kinetic analysis provide a novel measure of clustering similarity that incorporates information about the paths between them. This measure is robust to outliers and we illustrate the application to datasets containing multiple outliers.", "url": "https://arxiv.org/abs/2306.14346"}, {"metadata": {"arXiv": "2306.14347", "Date": "Sun, 25 Jun 2023 21:31:46 ", "Title": "Fast Classification with Sequential Feature Selection in Test Phase", "Authors": ["Ali Mirzaei", "Vahid Pourahmadi", "Hamid Sheikhzadeh", "Alireza Abdollahpourrostam"], "Categories": "cs.LG cs.IT eess.SP math.IT"}, "abstract": "This paper introduces a novel approach to active feature acquisition for classification, which is the task of sequentially selecting the most informative subset of features to achieve optimal prediction performance during testing while minimizing cost. The proposed approach involves a new lazy model that is significantly faster and more efficient compared to existing methods, while still producing comparable accuracy results. During the test phase, the proposed approach utilizes Fisher scores for feature ranking to identify the most important feature at each step. In the next step the training dataset is filtered based on the observed value of the selected feature and then we continue this process to reach to acceptable accuracy or limit of the budget for feature acquisition. The performance of the proposed approach was evaluated on synthetic and real datasets, including our new synthetic dataset, CUBE dataset and also real dataset Forest. The experimental results demonstrate that our approach achieves competitive accuracy results compared to existing methods, while significantly outperforming them in terms of speed. The source code of the algorithm is released at github with this link: https://github.com/alimirzaei/FCwSFS.", "url": "https://arxiv.org/abs/2306.14347"}, {"metadata": {"arXiv": "2306.14348", "Date": "Sun, 25 Jun 2023 21:43:05 ", "Title": "Collaborative and Distributed Bayesian Optimization via Consensus: Showcasing the Power of Collaboration for Optimal Design", "Authors": ["Xubo Yue", "Raed Al Kontar", "Albert S. Berahas", "Yang Liu", "Zhenghao Zai", "Kevin Edgar", "Blake N. Johnson"], "Categories": "cs.LG cs.DC", "Comments": ["43 pages"]}, "abstract": "Optimal design is a critical yet challenging task within many applications. This challenge arises from the need for extensive trial and error, often done through simulations or running field experiments. Fortunately, sequential optimal design, also referred to as Bayesian optimization when using surrogates with a Bayesian flavor, has played a key role in accelerating the design process through efficient sequential sampling strategies. However, a key opportunity exists nowadays. The increased connectivity of edge devices sets forth a new collaborative paradigm for Bayesian optimization. A paradigm whereby different clients collaboratively borrow strength from each other by effectively distributing their experimentation efforts to improve and fast-track their optimal design process. To this end, we bring the notion of consensus to Bayesian optimization, where clients agree (i.e., reach a consensus) on their next-to-sample designs. Our approach provides a generic and flexible framework that can incorporate different collaboration mechanisms. In lieu of this, we propose transitional collaborative mechanisms where clients initially rely more on each other to maneuver through the early stages with scant data, then, at the late stages, focus on their own objectives to get client-specific solutions. Theoretically, we show the sub-linear growth in regret for our proposed framework. Empirically, through simulated datasets and a real-world collaborative material discovery experiment, we show that our framework can effectively accelerate and improve the optimal design process and benefit all participants.", "url": "https://arxiv.org/abs/2306.14348"}, {"metadata": {"arXiv": "2306.14357", "Date": "Sun, 25 Jun 2023 22:17:25 ", "Title": "PolicyClusterGCN: Identifying Efficient Clusters for Training Graph Convolutional Networks", "Authors": ["Saket Gurukar", "Shaileshh Bojja Venkatakrishnan", "Balaraman Ravindran", "Srinivasan Parthasarathy"], "Categories": "cs.LG cs.SI"}, "abstract": "Graph convolutional networks (GCNs) have achieved huge success in several machine learning (ML) tasks on graph-structured data. Recently, several sampling techniques have been proposed for the efficient training of GCNs and to improve the performance of GCNs on ML tasks. Specifically, the subgraph-based sampling approaches such as ClusterGCN and GraphSAINT have achieved state-of-the-art performance on the node classification tasks. These subgraph-based sampling approaches rely on heuristics -- such as graph partitioning via edge cuts -- to identify clusters that are then treated as minibatches during GCN training. In this work, we hypothesize that rather than relying on such heuristics, one can learn a reinforcement learning (RL) policy to compute efficient clusters that lead to effective GCN performance. To that end, we propose PolicyClusterGCN, an online RL framework that can identify good clusters for GCN training. We develop a novel Markov Decision Process (MDP) formulation that allows the policy network to predict ``importance\" weights on the edges which are then utilized by a clustering algorithm (Graclus) to compute the clusters. We train the policy network using a standard policy gradient algorithm where the rewards are computed from the classification accuracies while training GCN using clusters given by the policy. Experiments on six real-world datasets and several synthetic datasets show that PolicyClusterGCN outperforms existing state-of-the-art models on node classification task.", "url": "https://arxiv.org/abs/2306.14357"}, {"metadata": {"arXiv": "2306.14375", "Date": "Mon, 26 Jun 2023 01:37:10 ", "Title": "Interpretable Sparsification of Brain Graphs: Better Practices and Effective Designs for Graph Neural Networks", "Authors": ["Gaotang Li", "Marlena Duda", "Xiang Zhang", "Danai Koutra", "Yujun Yan"], "Categories": "cs.LG q-bio.NC", "Comments": ["To appear in Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 23)"], "DOI": "10.1145/3580305.3599394"}, "abstract": "Brain graphs, which model the structural and functional relationships between brain regions, are crucial in neuroscientific and clinical applications involving graph classification. However, dense brain graphs pose computational challenges including high runtime and memory usage and limited interpretability. In this paper, we investigate effective designs in Graph Neural Networks (GNNs) to sparsify brain graphs by eliminating noisy edges. While prior works remove noisy edges based on explainability or task-irrelevant properties, their effectiveness in enhancing performance with sparsified graphs is not guaranteed. Moreover, existing approaches often overlook collective edge removal across multiple graphs. To address these issues, we introduce an iterative framework to analyze different sparsification models. Our findings are as follows: (i) methods prioritizing interpretability may not be suitable for graph sparsification as they can degrade GNNs' performance in graph classification tasks; (ii) simultaneously learning edge selection with GNN training is more beneficial than post-training; (iii) a shared edge selection across graphs outperforms separate selection for each graph; and (iv) task-relevant gradient information aids in edge selection. Based on these insights, we propose a new model, Interpretable Graph Sparsification (IGS), which enhances graph classification performance by up to 5.1% with 55.0% fewer edges. The retained edges identified by IGS provide neuroscientific interpretations and are supported by well-established literature.", "url": "https://arxiv.org/abs/2306.14375"}, {"metadata": {"arXiv": "2306.14381", "Date": "Mon, 26 Jun 2023 02:15:26 ", "Title": "Gradient Descent Converges Linearly for Logistic Regression on Separable Data", "Authors": ["Kyriakos Axiotis and Maxim Sviridenko"], "Categories": "cs.LG"}, "abstract": "We show that running gradient descent with variable learning rate guarantees loss $f(x) \\leq 1.1 \\cdot f(x^*) + \\epsilon$ for the logistic regression objective, where the error $\\epsilon$ decays exponentially with the number of iterations and polynomially with the magnitude of the entries of an arbitrary fixed solution $x^*$. This is in contrast to the common intuition that the absence of strong convexity precludes linear convergence of first-order methods, and highlights the importance of variable learning rates for gradient descent. We also apply our ideas to sparse logistic regression, where they lead to an exponential improvement of the sparsity-error tradeoff.", "url": "https://arxiv.org/abs/2306.14381"}, {"metadata": {"arXiv": "2306.14411", "Date": "Mon, 26 Jun 2023 04:12:40 ", "Title": "Score-based Source Separation with Applications to Digital Communication Signals", "Authors": ["Tejas Jayashankar", "Gary C.F. Lee", "Alejandro Lancho", "Amir Weiss", "Yury Polyanskiy", "Gregory W. Wornell"], "Categories": "cs.LG eess.SP"}, "abstract": "We propose a new method for separating superimposed sources using diffusion-based generative models. Our method relies only on separately trained statistical priors of independent sources to establish a new objective function guided by maximum a posteriori estimation with an $\\alpha$-posterior, across multiple levels of Gaussian smoothing. Motivated by applications in radio-frequency (RF) systems, we are interested in sources with underlying discrete nature and the recovery of encoded bits from a signal of interest, as measured by the bit error rate (BER). Experimental results with RF mixtures demonstrate that our method results in a BER reduction of 95% over classical and existing learning-based methods. Our analysis demonstrates that our proposed method yields solutions that asymptotically approach the modes of an underlying discrete distribution. Furthermore, our method can be viewed as a multi-source extension to the recently proposed score distillation sampling scheme, shedding additional light on its use beyond conditional sampling.", "url": "https://arxiv.org/abs/2306.14411"}, {"metadata": {"arXiv": "2306.14430", "Date": "Mon, 26 Jun 2023 05:58:17 ", "Title": "Enhanced multi-fidelity modelling for digital twin and uncertainty quantification", "Authors": ["AS Desai and Navaneeth N and S Adhikari and S Chakraborty"], "Categories": "cs.LG stat.ML"}, "abstract": "The increasing significance of digital twin technology across engineering and industrial domains, such as aerospace, infrastructure, and automotive, is undeniable. However, the lack of detailed application-specific information poses challenges to its seamless implementation in practical systems. Data-driven models play a crucial role in digital twins, enabling real-time updates and predictions by leveraging data and computational models. Nonetheless, the fidelity of available data and the scarcity of accurate sensor data often hinder the efficient learning of surrogate models, which serve as the connection between physical systems and digital twin models. To address this challenge, we propose a novel framework that begins by developing a robust multi-fidelity surrogate model, subsequently applied for tracking digital twin systems. Our framework integrates polynomial correlated function expansion (PCFE) with the Gaussian process (GP) to create an effective surrogate model called H-PCFE. Going a step further, we introduce deep-HPCFE, a cascading arrangement of models with different fidelities, utilizing nonlinear auto-regression schemes. These auto-regressive schemes effectively address the issue of erroneous predictions from low-fidelity models by incorporating space-dependent cross-correlations among the models. To validate the efficacy of the multi-fidelity framework, we first assess its performance in uncertainty quantification using benchmark numerical examples. Subsequently, we demonstrate its applicability in the context of digital twin systems.", "url": "https://arxiv.org/abs/2306.14430"}, {"metadata": {"arXiv": "2306.14443", "Date": "Mon, 26 Jun 2023 06:14:01 ", "Title": "Federated Learning on Non-iid Data via Local and Global Distillation", "Authors": ["Xiaolin Zheng", "Senci Ying", "Fei Zheng", "Jianwei Yin", "Longfei Zheng", "Chaochao Chen", "Fengqin Dong"], "Categories": "cs.LG cs.CR", "Comments": ["Accpeted in IEEE ICWS 2023"]}, "abstract": "Most existing federated learning algorithms are based on the vanilla FedAvg scheme. However, with the increase of data complexity and the number of model parameters, the amount of communication traffic and the number of iteration rounds for training such algorithms increases significantly, especially in non-independently and homogeneously distributed scenarios, where they do not achieve satisfactory performance. In this work, we propose FedND: federated learning with noise distillation. The main idea is to use knowledge distillation to optimize the model training process. In the client, we propose a self-distillation method to train the local model. In the server, we generate noisy samples for each client and use them to distill other clients. Finally, the global model is obtained by the aggregation of local models. Experimental results show that the algorithm achieves the best performance and is more communication-efficient than state-of-the-art methods.", "url": "https://arxiv.org/abs/2306.14443"}, {"metadata": {"arXiv": "2306.14479", "Date": "Mon, 26 Jun 2023 07:46:04 ", "Title": "Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization", "Authors": ["Jinxin Liu", "Hongyin Zhang", "Zifeng Zhuang", "Yachen Kang", "Donglin Wang", "Bin Wang"], "Categories": "cs.LG"}, "abstract": "In this work, we decouple the iterative bi-level offline RL from the offline training phase, forming a non-iterative bi-level paradigm and avoiding the iterative error propagation over two levels. Specifically, this non-iterative paradigm allows us to conduct inner-level optimization in training (for OOD issues), while performing outer-level optimization in testing (for reward maximizing). Naturally, such a paradigm raises three core questions that are \\textit{not} fully answered by prior non-iterative offline RL counterparts like reward-conditioned policy: Q1) What information should we transfer from the inner-level to the outer-level? Q2) What should we pay attention to when exploiting the transferred information in the outer-level optimization? Q3) What are the~benefits of concurrently conducting outer-level optimization during testing? Motivated by model-based optimization~{(MBO)}, we propose DROP (\\textbf{D}esign f\\textbf{RO}m \\textbf{P}olicies), which fully answers the above questions. Specifically, in the inner-level, DROP decomposes offline data into multiple subsets and learns an {MBO} score model~(A1). To keep safe exploitation to the score model in the outer-level, we explicitly learn a behavior embedding and introduce a conservative regularization (A2). During testing, we show that DROP permits test-time adaptation, enabling an adaptive inference across states~(A3). Empirically, we find that DROP, compared to prior non-iterative offline RL counterparts, gains an average improvement probability of more than 80\\%, and achieves comparable or better performance compared to prior iterative baselines.", "url": "https://arxiv.org/abs/2306.14479"}, {"metadata": {"arXiv": "2306.14511", "Date": "Mon, 26 Jun 2023 08:40:24 ", "Title": "TaylorPDENet: Learning PDEs from non-grid Data", "Authors": ["Paul Heinisch", "Andrzej Dulny", "Anna Krause", "Andreas Hotho"], "Categories": "cs.LG"}, "abstract": "Modeling data obtained from dynamical systems has gained attention in recent years as a challenging task for machine learning models. Previous approaches assume the measurements to be distributed on a grid. However, for real-world applications like weather prediction, the observations are taken from arbitrary locations within the spatial domain. In this paper, we propose TaylorPDENet - a novel machine learning method that is designed to overcome this challenge. Our algorithm uses the multidimensional Taylor expansion of a dynamical system at each observation point to estimate the spatial derivatives to perform predictions. TaylorPDENet is able to accomplish two objectives simultaneously: accurately forecast the evolution of a complex dynamical system and explicitly reconstruct the underlying differential equation describing the system. We evaluate our model on a variety of advection-diffusion equations with different parameters and show that it performs similarly to equivalent approaches on grid-structured data while being able to process unstructured data as well.", "url": "https://arxiv.org/abs/2306.14511"}, {"metadata": {"arXiv": "2306.14534", "Date": "Mon, 26 Jun 2023 09:18:30 ", "Title": "CEIL: Generalized Contextual Imitation Learning", "Authors": ["Jinxin Liu", "Li He", "Yachen Kang", "Zifeng Zhuang", "Donglin Wang", "Huazhe Xu"], "Categories": "cs.LG"}, "abstract": "In this paper, we present \\textbf{C}ont\\textbf{E}xtual \\textbf{I}mitation \\textbf{L}earning~(CEIL), a general and broadly applicable algorithm for imitation learning (IL). Inspired by the formulation of hindsight information matching, we derive CEIL by explicitly learning a hindsight embedding function together with a contextual policy using the hindsight embeddings. To achieve the expert matching objective for IL, we advocate for optimizing a contextual variable such that it biases the contextual policy towards mimicking expert behaviors. Beyond the typical learning from demonstrations (LfD) setting, CEIL is a generalist that can be effectively applied to multiple settings including: 1)~learning from observations (LfO), 2)~offline IL, 3)~cross-domain IL (mismatched experts), and 4) one-shot IL settings. Empirically, we evaluate CEIL on the popular MuJoCo tasks (online) and the D4RL dataset (offline). Compared to prior state-of-the-art baselines, we show that CEIL is more sample-efficient in most online IL tasks and achieves better or competitive performances in offline tasks.", "url": "https://arxiv.org/abs/2306.14534"}, {"metadata": {"arXiv": "2306.14574", "Date": "Mon, 26 Jun 2023 10:35:31 ", "Title": "On-Device Evaluation Toolkit for Machine Learning on Heterogeneous Low-Power System-on-Chip", "Authors": ["Zhaolan Huang", "Koen Zandberg", "Kaspar Schleiser and Emmanuel Baccelli"], "Categories": "cs.LG cs.PF"}, "abstract": "Network delays, throughput bottlenecks and privacy issues push Artificial Intelligence of Things (AIoT) designers towards evaluating the feasibility of moving model training and execution (inference) as near as possible to the terminals. Meanwhile, results from the TinyML community demonstrate that, in some cases, it is possible to execute model inference directly on the terminals themselves, even if these are small microcontroller-based devices. However, to date, researchers and practitioners in the domain lack convenient all-in-one toolkits to help them evaluate the feasibility of moving execution of arbitrary models to arbitrary low-power IoT hardware. To this effect, we present in this paper U-TOE, a universal toolkit we designed to facilitate the task of AIoT designers and researchers, by combining functionalities from a low-power embedded OS, a generic model transpiler and compiler, an integrated performance measurement module, and an open-access remote IoT testbed. We provide an open source implementation of U-TOE and we demonstrate its use to experimentally evaluate the performance of a wide variety of models, on a wide variety of low-power boards, based on popular microcontroller architectures (ARM Cortex-M and RISC-V). U-TOE thus allows easily reproducible and customisable comparative evaluation experiments in this domain, on a wide variety of IoT hardware all-at-once. The availability of a toolkit such as U-TOE is desirable to accelerate the field of AIoT, towards fully exploiting the potential of edge computing.", "url": "https://arxiv.org/abs/2306.14574"}, {"metadata": {"arXiv": "2306.14624", "Date": "Mon, 26 Jun 2023 11:56:00 ", "Title": "Insights From Insurance for Fair Machine Learning: Responsibility, Performativity and Aggregates", "Authors": ["Christian Fr\\\"ohlich and Robert C. Williamson"], "Categories": "cs.LG cs.CY"}, "abstract": "We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront three themes that have been largely overlooked in the machine learning literature: responsibility, performativity and tensions between aggregate and individual.", "url": "https://arxiv.org/abs/2306.14624"}, {"metadata": {"arXiv": "2306.14688", "Date": "Mon, 26 Jun 2023 13:32:11 ", "Title": "An Evolution Kernel Method for Graph Classification through Heat Diffusion Dynamics", "Authors": ["Xue Liu", "Dan Sun", "Wei Wei", "Zhiming Zheng"], "Categories": "cs.LG"}, "abstract": "Autonomous individuals establish a structural complex system through pairwise connections and interactions. Notably, the evolution reflects the dynamic nature of each complex system since it recodes a series of temporal changes from the past, the present into the future. Different systems follow distinct evolutionary trajectories, which can serve as distinguishing traits for system classification. However, modeling a complex system's evolution is challenging for the graph model because the graph is typically a snapshot of the static status of a system, and thereby hard to manifest the long-term evolutionary traits of a system entirely. To address this challenge, we suggest utilizing a heat-driven method to generate temporal graph augmentation. This approach incorporates the physics-based heat kernel and DropNode technique to transform each static graph into a sequence of temporal ones. This approach effectively describes the evolutional behaviours of the system, including the retention or disappearance of elements at each time point based on the distributed heat on each node. Additionally, we propose a dynamic time-wrapping distance GDTW to quantitatively measure the distance between pairwise evolutionary systems through optimal matching. The resulting approach, called the Evolution Kernel method, has been successfully applied to classification problems in real-world structural graph datasets. The results yield significant improvements in supervised classification accuracy over a series of baseline methods.", "url": "https://arxiv.org/abs/2306.14688"}, {"metadata": {"arXiv": "2306.14705", "Date": "Mon, 26 Jun 2023 13:54:52 ", "Title": "Augmenting Control over Exploration Space in Molecular Dynamics Simulators to Streamline De Novo Analysis through Generative Control Policies", "Authors": ["Paloma Gonzalez-Rojas", "Andrew Emmel", "Luis Martinez", "Neil Malur", "Gregory Rutledge"], "Categories": "cs.LG cond-mat.mtrl-sci", "Comments": ["ICML 2023 Workshop on Structured Probabilistic Inference (SPIGM) and Generative Modeling", "of the International Conference of Machine Learning (ICML)"], "Report-no": "03"}, "abstract": "This study introduces the P5 model - a foundational method that utilizes reinforcement learning (RL) to augment control, effectiveness, and scalability in molecular dynamics simulations (MD). Our innovative strategy optimizes the sampling of target polymer chain conformations, marking an efficiency improvement of over 37.1%. The RL-induced control policies function as an inductive bias, modulating Brownian forces to steer the system towards the preferred state, thereby expanding the exploration of the configuration space beyond what traditional MD allows. This broadened exploration generates a more varied set of conformations and targets specific properties, a feature pivotal for progress in polymer development, drug discovery, and material design. Our technique offers significant advantages when investigating new systems with limited prior knowledge, opening up new methodologies for tackling complex simulation problems with generative techniques.", "url": "https://arxiv.org/abs/2306.14705"}, {"metadata": {"arXiv": "2306.14759", "Date": "Mon, 26 Jun 2023 15:13:36 ", "Title": "PMaF: Deep Declarative Layers for Principal Matrix Features", "Authors": ["Zhiwei Xu", "Hao Wang", "Yanbin Liu", "Stephen Gould"], "Categories": "cs.LG", "Comments": ["Accepted to the Differentiable Almost Everything Workshop of the International Conference on Machine Learning (ICML) 2023"]}, "abstract": "We explore two differentiable deep declarative layers, namely least squares on sphere (LESS) and implicit eigen decomposition (IED), for learning the principal matrix features (PMaF). This can be used to represent data features with a low-dimension vector containing dominant information from a high-dimension matrix. We first solve the problems with iterative optimization in the forward pass and then backpropagate the solution for implicit gradients under a bi-level optimization framework. Particularly, adaptive descent steps with the backtracking line search method and descent decay in the tangent space are studied to improve the forward pass efficiency of LESS. Meanwhile, exploited data structures are used to greatly reduce the computational complexity in the backward pass of LESS and IED. Empirically, we demonstrate the superiority of our layers over the off-the-shelf baselines by comparing the solution optimality and computational requirements.", "url": "https://arxiv.org/abs/2306.14759"}, {"metadata": {"arXiv": "2306.14775", "Date": "Mon, 26 Jun 2023 15:35:27 ", "Title": "Parameter-Level Soft-Masking for Continual Learning", "Authors": ["Tatsuya Konishi", "Mori Kurokawa", "Chihiro Ono", "Zixuan Ke", "Gyuhak Kim", "Bing Liu"], "Categories": "cs.LG cs.CV", "Comments": ["ICML2023"]}, "abstract": "Existing research on task incremental learning in continual learning has primarily focused on preventing catastrophic forgetting (CF). Although several techniques have achieved learning with no CF, they attain it by letting each task monopolize a sub-network in a shared network, which seriously limits knowledge transfer (KT) and causes over-consumption of the network capacity, i.e., as more tasks are learned, the performance deteriorates. The goal of this paper is threefold: (1) overcoming CF, (2) encouraging KT, and (3) tackling the capacity problem. A novel technique (called SPG) is proposed that soft-masks (partially blocks) parameter updating in training based on the importance of each parameter to old tasks. Each task still uses the full network, i.e., no monopoly of any part of the network by any task, which enables maximum KT and reduction in capacity usage. To our knowledge, this is the first work that soft-masks a model at the parameter-level for continual learning. Extensive experiments demonstrate the effectiveness of SPG in achieving all three objectives. More notably, it attains significant transfer of knowledge not only among similar tasks (with shared knowledge) but also among dissimilar tasks (with little shared knowledge) while mitigating CF.", "url": "https://arxiv.org/abs/2306.14775"}, {"metadata": {"arXiv": "2306.14787", "Date": "Mon, 26 Jun 2023 15:46:08 ", "Title": "Distributive Pre-Training of Generative Modeling Using Matrix-Product States", "Authors": ["Sheng-Hsuan Lin", "Olivier Kuijpers", "Sebastian Peterhansl", "and Frank Pollmann"], "Categories": "cs.LG cond-mat.stat-mech quant-ph", "Comments": ["7+2 pages", "1+2 figures; Position paper in QTNML Workshop", "NeurIPS 2021; See https://tensorworkshop.github.io/NeurIPS2021/accepted_papers/MPS_MNIST.pdf"]}, "abstract": "Tensor networks have recently found applications in machine learning for both supervised learning and unsupervised learning. The most common approaches for training these models are gradient descent methods. In this work, we consider an alternative training scheme utilizing basic tensor network operations, e.g., summation and compression. The training algorithm is based on compressing the superposition state constructed from all the training data in product state representation. The algorithm could be parallelized easily and only iterates through the dataset once. Hence, it serves as a pre-training algorithm. We benchmark the algorithm on the MNIST dataset and show reasonable results for generating new images and classification tasks. Furthermore, we provide an interpretation of the algorithm as a compressed quantum kernel density estimation for the probability amplitude of input data.", "url": "https://arxiv.org/abs/2306.14787"}, {"metadata": {"arXiv": "2306.14799", "Date": "Mon, 26 Jun 2023 15:58:13 ", "Title": "On Imitation in Mean-field Games", "Authors": ["Giorgia Ramponi", "Pavel Kolev", "Olivier Pietquin", "Niao He", "Mathieu Lauri\\`ere", "Matthieu Geist"], "Categories": "cs.LG cs.GT"}, "abstract": "We explore the problem of imitation learning (IL) in the context of mean-field games (MFGs), where the goal is to imitate the behavior of a population of agents following a Nash equilibrium policy according to some unknown payoff function. IL in MFGs presents new challenges compared to single-agent IL, particularly when both the reward function and the transition kernel depend on the population distribution. In this paper, departing from the existing literature on IL for MFGs, we introduce a new solution concept called the Nash imitation gap. Then we show that when only the reward depends on the population distribution, IL in MFGs can be reduced to single-agent IL with similar guarantees. However, when the dynamics is population-dependent, we provide a novel upper-bound that suggests IL is harder in this setting. To address this issue, we propose a new adversarial formulation where the reinforcement learning problem is replaced by a mean-field control (MFC) problem, suggesting progress in IL within MFGs may have to build upon MFC.", "url": "https://arxiv.org/abs/2306.14799"}, {"metadata": {"arXiv": "2306.14808", "Date": "Mon, 26 Jun 2023 16:08:26 ", "Title": "Maximum State Entropy Exploration using Predecessor and Successor Representations", "Authors": ["Arnav Kumar Jain", "Lucas Lehnert", "Irina Rish", "Glen Berseth"], "Categories": "cs.LG"}, "abstract": "Animals have a developed ability to explore that aids them in important tasks such as locating food, exploring for shelter, and finding misplaced items. These exploration skills necessarily track where they have been so that they can plan for finding items with relative efficiency. Contemporary exploration algorithms often learn a less efficient exploration strategy because they either condition only on the current state or simply rely on making random open-loop exploratory moves. In this work, we propose $\\eta\\psi$-Learning, a method to learn efficient exploratory policies by conditioning on past episodic experience to make the next exploratory move. Specifically, $\\eta\\psi$-Learning learns an exploration policy that maximizes the entropy of the state visitation distribution of a single trajectory. Furthermore, we demonstrate how variants of the predecessor representation and successor representations can be combined to predict the state visitation entropy. Our experiments demonstrate the efficacy of $\\eta\\psi$-Learning to strategically explore the environment and maximize the state coverage with limited samples.", "url": "https://arxiv.org/abs/2306.14808"}, {"metadata": {"arXiv": "2306.14809", "Date": "Mon, 26 Jun 2023 16:11:11 ", "Title": "Tanimoto Random Features for Scalable Molecular Machine Learning", "Authors": ["Austin Tripp", "Sergio Bacallado", "Sukriti Singh", "Jos\\'e Miguel Hern\\'andez-Lobato"], "Categories": "cs.LG", "Comments": ["Work in progress: expect updates in the future. Article is 29 pages with 9 figures"]}, "abstract": "The Tanimoto coefficient is commonly used to measure the similarity between molecules represented as discrete fingerprints, either as a distance metric or a positive definite kernel. While many kernel methods can be accelerated using random feature approximations, at present there is a lack of such approximations for the Tanimoto kernel. In this paper we propose two kinds of novel random features to allow this kernel to scale to large datasets, and in the process discover a novel extension of the kernel to real vectors. We theoretically characterize these random features, and provide error bounds on the spectral norm of the Gram matrix. Experimentally, we show that the random features proposed in this work are effective at approximating the Tanimoto coefficient in real-world datasets and that the kernels explored in this work are useful for molecular property prediction and optimization tasks.", "url": "https://arxiv.org/abs/2306.14809"}, {"metadata": {"arXiv": "2306.14818", "Date": "Mon, 26 Jun 2023 16:24:31 ", "Title": "Accelerating Molecular Graph Neural Networks via Knowledge Distillation", "Authors": ["Filip Ekstr\\\"om Kelvinius", "Dimitar Georgiev", "Artur Petrov Toshev", "Johannes Gasteiger"], "Categories": "cs.LG physics.chem-ph"}, "abstract": "Recent advances in graph neural networks (GNNs) have allowed molecular simulations with accuracy on par with conventional gold-standard methods at a fraction of the computational cost. Nonetheless, as the field has been progressing to bigger and more complex architectures, state-of-the-art GNNs have become largely prohibitive for many large-scale applications. In this paper, we, for the first time, explore the utility of knowledge distillation (KD) for accelerating molecular GNNs. To this end, we devise KD strategies that facilitate the distillation of hidden representations in directional and equivariant GNNs and evaluate their performance on the regression task of energy and force prediction. We validate our protocols across different teacher-student configurations and demonstrate that they can boost the predictive accuracy of student models without altering their architecture. We also conduct comprehensive optimization of various components of our framework, and investigate the potential of data augmentation to further enhance performance. All in all, we manage to close as much as 59% of the gap in predictive accuracy between models like GemNet-OC and PaiNN with zero additional cost at inference.", "url": "https://arxiv.org/abs/2306.14818"}, {"metadata": {"arXiv": "2306.14852", "Date": "Mon, 26 Jun 2023 17:02:54 ", "Title": "CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation", "Authors": ["Danny Reidenbach", "Aditi S. Krishnapriyan"], "Categories": "cs.LG physics.chem-ph q-bio.BM", "Comments": ["10 pages", "3 figures"]}, "abstract": "Molecular conformer generation (MCG) is an important task in cheminformatics and drug discovery. The ability to efficiently generate low-energy 3D structures can avoid expensive quantum mechanical simulations, leading to accelerated screenings and enhanced structural exploration. Several generative models have been developed for MCG, but many struggle to consistently produce high-quality conformers. To address these issues, we introduce CoarsenConf, which coarse-grains molecular graphs based on torsional angles and integrates them into an SE(3)-equivariant hierarchical variational autoencoder. Through equivariant coarse-graining, we aggregate the fine-grained atomic coordinates of subgraphs connected via rotatable bonds, creating a variable-length coarse-grained latent representation. Our model uses a novel aggregated attention mechanism to restore fine-grained coordinates from the coarse-grained latent representation, enabling efficient autoregressive generation of large molecules. Furthermore, our work expands current conformer generation benchmarks and introduces new metrics to better evaluate the quality and viability of generated conformers. We demonstrate that CoarsenConf generates more accurate conformer ensembles compared to prior generative models and traditional cheminformatics methods.", "url": "https://arxiv.org/abs/2306.14852"}, {"metadata": {"arXiv": "2306.14859", "Date": "Mon, 26 Jun 2023 17:13:31 ", "Title": "Effective Minkowski Dimension of Deep Nonparametric Regression: Function Approximation and Statistical Theories", "Authors": ["Zixuan Zhang", "Minshuo Chen", "Mengdi Wang", "Wenjing Liao", "Tuo Zhao"], "Categories": "cs.LG stat.ML"}, "abstract": "Existing theories on deep nonparametric regression have shown that when the input data lie on a low-dimensional manifold, deep neural networks can adapt to the intrinsic data structures. In real world applications, such an assumption of data lying exactly on a low dimensional manifold is stringent. This paper introduces a relaxed assumption that the input data are concentrated around a subset of $\\mathbb{R}^d$ denoted by $\\mathcal{S}$, and the intrinsic dimension of $\\mathcal{S}$ can be characterized by a new complexity notation -- effective Minkowski dimension. We prove that, the sample complexity of deep nonparametric regression only depends on the effective Minkowski dimension of $\\mathcal{S}$ denoted by $p$. We further illustrate our theoretical findings by considering nonparametric regression with an anisotropic Gaussian random design $N(0,\\Sigma)$, where $\\Sigma$ is full rank. When the eigenvalues of $\\Sigma$ have an exponential or polynomial decay, the effective Minkowski dimension of such an Gaussian random design is $p=\\mathcal{O}(\\sqrt{\\log n})$ or $p=\\mathcal{O}(n^\\gamma)$, respectively, where $n$ is the sample size and $\\gamma\\in(0,1)$ is a small constant depending on the polynomial decay rate. Our theory shows that, when the manifold assumption does not hold, deep neural networks can still adapt to the effective Minkowski dimension of the data, and circumvent the curse of the ambient dimensionality for moderate sample sizes.", "url": "https://arxiv.org/abs/2306.14859"}, {"metadata": {"arXiv": "2306.14872", "Date": "Mon, 26 Jun 2023 17:38:45 ", "Title": "Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits", "Authors": ["Yuwei Luo", "Mohsen Bayati"], "Categories": "cs.LG stat.ML"}, "abstract": "This paper is motivated by recent developments in the linear bandit literature, which have revealed a discrepancy between the promising empirical performance of algorithms such as Thompson sampling and Greedy, when compared to their pessimistic theoretical regret bounds. The challenge arises from the fact that while these algorithms may perform poorly in certain problem instances, they generally excel in typical instances. To address this, we propose a new data-driven technique that tracks the geometry of the uncertainty ellipsoid, enabling us to establish an instance-dependent frequentist regret bound for a broad class of algorithms, including Greedy, OFUL, and Thompson sampling. This result empowers us to identify and ``course-correct\" instances in which the base algorithms perform poorly. The course-corrected algorithms achieve the minimax optimal regret of order $\\tilde{\\mathcal{O}}(d\\sqrt{T})$, while retaining most of the desirable properties of the base algorithms. We present simulation results to validate our findings and compare the performance of our algorithms with the baselines.", "url": "https://arxiv.org/abs/2306.14872"}, {"metadata": {"arXiv": "2306.14878", "Date": "Mon, 26 Jun 2023 17:48:25 ", "Title": "Restart Sampling for Improving Generative Processes", "Authors": ["Yilun Xu", "Mingyang Deng", "Xiang Cheng", "Yonglong Tian", "Ziming Liu", "Tommi Jaakkola"], "Categories": "cs.LG cs.CV stat.CO stat.ML", "Comments": ["Code is available at https://github.com/Newbeeer/diffusion_restart_sampling"]}, "abstract": "Generative processes that involve solving differential equations, such as diffusion models, frequently necessitate balancing speed and quality. ODE-based samplers are fast but plateau in performance while SDE-based samplers deliver higher sample quality at the cost of increased sampling time. We attribute this difference to sampling errors: ODE-samplers involve smaller discretization errors while stochasticity in SDE contracts accumulated errors. Based on these findings, we propose a novel sampling algorithm called Restart in order to better balance discretization errors and contraction. The sampling method alternates between adding substantial noise in additional forward steps and strictly following a backward ODE. Empirically, Restart sampler surpasses previous SDE and ODE samplers in both speed and accuracy. Restart not only outperforms the previous best SDE results, but also accelerates the sampling speed by 10-fold / 2-fold on CIFAR-10 / ImageNet $64 \\times 64$. In addition, it attains significantly better sample quality than ODE samplers within comparable sampling times. Moreover, Restart better balances text-image alignment/visual quality versus diversity than previous samplers in the large-scale text-to-image Stable Diffusion model pre-trained on LAION $512 \\times 512$. Code is available at https://github.com/Newbeeer/diffusion_restart_sampling", "url": "https://arxiv.org/abs/2306.14878"}, {"metadata": {"arXiv": "2306.14601", "Date": "Mon, 26 Jun 2023 11:24:03 ", "Title": "Safe Navigation in Unstructured Environments by Minimizing Uncertainty in Control and Perception", "Authors": ["Junwon Seo", "Jungwi Mun", "and Taekyung Kim"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["RSS 2023 Workshop on Inference and Decision Making for Autonomous Vehicles (IDMAV)"]}, "abstract": "Uncertainty in control and perception poses challenges for autonomous vehicle navigation in unstructured environments, leading to navigation failures and potential vehicle damage. This paper introduces a framework that minimizes control and perception uncertainty to ensure safe and reliable navigation. The framework consists of two uncertainty-aware models: a learning-based vehicle dynamics model and a self-supervised traversability estimation model. We train a vehicle dynamics model that can quantify the epistemic uncertainty of the model to perform active exploration, resulting in the efficient collection of training data and effective avoidance of uncertain state-action spaces. In addition, we employ meta-learning to train a traversability cost prediction network. The model can be trained with driving data from a variety of types of terrain, and it can online-adapt based on interaction experiences to reduce the aleatoric uncertainty. Integrating the dynamics model and traversability cost prediction model with a sampling-based model predictive controller allows for optimizing trajectories that avoid uncertain terrains and state-action spaces. Experimental results demonstrate that the proposed method reduces uncertainty in prediction and improves stability in autonomous vehicle navigation in unstructured environments.", "url": "https://arxiv.org/abs/2306.14601"}, {"metadata": {"arXiv": "2306.14846", "Date": "Mon, 26 Jun 2023 16:57:03 ", "Title": "ViNT: A Foundation Model for Visual Navigation", "Authors": ["Dhruv Shah", "Ajay Sridhar", "Nitish Dashora", "Kyle Stachowicz", "Kevin Black", "Noriaki Hirose", "Sergey Levine"], "Categories": "cs.RO cs.CV cs.LG"}, "abstract": "General-purpose pre-trained models (\"foundation models\") have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navigation from a variety of different robotic platforms, and exhibits positive transfer, outperforming specialist models trained on singular datasets. ViNT can be augmented with diffusion-based subgoal proposals to explore novel environments, and can solve kilometer-scale navigation problems when equipped with long-range heuristics. ViNT can also be adapted to novel task specifications with a technique inspired by prompt-tuning, where the goal encoder is replaced by an encoding of another task modality (e.g., GPS waypoints or routing commands) embedded into the same space of goal tokens. This flexibility and ability to accommodate a variety of downstream problem domains establishes ViNT as an effective foundation model for mobile robotics. For videos, code, and model checkpoints, see our project page at https://visualnav-transformer.github.io.", "url": "https://arxiv.org/abs/2306.14846"}, {"metadata": {"arXiv": "2306.13867", "Date": "Sat, 24 Jun 2023 05:24:48 ", "Title": "Physics-Informed Machine Learning for Modeling and Control of Dynamical Systems", "Authors": ["Truong X. Nghiem (1)", "J\\'an Drgo\\v{n}a (2)", "Colin Jones (3)", "Zoltan Nagy (4)", "Roland Schwan (3)", "Biswadip Dey (5)", "Ankush Chakrabarty (6)", "Stefano Di Cairano (6)", "Joel A. Paulson (7)", "Andrea Carron (8)", "Melanie N. Zeilinger (8)", "Wenceslao Shaw Cortez (2)", "and Draguna L. Vrabie (2) ((1) School of Informatics", "Computing", "and Cyber Systems", "Northern Arizona University", "Flagstaff", "USA", "(2) Pacific Northwest National Laboratory", "Richland", "USA", "(3) EPFL", "Switzerland", "(4) The University of Texas at Austin", "USA", "(5) Siemens Corporation Technology", "Princeton", "USA", "(6) Mitsubishi Electric Research Laboratories", "Cambridge", "USA", "(7) The Ohio State University", "Columbus", "USA", "(8) ETH Zurich", "Switzerland)"], "Categories": "eess.SY cs.LG cs.SY", "Comments": ["16 pages", "4 figures", "to be published in 2023 American Control Conference (ACC)"]}, "abstract": "Physics-informed machine learning (PIML) is a set of methods and tools that systematically integrate machine learning (ML) algorithms with physical constraints and abstract mathematical models developed in scientific and engineering domains. As opposed to purely data-driven methods, PIML models can be trained from additional information obtained by enforcing physical laws such as energy and mass conservation. More broadly, PIML models can include abstract properties and conditions such as stability, convexity, or invariance. The basic premise of PIML is that the integration of ML and physics can yield more effective, physically consistent, and data-efficient models. This paper aims to provide a tutorial-like overview of the recent advances in PIML for dynamical system modeling and control. Specifically, the paper covers an overview of the theory, fundamental concepts and methods, tools, and applications on topics of: 1) physics-informed learning for system identification; 2) physics-informed learning for control; 3) analysis and verification of PIML models; and 4) physics-informed digital twins. The paper is concluded with a perspective on open challenges and future research opportunities.", "url": "https://arxiv.org/abs/2306.13867"}, {"metadata": {"arXiv": "2306.14814", "Date": "Mon, 26 Jun 2023 16:18:20 ", "Title": "Probabilistic Risk Assessment of an Obstacle Detection System for GoA 4 Freight Trains", "Authors": ["Mario Gleirscher and Anne E. Haxthausen and Jan Peleska"], "Categories": "eess.SY cs.CV cs.LG cs.SY"}, "abstract": "In this paper, a quantitative risk assessment approach is discussed for the design of an obstacle detection function for low-speed freight trains with grade of automation (GoA)~4. In this 5-step approach, starting with single detection channels and ending with a three-out-of-three (3oo3) model constructed of three independent dual-channel modules and a voter, a probabilistic assessment is exemplified, using a combination of statistical methods and parametric stochastic model checking. It is illustrated that, under certain not unreasonable assumptions, the resulting hazard rate becomes acceptable for specific application settings. The statistical approach for assessing the residual risk of misclassifications in convolutional neural networks and conventional image processing software suggests that high confidence can be placed into the safety-critical obstacle detection function, even though its implementation involves realistic machine learning uncertainties.", "url": "https://arxiv.org/abs/2306.14814"}, {"metadata": {"arXiv": "2306.13657", "Date": "Sat, 03 Jun 2023 21:31:38 ", "Title": "On Computational Mechanisms for Shared Intentionality, and Speculation on Rationality and Consciousness", "Authors": ["John Rushby"], "Categories": "cs.AI q-bio.NC", "Comments": ["This paper builds on our earlier arXiv:2209.03956 and provides the background for our arXiv:2207.05058"], "ACM-class": "I.2.11"}, "abstract": "A singular attribute of humankind is our ability to undertake novel, cooperative behavior, or teamwork. This requires that we can communicate goals, plans, and ideas between the brains of individuals to create shared intentionality. Using the information processing model of David Marr, I derive necessary characteristics of basic mechanisms to enable shared intentionality between computational agents and indicate how these could be implemented in present-day AI-based robots. More speculatively, I suggest the mechanisms derived by this thought experiment apply to humans and extend to provide explanations for human rationality and aspects of intentional and phenomenal consciousness that accord with observation. This yields what I call the Shared Intentionality First Theory (SIFT) for rationality and consciousness.", "url": "https://arxiv.org/abs/2306.13657"}, {"metadata": {"arXiv": "2306.13659", "Date": "Thu, 08 Jun 2023 09:18:28 ", "Title": "Toward A Logical Theory Of Fairness and Bias", "Authors": ["Vaishak Belle"], "Categories": "cs.AI cs.LO", "Comments": ["Accepted to TPLP", "as part of ICLP 2023"]}, "abstract": "Fairness in machine learning is of considerable interest in recent years owing to the propensity of algorithms trained on historical data to amplify and perpetuate historical biases. In this paper, we argue for a formal reconstruction of fairness definitions, not so much to replace existing definitions but to ground their application in an epistemic setting and allow for rich environmental modelling. Consequently we look into three notions: fairness through unawareness, demographic parity and counterfactual fairness, and formalise these in the epistemic situation calculus.", "url": "https://arxiv.org/abs/2306.13659"}, {"metadata": {"arXiv": "2306.13660", "Date": "Thu, 08 Jun 2023 12:34:31 ", "Title": "Statistical relational learning and neuro-symbolic AI: what does first-order logic offer?", "Authors": ["Vaishak Belle"], "Categories": "cs.AI cs.LO"}, "abstract": "In this paper, our aim is to briefly survey and articulate the logical and philosophical foundations of using (first-order) logic to represent (probabilistic) knowledge in a non-technical fashion. Our motivation is three fold. First, for machine learning researchers unaware of why the research community cares about relational representations, this article can serve as a gentle introduction. Second, for logical experts who are newcomers to the learning area, such an article can help in navigating the differences between finite vs infinite, and subjective probabilities vs random-world semantics. Finally, for researchers from statistical relational learning and neuro-symbolic AI, who are usually embedded in finite worlds with subjective probabilities, appreciating what infinite domains and random-world semantics brings to the table is of utmost theoretical import.", "url": "https://arxiv.org/abs/2306.13660"}, {"metadata": {"arXiv": "2306.13683", "Date": "Thu, 22 Jun 2023 08:51:48 ", "Title": "Model Families for Multi-Criteria Decision Support: A COVID-19 Case Study", "Authors": ["Martin Bicher", "Claire Rippinger", "Christoph Urach", "Dominik Brunmeir", "Melanie Zechmeister", "Niki Popper"], "Categories": "cs.AI cs.CY", "Comments": ["20 pages + 8 pages appendix", "9 figures im main text", "1 figure and 4 tables in appendix", "source code to simulations is found on GitHub https://github.com/dwhGmbH/covid19_model_family"], "MSC-class": "92-10", "ACM-class": "I.6.4; I.6.5; I.6.7; I.6.8"}, "abstract": "Continued model-based decision support is associated with particular challenges, especially in long-term projects. Due to the regularly changing questions and the often changing understanding of the underlying system, the models used must be regularly re-evaluated, -modelled and -implemented with respect to changing modelling purpose, system boundaries and mapped causalities. Usually, this leads to models with continuously growing complexity and volume. In this work we aim to reevaluate the idea of the model family, dating back to the 1990s, and use it to promote this as a mindset in the creation of decision support frameworks in large research projects. The idea is to generally not develop and enhance a single standalone model, but to divide the research tasks into interacting smaller models which specifically correspond to the research question. This strategy comes with many advantages, which we explain using the example of a family of models for decision support in the COVID-19 crisis and corresponding success stories. We describe the individual models, explain their role within the family, and how they are used - individually and with each other.", "url": "https://arxiv.org/abs/2306.13683"}, {"metadata": {"arXiv": "2306.13723", "Date": "Fri, 23 Jun 2023 18:10:54 ", "Title": "Social AI and the Challenges of the Human-AI Ecosystem", "Authors": ["Dino Pedreschi", "Luca Pappalardo", "Ricardo Baeza-Yates", "Albert-Laszlo Barabasi", "Frank Dignum", "Virginia Dignum", "Tina Eliassi-Rad", "Fosca Giannotti", "Janos Kertesz", "Alistair Knott", "Yannis Ioannidis", "Paul Lukowicz", "Andrea Passarella", "Alex Sandy Pentland", "John Shawe-Taylor", "Alessandro Vespignani"], "Categories": "cs.AI"}, "abstract": "The rise of large-scale socio-technical systems in which humans interact with artificial intelligence (AI) systems (including assistants and recommenders, in short AIs) multiplies the opportunity for the emergence of collective phenomena and tipping points, with unexpected, possibly unintended, consequences. For example, navigation systems' suggestions may create chaos if too many drivers are directed on the same route, and personalised recommendations on social media may amplify polarisation, filter bubbles, and radicalisation. On the other hand, we may learn how to foster the \"wisdom of crowds\" and collective action effects to face social and environmental challenges. In order to understand the impact of AI on socio-technical systems and design next-generation AIs that team with humans to help overcome societal problems rather than exacerbate them, we propose to build the foundations of Social AI at the intersection of Complex Systems, Network Science and AI. In this perspective paper, we discuss the main open questions in Social AI, outlining possible technical and scientific challenges and suggesting research avenues.", "url": "https://arxiv.org/abs/2306.13723"}, {"metadata": {"arXiv": "2306.13732", "Date": "Fri, 23 Jun 2023 18:42:27 ", "Title": "Reinforcement Learning with Temporal-Logic-Based Causal Diagrams", "Authors": ["Yash Paliwal", "Rajarshi Roy", "Jean-Rapha\\\"el Gaglione", "Nasim Baharisangari", "Daniel Neider", "Xiaoming Duan", "Ufuk Topcu", "Zhe Xu"], "Categories": "cs.AI cs.FL"}, "abstract": "We study a class of reinforcement learning (RL) tasks where the objective of the agent is to accomplish temporally extended goals. In this setting, a common approach is to represent the tasks as deterministic finite automata (DFA) and integrate them into the state-space for RL algorithms. However, while these machines model the reward function, they often overlook the causal knowledge about the environment. To address this limitation, we propose the Temporal-Logic-based Causal Diagram (TL-CD) in RL, which captures the temporal causal relationships between different properties of the environment. We exploit the TL-CD to devise an RL algorithm in which an agent requires significantly less exploration of the environment. To this end, based on a TL-CD and a task DFA, we identify configurations where the agent can determine the expected rewards early during an exploration. Through a series of case studies, we demonstrate the benefits of using TL-CDs, particularly the faster convergence of the algorithm to an optimal policy due to reduced exploration of the environment.", "url": "https://arxiv.org/abs/2306.13732"}, {"metadata": {"arXiv": "2306.13760", "Date": "Fri, 23 Jun 2023 19:50:48 ", "Title": "Task-Driven Graph Attention for Hierarchical Relational Object Navigation", "Authors": ["Michael Lingelbach", "Chengshu Li", "Minjune Hwang", "Andrey Kurenkov", "Alan Lou", "Roberto Mart\\'in-Mart\\'in", "Ruohan Zhang", "Li Fei-Fei", "Jiajun Wu"], "Categories": "cs.AI"}, "abstract": "Embodied AI agents in large scenes often need to navigate to find objects. In this work, we study a naturally emerging variant of the object navigation task, hierarchical relational object navigation (HRON), where the goal is to find objects specified by logical predicates organized in a hierarchical structure - objects related to furniture and then to rooms - such as finding an apple on top of a table in the kitchen. Solving such a task requires an efficient representation to reason about object relations and correlate the relations in the environment and in the task goal. HRON in large scenes (e.g. homes) is particularly challenging due to its partial observability and long horizon, which invites solutions that can compactly store the past information while effectively exploring the scene. We demonstrate experimentally that scene graphs are the best-suited representation compared to conventional representations such as images or 2D maps. We propose a solution that uses scene graphs as part of its input and integrates graph neural networks as its backbone, with an integrated task-driven attention mechanism, and demonstrate its better scalability and learning efficiency than state-of-the-art baselines.", "url": "https://arxiv.org/abs/2306.13760"}, {"metadata": {"arXiv": "2306.13780", "Date": "Fri, 23 Jun 2023 20:42:22 ", "Title": "Achieving Diversity in Objective Space for Sample-efficient Search of Multiobjective Optimization Problems", "Authors": ["Eric Hans Lee", "Bolong Cheng", "Michael McCourt"], "Categories": "cs.AI math.OC", "Journal-ref": "2022 Winter Simulation Conference", "DOI": "10.1109/WSC57314.2022.10015472"}, "abstract": "Efficiently solving multi-objective optimization problems for simulation optimization of important scientific and engineering applications such as materials design is becoming an increasingly important research topic. This is due largely to the expensive costs associated with said applications, and the resulting need for sample-efficient, multiobjective optimization methods that efficiently explore the Pareto frontier to expose a promising set of design solutions. We propose moving away from using explicit optimization to identify the Pareto frontier and instead suggest searching for a diverse set of outcomes that satisfy user-specified performance criteria. This method presents decision makers with a robust pool of promising design decisions and helps them better understand the space of good solutions. To achieve this outcome, we introduce the Likelihood of Metric Satisfaction (LMS) acquisition function, analyze its behavior and properties, and demonstrate its viability on various problems.", "url": "https://arxiv.org/abs/2306.13780"}, {"metadata": {"arXiv": "2306.13817", "Date": "Fri, 23 Jun 2023 23:53:49 ", "Title": "The Double Helix inside the NLP Transformer", "Authors": ["Jason H.J. Lu", "Qingzhen Guo"], "Categories": "cs.AI cs.CL", "Comments": ["Submitted to IEEE Transactions on Neural Networks and Learning Systems 21-Jun-2023. 12 pages", "14 figures"], "ACM-class": "I.2"}, "abstract": "We introduce a framework for analyzing various types of information in an NLP Transformer. In this approach, we distinguish four layers of information: positional, syntactic, semantic, and contextual. We also argue that the common practice of adding positional information to semantic embedding is sub-optimal and propose instead a Linear-and-Add approach. Our analysis reveals an autogenetic separation of positional information through the deep layers. We show that the distilled positional components of the embedding vectors follow the path of a helix, both on the encoder side and on the decoder side. We additionally show that on the encoder side, the conceptual dimensions generate Part-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram approach helps to reveal the PoS clusters of the next token. Our approach paves a way to elucidate the processing of information through the deep layers of an NLP Transformer.", "url": "https://arxiv.org/abs/2306.13817"}, {"metadata": {"arXiv": "2306.13885", "Date": "Sat, 24 Jun 2023 07:21:28 ", "Title": "Manipulation Risks in Explainable AI: The Implications of the Disagreement Problem", "Authors": ["Sofie Goethals and David Martens and Theodoros Evgeniou"], "Categories": "cs.AI"}, "abstract": "Artificial Intelligence (AI) systems are increasingly used in high-stakes domains of our life, increasing the need to explain these decisions and to make sure that they are aligned with how we want the decision to be made. The field of Explainable AI (XAI) has emerged in response. However, it faces a significant challenge known as the disagreement problem, where multiple explanations are possible for the same AI decision or prediction. While the existence of the disagreement problem is acknowledged, the potential implications associated with this problem have not yet been widely studied. First, we provide an overview of the different strategies explanation providers could deploy to adapt the returned explanation to their benefit. We make a distinction between strategies that attack the machine learning model or underlying data to influence the explanations, and strategies that leverage the explanation phase directly. Next, we analyse several objectives and concrete scenarios the providers could have to engage in this behavior, and the potential dangerous consequences this manipulative behavior could have on society. We emphasize that it is crucial to investigate this issue now, before these methods are widely implemented, and propose some mitigation strategies.", "url": "https://arxiv.org/abs/2306.13885"}, {"metadata": {"arXiv": "2306.13935", "Date": "Sat, 24 Jun 2023 10:50:42 ", "Title": "Are Good Explainers Secretly Human-in-the-Loop Active Learners?", "Authors": ["Emma Thuong Nguyen", "Abhishek Ghose"], "Categories": "cs.AI", "Comments": ["Accepted in AI & HCI workshop", "ICML 2023"]}, "abstract": "Explainable AI (XAI) techniques have become popular for multiple use-cases in the past few years. Here we consider its use in studying model predictions to gather additional training data. We argue that this is equivalent to Active Learning, where the query strategy involves a human-in-the-loop. We provide a mathematical approximation for the role of the human, and present a general formalization of the end-to-end workflow. This enables us to rigorously compare this use with standard Active Learning algorithms, while allowing for extensions to the workflow. An added benefit is that their utility can be assessed via simulation instead of conducting expensive user-studies. We also present some initial promising results.", "url": "https://arxiv.org/abs/2306.13935"}, {"metadata": {"arXiv": "2306.13956", "Date": "Sat, 24 Jun 2023 13:07:08 ", "Title": "Pointwise-in-Time Explanation for Linear Temporal Logic Rules", "Authors": ["Noel Brindise and Cedric Langbort"], "Categories": "cs.AI", "Comments": ["Submitted to CDC 2023"]}, "abstract": "This work introduces a framework to assess the relevance of individual linear temporal logic (LTL) constraints at specific times in a given path plan, a task we refer to as \"pointwise-in-time\" explanation. We develop this framework, featuring a status assessment algorithm, for agents which execute finite plans in a discrete-time, discrete-space setting expressible via a Kripke structure. Given a plan on this structure and a set of LTL rules which are known to constrain the agent, the algorithm responds to two types of user queries to produce explanation. For the selected query time, explanations identify which rules are active, which have just been satisfied, and which are inactive, where the framework status criteria are formally and intuitively defined. Explanations may also include the status of individual rule arguments to provide further insight. In this paper, we systematically present this novel framework and provide an example of its implementation.", "url": "https://arxiv.org/abs/2306.13956"}, {"metadata": {"arXiv": "2306.13961", "Date": "Sat, 24 Jun 2023 13:30:04 ", "Title": "Categorical Approach to Conflict Resolution: Integrating Category Theory into the Graph Model for Conflict Resolution", "Authors": ["Yukiko Kato"], "Categories": "cs.AI cs.GT"}, "abstract": "This paper introduces the Categorical Graph Model for Conflict Resolution (C-GMCR), a novel framework that integrates category theory into the traditional Graph Model for Conflict Resolution (GMCR). The C-GMCR framework provides a more abstract and general way to model and analyze conflict resolution, enabling researchers to uncover deeper insights and connections. We present the basic concepts, methods, and application of the C-GMCR framework to the well-known Prisoner's Dilemma and other representative cases. The findings suggest that the categorical approach offers new perspectives on stability concepts and can potentially lead to the development of more effective conflict resolution strategies.", "url": "https://arxiv.org/abs/2306.13961"}, {"metadata": {"arXiv": "2306.14062", "Date": "Sat, 24 Jun 2023 21:08:15 ", "Title": "On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions", "Authors": ["Reza Fayyazi", "Shanchieh Jay Yang"], "Categories": "cs.AI cs.CR"}, "abstract": "The volume, variety, and velocity of change in vulnerabilities and exploits have made incident threat analysis challenging with human expertise and experience along. The MITRE AT&CK framework employs Tactics, Techniques, and Procedures (TTPs) to describe how and why attackers exploit vulnerabilities. However, a TTP description written by one security professional can be interpreted very differently by another, leading to confusion in cybersecurity operations or even business, policy, and legal decisions. Meanwhile, advancements in AI have led to the increasing use of Natural Language Processing (NLP) algorithms to assist the various tasks in cyber operations. With the rise of Large Language Models (LLMs), NLP tasks have significantly improved because of the LLM's semantic understanding and scalability. This leads us to question how well LLMs can interpret TTP or general cyberattack descriptions. We propose and analyze the direct use of LLMs as well as training BaseLLMs with ATT&CK descriptions to study their capability in predicting ATT&CK tactics. Our results reveal that the BaseLLMs with supervised training provide a more focused and clearer differentiation between the ATT&CK tactics (if such differentiation exists). On the other hand, LLMs offer a broader interpretation of cyberattack techniques. Despite the power of LLMs, inherent ambiguity exists within their predictions. We thus summarize the existing challenges and recommend research directions on LLMs to deal with the inherent ambiguity of TTP descriptions.", "url": "https://arxiv.org/abs/2306.14062"}, {"metadata": {"arXiv": "2306.14077", "Date": "Sat, 24 Jun 2023 23:33:00 ", "Title": "Full Automation of Goal-driven LLM Dialog Threads with And-Or Recursors and Refiner Oracles", "Authors": ["Paul Tarau"], "Categories": "cs.AI cs.LO", "Comments": ["23 pages", "1 figure", "more information at https://github.com/ptarau/recursors"]}, "abstract": "We automate deep step-by step reasoning in an LLM dialog thread by recursively exploring alternatives (OR-nodes) and expanding details (AND-nodes) up to a given depth. Starting from a single succinct task-specific initiator we steer the automated dialog thread to stay focussed on the task by synthesizing a prompt that summarizes the depth-first steps taken so far. Our algorithm is derived from a simple recursive descent implementation of a Horn Clause interpreter, except that we accommodate our logic engine to fit the natural language reasoning patterns LLMs have been trained on. Semantic similarity to ground-truth facts or oracle advice from another LLM instance is used to restrict the search space and validate the traces of justification steps returned as answers. At the end, the unique minimal model of a generated Horn Clause program collects the results of the reasoning process. As applications, we sketch implementations of consequence predictions, causal explanations, recommendation systems and topic-focussed exploration of scientific literature.", "url": "https://arxiv.org/abs/2306.14077"}, {"metadata": {"arXiv": "2306.14165", "Date": "Sun, 25 Jun 2023 08:18:03 ", "Title": "Interactive Design by Integrating a Large Pre-Trained Language Model and Building Information Modeling", "Authors": ["Suhyung Jang and Ghang Lee"], "Categories": "cs.AI cs.HC"}, "abstract": "This study explores the potential of generative artificial intelligence (AI) models, specifically OpenAI's generative pre-trained transformer (GPT) series, when integrated with building information modeling (BIM) tools as an interactive design assistant for architectural design. The research involves the development and implementation of three key components: 1) BIM2XML, a component that translates BIM data into extensible markup language (XML) format; 2) Generative AI-enabled Interactive Architectural design (GAIA), a component that refines the input design in XML by identifying designer intent, relevant objects, and their attributes, using pre-trained language models; and 3) XML2BIM, a component that converts AI-generated XML data back into a BIM tool. This study validated the proposed approach through a case study involving design detailing, using the GPT series and Revit. Our findings demonstrate the effectiveness of state-of-the-art language models in facilitating dynamic collaboration between architects and AI systems, highlighting the potential for further advancements.", "url": "https://arxiv.org/abs/2306.14165"}, {"metadata": {"arXiv": "2306.14256", "Date": "Sun, 25 Jun 2023 14:28:12 ", "Title": "A Multilingual Translator to SQL with Database Schema Pruning to Improve Self-Attention", "Authors": ["Marcelo Archanjo Jose and Fabio Gagliardi Cozman"], "Categories": "cs.AI", "Comments": ["This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in International Journal of Information Technology", "and is available online at https://doi.org/10.1007/s41870-023-01342-3 . SharedIt link: https://rdcu.be/dff19"], "MSC-class": "68T07, 68T50", "ACM-class": "I.2.7; H.3.3", "DOI": "10.1007/s41870-023-01342-3"}, "abstract": "Long sequences of text are challenging in the context of transformers, due to quadratic memory increase in the self-attention mechanism. As this issue directly affects the translation from natural language to SQL queries (as techniques usually take as input a concatenated text with the question and the database schema), we present techniques that allow long text sequences to be handled by transformers with up to 512 input tokens. We propose a training process with database schema pruning (removal of tables and columns names that are useless for the query of interest). In addition, we used a multilingual approach with the mT5-large model fine-tuned with a data-augmented Spider dataset in four languages simultaneously: English, Portuguese, Spanish, and French. Our proposed technique used the Spider dataset and increased the exact set match accuracy results from 0.718 to 0.736 in a validation dataset (Dev). Source code, evaluations, and checkpoints are available at: \\underline{https://github.com/C4AI/gap-text2sql}.", "url": "https://arxiv.org/abs/2306.14256"}, {"metadata": {"arXiv": "2306.14349", "Date": "Sun, 25 Jun 2023 21:50:14 ", "Title": "Utilizing deep learning for automated tuning of database management systems", "Authors": ["Karthick Prasad Gunasekaran", "Kajal Tiwari", "Rachana Acharya"], "Categories": "cs.AI cs.DB", "Journal-ref": "2023 International Conference on Robotics, Machine Learning and Signal Processing"}, "abstract": "Managing the configurations of a database system poses significant challenges due to the multitude of configuration knobs that impact various system aspects.The lack of standardization, independence, and universality among these knobs further complicates the task of determining the optimal settings.To address this issue, an automated solution leveraging supervised and unsupervised machine learning techniques was developed.This solution aims to identify influential knobs, analyze previously unseen workloads, and provide recommendations for knob settings.The effectiveness of this approach is demonstrated through the evaluation of a new tool called OtterTune [1] on three different database management systems (DBMSs).The results indicate that OtterTune's recommendations are comparable to or even surpass the configurations generated by existing tools or human experts.In this study, we build upon the automated technique introduced in the original OtterTune paper, utilizing previously collected training data to optimize new DBMS deployments.By employing supervised and unsupervised machine learning methods, we focus on improving latency prediction.Our approach expands upon the methods proposed in the original paper by incorporating GMM clustering to streamline metrics selection and combining ensemble models (such as RandomForest) with non-linear models (like neural networks) for more accurate prediction modeling.", "url": "https://arxiv.org/abs/2306.14349"}, {"metadata": {"arXiv": "2306.14356", "Date": "Sun, 25 Jun 2023 22:16:59 ", "Title": "Smart Transformation of EFL Teaching and Learning Approaches", "Authors": ["Md. Russell Talukder"], "Categories": "cs.AI", "Comments": ["59 pages ", "7 figures", "30 tables", "multidisciplinary research article"], "DOI": "10.33166/AETiC.2023.03.002"}, "abstract": "The calibration of the EFL teaching and learning approaches with Artificial Intelligence can potentially facilitate a smart transformation, fostering a personalized and engaging experience in teaching and learning among the stakeholders. The paper focuses on developing an EFL Big Data Ecosystem that is based on Big Data, Analytics, Machine Learning and cluster domain of EFL teaching and learning contents. Accordingly, the paper uses two membranes to construe its framework, namely (i) Open Big Data Membrane that stores random data collected from various source domains and (ii) Machine Learning Membrane that stores specially prepared structured and semi-structured data. Theoretically, the structured and semi structured data are to be prepared skill-wise, attribute-wise, method-wise, and preference-wise to accommodate the personalized preferences and diverse teaching and learning needs of different individuals. The ultimate goal is to optimize the learning experience by leveraging machine learning to create tailored content that aligns with the diverse teaching and learning needs of the EFL communities.", "url": "https://arxiv.org/abs/2306.14356"}, {"metadata": {"arXiv": "2306.14421", "Date": "Mon, 26 Jun 2023 05:03:24 ", "Title": "A Preference-aware Meta-optimization Framework for Personalized Vehicle Energy Consumption Estimation", "Authors": ["Siqi Lai (1)", "Weijia Zhang (1)", "Hao Liu (1", "2) ((1) The Hong Kong University of Science and Technology (Guangzhou)", "(2) The Hong Kong University of Science and Technology)"], "Categories": "cs.AI", "DOI": "10.1145/3580305.3599767"}, "abstract": "Vehicle Energy Consumption (VEC) estimation aims to predict the total energy required for a given trip before it starts, which is of great importance to trip planning and transportation sustainability. Existing approaches mainly focus on extracting statistically significant factors from typical trips to improve the VEC estimation. However, the energy consumption of each vehicle may diverge widely due to the personalized driving behavior under varying travel contexts. To this end, this paper proposes a preference-aware meta-optimization framework Meta-Pec for personalized vehicle energy consumption estimation. Specifically, we first propose a spatiotemporal behavior learning module to capture the latent driver preference hidden in historical trips. Moreover, based on the memorization of driver preference, we devise a selection-based driving behavior prediction module to infer driver-specific driving patterns on a given route, which provides additional basis and supervision signals for VEC estimation. Besides, a driver-specific meta-optimization scheme is proposed to enable fast model adaption by learning and sharing transferable knowledge globally. Extensive experiments on two real-world datasets show the superiority of our proposed framework against ten numerical and data-driven machine learning baselines. The source code is available at https://github.com/usail-hkust/Meta-Pec.", "url": "https://arxiv.org/abs/2306.14421"}, {"metadata": {"arXiv": "2306.14535", "Date": "Mon, 26 Jun 2023 09:19:01 ", "Title": "About the Cost of Global Privacy in Density Estimation", "Authors": ["Cl\\'ement Lalanne (ENS de Lyon", "OCKHAM)", "Aur\\'elien Garivier (UMPA-ENSL", "MC2)", "R\\'emi Gribonval (OCKHAM)"], "Categories": "cs.AI math.ST stat.TH"}, "abstract": "We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces, and under global privacy. In particular, we investigate regimes where the privacy budget is not supposed to be constant. We consider the classical definition of global differential privacy, but also the more recent notion of global concentrated differential privacy. We recover the result of Barber \\& Duchi (2014) stating that histogram estimators are optimal against Lipschitz distributions for the L2 risk, and under regular differential privacy, and we extend it to other norms and notions of privacy. Then, we investigate higher degrees of smoothness, drawing two conclusions: First, and contrary to what happens with constant privacy budget (Wasserman \\& Zhou, 2010), there are regimes where imposing privacy degrades the regular minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are near-optimal against the same classes of densities in this new setup with pure differential privacy, but contrary to the constant privacy budget case, it comes at the cost of relaxation. With zero concentrated differential privacy, there is no need for relaxation, and we prove that the estimation is optimal.", "url": "https://arxiv.org/abs/2306.14535"}, {"metadata": {"arXiv": "2306.14545", "Date": "Mon, 26 Jun 2023 09:35:56 ", "Title": "Neural State-Dependent Delay Differential Equations", "Authors": ["Thibault Monsel (DATAFLOT", "TAU)", "Onofrio Semeraro (DATAFLOT)", "Lionel Mathelin (DATAFLOT)", "Guillaume Charpiat (TAU)"], "Categories": "cs.AI math.DS"}, "abstract": "Discontinuities and delayed terms are encountered in the governing equations of a large class of problems ranging from physics, engineering, medicine to economics. These systems are impossible to be properly modelled and simulated with standard Ordinary Differential Equations (ODE), or any data-driven approximation including Neural Ordinary Differential Equations (NODE). To circumvent this issue, latent variables are typically introduced to solve the dynamics of the system in a higher dimensional space and obtain the solution as a projection to the original space. However, this solution lacks physical interpretability. In contrast, Delay Differential Equations (DDEs) and their data-driven, approximated counterparts naturally appear as good candidates to characterize such complicated systems. In this work we revisit the recently proposed Neural DDE by introducing Neural State-Dependent DDE (SDDDE), a general and flexible framework featuring multiple and state-dependent delays. The developed framework is auto-differentiable and runs efficiently on multiple backends. We show that our method is competitive and outperforms other continuous-class models on a wide variety of delayed dynamical systems.", "url": "https://arxiv.org/abs/2306.14545"}, {"metadata": {"arXiv": "2306.14546", "Date": "Mon, 26 Jun 2023 09:39:05 ", "Title": "logLTN: Differentiable Fuzzy Logic in the Logarithm Space", "Authors": ["Samy Badreddine", "Luciano Serafini", "Michael Spranger"], "Categories": "cs.AI"}, "abstract": "The AI community is increasingly focused on merging logic with deep learning to create Neuro-Symbolic (NeSy) paradigms and assist neural approaches with symbolic knowledge. A significant trend in the literature involves integrating axioms and facts in loss functions by grounding logical symbols with neural networks and operators with fuzzy semantics. Logic Tensor Networks (LTN) is one of the main representatives in this category, known for its simplicity, efficiency, and versatility. However, it has been previously shown that not all fuzzy operators perform equally when applied in a differentiable setting. Researchers have proposed several configurations of operators, trading off between effectiveness, numerical stability, and generalization to different formulas. This paper presents a configuration of fuzzy operators for grounding formulas end-to-end in the logarithm space. Our goal is to develop a configuration that is more effective than previous proposals, able to handle any formula, and numerically stable. To achieve this, we propose semantics that are best suited for the logarithm space and introduce novel simplifications and improvements that are crucial for optimization via gradient-descent. We use LTN as the framework for our experiments, but the conclusions of our work apply to any similar NeSy framework. Our findings, both formal and empirical, show that the proposed configuration outperforms the state-of-the-art and that each of our modifications is essential in achieving these results.", "url": "https://arxiv.org/abs/2306.14546"}, {"metadata": {"arXiv": "2306.14551", "Date": "Mon, 26 Jun 2023 09:49:51 ", "Title": "Creating user stereotypes for persona development from qualitative data through semi-automatic subspace clustering", "Authors": ["Dannie Korsgaard", "Thomas Bjorner", "Pernille Krog Sorensen", "Paolo Burelli"], "Categories": "cs.AI cs.HC", "Journal-ref": "User modeling and user-adapted interaction 2020", "DOI": "10.1007/s11257-019-09252-5"}, "abstract": "Personas are models of users that incorporate motivations, wishes, and objectives; These models are employed in user-centred design to help design better user experiences and have recently been employed in adaptive systems to help tailor the personalized user experience. Designing with personas involves the production of descriptions of fictitious users, which are often based on data from real users. The majority of data-driven persona development performed today is based on qualitative data from a limited set of interviewees and transformed into personas using labour-intensive manual techniques. In this study, we propose a method that employs the modelling of user stereotypes to automate part of the persona creation process and addresses the drawbacks of the existing semi-automated methods for persona development. The description of the method is accompanied by an empirical comparison with a manual technique and a semi-automated alternative (multiple correspondence analysis). The results of the comparison show that manual techniques differ between human persona designers leading to different results. The proposed algorithm provides similar results based on parameter input, but was more rigorous and will find optimal clusters, while lowering the labour associated with finding the clusters in the dataset. The output of the method also represents the largest variances in the dataset identified by the multiple correspondence analysis.", "url": "https://arxiv.org/abs/2306.14551"}, {"metadata": {"arXiv": "2306.14694", "Date": "Mon, 26 Jun 2023 13:39:36 ", "Title": "DR-HAI: Argumentation-based Dialectical Reconciliation in Human-AI Interactions", "Authors": ["Stylianos Loukas Vasileiou", "Ashwin Kumar", "William Yeoh", "Tran Cao Son", "Francesca Toni"], "Categories": "cs.AI cs.HC cs.LO"}, "abstract": "We introduce DR-HAI -- a novel argumentation-based framework designed to extend model reconciliation approaches, commonly used in explainable AI planning, for enhanced human-AI interaction. By adopting a multi-shot reconciliation paradigm and not assuming a-priori knowledge of the human user's model, DR-HAI enables interactive reconciliation to address knowledge discrepancies between an explainer and an explainee. We formally describe the operational semantics of DR-HAI, provide theoretical guarantees related to termination and success, and empirically evaluate its efficacy. Our findings suggest that DR-HAI offers a promising direction for fostering effective human-AI interactions.", "url": "https://arxiv.org/abs/2306.14694"}, {"metadata": {"arXiv": "2306.14722", "Date": "Mon, 26 Jun 2023 14:19:46 ", "Title": "FC-KBQA: A Fine-to-Coarse Composition Framework for Knowledge Base Question Answering", "Authors": ["Lingxi Zhang", "Jing Zhang", "Yanling Wang", "Shulin Cao", "Xinmei Huang", "Cuiping Li", "Hong Chen", "Juanzi Li"], "Categories": "cs.AI"}, "abstract": "The generalization problem on KBQA has drawn considerable attention. Existing research suffers from the generalization issue brought by the entanglement in the coarse-grained modeling of the logical expression, or inexecutability issues due to the fine-grained modeling of disconnected classes and relations in real KBs. We propose a Fine-to-Coarse Composition framework for KBQA (FC-KBQA) to both ensure the generalization ability and executability of the logical expression. The main idea of FC-KBQA is to extract relevant fine-grained knowledge components from KB and reformulate them into middle-grained knowledge pairs for generating the final logical expressions. FC-KBQA derives new state-of-the-art performance on GrailQA and WebQSP, and runs 4 times faster than the baseline.", "url": "https://arxiv.org/abs/2306.14722"}, {"metadata": {"arXiv": "2306.14816", "Date": "Mon, 26 Jun 2023 16:22:13 ", "Title": "Experiments with Detecting and Mitigating AI Deception", "Authors": ["Ismail Sahbane", "Francis Rhys Ward", "C Henrik {\\AA}slund"], "Categories": "cs.AI", "Comments": ["4 pages", "2 figures", "3 algorithms", "1 table"]}, "abstract": "How to detect and mitigate deceptive AI systems is an open problem for the field of safe and trustworthy AI. We analyse two algorithms for mitigating deception: The first is based on the path-specific objectives framework where paths in the game that incentivise deception are removed. The second is based on shielding, i.e., monitoring for unsafe policies and replacing them with a safe reference policy. We construct two simple games and evaluate our algorithms empirically. We find that both methods ensure that our agent is not deceptive, however, shielding tends to achieve higher reward.", "url": "https://arxiv.org/abs/2306.14816"}, {"metadata": {"arXiv": "2306.13725", "Date": "Fri, 23 Jun 2023 18:14:26 ", "Title": "Improving Panoptic Segmentation for Nighttime or Low-Illumination Urban Driving Scenes", "Authors": ["Ankur Chrungoo"], "Categories": "cs.CV cs.AI", "Comments": ["12 pages", "6 figures"]}, "abstract": "Autonomous vehicles and driving systems use scene parsing as an essential tool to understand the surrounding environment. Panoptic segmentation is a state-of-the-art technique which proves to be pivotal in this use case. Deep learning-based architectures have been utilized for effective and efficient Panoptic Segmentation in recent times. However, when it comes to adverse conditions like dark scenes with poor illumination or nighttime images, existing methods perform poorly in comparison to daytime images. One of the main factors for poor results is the lack of sufficient and accurately annotated nighttime images for urban driving scenes. In this work, we propose two new methods, first to improve the performance, and second to improve the robustness of panoptic segmentation in nighttime or poor illumination urban driving scenes using a domain translation approach. The proposed approach makes use of CycleGAN (Zhu et al., 2017) to translate daytime images with existing panoptic annotations into nighttime images, which are then utilized to retrain a Panoptic segmentation model to improve performance and robustness under poor illumination and nighttime conditions. In our experiments, Approach-1 demonstrates a significant improvement in the Panoptic segmentation performance on the converted Cityscapes dataset with more than +10% PQ, +12% RQ, +2% SQ, +14% mIoU and +10% AP50 absolute gain. Approach-2 demonstrates improved robustness to varied nighttime driving environments. Both the approaches are supported via comprehensive quantitative and qualitative analysis.", "url": "https://arxiv.org/abs/2306.13725"}, {"metadata": {"arXiv": "2306.13856", "Date": "Sat, 24 Jun 2023 04:11:31 ", "Title": "Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification", "Authors": ["Rui Wang", "Peipei Li", "Huaibo Huang", "Chunshui Cao", "Ran He", "Zhaofeng He"], "Categories": "cs.CV cs.AI", "Comments": ["14 pages", "6 figures"]}, "abstract": "We present a novel language-driven ordering alignment method for ordinal classification. The labels in ordinal classification contain additional ordering relations, making them prone to overfitting when relying solely on training data. Recent developments in pre-trained vision-language models inspire us to leverage the rich ordinal priors in human language by converting the original task into a vision-language alignment task. Consequently, we propose L2RCLIP, which fully utilizes the language priors from two perspectives. First, we introduce a complementary prompt tuning technique called RankFormer, designed to enhance the ordering relation of original rank prompts. It employs token-level attention with residual-style prompt blending in the word embedding space. Second, to further incorporate language priors, we revisit the approximate bound optimization of vanilla cross-entropy loss and restructure it within the cross-modal embedding space. Consequently, we propose a cross-modal ordinal pairwise loss to refine the CLIP feature space, where texts and images maintain both semantic alignment and ordering alignment. Extensive experiments on three ordinal classification tasks, including facial age estimation, historical color image (HCI) classification, and aesthetic assessment demonstrate its promising performance.", "url": "https://arxiv.org/abs/2306.13856"}, {"metadata": {"arXiv": "2306.14106", "Date": "Sun, 25 Jun 2023 02:54:03 ", "Title": "Semi-supervised Object Detection: A Survey on Recent Research and Progress", "Authors": ["Yanyang Wang", "Zhaoxiang Liu", "Shiguo Lian"], "Categories": "cs.CV cs.AI", "Comments": ["10 pages", "20 figures", "2 tables"]}, "abstract": "In recent years, deep learning technology has been maturely applied in the field of object detection, and most algorithms tend to be supervised learning. However, a large amount of labeled data requires high costs of human resources, which brings about low efficiency and limitations. Semi-supervised object detection (SSOD) has been paid more and more attentions due to its high research value and practicability. It is designed to learn information by using small amounts of labeled data and large amounts of unlabeled data. In this paper, we present a comprehensive and up-to-date survey on the SSOD approaches from five aspects. We first briefly introduce several ways of data augmentation. Then, we dive the mainstream semi-supervised strategies into pseudo labels, consistent regularization, graph based and transfer learning based methods, and introduce some methods in challenging settings. We further present widely-used loss functions, and then we outline the common benchmark datasets and compare the accuracy among different representative approaches. Finally, we conclude this paper and present some promising research directions for the future. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches developed over the past few years.", "url": "https://arxiv.org/abs/2306.14106"}, {"metadata": {"arXiv": "2306.14169", "Date": "Sun, 25 Jun 2023 08:23:44 ", "Title": "A Web-based Mpox Skin Lesion Detection System Using State-of-the-art Deep Learning Models Considering Racial Diversity", "Authors": ["Shams Nafisa Ali", "Md. Tazuddin Ahmed", "Tasnim Jahan", "Joydip Paul", "S. M. Sakeef Sani", "Nawsabah Noor", "Anzirun Nahar Asma", "Taufiq Hasan"], "Categories": "cs.CV cs.AI"}, "abstract": "The recent 'Mpox' outbreak, formerly known as 'Monkeypox', has become a significant public health concern and has spread to over 110 countries globally. The challenge of clinically diagnosing mpox early on is due, in part, to its similarity to other types of rashes. Computer-aided screening tools have been proven valuable in cases where Polymerase Chain Reaction (PCR) based diagnosis is not immediately available. Deep learning methods are powerful in learning complex data representations, but their efficacy largely depends on adequate training data. To address this challenge, we present the \"Mpox Skin Lesion Dataset Version 2.0 (MSLD v2.0)\" as a follow-up to the previously released openly accessible dataset, one of the first datasets containing mpox lesion images. This dataset contains images of patients with mpox and five other non-mpox classes (chickenpox, measles, hand-foot-mouth disease, cowpox, and healthy). We benchmark the performance of several state-of-the-art deep learning models, including VGG16, ResNet50, DenseNet121, MobileNetV2, EfficientNetB3, InceptionV3, and Xception, to classify mpox and other infectious skin diseases. In order to reduce the impact of racial bias, we utilize a color space data augmentation method to increase skin color variability during training. Additionally, by leveraging transfer learning implemented with pre-trained weights generated from the HAM10000 dataset, an extensive collection of pigmented skin lesion images, we achieved the best overall accuracy of $83.59\\pm2.11\\%$. Finally, the developed models are incorporated within a prototype web application to analyze uploaded skin images by a user and determine whether a subject is a suspected mpox patient.", "url": "https://arxiv.org/abs/2306.14169"}, {"metadata": {"arXiv": "2306.14182", "Date": "Sun, 25 Jun 2023 09:28:40 ", "Title": "Switch-BERT: Learning to Model Multimodal Interactions by Switching Attention and Input", "Authors": ["Qingpei Guo", "Kaisheng Yao and Wei Chu"], "Categories": "cs.CV cs.AI", "Comments": ["Accepted by ECCV2022"]}, "abstract": "The ability to model intra-modal and inter-modal interactions is fundamental in multimodal machine learning. The current state-of-the-art models usually adopt deep learning models with fixed structures. They can achieve exceptional performances on specific tasks, but face a particularly challenging problem of modality mismatch because of diversity of input modalities and their fixed structures. In this paper, we present \\textbf{Switch-BERT} for joint vision and language representation learning to address this problem. Switch-BERT extends BERT architecture by introducing learnable layer-wise and cross-layer interactions. It learns to optimize attention from a set of attention modes representing these interactions. One specific property of the model is that it learns to attend outputs from various depths, therefore mitigates the modality mismatch problem. We present extensive experiments on visual question answering, image-text retrieval and referring expression comprehension experiments. Results confirm that, whereas alternative architectures including ViLBERT and UNITER may excel in particular tasks, Switch-BERT can consistently achieve better or comparable performances than the current state-of-the-art models in these tasks. Ablation studies indicate that the proposed model achieves superior performances due to its ability in learning task-specific multimodal interactions.", "url": "https://arxiv.org/abs/2306.14182"}, {"metadata": {"arXiv": "2306.14209", "Date": "Sun, 25 Jun 2023 11:19:47 ", "Title": "Deep image prior inpainting of ancient frescoes in the Mediterranean Alpine arc", "Authors": ["Fabio Merizzi", "Perrine Saillard", "Oceane Acquier", "Elena Morotti", "Elena Loli Piccolomini", "Luca Calatroni and Rosa Maria Dess\\`i"], "Categories": "cs.CV cs.AI", "Comments": ["20 pages", "12 figures"]}, "abstract": "The unprecedented success of image reconstruction approaches based on deep neural networks has revolutionised both the processing and the analysis paradigms in several applied disciplines. In the field of digital humanities, the task of digital reconstruction of ancient frescoes is particularly challenging due to the scarce amount of available training data caused by ageing, wear, tear and retouching over time. To overcome these difficulties, we consider the Deep Image Prior (DIP) inpainting approach which computes appropriate reconstructions by relying on the progressive updating of an untrained convolutional neural network so as to match the reliable piece of information in the image at hand while promoting regularisation elsewhere. In comparison with state-of-the-art approaches (based on variational/PDEs and patch-based methods), DIP-based inpainting reduces artefacts and better adapts to contextual/non-local information, thus providing a valuable and effective tool for art historians. As a case study, we apply such approach to reconstruct missing image contents in a dataset of highly damaged digital images of medieval paintings located into several chapels in the Mediterranean Alpine Arc and provide a detailed description on how visible and invisible (e.g., infrared) information can be integrated for identifying and reconstructing damaged image regions.", "url": "https://arxiv.org/abs/2306.14209"}, {"metadata": {"arXiv": "2306.14293", "Date": "Sun, 25 Jun 2023 16:55:32 ", "Title": "Multi-Scale Cross Contrastive Learning for Semi-Supervised Medical Image Segmentation", "Authors": ["Qianying Liu", "Xiao Gu", "Paul Henderson", "Fani Deligianni"], "Categories": "cs.CV cs.AI"}, "abstract": "Semi-supervised learning has demonstrated great potential in medical image segmentation by utilizing knowledge from unlabeled data. However, most existing approaches do not explicitly capture high-level semantic relations between distant regions, which limits their performance. In this paper, we focus on representation learning for semi-supervised learning, by developing a novel Multi-Scale Cross Supervised Contrastive Learning (MCSC) framework, to segment structures in medical images. We jointly train CNN and Transformer models, regularising their features to be semantically consistent across different scales. Our approach contrasts multi-scale features based on ground-truth and cross-predicted labels, in order to extract robust feature representations that reflect intra- and inter-slice relationships across the whole dataset. To tackle class imbalance, we take into account the prevalence of each class to guide contrastive learning and ensure that features adequately capture infrequent classes. Extensive experiments on two multi-structure medical segmentation datasets demonstrate the effectiveness of MCSC. It not only outperforms state-of-the-art semi-supervised methods by more than 3.0% in Dice, but also greatly reduces the performance gap with fully supervised methods.", "url": "https://arxiv.org/abs/2306.14293"}, {"metadata": {"arXiv": "2306.14300", "Date": "Sun, 25 Jun 2023 18:02:01 ", "Title": "Screening Autism Spectrum Disorder in childrens using Deep Learning Approach : Evaluating the classification model of YOLOv8 by comparing with other models", "Authors": ["Subash Gautam", "Prabin Sharma", "Kisan Thapa", "Mala Deep Upadhaya", "Dikshya Thapa", "Salik Ram Khanal", "V\\'itor Manuel de Jesus Filipe"], "Categories": "cs.CV cs.AI", "Comments": ["17 pages,12 figures"]}, "abstract": "Autism spectrum disorder (ASD) is a developmental condition that presents significant challenges in social interaction, communication, and behavior. Early intervention plays a pivotal role in enhancing cognitive abilities and reducing autistic symptoms in children with ASD. Numerous clinical studies have highlighted distinctive facial characteristics that distinguish ASD children from typically developing (TD) children. In this study, we propose a practical solution for ASD screening using facial images using YoloV8 model. By employing YoloV8, a deep learning technique, on a dataset of Kaggle, we achieved exceptional results. Our model achieved a remarkable 89.64% accuracy in classification and an F1-score of 0.89. Our findings provide support for the clinical observations regarding facial feature discrepancies between children with ASD. The high F1-score obtained demonstrates the potential of deep learning models in screening children with ASD. We conclude that the newest version of YoloV8 which is usually used for object detection can be used for classification problem of Austistic and Non-autistic images.", "url": "https://arxiv.org/abs/2306.14300"}, {"metadata": {"arXiv": "2306.14448", "Date": "Mon, 26 Jun 2023 06:34:53 ", "Title": "Progressive Energy-Based Cooperative Learning for Multi-Domain Image-to-Image Translation", "Authors": ["Weinan Song", "Yaxuan Zhu", "Lei He", "Yingnian Wu", "and Jianwen Xie"], "Categories": "cs.CV cs.AI"}, "abstract": "This paper studies a novel energy-based cooperative learning framework for multi-domain image-to-image translation. The framework consists of four components: descriptor, translator, style encoder, and style generator. The descriptor is a multi-head energy-based model that represents a multi-domain image distribution. The components of translator, style encoder, and style generator constitute a diversified image generator. Specifically, given an input image from a source domain, the translator turns it into a stylised output image of the target domain according to a style code, which can be inferred by the style encoder from a reference image or produced by the style generator from a random noise. Since the style generator is represented as an domain-specific distribution of style codes, the translator can provide a one-to-many transformation (i.e., diversified generation) between source domain and target domain. To train our framework, we propose a likelihood-based multi-domain cooperative learning algorithm to jointly train the multi-domain descriptor and the diversified image generator (including translator, style encoder, and style generator modules) via multi-domain MCMC teaching, in which the descriptor guides the diversified image generator to shift its probability density toward the data distribution, while the diversified image generator uses its randomly translated images to initialize the descriptor's Langevin dynamics process for efficient sampling.", "url": "https://arxiv.org/abs/2306.14448"}, {"metadata": {"arXiv": "2306.14490", "Date": "Mon, 26 Jun 2023 08:04:24 ", "Title": "TaiChi Action Capture and Performance Analysis with Multi-view RGB Cameras", "Authors": ["Jianwei Li", "Siyu Mo", "Yanfei Shen"], "Categories": "cs.CV cs.AI"}, "abstract": "Recent advances in computer vision and deep learning have influenced the field of sports performance analysis for researchers to track and reconstruct freely moving humans without any marker attachment. However, there are few works for vision-based motion capture and intelligent analysis for professional TaiChi movement. In this paper, we propose a framework for TaiChi performance capture and analysis with multi-view geometry and artificial intelligence technology. The main innovative work is as follows: 1) A multi-camera system suitable for TaiChi motion capture is built and the multi-view TaiChi data is collected and processed; 2) A combination of traditional visual method and implicit neural radiance field is proposed to achieve sparse 3D skeleton fusion and dense 3D surface reconstruction. 3) The normalization modeling of movement sequences is carried out based on motion transfer, so as to realize TaiChi performance analysis for different groups. We have carried out evaluation experiments, and the experimental results have shown the efficiency of our method.", "url": "https://arxiv.org/abs/2306.14490"}, {"metadata": {"arXiv": "2306.14505", "Date": "Mon, 26 Jun 2023 08:24:37 ", "Title": "AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor", "Authors": ["Yu-Jen Chen", "Xinrong Hu", "Yiyu Shi", "Tsung-Yi Ho"], "Categories": "cs.CV cs.AI", "Comments": ["arXiv admin note: text overlap with arXiv:2306.05476"]}, "abstract": "Magnetic resonance imaging (MRI) is commonly used for brain tumor segmentation, which is critical for patient evaluation and treatment planning. To reduce the labor and expertise required for labeling, weakly-supervised semantic segmentation (WSSS) methods with class activation mapping (CAM) have been proposed. However, existing CAM methods suffer from low resolution due to strided convolution and pooling layers, resulting in inaccurate predictions. In this study, we propose a novel CAM method, Attentive Multiple-Exit CAM (AME-CAM), that extracts activation maps from multiple resolutions to hierarchically aggregate and improve prediction accuracy. We evaluate our method on the BraTS 2021 dataset and show that it outperforms state-of-the-art methods.", "url": "https://arxiv.org/abs/2306.14505"}, {"metadata": {"arXiv": "2306.14565", "Date": "Mon, 26 Jun 2023 10:26:33 ", "Title": "Aligning Large Multi-Modal Model with Robust Instruction Tuning", "Authors": ["Fuxiao Liu", "Kevin Lin", "Linjie Li", "Jianfeng Wang", "Yaser Yacoob", "Lijuan Wang"], "Categories": "cs.CV cs.AI cs.CE cs.CL cs.MM", "Comments": ["35 pages", "27 figures. Under Review"]}, "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to evaluate visual instruction tuning without the need for human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate that existing LMMs exhibit significant hallucination when presented with our negative instructions, particularly with Existent Element Manipulation instructions. Moreover, by finetuning MiniGPT4 on LRV-Instruction, we successfully mitigate hallucination while improving performance on public datasets using less training data compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Our project link is available at https://fuxiaoliu.github.io/LRV/.", "url": "https://arxiv.org/abs/2306.14565"}, {"metadata": {"arXiv": "2306.14620", "Date": "Mon, 26 Jun 2023 11:52:22 ", "Title": "Video object detection for privacy-preserving patient monitoring in intensive care", "Authors": ["Raphael Emberger (1)", "Jens Michael Boss (2)", "Daniel Baumann (2)", "Marko Seric (2)", "Shufan Huo (2 and 3)", "Lukas Tuggener (1)", "Emanuela Keller (2)", "Thilo Stadelmann (1 and 4) ((1) Centre for Artificial Intelligence", "ZHAW School of Engineering", "Winterthur", "Switzerland", "(2) Neurocritical Care Unit", "Department of Neurosurgery and Institute of Intensive Care Medicine", "Clinical Neuroscience Center", "University Hospital Zurich and University of Zurich", "Switzerland", "(3) Neurology", "Charit\\'e - University Medicine Berlin", "Berlin", "Germany", "(4) European Centre for Living Technology (ECLT)", "Ca' Bottacin", "Venice", "Italy)"], "Categories": "cs.CV cs.AI", "Comments": ["4 pages", "3 figures", "2023 10th Swiss Conference on Data Science (SDS)", "code available at https://github.com/raember/yolov5r_autodidact and https://github.com/raember/VideoProc"], "ACM-class": "I.2.10", "DOI": "10.21256/zhaw-27810"}, "abstract": "Patient monitoring in intensive care units, although assisted by biosensors, needs continuous supervision of staff. To reduce the burden on staff members, IT infrastructures are built to record monitoring data and develop clinical decision support systems. These systems, however, are vulnerable to artifacts (e.g. muscle movement due to ongoing treatment), which are often indistinguishable from real and potentially dangerous signals. Video recordings could facilitate the reliable classification of biosignals using object detection (OD) methods to find sources of unwanted artifacts. Due to privacy restrictions, only blurred videos can be stored, which severely impairs the possibility to detect clinically relevant events such as interventions or changes in patient status with standard OD methods. Hence, new kinds of approaches are necessary that exploit every kind of available information due to the reduced information content of blurred footage and that are at the same time easily implementable within the IT infrastructure of a normal hospital. In this paper, we propose a new method for exploiting information in the temporal succession of video frames. To be efficiently implementable using off-the-shelf object detectors that comply with given hardware constraints, we repurpose the image color channels to account for temporal consistency, leading to an improved detection rate of the object classes. Our method outperforms a standard YOLOv5 baseline model by +1.7% mAP@.5 while also training over ten times faster on our proprietary dataset. We conclude that this approach has shown effectiveness in the preliminary experiments and holds potential for more general video OD in the future.", "url": "https://arxiv.org/abs/2306.14620"}, {"metadata": {"arXiv": "2306.14685", "Date": "Mon, 26 Jun 2023 13:30:38 ", "Title": "DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models", "Authors": ["Ximing Xing", "Chuang Wang", "Haitao Zhou", "Jing Zhang", "Qian Yu", "Dong Xu"], "Categories": "cs.CV cs.AI", "Comments": ["13 pages"]}, "abstract": "Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates vectorized free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of Bezier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than prior work.", "url": "https://arxiv.org/abs/2306.14685"}, {"metadata": {"arXiv": "2306.14899", "Date": "Mon, 26 Jun 2023 17:59:55 ", "Title": "FunQA: Towards Surprising Video Comprehension", "Authors": ["Binzhu Xie", "Sicheng Zhang", "Zitang Zhou", "Bo Li", "Yuanhan Zhang", "Jack Hessel", "Jingkang Yang", "Ziwei Liu"], "Categories": "cs.CV cs.AI cs.CL cs.MM", "Comments": ["Ask VLMs about humor", "creation", "and magics. Project Page: https://funqa-benchmark.github.io/ Codebase: https://github.com/Jingkang50/FunQA"]}, "abstract": "Surprising videos, e.g., funny clips, creative performances, or visual illusions, attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA, a challenging video question answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter-intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts, e.g., cooking or instructional videos, FunQA covers three previously unexplored types of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous QA tasks designed to assess the model's capability in counter-intuitive timestamp localization, detailed video description, and reasoning around counter-intuitiveness. We also pose higher-level tasks, such as attributing a fitting and vivid title to the video, and scoring the video creativity. In total, the FunQA benchmark consists of 312K free-text QA pairs derived from 4.3K video clips, spanning a total of 24 video hours. Extensive experiments with existing VideoQA models reveal significant performance gaps for the FunQA videos across spatial-temporal reasoning, visual-centered reasoning, and free-text generation.", "url": "https://arxiv.org/abs/2306.14899"}, {"metadata": {"arXiv": "2306.14055", "Date": "Sat, 24 Jun 2023 20:47:36 ", "Title": "Transforming a Quadruped into a Guide Robot for the Visually Impaired: Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism", "Authors": ["J. Taery Kim", "Wenhao Yu", "Yash Kothari", "Jie Tan", "Greg Turk", "Sehoon Ha"], "Categories": "cs.RO cs.AI", "Comments": ["16 pages", "8 figures"]}, "abstract": "This paper explores the principles for transforming a quadrupedal robot into a guide robot for individuals with visual impairments. A guide robot has great potential to resolve the limited availability of guide animals that are accessible to only two to three percent of the potential blind or visually impaired (BVI) users. To build a successful guide robot, our paper explores three key topics: (1) formalizing the navigation mechanism of a guide dog and a human, (2) developing a data-driven model of their interaction, and (3) improving user safety. First, we formalize the wayfinding task of the human-guide robot team using Markov Decision Processes based on the literature and interviews. Then we collect real human-robot interaction data from three visually impaired and six sighted people and develop an interaction model called the ``Delayed Harness'' to effectively simulate the navigation behaviors of the team. Additionally, we introduce an action shielding mechanism to enhance user safety by predicting and filtering out dangerous actions. We evaluate the developed interaction model and the safety mechanism in simulation, which greatly reduce the prediction errors and the number of collisions, respectively. We also demonstrate the integrated system on a quadrupedal robot with a rigid harness, by guiding users over $100+$~m trajectories.", "url": "https://arxiv.org/abs/2306.14055"}, {"metadata": {"arXiv": "2306.14437", "Date": "Mon, 26 Jun 2023 06:06:49 ", "Title": "A Self-supervised Contrastive Learning Method for Grasp Outcomes Prediction", "Authors": ["Chengliang Liu", "Binhua Huang", "Yiwen Liu", "Yuanzhe Su", "Ke Mai", "Yupo Zhang", "Zhengkun Yi", "Xinyu Wu"], "Categories": "cs.RO cs.AI", "Comments": ["Manuscript accepted to RCAR 2023"]}, "abstract": "In this paper, we investigate the effectiveness of contrastive learning methods for predicting grasp outcomes in an unsupervised manner. By utilizing a publicly available dataset, we demonstrate that contrastive learning methods perform well on the task of grasp outcomes prediction. Specifically, the dynamic-dictionary-based method with the momentum updating technique achieves a satisfactory accuracy of 81.83% using data from one single tactile sensor, outperforming other unsupervised methods. Our results reveal the potential of contrastive learning methods for applications in the field of robot grasping and highlight the importance of accurate grasp prediction for achieving stable grasps.", "url": "https://arxiv.org/abs/2306.14437"}, {"metadata": {"arXiv": "2306.14489", "Date": "Mon, 26 Jun 2023 08:02:55 ", "Title": "Decentralized Multi-Robot Formation Control Using Reinforcement Learning", "Authors": ["Juraj Obradovic", "Marko Krizmancic", "Stjepan Bogdan"], "Categories": "cs.RO cs.AI cs.MA", "Comments": ["7 pages", "10 figures. To be published in 2023 International Conference on Information", "Communication and Automation Technologies (ICAT)"]}, "abstract": "This paper presents a decentralized leader-follower multi-robot formation control based on a reinforcement learning (RL) algorithm applied to a swarm of small educational Sphero robots. Since the basic Q-learning method is known to require large memory resources for Q-tables, this work implements the Double Deep Q-Network (DDQN) algorithm, which has achieved excellent results in many robotic problems. To enhance the system behavior, we trained two different DDQN models, one for reaching the formation and the other for maintaining it. The models use a discrete set of robot motions (actions) to adapt the continuous nonlinear system to the discrete nature of RL. The presented approach has been tested in simulation and real experiments which show that the multi-robot system can achieve and maintain a stable formation without the need for complex mathematical models and nonlinear control laws.", "url": "https://arxiv.org/abs/2306.14489"}, {"metadata": {"arXiv": "2306.14619", "Date": "Mon, 26 Jun 2023 11:52:14 ", "Title": "Verification of Neural Network Control Systems using Symbolic Zonotopes and Polynotopes", "Authors": ["Carlos Trapiello", "Christophe Combastel", "Ali Zolghadri"], "Categories": "eess.SY cs.AI cs.SY", "Comments": ["14 pages", "6 figures", "3 tables"], "MSC-class": "93-08 (Primary) 93B03, 93C30, 93C41 (Secondary)", "ACM-class": "I.2.8; I.6.4; J.6"}, "abstract": "Verification and safety assessment of neural network controlled systems (NNCSs) is an emerging challenge. To provide guarantees, verification tools must efficiently capture the interplay between the neural network and the physical system within the control loop. In this paper, a compositional approach focused on inclusion preserving long term symbolic dependency modeling is proposed for the analysis of NNCSs. First of all, the matrix structure of symbolic zonotopes is exploited to efficiently abstract the input/output mapping of the loop elements through (inclusion preserving) affine symbolic expressions, thus maintaining linear dependencies between interacting blocks. Then, two further extensions are studied. Firstly, symbolic polynotopes are used to abstract the loop elements behaviour by means of polynomial symbolic expressions and dependencies. Secondly, an original input partitioning algorithm takes advantage of symbol preservation to assess the sensitivity of the computed approximation to some input directions. The approach is evaluated via different numerical examples and benchmarks. A good trade-off between low conservatism and computational efficiency is obtained.", "url": "https://arxiv.org/abs/2306.14619"}, {"metadata": {"arXiv": "2306.13761", "Date": "Fri, 23 Jun 2023 19:55:41 ", "Title": "CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation", "Authors": ["Amal Feriani", "Di Wu", "Steve Liu", "Greg Dudek"], "Categories": "cs.AI cs.LG"}, "abstract": "Deep learning has been extensively used in wireless communication problems, including channel estimation. Although several data-driven approaches exist, a fair and realistic comparison between them is difficult due to inconsistencies in the experimental conditions and the lack of a standardized experimental design. In addition, the performance of data-driven approaches is often compared based on empirical analysis. The lack of reproducibility and availability of standardized evaluation tools (e.g., datasets, codebases) hinder the development and progress of data-driven methods for channel estimation and wireless communication in general. In this work, we introduce an initiative to build benchmarks that unify several data-driven OFDM channel estimation approaches. Specifically, we present CeBed (a testbed for channel estimation) including different datasets covering various systems models and propagation conditions along with the implementation of ten deep and traditional baselines. This benchmark considers different practical aspects such as the robustness of the data-driven models, the number and the arrangement of pilots, and the number of receive antennas. This work offers a comprehensive and unified framework to help researchers evaluate and design data-driven channel estimation algorithms.", "url": "https://arxiv.org/abs/2306.13761"}, {"metadata": {"arXiv": "2306.13803", "Date": "Fri, 23 Jun 2023 22:35:51 ", "Title": "Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring", "Authors": ["Leandra Brickson", "Fritz Vollrath", "Alexander J. Titus"], "Categories": "cs.AI cs.LG"}, "abstract": "Artificial intelligence (AI) and machine learning (ML) present revolutionary opportunities to enhance our understanding of animal behavior and conservation strategies. Using elephants, a crucial species in Africa's protected areas, as our focal point, we delve into the role of AI and ML in their conservation. Given the increasing amounts of data gathered from a variety of sensors like cameras, microphones, geophones, drones, and satellites, the challenge lies in managing and interpreting this vast data. New AI and ML techniques offer solutions to streamline this process, helping us extract vital information that might otherwise be overlooked. This paper focuses on the different AI-driven monitoring methods and their potential for improving elephant conservation. Collaborative efforts between AI experts and ecological researchers are essential in leveraging these innovative technologies for enhanced wildlife conservation, setting a precedent for numerous other species.", "url": "https://arxiv.org/abs/2306.13803"}, {"metadata": {"arXiv": "2306.13995", "Date": "Sat, 24 Jun 2023 15:00:47 ", "Title": "A clustering and graph deep learning-based framework for COVID-19 drug repurposing", "Authors": ["Chaarvi Bansal", "Rohitash Chandra", "Vinti Agarwal", "P. R. Deepa"], "Categories": "cs.AI cs.LG"}, "abstract": "Drug repurposing (or repositioning) is the process of finding new therapeutic uses for drugs already approved by drug regulatory authorities (e.g., the Food and Drug Administration (FDA) and Therapeutic Goods Administration (TGA)) for other diseases. This involves analyzing the interactions between different biological entities, such as drug targets (genes/proteins and biological pathways) and drug properties, to discover novel drug-target or drug-disease relations. Artificial intelligence methods such as machine learning and deep learning have successfully analyzed complex heterogeneous data in the biomedical domain and have also been used for drug repurposing. This study presents a novel unsupervised machine learning framework that utilizes a graph-based autoencoder for multi-feature type clustering on heterogeneous drug data. The dataset consists of 438 drugs, of which 224 are under clinical trials for COVID-19 (category A). The rest are systematically filtered to ensure the safety and efficacy of the treatment (category B). The framework solely relies on reported drug data, including its pharmacological properties, chemical/physical properties, interaction with the host, and efficacy in different publicly available COVID-19 assays. Our machine-learning framework reveals three clusters of interest and provides recommendations featuring the top 15 drugs for COVID-19 drug repurposing, which were shortlisted based on the predicted clusters that were dominated by category A drugs. The anti-COVID efficacy of the drugs should be verified by experimental studies. Our framework can be extended to support other datasets and drug repurposing studies, given open-source code and data availability.", "url": "https://arxiv.org/abs/2306.13995"}, {"metadata": {"arXiv": "2306.14325", "Date": "Sun, 25 Jun 2023 19:38:01 ", "Title": "The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs", "Authors": ["Lance Ying", "Katherine M. Collins", "Megan Wei", "Cedegao E. Zhang", "Tan Zhi-Xuan", "Adrian Weller", "Joshua B. Tenenbaum", "Lionel Wong"], "Categories": "cs.AI cs.LG", "Comments": ["To appear at ICML Workshop on Theory of Mind in Communicating Agents"]}, "abstract": "Human beings are social creatures. We routinely reason about other agents, and a crucial component of this social reasoning is inferring people's goals as we learn about their actions. In many settings, we can perform intuitive but reliable goal inference from language descriptions of agents, actions, and the background environments. In this paper, we study this process of language driving and influencing social reasoning in a probabilistic goal inference domain. We propose a neuro-symbolic model that carries out goal inference from linguistic inputs of agent scenarios. The \"neuro\" part is a large language model (LLM) that translates language descriptions to code representations, and the \"symbolic\" part is a Bayesian inverse planning engine. To test our model, we design and run a human experiment on a linguistic goal inference task. Our model closely matches human response patterns and better predicts human judgements than using an LLM alone.", "url": "https://arxiv.org/abs/2306.14325"}, {"metadata": {"arXiv": "2306.14483", "Date": "Mon, 26 Jun 2023 07:50:32 ", "Title": "Medical Federated Model with Mixture of Personalized and Sharing Components", "Authors": ["Yawei Zhao", "Qinghe Liu", "Xinwang Liu", "Kunlun He"], "Categories": "cs.AI cs.LG", "Comments": ["Medical data", "federated learning", "personalized model", "similarity network"]}, "abstract": "Although data-driven methods usually have noticeable performance on disease diagnosis and treatment, they are suspected of leakage of privacy due to collecting data for model training. Recently, federated learning provides a secure and trustable alternative to collaboratively train model without any exchange of medical data among multiple institutes. Therefore, it has draw much attention due to its natural merit on privacy protection. However, when heterogenous medical data exists between different hospitals, federated learning usually has to face with degradation of performance. In the paper, we propose a new personalized framework of federated learning to handle the problem. It successfully yields personalized models based on awareness of similarity between local data, and achieves better tradeoff between generalization and personalization than existing methods. After that, we further design a differentially sparse regularizer to improve communication efficiency during procedure of model training. Additionally, we propose an effective method to reduce the computational cost, which improves computation efficiency significantly. Furthermore, we collect 5 real medical datasets, including 2 public medical image datasets and 3 private multi-center clinical diagnosis datasets, and evaluate its performance by conducting nodule classification, tumor segmentation, and clinical risk prediction tasks. Comparing with 13 existing related methods, the proposed method successfully achieves the best model performance, and meanwhile up to 60% improvement of communication efficiency. Source code is public, and can be accessed at: https://github.com/ApplicationTechnologyOfMedicalBigData/pFedNet-code.", "url": "https://arxiv.org/abs/2306.14483"}, {"metadata": {"arXiv": "2306.14626", "Date": "Mon, 26 Jun 2023 12:00:05 ", "Title": "Estimating player completion rate in mobile puzzle games using reinforcement learning", "Authors": ["Jeppe Theiss Kristensen", "Arturo Valdivia", "Paolo Burelli"], "Categories": "cs.AI cs.LG", "Journal-ref": "Proceedings of the 2020 IEEE Conference on Games (CoG)", "DOI": "10.1109/CoG47356.2020.9231581"}, "abstract": "In this work we investigate whether it is plausible to use the performance of a reinforcement learning (RL) agent to estimate the difficulty measured as the player completion rate of different levels in the mobile puzzle game Lily's Garden.For this purpose we train an RL agent and measure the number of moves required to complete a level. This is then compared to the level completion rate of a large sample of real players.We find that the strongest predictor of player completion rate for a level is the number of moves taken to complete a level of the ~5% best runs of the agent on a given level. A very interesting observation is that, while in absolute terms, the agent is unable to reach human-level performance across all levels, the differences in terms of behaviour between levels are highly correlated to the differences in human behaviour. Thus, despite performing sub-par, it is still possible to use the performance of the agent to estimate, and perhaps further model, player metrics.", "url": "https://arxiv.org/abs/2306.14626"}, {"metadata": {"arXiv": "2306.14650", "Date": "Mon, 26 Jun 2023 12:40:12 ", "Title": "PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture", "Authors": ["Mohit Vaishnav"], "Categories": "cs.AI cs.CV cs.LG cs.SC", "Comments": ["PhD Thesis", "152 pages", "32 figures", "6 tables"]}, "abstract": "We investigate the role of attention and memory in complex reasoning tasks. We analyze Transformer-based self-attention as a model and extend it with memory. By studying a synthetic visual reasoning test, we refine the taxonomy of reasoning tasks. Incorporating self-attention with ResNet50, we enhance feature maps using feature-based and spatial attention, achieving efficient solving of challenging visual reasoning tasks. Our findings contribute to understanding the attentional needs of SVRT tasks. Additionally, we propose GAMR, a cognitive architecture combining attention and memory, inspired by active vision theory. GAMR outperforms other architectures in sample efficiency, robustness, and compositionality, and shows zero-shot generalization on new reasoning tasks.", "url": "https://arxiv.org/abs/2306.14650"}, {"metadata": {"arXiv": "2306.14683", "Date": "Mon, 26 Jun 2023 13:27:11 ", "Title": "Multi-Agent Deep Reinforcement Learning for Dynamic Avatar Migration in AIoT-enabled Vehicular Metaverses with Trajectory Prediction", "Authors": ["Junlong Chen", "Jiawen Kang", "Minrui Xu", "Zehui Xiong", "Dusit Niyato", "Chuan Chen", "Abbas Jamalipour", "and Shengli Xie"], "Categories": "cs.AI cs.LG eess.SP"}, "abstract": "Avatars, as promising digital assistants in Vehicular Metaverses, can enable drivers and passengers to immerse in 3D virtual spaces, serving as a practical emerging example of Artificial Intelligence of Things (AIoT) in intelligent vehicular environments. The immersive experience is achieved through seamless human-avatar interaction, e.g., augmented reality navigation, which requires intensive resources that are inefficient and impractical to process on intelligent vehicles locally. Fortunately, offloading avatar tasks to RoadSide Units (RSUs) or cloud servers for remote execution can effectively reduce resource consumption. However, the high mobility of vehicles, the dynamic workload of RSUs, and the heterogeneity of RSUs pose novel challenges to making avatar migration decisions. To address these challenges, in this paper, we propose a dynamic migration framework for avatar tasks based on real-time trajectory prediction and Multi-Agent Deep Reinforcement Learning (MADRL). Specifically, we propose a model to predict the future trajectories of intelligent vehicles based on their historical data, indicating the future workloads of RSUs.Based on the expected workloads of RSUs, we formulate the avatar task migration problem as a long-term mixed integer programming problem. To tackle this problem efficiently, the problem is transformed into a Partially Observable Markov Decision Process (POMDP) and solved by multiple DRL agents with hybrid continuous and discrete actions in decentralized. Numerical results demonstrate that our proposed algorithm can effectively reduce the latency of executing avatar tasks by around 25% without prediction and 30% with prediction and enhance user immersive experiences in the AIoT-enabled Vehicular Metaverse (AeVeM).", "url": "https://arxiv.org/abs/2306.14683"}, {"metadata": {"arXiv": "2306.14786", "Date": "Fri, 23 Jun 2023 16:47:49 ", "Title": "Evolutionary approaches to explainable machine learning", "Authors": ["Ryan Zhou", "Ting Hu"], "Categories": "cs.AI cs.LG cs.NE"}, "abstract": "Machine learning models are increasingly being used in critical sectors, but their black-box nature has raised concerns about accountability and trust. The field of explainable artificial intelligence (XAI) or explainable machine learning (XML) has emerged in response to the need for human understanding of these models. Evolutionary computing, as a family of powerful optimization and learning tools, has significant potential to contribute to XAI/XML. In this chapter, we provide a brief introduction to XAI/XML and review various techniques in current use for explaining machine learning models. We then focus on how evolutionary computing can be used in XAI/XML, and review some approaches which incorporate EC techniques. We also discuss some open challenges in XAI/XML and opportunities for future research in this field using EC. Our aim is to demonstrate that evolutionary computing is well-suited for addressing current problems in explainability, and to encourage further exploration of these methods to contribute to the development of more transparent, trustworthy and accountable machine learning models.", "url": "https://arxiv.org/abs/2306.14786"}, {"metadata": {"arXiv": "2306.13960", "Date": "Sat, 24 Jun 2023 13:29:54 ", "Title": "Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis", "Authors": ["Thijs P. Kuipers and Erik J. Bekkers"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["10 pages", "1 figure", "2 tables", "to be published at MICCAI 2023"]}, "abstract": "Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5% gain in accuracy over regular CNNs.", "url": "https://arxiv.org/abs/2306.13960"}, {"metadata": {"arXiv": "2306.14221", "Date": "Sun, 25 Jun 2023 12:05:46 ", "Title": "Feature Adversarial Distillation for Point Cloud Classification", "Authors": ["YuXing Lee", "Wei Wu"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Accepted to ICIP2023"]}, "abstract": "Due to the point cloud's irregular and unordered geometry structure, conventional knowledge distillation technology lost a lot of information when directly used on point cloud tasks. In this paper, we propose Feature Adversarial Distillation (FAD) method, a generic adversarial loss function in point cloud distillation, to reduce loss during knowledge transfer.In the feature extraction stage, the features extracted by the teacher are used as the discriminator, and the students continuously generate new features in the training stage. The feature of the student is obtained by attacking the feedback from the teacher and getting a score to judge whether the student has learned the knowledge well or not. In experiments on standard point cloud classification on ModelNet40 and ScanObjectNN datasets, our method reduced the information loss of knowledge transfer in distillation in 40x model compression while maintaining competitive performance.", "url": "https://arxiv.org/abs/2306.14221"}, {"metadata": {"arXiv": "2306.14658", "Date": "Mon, 26 Jun 2023 12:51:32 ", "Title": "Beyond AUROC & co. for evaluating out-of-distribution detection performance", "Authors": ["Galadrielle Humblot-Renaux", "Sergio Escalera", "Thomas B. Moeslund"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["published in SAIAD CVPRW'23 (Safe Artificial Intelligence for All Domains CVPR workshop)"]}, "abstract": "While there has been a growing research interest in developing out-of-distribution (OOD) detection methods, there has been comparably little discussion around how these methods should be evaluated. Given their relevance for safe(r) AI, it is important to examine whether the basis for comparing OOD detection methods is consistent with practical needs. In this work, we take a closer look at the go-to metrics for evaluating OOD detection, and question the approach of exclusively reducing OOD detection to a binary classification task with little consideration for the detection threshold. We illustrate the limitations of current metrics (AUROC & its friends) and propose a new metric - Area Under the Threshold Curve (AUTC), which explicitly penalizes poor separation between ID and OOD samples. Scripts and data are available at https://github.com/glhr/beyond-auroc", "url": "https://arxiv.org/abs/2306.14658"}, {"metadata": {"arXiv": "2306.14789", "Date": "Mon, 26 Jun 2023 15:46:49 ", "Title": "Segmentation of Industrial Burner Flames: A Comparative Study from Traditional Image Processing to Machine and Deep Learning", "Authors": ["Steven Landgraf", "Markus Hillemann", "Moritz Aberle", "Valentin Jung", "Markus Ulrich"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["8 Pages", "5 figures", "submitted to the Geospatial Week 2023"]}, "abstract": "In many industrial processes, such as power generation, chemical production, and waste management, accurately monitoring industrial burner flame characteristics is crucial for safe and efficient operation. A key step involves separating the flames from the background through binary segmentation. Decades of machine vision research have produced a wide range of possible solutions, from traditional image processing to traditional machine learning and modern deep learning methods. In this work, we present a comparative study of multiple segmentation approaches, namely Global Thresholding, Region Growing, Support Vector Machines, Random Forest, Multilayer Perceptron, U-Net, and DeepLabV3+, that are evaluated on a public benchmark dataset of industrial burner flames. We provide helpful insights and guidance for researchers and practitioners aiming to select an appropriate approach for the binary segmentation of industrial burner flames and beyond. For the highest accuracy, deep learning is the leading approach, while for fast and simple solutions, traditional image processing techniques remain a viable option.", "url": "https://arxiv.org/abs/2306.14789"}, {"metadata": {"arXiv": "2306.13682", "Date": "Wed, 21 Jun 2023 21:57:58 ", "Title": "Evaluating the overall sensitivity of saliency-based explanation methods", "Authors": ["Harshinee Sriram and Cristina Conati"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted to the IJCAI-XAI 2023 workshop"]}, "abstract": "We address the need to generate faithful explanations of \"black box\" Deep Learning models. Several tests have been proposed to determine aspects of faithfulness of explanation methods, but they lack cross-domain applicability and a rigorous methodology. Hence, we select an existing test that is model agnostic and is well-suited for comparing one aspect of faithfulness (i.e., sensitivity) of multiple explanation methods, and extend it by specifying formal thresh-olds and building criteria to determine the over-all sensitivity of the explanation method. We present examples of how multiple explanation methods for Convolutional Neural Networks can be compared using this extended methodology. Finally, we discuss the relationship between sensitivity and faithfulness and consider how the test can be adapted to assess different explanation methods in other domains.", "url": "https://arxiv.org/abs/2306.13682"}, {"metadata": {"arXiv": "2306.13841", "Date": "Sat, 24 Jun 2023 02:26:45 ", "Title": "Is Pre-training Truly Better Than Meta-Learning?", "Authors": ["Brando Miranda", "Patrick Yu", "Saumya Goyal", "Yu-Xiong Wang", "Sanmi Koyejo"], "Categories": "cs.LG cs.AI cs.CL cs.CV", "Journal-ref": "Proceedings of the 40th International Conference on Machine Learning 2023 DMLR Workshop"}, "abstract": "In the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is high, MAML beats PT on average. The caveat is that the magnitude of the average difference between a PT vs. MAML using the effect size is low (according to classical statistical thresholds) -- less than 0.2. Nevertheless, this observation is contrary to the currently held belief that a pre-trained model is always better than a meta-learning model. Our extensive experiments consider 21 few-shot learning benchmarks, including the large-scale few-shot learning dataset Meta-Data set. We also show no significant difference between a MAML model vs. a PT model with GPT-2 on Openwebtext. We, therefore, conclude that a pre-trained model does not always beat a meta-learned model and that the formal diversity of a dataset is a driving factor.", "url": "https://arxiv.org/abs/2306.13841"}, {"metadata": {"arXiv": "2306.13854", "Date": "Sat, 24 Jun 2023 04:02:50 ", "Title": "Similarity Preserving Adversarial Graph Contrastive Learning", "Authors": ["Yeonjun In", "Kanghoon Yoon", "Chanyoung Park"], "Categories": "cs.LG cs.AI cs.CR", "Comments": ["9 pages; KDD'23"]}, "abstract": "Recent works demonstrate that GNN models are vulnerable to adversarial attacks, which refer to imperceptible perturbation on the graph structure and node features. Among various GNN models, graph contrastive learning (GCL) based methods specifically suffer from adversarial attacks due to their inherent design that highly depends on the self-supervision signals derived from the original graph, which however already contains noise when the graph is attacked. To achieve adversarial robustness against such attacks, existing methods adopt adversarial training (AT) to the GCL framework, which considers the attacked graph as an augmentation under the GCL framework. However, we find that existing adversarially trained GCL methods achieve robustness at the expense of not being able to preserve the node feature similarity. In this paper, we propose a similarity-preserving adversarial graph contrastive learning (SP-AGCL) framework that contrasts the clean graph with two auxiliary views of different properties (i.e., the node similarity-preserving view and the adversarial view). Extensive experiments demonstrate that SP-AGCL achieves a competitive performance on several downstream tasks, and shows its effectiveness in various scenarios, e.g., a network with adversarial attacks, noisy labels, and heterophilous neighbors. Our code is available at https://github.com/yeonjun-in/torch-SP-AGCL.", "url": "https://arxiv.org/abs/2306.13854"}, {"metadata": {"arXiv": "2306.13892", "Date": "Sat, 24 Jun 2023 07:46:00 ", "Title": "Differentially Private Decentralized Deep Learning with Consensus Algorithms", "Authors": ["Jasmine Bayrooti", "Zhan Gao", "Amanda Prorok"], "Categories": "cs.LG cs.AI"}, "abstract": "Cooperative decentralized deep learning relies on direct information exchange between communicating agents, each with access to a local dataset which should be kept private. The goal is for all agents to achieve consensus on model parameters after training. However, sharing parameters with untrustworthy neighboring agents could leak exploitable information about local datasets. To combat this, we introduce differentially private decentralized learning that secures each agent's local dataset during and after cooperative training. In our approach, we generalize Differentially Private Stochastic Gradient Descent (DP-SGD) -- a popular differentially private training method for centralized deep learning -- to practical subgradient- and ADMM-based decentralized learning methods. Our algorithms' differential privacy guarantee holds for arbitrary deep learning objective functions, and we analyze the convergence properties for strongly convex objective functions. We compare our algorithms against centrally trained models on standard classification tasks and evaluate the relationships between performance, privacy budget, graph connectivity, and degree of training data overlap among agents. We find that differentially private gradient tracking is resistant to performance degradation under sparse graphs and non-uniform data distributions. Furthermore, we show that it is possible to learn a model achieving high accuracies, within 3% of DP-SGD on MNIST under (1, 10^-5)-differential privacy and within 6% of DP-SGD on CIFAR-100 under (10, 10^-5)-differential privacy, without ever sharing raw data with other agents. Open source code can be found at: https://github.com/jbayrooti/dp-dec-learning.", "url": "https://arxiv.org/abs/2306.13892"}, {"metadata": {"arXiv": "2306.13923", "Date": "Sat, 24 Jun 2023 10:07:35 ", "Title": "Active Data Acquisition in Autonomous Driving Simulation", "Authors": ["Jianyu Lai", "Zexuan Jia", "Boao Li"], "Categories": "cs.LG cs.AI"}, "abstract": "Autonomous driving algorithms rely heavily on learning-based models, which require large datasets for training. However, there is often a large amount of redundant information in these datasets, while collecting and processing these datasets can be time-consuming and expensive. To address this issue, this paper proposes the concept of an active data-collecting strategy. For high-quality data, increasing the collection density can improve the overall quality of the dataset, ultimately achieving similar or even better results than the original dataset with lower labeling costs and smaller dataset sizes. In this paper, we design experiments to verify the quality of the collected dataset and to demonstrate this strategy can significantly reduce labeling costs and dataset size while improving the overall quality of the dataset, leading to better performance of autonomous driving systems. The source code implementing the proposed approach is publicly available on https://github.com/Th1nkMore/carla_dataset_tools.", "url": "https://arxiv.org/abs/2306.13923"}, {"metadata": {"arXiv": "2306.13929", "Date": "Sat, 24 Jun 2023 10:27:08 ", "Title": "Evaluating the Utility of GAN Generated Synthetic Tabular Data for Class Balancing and Low Resource Settings", "Authors": ["Nagarjuna Chereddy and Bharath Kumar Bolla"], "Categories": "cs.LG cs.AI", "DOI": "10.1007/978-3-031-36402-0_4"}, "abstract": "The present study aimed to address the issue of imbalanced data in classification tasks and evaluated the suitability of SMOTE, ADASYN, and GAN techniques in generating synthetic data to address the class imbalance and improve the performance of classification models in low-resource settings. The study employed the Generalised Linear Model (GLM) algorithm for class balancing experiments and the Random Forest (RF) algorithm for low-resource setting experiments to assess model performance under varying training data. The recall metric was the primary evaluation metric for all classification models. The results of the class balancing experiments showed that the GLM model trained on GAN-balanced data achieved the highest recall value. Similarly, in low-resource experiments, models trained on data enhanced with GAN-synthesized data exhibited better recall values than original data. These findings demonstrate the potential of GAN-generated synthetic data for addressing the challenge of imbalanced data in classification tasks and improving model performance in low-resource settings.", "url": "https://arxiv.org/abs/2306.13929"}, {"metadata": {"arXiv": "2306.13931", "Date": "Sat, 24 Jun 2023 10:38:08 ", "Title": "Comparative Study of Predicting Stock Index Using Deep Learning Models", "Authors": ["Harshal Patel", "Bharath Kumar Bolla", "Sabeesh E", "Dinesh Reddy"], "Categories": "cs.LG cs.AI"}, "abstract": "Time series forecasting has seen many methods attempted over the past few decades, including traditional technical analysis, algorithmic statistical models, and more recent machine learning and artificial intelligence approaches. Recently, neural networks have been incorporated into the forecasting scenario, such as the LSTM and conventional RNN approaches, which utilize short-term and long-term dependencies. This study evaluates traditional forecasting methods, such as ARIMA, SARIMA, and SARIMAX, and newer neural network approaches, such as DF-RNN, DSSM, and Deep AR, built using RNNs. The standard NIFTY-50 dataset from Kaggle is used to assess these models using metrics such as MSE, RMSE, MAPE, POCID, and Theil's U. Results show that Deep AR outperformed all other conventional deep learning and traditional approaches, with the lowest MAPE of 0.01 and RMSE of 189. Additionally, the performance of Deep AR and GRU did not degrade when the amount of training data was reduced, suggesting that these models may not require a large amount of data to achieve consistent and reliable performance. The study demonstrates that incorporating deep learning approaches in a forecasting scenario significantly outperforms conventional approaches and can handle complex datasets, with potential applications in various domains, such as weather predictions and other time series applications in a real-world scenario.", "url": "https://arxiv.org/abs/2306.13931"}, {"metadata": {"arXiv": "2306.13944", "Date": "Sat, 24 Jun 2023 12:02:50 ", "Title": "Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery", "Authors": ["Xiao Zhang", "Hai Zhang", "Hongtu Zhou", "Chang Huang", "Di Zhang", "Chen Ye", "Junqiao Zhao"], "Categories": "cs.LG cs.AI", "Comments": ["8 pages", "5 figures"]}, "abstract": "Safety is one of the main challenges in applying reinforcement learning to realistic environmental tasks. To ensure safety during and after training process, existing methods tend to adopt overly conservative policy to avoid unsafe situations. However, overly conservative policy severely hinders the exploration, and makes the algorithms substantially less rewarding. In this paper, we propose a method to construct a boundary that discriminates safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pretrained on an offline dataset, in which the safety critic evaluates upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent to interact with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with less safety violations than state-of-the-art algorithms.", "url": "https://arxiv.org/abs/2306.13944"}, {"metadata": {"arXiv": "2306.13945", "Date": "Sat, 24 Jun 2023 12:06:26 ", "Title": "Large Sequence Models for Sequential Decision-Making: A Survey", "Authors": ["Muning Wen", "Runji Lin", "Hanjing Wang", "Yaodong Yang", "Ying Wen", "Luo Mai", "Jun Wang", "Haifeng Zhang and Weinan Zhang"], "Categories": "cs.LG cs.AI cs.MA", "Comments": ["25 pages", "4 figures", "2 tables"]}, "abstract": "Transformer architectures have facilitated the development of large-scale and general-purpose sequence models for prediction tasks in natural language processing and computer vision, e.g., GPT-3 and Swin Transformer. Although originally designed for prediction problems, it is natural to inquire about their suitability for sequential decision-making and reinforcement learning problems, which are typically beset by long-standing issues involving sample efficiency, credit assignment, and partial observability. In recent years, sequence models, especially the Transformer, have attracted increasing interest in the RL communities, spawning numerous approaches with notable effectiveness and generalizability. This survey presents a comprehensive overview of recent works aimed at solving sequential decision-making tasks with sequence models such as the Transformer, by discussing the connection between sequential decision-making and sequence modeling, and categorizing them based on the way they utilize the Transformer. Moreover, this paper puts forth various potential avenues for future research intending to improve the effectiveness of large sequence models for sequential decision-making, encompassing theoretical foundations, network architectures, algorithms, and efficient training systems. As this article has been accepted by the Frontiers of Computer Science, here is an early version, and the most up-to-date version can be found at https://journal.hep.com.cn/fcs/EN/10.1007/s11704-023-2689-5", "url": "https://arxiv.org/abs/2306.13945"}, {"metadata": {"arXiv": "2306.13948", "Date": "Sat, 24 Jun 2023 12:10:16 ", "Title": "Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset", "Authors": ["Jingwei Zuo", "Wenbin Li", "Michele Baldo and Hakim Hacid"], "Categories": "cs.LG cs.AI"}, "abstract": "Air quality forecasting has garnered significant attention recently, with data-driven models taking center stage due to advancements in machine learning and deep learning models. However, researchers face challenges with complex data acquisition and the lack of open-sourced datasets, hindering efficient model validation. This paper introduces PurpleAirSF, a comprehensive and easily accessible dataset collected from the PurpleAir network. With its high temporal resolution, various air quality measures, and diverse geographical coverage, this dataset serves as a useful tool for researchers aiming to develop novel forecasting models, study air pollution patterns, and investigate their impacts on health and the environment. We present a detailed account of the data collection and processing methods employed to build PurpleAirSF. Furthermore, we conduct preliminary experiments using both classic and modern spatio-temporal forecasting models, thereby establishing a benchmark for future air quality forecasting tasks.", "url": "https://arxiv.org/abs/2306.13948"}, {"metadata": {"arXiv": "2306.14031", "Date": "Sat, 24 Jun 2023 18:19:29 ", "Title": "Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression", "Authors": ["Tianhong Huang", "Victor Agostinelli", "Lizhong Chen"], "Categories": "cs.LG cs.AI"}, "abstract": "Compactness in deep learning can be critical to a model's viability in low-resource applications, and a common approach to extreme model compression is quantization. We consider Iterative Product Quantization (iPQ) with Quant-Noise to be state-of-the-art in this area, but this quantization framework suffers from preventable inference quality degradation due to prevalent empty clusters. In this paper, we propose several novel enhancements aiming to improve the accuracy of iPQ with Quant-Noise by focusing on resolving empty clusters. Our contribution, which we call Partitioning-Guided k-means (PG k-means), is a heavily augmented k-means implementation composed of three main components. First, we propose a partitioning-based pre-assignment strategy that ensures no initial empty clusters and encourages an even weight-to-cluster distribution. Second, we propose an empirically superior empty cluster resolution heuristic executed via cautious partitioning of large clusters. Finally, we construct an optional optimization step that consolidates intuitively dense clusters of weights to ensure shared representation. The proposed approach consistently reduces the number of empty clusters in iPQ with Quant-Noise by 100x on average, uses 8x fewer iterations during empty cluster resolution, and improves overall model accuracy by up to 12%, when applied to RoBERTa on a variety of tasks in the GLUE benchmark.", "url": "https://arxiv.org/abs/2306.14031"}, {"metadata": {"arXiv": "2306.14043", "Date": "Sat, 24 Jun 2023 19:50:08 ", "Title": "Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks", "Authors": ["Pranav Dahiya", "Ilia Shumailov", "Ross Anderson"], "Categories": "cs.LG cs.AI cs.CR"}, "abstract": "Randomness supports many critical functions in the field of machine learning (ML) including optimisation, data selection, privacy, and security. ML systems outsource the task of generating or harvesting randomness to the compiler, the cloud service provider or elsewhere in the toolchain. Yet there is a long history of attackers exploiting poor randomness, or even creating it -- as when the NSA put backdoors in random number generators to break cryptography. In this paper we consider whether attackers can compromise an ML system using only the randomness on which they commonly rely. We focus our effort on Randomised Smoothing, a popular approach to train certifiably robust models, and to certify specific input datapoints of an arbitrary model. We choose Randomised Smoothing since it is used for both security and safety -- to counteract adversarial examples and quantify uncertainty respectively. Under the hood, it relies on sampling Gaussian noise to explore the volume around a data point to certify that a model is not vulnerable to adversarial examples. We demonstrate an entirely novel attack against it, where an attacker backdoors the supplied randomness to falsely certify either an overestimate or an underestimate of robustness. We demonstrate that such attacks are possible, that they require very small changes to randomness to succeed, and that they can be hard to detect. As an example, we hide an attack in the random number generator and show that the randomness tests suggested by NIST fail to detect it. We advocate updating the NIST guidelines on random number testing to make them more appropriate for safety-critical and security-critical machine-learning applications.", "url": "https://arxiv.org/abs/2306.14043"}, {"metadata": {"arXiv": "2306.14047", "Date": "Sat, 24 Jun 2023 20:07:51 ", "Title": "Towards Optimal Pricing of Demand Response -- A Nonparametric Constrained Policy Optimization Approach", "Authors": ["Jun Song and Chaoyue Zhao"], "Categories": "cs.LG cs.AI cs.SY eess.SY", "Comments": ["2023 IEEE PES General Meeting. arXiv admin note: text overlap with arXiv:2006.07815"]}, "abstract": "Demand response (DR) has been demonstrated to be an effective method for reducing peak load and mitigating uncertainties on both the supply and demand sides of the electricity market. One critical question for DR research is how to appropriately adjust electricity prices in order to shift electrical load from peak to off-peak hours. In recent years, reinforcement learning (RL) has been used to address the price-based DR problem because it is a model-free technique that does not necessitate the identification of models for end-use customers. However, the majority of RL methods cannot guarantee the stability and optimality of the learned pricing policy, which is undesirable in safety-critical power systems and may result in high customer bills. In this paper, we propose an innovative nonparametric constrained policy optimization approach that improves optimality while ensuring stability of the policy update, by removing the restrictive assumption on policy representation that the majority of the RL literature adopts: the policy must be parameterized or fall into a certain distribution class. We derive a closed-form expression of optimal policy update for each iteration and develop an efficient on-policy actor-critic algorithm to address the proposed constrained policy optimization problem. The experiments on two DR cases show the superior performance of our proposed nonparametric constrained policy optimization method compared with state-of-the-art RL algorithms.", "url": "https://arxiv.org/abs/2306.14047"}, {"metadata": {"arXiv": "2306.14063", "Date": "Sat, 24 Jun 2023 21:48:28 ", "Title": "Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data", "Authors": ["Sunil Madhow", "Dan Xiao", "Ming Yin", "Yu-Xiang Wang"], "Categories": "cs.LG cs.AI"}, "abstract": "Developing theoretical guarantees on the sample complexity of offline RL methods is an important step towards making data-hungry RL algorithms practically viable. Currently, most results hinge on unrealistic assumptions about the data distribution -- namely that it comprises a set of i.i.d. trajectories collected by a single logging policy. We consider a more general setting where the dataset may have been gathered adaptively. We develop theory for the TMIS Offline Policy Evaluation (OPE) estimator in this generalized setting for tabular MDPs, deriving high-probability, instance-dependent bounds on its estimation error. We also recover minimax-optimal offline learning in the adaptive setting. Finally, we conduct simulations to empirically analyze the behavior of these estimators under adaptive and non-adaptive regimes.", "url": "https://arxiv.org/abs/2306.14063"}, {"metadata": {"arXiv": "2306.14072", "Date": "Sat, 24 Jun 2023 22:57:40 ", "Title": "Intensity-free Convolutional Temporal Point Process: Incorporating Local and Global Event Contexts", "Authors": ["Wang-Tao Zhou", "Zhao Kang", "Ling Tian", "Yi Su"], "Categories": "cs.LG cs.AI cs.SI", "Comments": ["Accepted to Information Sciences"]}, "abstract": "Event prediction in the continuous-time domain is a crucial but rather difficult task. Temporal point process (TPP) learning models have shown great advantages in this area. Existing models mainly focus on encoding global contexts of events using techniques like recurrent neural networks (RNNs) or self-attention mechanisms. However, local event contexts also play an important role in the occurrences of events, which has been largely ignored. Popular convolutional neural networks, which are designated for local context capturing, have never been applied to TPP modelling due to their incapability of modelling in continuous time. In this work, we propose a novel TPP modelling approach that combines local and global contexts by integrating a continuous-time convolutional event encoder with an RNN. The presented framework is flexible and scalable to handle large datasets with long sequences and complex latent patterns. The experimental result shows that the proposed model improves the performance of probabilistic sequential modelling and the accuracy of event prediction. To our best knowledge, this is the first work that applies convolutional neural networks to TPP modelling.", "url": "https://arxiv.org/abs/2306.14072"}, {"metadata": {"arXiv": "2306.14079", "Date": "Sat, 24 Jun 2023 23:40:58 ", "Title": "Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching", "Authors": ["H.J. Terry Suh", "Glen Chou", "Hongkai Dai", "Lujie Yang", "Abhishek Gupta", "Russ Tedrake"], "Categories": "cs.LG cs.AI cs.RO cs.SY eess.SY", "Comments": ["Glen Chou", "Hongkai Dai", "and Lujie Yang contributed equally to this work"]}, "abstract": "Offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL) allow policy search algorithms to make use of offline data, but require careful incorporation of uncertainty in order to circumvent the challenges of distribution shift. Gradient-based policy search methods are a promising direction due to their effectiveness in high dimensions; however, we require a more careful consideration of how these methods interplay with uncertainty estimation. We claim that in order for an uncertainty metric to be amenable for gradient-based optimization, it must be (i) stably convergent to data when uncertainty is minimized with gradients, and (ii) not prone to underestimation of true uncertainty. We investigate smoothed distance to data as a metric, and show that it not only stably converges to data, but also allows us to analyze model bias with Lipschitz constants. Moreover, we establish an equivalence between smoothed distance to data and data likelihood, which allows us to use score-matching techniques to learn gradients of distance to data. Importantly, we show that offline model-based policy search problems that maximize data likelihood do not require values of likelihood; but rather only the gradient of the log likelihood (the score function). Using this insight, we propose Score-Guided Planning (SGP), a planning algorithm for offline RL that utilizes score-matching to enable first-order planning in high-dimensional problems, where zeroth-order methods were unable to scale, and ensembles were unable to overcome local minima. Website: https://sites.google.com/view/score-guided-planning/home", "url": "https://arxiv.org/abs/2306.14079"}, {"metadata": {"arXiv": "2306.14101", "Date": "Sun, 25 Jun 2023 02:39:19 ", "Title": "Language models are weak learners", "Authors": ["Hariharan Manikandan", "Yiding Jiang", "J Zico Kolter"], "Categories": "cs.LG cs.AI", "Comments": ["23 pages", "6 figures"]}, "abstract": "A central notion in practical and theoretical machine learning is that of a $\\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree-based boosting. The model outperforms both few-shot learning and occasionally even more involved fine-tuning procedures, particularly for tasks involving small numbers of data points. The results illustrate the potential for prompt-based LLMs to function not just as few-shot learners themselves, but as components of larger machine learning pipelines.", "url": "https://arxiv.org/abs/2306.14101"}, {"metadata": {"arXiv": "2306.14111", "Date": "Sun, 25 Jun 2023 03:18:15 ", "Title": "Is RLHF More Difficult than Standard RL?", "Authors": ["Yuanhao Wang", "Qinghua Liu", "Chi Jin"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the final state. We instantiate all reward-based RL subroutines by concrete provable algorithms, and apply our theory to a large class of models including tabular MDPs and MDPs with generic function approximation. We further provide guarantees when K-wise comparisons are available.", "url": "https://arxiv.org/abs/2306.14111"}, {"metadata": {"arXiv": "2306.14114", "Date": "Sun, 25 Jun 2023 03:31:47 ", "Title": "TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences", "Authors": ["Ruichu Cai", "Yuequn Liu", "Wei Chen", "Jie Qiao", "Yuguang Yan", "Zijian Li", "Keli Zhang", "Zhifeng Hao"], "Categories": "cs.LG cs.AI"}, "abstract": "Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a two-stage unified topological neural Poisson auto-regressive model. During the generation stage, we employ a variant of the neural Poisson process to model the event sequences, considering influences from both the topological network and the Granger causal structure. In the inference stage, we formulate an amortized inference algorithm to infer the latent Granger causal structure. We encapsulate these two stages within a unified likelihood function, providing an end-to-end framework for this task.", "url": "https://arxiv.org/abs/2306.14114"}, {"metadata": {"arXiv": "2306.14115", "Date": "Sun, 25 Jun 2023 03:34:06 ", "Title": "Towards Trustworthy Explanation: On Causal Rationalization", "Authors": ["Wenbo Zhang", "Tong Wu", "Yunlong Wang", "Yong Cai", "Hengrui Cai"], "Categories": "cs.LG cs.AI cs.CL stat.ME stat.ML", "Comments": ["In Proceedings of the 40th International Conference on Machine Learning (ICML) GitHub Repository: https://github.com/onepounchman/Causal-Retionalization"]}, "abstract": "With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets with extensive experiments compared to state-of-the-art methods.", "url": "https://arxiv.org/abs/2306.14115"}, {"metadata": {"arXiv": "2306.14123", "Date": "Sun, 25 Jun 2023 04:38:19 ", "Title": "Privacy and Fairness in Federated Learning: on the Perspective of Trade-off", "Authors": ["Huiqiang Chen", "Tianqing Zhu", "Tao Zhang", "Wanlei Zhou", "Philip S. Yu"], "Categories": "cs.LG cs.AI cs.CR cs.CY"}, "abstract": "Federated learning (FL) has been a hot topic in recent years. Ever since it was introduced, researchers have endeavored to devise FL systems that protect privacy or ensure fair results, with most research focusing on one or the other. As two crucial ethical notions, the interactions between privacy and fairness are comparatively less studied. However, since privacy and fairness compete, considering each in isolation will inevitably come at the cost of the other. To provide a broad view of these two critical topics, we presented a detailed literature review of privacy and fairness issues, highlighting unique challenges posed by FL and solutions in federated settings. We further systematically surveyed different interactions between privacy and fairness, trying to reveal how privacy and fairness could affect each other and point out new research directions in fair and private FL.", "url": "https://arxiv.org/abs/2306.14123"}, {"metadata": {"arXiv": "2306.14133", "Date": "Sun, 25 Jun 2023 05:41:38 ", "Title": "Provably Convergent Policy Optimization via Metric-aware Trust Region Methods", "Authors": ["Jun Song", "Niao He", "Lijun Ding and Chaoyue Zhao"], "Categories": "cs.LG cs.AI math.OC", "Journal-ref": "Transactions on Machine Learning Research, 2023"}, "abstract": "Trust-region methods based on Kullback-Leibler divergence are pervasively used to stabilize policy optimization in reinforcement learning. In this paper, we exploit more flexible metrics and examine two natural extensions of policy optimization with Wasserstein and Sinkhorn trust regions, namely Wasserstein policy optimization (WPO) and Sinkhorn policy optimization (SPO). Instead of restricting the policy to a parametric distribution class, we directly optimize the policy distribution and derive their closed-form policy updates based on the Lagrangian duality. Theoretically, we show that WPO guarantees a monotonic performance improvement, and SPO provably converges to WPO as the entropic regularizer diminishes. Moreover, we prove that with a decaying Lagrangian multiplier to the trust region constraint, both methods converge to global optimality. Experiments across tabular domains, robotic locomotion, and continuous control tasks further demonstrate the performance improvement of both approaches, more robustness of WPO to sample insufficiency, and faster convergence of SPO, over state-of-art policy gradient methods.", "url": "https://arxiv.org/abs/2306.14133"}, {"metadata": {"arXiv": "2306.14275", "Date": "Sun, 25 Jun 2023 15:53:31 ", "Title": "Enhancing Adversarial Training via Reweighting Optimization Trajectory", "Authors": ["Tianjin Huang", "Shiwei Liu", "Tianlong Chen", "Meng Fang", "Li Shen", "Vlaod Menkovski", "Lu Yin", "Yulong Pei and Mykola Pechenizkiy"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted by ECML 2023"], "Journal-ref": "ECML 2023"}, "abstract": "Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \\textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with the existing adversarial training methods and consistently overcomes the robust overfitting issue, resulting in better adversarial robustness. For example, WOT boosts the robust accuracy of AT-PGD under AA-$L_{\\infty}$ attack by 1.53\\% $\\sim$ 6.11\\% and meanwhile increases the clean accuracy by 0.55\\%$\\sim$5.47\\% across SVHN, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.", "url": "https://arxiv.org/abs/2306.14275"}, {"metadata": {"arXiv": "2306.14340", "Date": "Sun, 25 Jun 2023 20:57:35 ", "Title": "GPatcher: A Simple and Adaptive MLP Model for Alleviating Graph Heterophily", "Authors": ["Shuaicheng Zhang", "Haohui Wang", "Si Zhang", "Dawei Zhou"], "Categories": "cs.LG cs.AI cs.SI"}, "abstract": "While graph heterophily has been extensively studied in recent years, a fundamental research question largely remains nascent: How and to what extent will graph heterophily affect the prediction performance of graph neural networks (GNNs)? In this paper, we aim to demystify the impact of graph heterophily on GNN spectral filters. Our theoretical results show that it is essential to design adaptive polynomial filters that adapts different degrees of graph heterophily to guarantee the generalization performance of GNNs. Inspired by our theoretical findings, we propose a simple yet powerful GNN named GPatcher by leveraging the MLP-Mixer architectures. Our approach comprises two main components: (1) an adaptive patch extractor function that automatically transforms each node's non-Euclidean graph representations to Euclidean patch representations given different degrees of heterophily, and (2) an efficient patch mixer function that learns salient node representation from both the local context information and the global positional information. Through extensive experiments, the GPatcher model demonstrates outstanding performance on node classification compared with popular homophily GNNs and state-of-the-art heterophily GNNs.", "url": "https://arxiv.org/abs/2306.14340"}, {"metadata": {"arXiv": "2306.14369", "Date": "Mon, 26 Jun 2023 00:27:48 ", "Title": "Few-Shot Continual Learning via Flat-to-Wide Approaches", "Authors": ["Muhammad Anwar Ma'sum", "Mahardhika Pratama", "Lin Liu", "Edwin Lughofer", "Habibullah", "Ryszard Kowalczyk"], "Categories": "cs.LG cs.AI"}, "abstract": "Existing approaches on continual learning call for a lot of samples in their training processes. Such approaches are impractical for many real-world problems having limited samples because of the overfitting problem. This paper proposes a few-shot continual learning approach, termed FLat-tO-WidE AppRoach (FLOWER), where a flat-to-wide learning process finding the flat-wide minima is proposed to address the catastrophic forgetting problem. The issue of data scarcity is overcome with a data augmentation approach making use of a ball generator concept to restrict the sampling space into the smallest enclosing ball. Our numerical studies demonstrate the advantage of FLOWER achieving significantly improved performances over prior arts notably in the small base tasks. For further study, source codes of FLOWER, competitor algorithms and experimental logs are shared publicly in \\url{https://github.com/anwarmaxsum/FLOWER}.", "url": "https://arxiv.org/abs/2306.14369"}, {"metadata": {"arXiv": "2306.14403", "Date": "Mon, 26 Jun 2023 03:32:57 ", "Title": "Anomaly Detection with Score Distribution Discrimination", "Authors": ["Minqi Jiang", "Songqiao Han", "Hailiang Huang"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["Accepted by KDD 2023. Detailed discussions can be found in https://openreview.net/forum?id=P1Worw-M1Tf&referrer=[the%20profile%20of%20Minqi%20Jiang](/profile?id=~Minqi_Jiang2)"], "DOI": "10.1145/3580305.3599258"}, "abstract": "Recent studies give more attention to the anomaly detection (AD) methods that can leverage a handful of labeled anomalies along with abundant unlabeled data. These existing anomaly-informed AD methods rely on manually predefined score target(s), e.g., prior constant or margin hyperparameter(s), to realize discrimination in anomaly scores between normal and abnormal data. However, such methods would be vulnerable to the existence of anomaly contamination in the unlabeled data, and also lack adaptation to different data scenarios. In this paper, we propose to optimize the anomaly scoring function from the view of score distribution, thus better retaining the diversity and more fine-grained information of input data, especially when the unlabeled data contains anomaly noises in more practical AD scenarios. We design a novel loss function called Overlap loss that minimizes the overlap area between the score distributions of normal and abnormal samples, which no longer depends on prior anomaly score targets and thus acquires adaptability to various datasets. Overlap loss consists of Score Distribution Estimator and Overlap Area Calculation, which are introduced to overcome challenges when estimating arbitrary score distributions, and to ensure the boundness of training loss. As a general loss component, Overlap loss can be effectively integrated into multiple network architectures for constructing AD models. Extensive experimental results indicate that Overlap loss based AD models significantly outperform their state-of-the-art counterparts, and achieve better performance on different types of anomalies.", "url": "https://arxiv.org/abs/2306.14403"}, {"metadata": {"arXiv": "2306.14468", "Date": "Mon, 26 Jun 2023 07:20:25 ", "Title": "A General Framework for Sequential Decision-Making under Adaptivity Constraints", "Authors": ["Nuoya Xiong", "Zhuoran Yang", "Zhaoran Wang"], "Categories": "cs.LG cs.AI", "Comments": ["48 pages"]}, "abstract": "We take the first step in studying general sequential decision-making under two adaptivity constraints: rare policy switch and batch learning. First, we provide a general class called the Eluder Condition class, which includes a wide range of reinforcement learning classes. Then, for the rare policy switch constraint, we provide a generic algorithm to achieve a $\\widetilde{\\mathcal{O}}(\\log K) $ switching cost with a $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ regret on the EC class. For the batch learning constraint, we provide an algorithm that provides a $\\widetilde{\\mathcal{O}}(\\sqrt{K}+K/B)$ regret with the number of batches $B.$ This paper is the first work considering rare policy switch and batch learning under general function classes, which covers nearly all the models studied in the previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020), linear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong et al. 2021; Gao et al. 2021), generalized linear function approximation (Qiao et al. 2023), and also some new classes such as the low $D_\\Delta$-type Bellman eluder dimension problem, linear mixture MDP, kernelized nonlinear regulator and undercomplete partially observed Markov decision process (POMDP).", "url": "https://arxiv.org/abs/2306.14468"}, {"metadata": {"arXiv": "2306.14476", "Date": "Mon, 26 Jun 2023 07:37:50 ", "Title": "STEF-DHNet: Spatiotemporal External Factors Based Deep Hybrid Network for Enhanced Long-Term Taxi Demand Prediction", "Authors": ["Sheraz Hassan", "Muhammad Tahir", "Momin Uppal", "Zubair Khalid", "Ivan Gorban", "Selim Turki"], "Categories": "cs.LG cs.AI", "Comments": ["8 pages", "3 Figures"]}, "abstract": "Accurately predicting the demand for ride-hailing services can result in significant benefits such as more effective surge pricing strategies, improved driver positioning, and enhanced customer service. By understanding the demand fluctuations, companies can anticipate and respond to consumer requirements more efficiently, leading to increased efficiency and revenue. However, forecasting demand in a particular region can be challenging, as it is influenced by several external factors, such as time of day, weather conditions, and location. Thus, understanding and evaluating these factors is essential for predicting consumer behavior and adapting to their needs effectively. Grid-based deep learning approaches have proven effective in predicting regional taxi demand. However, these models have limitations in integrating external factors in their spatiotemporal complexity and maintaining high accuracy over extended time horizons without continuous retraining, which makes them less suitable for practical and commercial applications. To address these limitations, this paper introduces STEF-DHNet, a demand prediction model that combines Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) to integrate external features as spatiotemporal information and capture their influence on ride-hailing demand. The proposed model is evaluated using a long-term performance metric called the rolling error, which assesses its ability to maintain high accuracy over long periods without retraining. The results show that STEF-DHNet outperforms existing state-of-the-art methods on three diverse datasets, demonstrating its potential for practical use in real-world scenarios.", "url": "https://arxiv.org/abs/2306.14476"}, {"metadata": {"arXiv": "2306.14606", "Date": "Mon, 26 Jun 2023 11:30:33 ", "Title": "Multivariate Time Series Early Classification Across Channel and Time Dimensions", "Authors": ["Leonardos Pantiskas", "Kees Verstoep", "Mark Hoogendoorn", "Henri Bal"], "Categories": "cs.LG cs.AI"}, "abstract": "Nowadays, the deployment of deep learning models on edge devices for addressing real-world classification problems is becoming more prevalent. Moreover, there is a growing popularity in the approach of early classification, a technique that involves classifying the input data after observing only an early portion of it, aiming to achieve reduced communication and computation requirements, which are crucial parameters in edge intelligence environments. While early classification in the field of time series analysis has been broadly researched, existing solutions for multivariate time series problems primarily focus on early classification along the temporal dimension, treating the multiple input channels in a collective manner. In this study, we propose a more flexible early classification pipeline that offers a more granular consideration of input channels and extends the early classification paradigm to the channel dimension. To implement this method, we utilize reinforcement learning techniques and introduce constraints to ensure the feasibility and practicality of our objective. To validate its effectiveness, we conduct experiments using synthetic data and we also evaluate its performance on real datasets. The comprehensive results from our experiments demonstrate that, for multiple datasets, our method can enhance the early classification paradigm by achieving improved accuracy for equal input utilization.", "url": "https://arxiv.org/abs/2306.14606"}, {"metadata": {"arXiv": "2306.14609", "Date": "Mon, 26 Jun 2023 11:32:40 ", "Title": "The race to robustness: exploiting fragile models for urban camouflage and the imperative for machine learning security", "Authors": ["Harriet Farlow", "Matthew Garratt", "Gavin Mount and Tim Lynar"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Accepted to IEEE TENSYMP 2023"]}, "abstract": "Adversarial Machine Learning (AML) represents the ability to disrupt Machine Learning (ML) algorithms through a range of methods that broadly exploit the architecture of deep learning optimisation. This paper presents Distributed Adversarial Regions (DAR), a novel method that implements distributed instantiations of computer vision-based AML attack methods that may be used to disguise objects from image recognition in both white and black box settings. We consider the context of object detection models used in urban environments, and benchmark the MobileNetV2, NasNetMobile and DenseNet169 models against a subset of relevant images from the ImageNet dataset. We evaluate optimal parameters (size, number and perturbation method), and compare to state-of-the-art AML techniques that perturb the entire image. We find that DARs can cause a reduction in confidence of 40.4% on average, but with the benefit of not requiring the entire image, or the focal object, to be perturbed. The DAR method is a deliberately simple approach where the intention is to highlight how an adversary with very little skill could attack models that may already be productionised, and to emphasise the fragility of foundational object detection models. We present this as a contribution to the field of ML security as well as AML. This paper contributes a novel adversarial method, an original comparison between DARs and other AML methods, and frames it in a new context - that of urban camouflage and the necessity for ML security and model robustness.", "url": "https://arxiv.org/abs/2306.14609"}, {"metadata": {"arXiv": "2306.14701", "Date": "Mon, 26 Jun 2023 13:47:38 ", "Title": "Hard Sample Mining Enabled Contrastive Feature Learning for Wind Turbine Pitch System Fault Diagnosis", "Authors": ["Zixuan Wang", "Bo Qin", "Mengxuan Li", "Mark D. Butala", "Haibo Wang", "Peng Peng", "Hongwei Wang"], "Categories": "cs.LG cs.AI"}, "abstract": "The efficient utilization of wind power by wind turbines relies on the ability of their pitch systems to adjust blade pitch angles in response to varying wind speeds. However, the presence of multiple fault types in the pitch system poses challenges in accurately classifying these faults. This paper proposes a novel method based on hard sample mining-enabled contrastive feature learning (HSMCFL) to address this problem. The proposed method employs cosine similarity to identify hard samples and subsequently leverages contrastive feature learning to enhance representation learning through the construction of hard sample pairs. Furthermore, a multilayer perceptron is trained using the learned discriminative representations to serve as an efficient classifier. To evaluate the effectiveness of the proposed method, two real datasets comprising wind turbine pitch system cog belt fracture data are utilized. The fault diagnosis performance of the proposed method is compared against existing methods, and the results demonstrate its superior performance. The proposed approach exhibits significant improvements in fault diagnosis accuracy, providing promising prospects for enhancing the reliability and efficiency of wind turbine pitch system fault diagnosis.", "url": "https://arxiv.org/abs/2306.14701"}, {"metadata": {"arXiv": "2306.14744", "Date": "Mon, 26 Jun 2023 14:59:56 ", "Title": "ChiPFormer: Transferable Chip Placement via Offline Decision Transformer", "Authors": ["Yao Lai", "Jinxin Liu", "Zhentao Tang", "Bin Wang", "Jianye Hao", "Ping Luo"], "Categories": "cs.LG cs.AI"}, "abstract": "Placement is a critical step in modern chip design, aiming to determine the positions of circuit modules on the chip canvas. Recent works have shown that reinforcement learning (RL) can improve human performance in chip placement. However, such an RL-based approach suffers from long training time and low transfer ability in unseen chip circuits. To resolve these challenges, we cast the chip placement as an offline RL formulation and present ChiPFormer that enables learning a transferable placement policy from fixed offline data. ChiPFormer has several advantages that prior arts do not have. First, ChiPFormer can exploit offline placement designs to learn transferable policies more efficiently in a multi-task setting. Second, ChiPFormer can promote effective finetuning for unseen chip circuits, reducing the placement runtime from hours to minutes. Third, extensive experiments on 32 chip circuits demonstrate that ChiPFormer achieves significantly better placement quality while reducing the runtime by 10x compared to recent state-of-the-art approaches in both public benchmarks and realistic industrial tasks. The deliverables are released at https://sites.google.com/view/chipformer/home.", "url": "https://arxiv.org/abs/2306.14744"}, {"metadata": {"arXiv": "2306.14770", "Date": "Mon, 26 Jun 2023 15:26:24 ", "Title": "ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided Diffusion", "Authors": ["Yingjun Du", "Zehao Xiao", "Shengcai Liao", "Cees Snoek"], "Categories": "cs.LG cs.AI", "Comments": ["Under review"]}, "abstract": "Prototype-based meta-learning has emerged as a powerful technique for addressing few-shot learning challenges. However, estimating a deterministic prototype using a simple average function from a limited number of examples remains a fragile process. To overcome this limitation, we introduce ProtoDiff, a novel framework that leverages a task-guided diffusion model during the meta-training phase to gradually generate prototypes, thereby providing efficient class representations. Specifically, a set of prototypes is optimized to achieve per-task prototype overfitting, enabling accurately obtaining the overfitted prototypes for individual tasks. Furthermore, we introduce a task-guided diffusion process within the prototype space, enabling the meta-learning of a generative process that transitions from a vanilla prototype to an overfitted prototype. ProtoDiff gradually generates task-specific prototypes from random noise during the meta-test stage, conditioned on the limited samples available for the new task. Furthermore, to expedite training and enhance ProtoDiff's performance, we propose the utilization of residual prototype learning, which leverages the sparsity of the residual prototype. We conduct thorough ablation studies to demonstrate its ability to accurately capture the underlying prototype distribution and enhance generalization. The new state-of-the-art performance on within-domain, cross-domain, and few-task few-shot classification further substantiates the benefit of ProtoDiff.", "url": "https://arxiv.org/abs/2306.14770"}, {"metadata": {"arXiv": "2306.14884", "Date": "Mon, 26 Jun 2023 17:53:05 ", "Title": "Learning to Modulate pre-trained Models in RL", "Authors": ["Thomas Schmied", "Markus Hofmarcher", "Fabian Paischer", "Razvan Pascanu", "Sepp Hochreiter"], "Categories": "cs.LG cs.AI", "Comments": ["10 pages (+ references and appendix)", "Code: https://github.com/ml-jku/L2M"]}, "abstract": "Reinforcement Learning (RL) has been successful in various domains like robotics, game playing, and simulation. While RL agents have shown impressive capabilities in their specific tasks, they insufficiently adapt to new tasks. In supervised learning, this adaptation problem is addressed by large-scale pre-training followed by fine-tuning to new down-stream tasks. Recently, pre-training on multiple tasks has been gaining traction in RL. However, fine-tuning a pre-trained model often suffers from catastrophic forgetting, that is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks. To investigate the catastrophic forgetting phenomenon, we first jointly pre-train a model on datasets from two benchmark suites, namely Meta-World and DMControl. Then, we evaluate and compare a variety of fine-tuning methods prevalent in natural language processing, both in terms of performance on new tasks, and how well performance on pre-training tasks is retained. Our study shows that with most fine-tuning approaches, the performance on pre-training tasks deteriorates significantly. Therefore, we propose a novel method, Learning-to-Modulate (L2M), that avoids the degradation of learned skills by modulating the information flow of the frozen pre-trained model via a learnable modulation pool. Our method achieves state-of-the-art performance on the Continual-World benchmark, while retaining performance on the pre-training tasks. Finally, to aid future research in this area, we release a dataset encompassing 50 Meta-World and 16 DMControl tasks.", "url": "https://arxiv.org/abs/2306.14884"}, {"metadata": {"arXiv": "2306.14892", "Date": "Mon, 26 Jun 2023 17:58:50 ", "Title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning", "Authors": ["Jonathan N. Lee", "Annie Xie", "Aldo Pacchiano", "Yash Chandak", "Chelsea Finn", "Ofir Nachum", "Emma Brunskill"], "Categories": "cs.LG cs.AI"}, "abstract": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.", "url": "https://arxiv.org/abs/2306.14892"}, {"metadata": {"arXiv": "2306.14178", "Date": "Sun, 25 Jun 2023 09:08:41 ", "Title": "A Framework for dynamically meeting performance objectives on a service mesh", "Authors": ["Forough Shahab Samani and Rolf Stadler"], "Categories": "eess.SY cs.AI cs.LG cs.SY"}, "abstract": "We present a framework for achieving end-to-end management objectives for multiple services that concurrently execute on a service mesh. We apply reinforcement learning (RL) techniques to train an agent that periodically performs control actions to reallocate resources. We develop and evaluate the framework using a laboratory testbed where we run information and computing services on a service mesh, supported by the Istio and Kubernetes platforms. We investigate different management objectives that include end-to-end delay bounds on service requests, throughput objectives, cost-related objectives, and service differentiation. We compute the control policies on a simulator rather than on the testbed, which speeds up the training time by orders of magnitude for the scenarios we study. Our proposed framework is novel in that it advocates a top-down approach whereby the management objectives are defined first and then mapped onto the available control actions. It allows us to execute several types of control actions simultaneously. By first learning the system model and the operating region from testbed traces, we can train the agent for different management objectives in parallel.", "url": "https://arxiv.org/abs/2306.14178"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
