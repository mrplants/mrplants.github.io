<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2306.08200", "Date": "Wed, 14 Jun 2023 02:09:26 ", "Title": "POP: Prompt Of Prompts for Continual Learning", "Authors": ["Zhiyuan Hu", "Jiancheng Lyu", "Dashan Gao", "Nuno Vasconcelos"], "Categories": "cs.CV cs.LG"}, "abstract": "Continual learning (CL) has attracted increasing attention in the recent past. It aims to mimic the human ability to learn new concepts without catastrophic forgetting. While existing CL methods accomplish this to some extent, they are still prone to semantic drift of the learned feature space. Foundation models, which are endowed with a robust feature representation, learned from very large datasets, provide an interesting substrate for the solution of the CL problem. Recent work has also shown that they can be adapted to specific tasks by prompt tuning techniques that leave the generality of the representation mostly unscathed. An open question is, however, how to learn both prompts that are task specific and prompts that are global, i.e. capture cross-task information. In this work, we propose the Prompt Of Prompts (POP) model, which addresses this goal by progressively learning a group of task-specified prompts and a group of global prompts, denoted as POP, to integrate information from the former. We show that a foundation model equipped with POP learning is able to outperform classic CL methods by a significant margin. Moreover, as prompt tuning only requires a small set of training samples, POP is able to perform CL in the few-shot setting, while still outperforming competing methods trained on the entire dataset.", "url": "https://arxiv.org/abs/2306.08200"}, {"metadata": {"arXiv": "2306.08243", "Date": "Wed, 14 Jun 2023 05:04:11 ", "Title": "MMASD: A Multimodal Dataset for Autism Intervention Analysis", "Authors": ["Jicheng Li", "Vuthea Chheang", "Pinar Kullu", "Eli Brignac", "Zhang Guo", "Kenneth E. Barner", "Anjana Bhat", "Roghayeh Leila Barmaki Name"], "Categories": "cs.CV cs.LG", "Comments": ["8 pages", "2 figures"]}, "abstract": "Autism spectrum disorder (ASD) is a developmental disorder characterized by significant social communication impairments and difficulties perceiving and presenting communication cues. Machine learning techniques have been broadly adopted to facilitate autism studies and assessments. However, computational models are primarily concentrated on specific analysis and validated on private datasets in the autism community, which limits comparisons across models due to privacy-preserving data sharing complications. This work presents a novel privacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions of children with Autism. MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from over 100 hours of intervention recordings. To promote public access, each data sample consists of four privacy-preserving modalities of data: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evaluation scores of children, e.g., ADOS scores. MMASD aims to assist researchers and therapists in understanding children's cognitive status, monitoring their progress during therapy, and customizing the treatment plan accordingly. It also has inspiration for downstream tasks such as action quality assessment and interpersonal synchrony estimation. MMASD dataset can be easily accessed at https://github.com/Li-Jicheng/MMASD-A-Multimodal-Dataset-for-Autism-Intervention-Analysis.", "url": "https://arxiv.org/abs/2306.08243"}, {"metadata": {"arXiv": "2306.08591", "Date": "Wed, 14 Jun 2023 15:53:54 ", "Title": "Self-Supervised Polyp Re-Identification in Colonoscopy", "Authors": ["Yotam Intrator", "Natalie Aizenberg", "Amir Livne", "Ehud Rivlin", "Roman Goldenberg"], "Categories": "cs.CV cs.LG", "Comments": ["10 pages", "2 figures", "4 tables", "an supplementary materials (4 figures)"], "MSC-class": "ACM-class: I.2.10, I.2.6, J.3, I.4.8, I.4.9"}, "abstract": "Computer-aided polyp detection (CADe) is becoming a standard, integral part of any modern colonoscopy system. A typical colonoscopy CADe detects a polyp in a single frame and does not track it through the video sequence. Yet, many downstream tasks including polyp characterization (CADx), quality metrics, automatic reporting, require aggregating polyp data from multiple frames. In this work we propose a robust long term polyp tracking method based on re-identification by visual appearance. Our solution uses an attention-based self-supervised ML model, specifically designed to leverage the temporal nature of video input. We quantitatively evaluate method's performance and demonstrate its value for the CADx task.", "url": "https://arxiv.org/abs/2306.08591"}, {"metadata": {"arXiv": "2306.08593", "Date": "Wed, 14 Jun 2023 15:54:42 ", "Title": "Heterogeneous Continual Learning", "Authors": ["Divyam Madaan", "Hongxu Yin", "Wonmin Byeon", "Jan Kautz", "Pavlo Molchanov"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted to CVPR 2023"]}, "abstract": "We propose a novel framework and a solution to tackle the continual learning (CL) problem with changing network architectures. Most CL methods focus on adapting a single architecture to a new task/class by modifying its weights. However, with rapid progress in architecture design, the problem of adapting existing solutions to novel architectures becomes relevant. To address this limitation, we propose Heterogeneous Continual Learning (HCL), where a wide range of evolving network architectures emerge continually together with novel data/tasks. As a solution, we build on top of the distillation family of techniques and modify it to a new setting where a weaker model takes the role of a teacher; meanwhile, a new stronger architecture acts as a student. Furthermore, we consider a setup of limited access to previous data and propose Quick Deep Inversion (QDI) to recover prior task visual features to support knowledge transfer. QDI significantly reduces computational costs compared to previous solutions and improves overall performance. In summary, we propose a new setup for CL with a modified knowledge distillation paradigm and design a quick data inversion method to enhance distillation. Our evaluation of various benchmarks shows a significant improvement on accuracy in comparison to state-of-the-art methods over various networks architectures.", "url": "https://arxiv.org/abs/2306.08593"}, {"metadata": {"arXiv": "2306.08645", "Date": "Wed, 14 Jun 2023 17:23:07 ", "Title": "Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis", "Authors": ["Zhiyu Jin and Xuli Shen and Bin Li and Xiangyang Xue"], "Categories": "cs.CV cs.LG eess.IV", "Comments": ["21 pages", "12 figures"]}, "abstract": "Diffusion models (DMs) have recently gained attention with state-of-the-art performance in text-to-image synthesis. Abiding by the tradition in deep learning, DMs are trained and evaluated on the images with fixed sizes. However, users are demanding for various images with specific sizes and various aspect ratio. This paper focuses on adapting text-to-image diffusion models to handle such variety while maintaining visual fidelity. First we observe that, during the synthesis, lower resolution images suffer from incomplete object portrayal, while higher resolution images exhibit repetitive presentation. Next, we establish a statistical relationship indicating that attention entropy changes with token quantity, suggesting that models aggregate spatial information in proportion to image resolution. The subsequent interpretation on our observations is that objects are incompletely depicted due to limited spatial information for low resolutions, while repetitive presentation arises from redundant spatial information for high resolutions. From this perspective, we propose a scaling factor to alleviate the change of attention entropy and mitigate the defective pattern observed. Extensive experimental results validate the efficacy of the proposed scaling factor, which enables the model to achieve better visual effects, image quality, and text alignment. Notably, these improvements are achieved without additional training or fine-tuning techniques.", "url": "https://arxiv.org/abs/2306.08645"}, {"metadata": {"arXiv": "2306.08657", "Date": "Wed, 14 Jun 2023 17:52:37 ", "Title": "EMERSK -- Explainable Multimodal Emotion Recognition with Situational Knowledge", "Authors": ["Mijanur Palash", "Bharat Bhargava"], "Categories": "cs.CV cs.LG cs.MM", "Comments": ["Emotion Recognition", "Deep Learning", "Multi-modal", "Convolutional neural network (CNN)", "LSTM", "Situational-Knowledge", "Novelty"]}, "abstract": "Automatic emotion recognition has recently gained significant attention due to the growing popularity of deep learning algorithms. One of the primary challenges in emotion recognition is effectively utilizing the various cues (modalities) available in the data. Another challenge is providing a proper explanation of the outcome of the learning.To address these challenges, we present Explainable Multimodal Emotion Recognition with Situational Knowledge (EMERSK), a generalized and modular system for human emotion recognition and explanation using visual information. Our system can handle multiple modalities, including facial expressions, posture, and gait, in a flexible and modular manner. The network consists of different modules that can be added or removed depending on the available data. We utilize a two-stream network architecture with convolutional neural networks (CNNs) and encoder-decoder style attention mechanisms to extract deep features from face images. Similarly, CNNs and recurrent neural networks (RNNs) with Long Short-term Memory (LSTM) are employed to extract features from posture and gait data. We also incorporate deep features from the background as contextual information for the learning process. The deep features from each module are fused using an early fusion network. Furthermore, we leverage situational knowledge derived from the location type and adjective-noun pair (ANP) extracted from the scene, as well as the spatio-temporal average distribution of emotions, to generate explanations. Ablation studies demonstrate that each sub-network can independently perform emotion recognition, and combining them in a multimodal approach significantly improves overall recognition performance. Extensive experiments conducted on various benchmark datasets, including GroupWalk, validate the superior performance of our approach compared to other state-of-the-art methods.", "url": "https://arxiv.org/abs/2306.08657"}, {"metadata": {"arXiv": "2306.08733", "Date": "Wed, 14 Jun 2023 20:34:07 ", "Title": "Continuous Learning Based Novelty Aware Emotion Recognition System", "Authors": ["Mijanur Palash", "Bharat Bhargava"], "Categories": "cs.CV cs.LG cs.MM", "Comments": ["Automatic Emotion Detection", "Novelty", "Deep Learning"], "Journal-ref": "AAAI Spring Symposium 2022"}, "abstract": "Current works in human emotion recognition follow the traditional closed learning approach governed by rigid rules without any consideration of novelty. Classification models are trained on some collected datasets and expected to have the same data distribution in the real-world deployment. Due to the fluid and constantly changing nature of the world we live in, it is possible to have unexpected and novel sample distribution which can lead the model to fail. Hence, in this work, we propose a continuous learning based approach to deal with novelty in the automatic emotion recognition task.", "url": "https://arxiv.org/abs/2306.08733"}, {"metadata": {"arXiv": "2306.08842", "Date": "Thu, 15 Jun 2023 04:06:24 ", "Title": "ViP: A Differentially Private Foundation Model for Computer Vision", "Authors": ["Yaodong Yu and Maziar Sanjabi and Yi Ma and Kamalika Chaudhuri and Chuan Guo"], "Categories": "cs.CV cs.CR cs.LG", "Comments": ["19 pages", "6 figures"]}, "abstract": "Artificial intelligence (AI) has seen a tremendous surge in capabilities thanks to the use of foundation models trained on internet-scale data. On the flip side, the uncurated nature of internet-scale data also poses significant privacy and legal risks, as they often contain personal information or copyrighted material that should not be trained on without permission. In this work, we propose as a mitigation measure a recipe to train foundation vision models with differential privacy (DP) guarantee. We identify masked autoencoders as a suitable learning algorithm that aligns well with DP-SGD, and train ViP -- a Vision transformer with differential Privacy -- under a strict privacy budget of $\\epsilon=8$ on the LAION400M dataset. We evaluate the quality of representation learned by ViP using standard downstream vision tasks; in particular, ViP achieves a (non-private) linear probing accuracy of $55.7\\%$ on ImageNet, comparable to that of end-to-end trained AlexNet (trained and evaluated on ImageNet). Our result suggests that scaling to internet-scale data can be practical for private learning.", "url": "https://arxiv.org/abs/2306.08842"}, {"metadata": {"arXiv": "2306.08865", "Date": "Thu, 15 Jun 2023 05:27:46 ", "Title": "One-Shot Learning of Visual Path Navigation for Autonomous Vehicles", "Authors": ["Zhongying CuiZhu", "Francois Charette", "Amin Ghafourian", "Debo Shi", "Matthew Cui", "Anjali Krishnamachar", "Iman Soltani"], "Categories": "cs.CV cs.LG", "Comments": ["Machine Learning for Autonomous Driving Workshop at the 35th Conference on Neural Information Processing Systems (NeurIPS 20222)", "New Orleans", "USA"]}, "abstract": "Autonomous driving presents many challenges due to the large number of scenarios the autonomous vehicle (AV) may encounter. End-to-end deep learning models are comparatively simplistic models that can handle a broad set of scenarios. However, end-to-end models require large amounts of diverse data to perform well. This paper presents a novel deep neural network that performs image-to-steering path navigation that helps with the data problem by adding one-shot learning to the system. Presented with a previously unseen path, the vehicle can drive the path autonomously after being shown the path once and without model retraining. In fact, the full path is not needed and images of the road junctions is sufficient. In-vehicle testing and offline testing are used to verify the performance of the proposed navigation and to compare different candidate architectures.", "url": "https://arxiv.org/abs/2306.08865"}, {"metadata": {"arXiv": "2306.08960", "Date": "Thu, 15 Jun 2023 08:52:00 ", "Title": "Neural Network Compression using Binarization and Few Full-Precision Weights", "Authors": ["Franco Maria Nardini", "Cosimo Rulli", "Salvatore Trani", "Rossano Venturini"], "Categories": "cs.CV cs.LG", "Comments": ["14 pages", "6 figures", "3 tables"], "ACM-class": "I.2.6"}, "abstract": "Quantization and pruning are known to be two effective Deep Neural Networks model compression methods. In this paper, we propose Automatic Prune Binarization (APB), a novel compression technique combining quantization with pruning. APB enhances the representational capability of binary networks using a few full-precision weights. Our technique jointly maximizes the accuracy of the network while minimizing its memory impact by deciding whether each weight should be binarized or kept in full precision. We show how to efficiently perform a forward pass through layers compressed using APB by decomposing it into a binary and a sparse-dense matrix multiplication. Moreover, we design two novel efficient algorithms for extremely quantized matrix multiplication on CPU, leveraging highly efficient bitwise operations. The proposed algorithms are 6.9x and 1.5x faster than available state-of-the-art solutions. We perform an extensive evaluation of APB on two widely adopted model compression datasets, namely CIFAR10 and ImageNet. APB shows to deliver better accuracy/memory trade-off compared to state-of-the-art methods based on i) quantization, ii) pruning, and iii) combination of pruning and quantization. APB outperforms quantization also in the accuracy/efficiency trade-off, being up to 2x faster than the 2-bits quantized model with no loss in accuracy.", "url": "https://arxiv.org/abs/2306.08960"}, {"metadata": {"arXiv": "2306.09192", "Date": "Thu, 15 Jun 2023 15:19:25 ", "Title": "Training Diffusion Classifiers with Denoising Assistance", "Authors": ["Chandramouli Sastry", "Sri Harsha Dumpala", "Sageev Oore"], "Categories": "cs.CV cs.LG", "Comments": ["Shorter version of this work was accepted in the CVPR 2023 Workshop on Generative Models"]}, "abstract": "Score-matching and diffusion models have emerged as state-of-the-art generative models for both conditional and unconditional generation. Classifier-guided diffusion models are created by training a classifier on samples obtained from the forward-diffusion process (i.e., from data to noise). In this paper, we propose denoising-assisted (DA) classifiers wherein the diffusion classifier is trained using both noisy and denoised examples as simultaneous inputs to the model. We differentiate between denoising-assisted (DA) classifiers and noisy classifiers, which are diffusion classifiers that are only trained on noisy examples. Our experiments on Cifar10 and Imagenet show that DA-classifiers improve over noisy classifiers both quantitatively in terms of generalization to test data and qualitatively in terms of perceptually-aligned classifier-gradients and generative modeling metrics. Finally, we describe a semi-supervised framework for training diffusion classifiers and our experiments, that also include positive-unlabeled settings, demonstrate improved generalization of DA-classifiers over noisy classifiers.", "url": "https://arxiv.org/abs/2306.09192"}, {"metadata": {"arXiv": "2306.09269", "Date": "Thu, 15 Jun 2023 16:43:07 ", "Title": "Zero-Shot Anomaly Detection with Pre-trained Segmentation Models", "Authors": ["Matthew Baugh", "James Batten", "Johanna P. M\\\"uller", "Bernhard Kainz"], "Categories": "cs.CV cs.LG", "Comments": ["Ranked 3rd in zero-shot track of the Visual Anomaly and Novelty Detection (VAND) 2023 Challenge"]}, "abstract": "This technical report outlines our submission to the zero-shot track of the Visual Anomaly and Novelty Detection (VAND) 2023 Challenge. Building on the performance of the WINCLIP framework, we aim to enhance the system's localization capabilities by integrating zero-shot segmentation models. In addition, we perform foreground instance segmentation which enables the model to focus on the relevant parts of the image, thus allowing the models to better identify small or subtle deviations. Our pipeline requires no external data or information, allowing for it to be directly applied to new datasets. Our team (Variance Vigilance Vanguard) ranked third in the zero-shot track of the VAND challenge, and achieve an average F1-max score of 81.5/24.2 at a sample/pixel level on the VisA dataset.", "url": "https://arxiv.org/abs/2306.09269"}, {"metadata": {"arXiv": "2306.09295", "Date": "Thu, 15 Jun 2023 17:20:35 ", "Title": "Neural Fine-Tuning Search for Few-Shot Learning", "Authors": ["Panagiotis Eustratiadis", "{\\L}ukasz Dudziak", "Da Li", "Timothy Hospedales"], "Categories": "cs.CV cs.LG"}, "abstract": "In few-shot recognition, a classifier that has been trained on one set of classes is required to rapidly adapt and generalize to a disjoint, novel set of classes. To that end, recent studies have shown the efficacy of fine-tuning with carefully crafted adaptation architectures. However this raises the question of: How can one design the optimal adaptation strategy? In this paper, we study this question through the lens of neural architecture search (NAS). Given a pre-trained neural network, our algorithm discovers the optimal arrangement of adapters, which layers to keep frozen and which to fine-tune. We demonstrate the generality of our NAS method by applying it to both residual networks and vision transformers and report state-of-the-art performance on Meta-Dataset and Meta-Album.", "url": "https://arxiv.org/abs/2306.09295"}, {"metadata": {"arXiv": "2306.09344", "Date": "Thu, 15 Jun 2023 17:59:50 ", "Title": "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data", "Authors": ["Stephanie Fu", "Netanel Tamir", "Shobhita Sundaram", "Lucy Chai", "Richard Zhang", "Tali Dekel", "Phillip Isola"], "Categories": "cs.CV cs.LG", "Comments": ["Website: https://dreamsim-nights.github.io/ Code: https://github.com/ssundaram21/dreamsim"]}, "abstract": "Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.", "url": "https://arxiv.org/abs/2306.09344"}, {"metadata": {"arXiv": "2306.09345", "Date": "Thu, 15 Jun 2023 17:59:51 ", "Title": "Evaluating Data Attribution for Text-to-Image Models", "Authors": ["Sheng-Yu Wang", "Alexei A. Efros", "Jun-Yan Zhu", "Richard Zhang"], "Categories": "cs.CV cs.LG", "Comments": ["Project page: https://peterwang512.github.io/GenDataAttribution"]}, "abstract": "While large text-to-image models are able to synthesize \"novel\" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through \"customization\" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.", "url": "https://arxiv.org/abs/2306.09345"}, {"metadata": {"arXiv": "2306.09347", "Date": "Thu, 15 Jun 2023 17:59:54 ", "Title": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models", "Authors": ["Youquan Liu and Lingdong Kong and Jun Cen and Runnan Chen and Wenwei Zhang and Liang Pan and Kai Chen and Ziwei Liu"], "Categories": "cs.CV cs.LG cs.RO", "Comments": ["Preprint; 36 pages", "16 figures", "14 tables; Code at https://github.com/youquanl/Segment-Any-Point-Cloud"]}, "abstract": "Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, eliminating the need for annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment stages, facilitating cross-modal representation learning. iii) Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets. Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal. Notably, Seal achieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassing random initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU. Moreover, Seal demonstrates significant performance gains over existing methods across 20 different few-shot fine-tuning tasks on all eleven tested point cloud datasets.", "url": "https://arxiv.org/abs/2306.09347"}, {"metadata": {"arXiv": "2306.08008", "Date": "Tue, 13 Jun 2023 09:13:13 ", "Title": "Dynamic Interval Restrictions on Action Spaces in Deep Reinforcement Learning for Obstacle Avoidance", "Authors": ["Tim Grams"], "Categories": "cs.LG cs.RO", "Comments": ["Master Thesis"]}, "abstract": "Deep reinforcement learning algorithms typically act on the same set of actions. However, this is not sufficient for a wide range of real-world applications where different subsets are available at each step. In this thesis, we consider the problem of interval restrictions as they occur in pathfinding with dynamic obstacles. When actions that lead to collisions are avoided, the continuous action space is split into variable parts. Recent research learns with strong assumptions on the number of intervals, is limited to convex subsets, and the available actions are learned from the observations. Therefore, we propose two approaches that are independent of the state of the environment by extending parameterized reinforcement learning and ConstraintNet to handle an arbitrary number of intervals. We demonstrate their performance in an obstacle avoidance task and compare the methods to penalties, projection, replacement, as well as discrete and continuous masking from the literature. The results suggest that discrete masking of action-values is the only effective method when constraints did not emerge during training. When restrictions are learned, the decision between projection, masking, and our ConstraintNet modification seems to depend on the task at hand. We compare the results with varying complexity and give directions for future work.", "url": "https://arxiv.org/abs/2306.08008"}, {"metadata": {"arXiv": "2306.08076", "Date": "Tue, 13 Jun 2023 18:46:28 ", "Title": "Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization", "Authors": ["Xiner Li", "Shurui Gui", "Youzhi Luo", "Shuiwang Ji"], "Categories": "cs.LG"}, "abstract": "Out-of-distribution (OOD) generalization deals with the prevalent learning scenario where test distribution shifts from training distribution. With rising application demands and inherent complexity, graph OOD problems call for specialized solutions. While data-centric methods exhibit performance enhancements on many generic machine learning tasks, there is a notable absence of data augmentation methods tailored for graph OOD generalization. In this work, we propose to achieve graph OOD generalization with the novel design of non-Euclidean-space linear extrapolation. The proposed augmentation strategy extrapolates both structure and feature spaces to generate OOD graph data. Our design tailors OOD samples for specific shifts without corrupting underlying causal mechanisms. Theoretical analysis and empirical results evidence the effectiveness of our method in solving target shifts, showing substantial and constant improvements across various graph OOD tasks.", "url": "https://arxiv.org/abs/2306.08076"}, {"metadata": {"arXiv": "2306.08107", "Date": "Tue, 13 Jun 2023 19:51:22 ", "Title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks", "Authors": ["Alexander Tornede", "Difan Deng", "Theresa Eimer", "Joseph Giovanelli", "Aditya Mohan", "Tim Ruhkopf", "Sarah Segel", "Daphne Theodorakopoulos", "Tanja Tornede", "Henning Wachsmuth", "Marius Lindauer"], "Categories": "cs.LG cs.CL"}, "abstract": "The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersection of AutoML and LLMs.", "url": "https://arxiv.org/abs/2306.08107"}, {"metadata": {"arXiv": "2306.08109", "Date": "Tue, 13 Jun 2023 19:55:46 ", "Title": "Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity", "Authors": ["Fangshuo Liao", "Anastasios Kyrillidis"], "Categories": "cs.LG math.OC"}, "abstract": "Current state-of-the-art analyses on the convergence of gradient descent for training neural networks focus on characterizing properties of the loss landscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted strong convexity. While gradient descent converges linearly under such conditions, it remains an open question whether Nesterov's momentum enjoys accelerated convergence under similar settings and assumptions. In this work, we consider a new class of objective functions, where only a subset of the parameters satisfies strong convexity, and show Nesterov's momentum achieves acceleration in theory for this objective class. We provide two realizations of the problem class, one of which is deep ReLU networks, which --to the best of our knowledge--constitutes this work the first that proves accelerated convergence rate for non-trivial neural network architectures.", "url": "https://arxiv.org/abs/2306.08109"}, {"metadata": {"arXiv": "2306.08147", "Date": "Tue, 13 Jun 2023 21:35:24 ", "Title": "Multi-market Energy Optimization with Renewables via Reinforcement Learning", "Authors": ["Lucien Werner and Peeyush Kumar"], "Categories": "cs.LG"}, "abstract": "This paper introduces a deep reinforcement learning (RL) framework for optimizing the operations of power plants pairing renewable energy with storage. The objective is to maximize revenue from energy markets while minimizing storage degradation costs and renewable curtailment. The framework handles complexities such as time coupling by storage devices, uncertainty in renewable generation and energy prices, and non-linear storage models. The study treats the problem as a hierarchical Markov Decision Process (MDP) and uses component-level simulators for storage. It utilizes RL to incorporate complex storage models, overcoming restrictions of optimization-based methods that require convex and differentiable component models. A significant aspect of this approach is ensuring policy actions respect system constraints, achieved via a novel method of projecting potentially infeasible actions onto a safe state-action set. The paper demonstrates the efficacy of this approach through extensive experiments using data from US and Indian electricity markets, comparing the learned RL policies with a baseline control policy and a retrospective optimal control policy. It validates the adaptability of the learning framework with various storage models and shows the effectiveness of RL in a complex energy optimization setting, in the context of multi-market bidding, probabilistic forecasts, and accurate storage component models.", "url": "https://arxiv.org/abs/2306.08147"}, {"metadata": {"arXiv": "2306.08153", "Date": "Tue, 13 Jun 2023 21:53:17 ", "Title": "(Amplified) Banded Matrix Factorization: A unified approach to private training", "Authors": ["Christopher A. Choquette-Choo", "Arun Ganesh", "Ryan McKenna", "H. Brendan McMahan", "Keith Rush", "Abhradeep Guha Thakurta", "and Zheng Xu"], "Categories": "cs.LG cs.CR", "Comments": ["34 pages", "13 figures"]}, "abstract": "Matrix factorization (MF) mechanisms for differential privacy (DP) have substantially improved the state-of-the-art in privacy-utility-computation tradeoffs for ML applications in a variety of scenarios, but in both the centralized and federated settings there remain instances where either MF cannot be easily applied, or other algorithms provide better tradeoffs (typically, as $\\epsilon$ becomes small). In this work, we show how MF can subsume prior state-of-the-art algorithms in both federated and centralized training settings, across all privacy budgets. The key technique throughout is the construction of MF mechanisms with banded matrices. For cross-device federated learning (FL), this enables multiple-participations with a relaxed device participation schema compatible with practical FL infrastructure (as demonstrated by a production deployment). In the centralized setting, we prove that banded matrices enjoy the same privacy amplification results as for the ubiquitous DP-SGD algorithm, but can provide strictly better performance in most scenarios -- this lets us always at least match DP-SGD, and often outperform it even at $\\epsilon\\ll2$. Finally, $\\hat{b}$-banded matrices substantially reduce the memory and time complexity of per-step noise generation from $\\mathcal{O}(n)$, $n$ the total number of iterations, to a constant $\\mathcal{O}(\\hat{b})$, compared to general MF mechanisms.", "url": "https://arxiv.org/abs/2306.08153"}, {"metadata": {"arXiv": "2306.08173", "Date": "Tue, 13 Jun 2023 23:32:09 ", "Title": "Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training", "Authors": ["Alyssa Huang", "Peihan Liu", "Ryumei Nakada", "Linjun Zhang", "Wanrong Zhang"], "Categories": "cs.LG cs.CR cs.IT math.IT stat.ML"}, "abstract": "The surge in multimodal AI's success has sparked concerns over data privacy in vision-and-language tasks. While CLIP has revolutionized multimodal learning through joint training on images and text, its potential to unintentionally disclose sensitive information necessitates the integration of privacy-preserving mechanisms. We introduce a differentially private adaptation of the Contrastive Language-Image Pretraining (CLIP) model that effectively addresses privacy concerns while retaining accuracy. Our proposed method, Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse vision-and-language tasks such as image classification and visual question answering. We demonstrate that our approach retains performance on par with the standard non-private CLIP model. Furthermore, we analyze our proposed algorithm under linear representation settings. We derive the convergence rate of our algorithm and show a trade-off between utility and privacy when gradients are clipped per-batch and the loss function does not satisfy smoothness conditions assumed in the literature for the analysis of DP-SGD.", "url": "https://arxiv.org/abs/2306.08173"}, {"metadata": {"arXiv": "2306.08191", "Date": "Wed, 14 Jun 2023 01:24:42 ", "Title": "Solving Large-scale Spatial Problems with Convolutional Neural Networks", "Authors": ["Damian Owerko", "Charilaos I. Kanatsoulis", "Charilaos I. Kanatsoulis"], "Categories": "cs.LG eess.SP", "Comments": ["6 pages", "2 figures", "submitted to Asilomar Conference on Signals", "Systems", "and Computers 2023"]}, "abstract": "Over the past decade, deep learning research has been accelerated by increasingly powerful hardware, which facilitated rapid growth in the model complexity and the amount of data ingested. This is becoming unsustainable and therefore refocusing on efficiency is necessary. In this paper, we employ transfer learning to improve training efficiency for large-scale spatial problems. We propose that a convolutional neural network (CNN) can be trained on small windows of signals, but evaluated on arbitrarily large signals with little to no performance degradation, and provide a theoretical bound on the resulting generalization error. Our proof leverages shift-equivariance of CNNs, a property that is underexploited in transfer learning. The theoretical results are experimentally supported in the context of mobile infrastructure on demand (MID). The proposed approach is able to tackle MID at large scales with hundreds of agents, which was computationally intractable prior to this work.", "url": "https://arxiv.org/abs/2306.08191"}, {"metadata": {"arXiv": "2306.08192", "Date": "Wed, 14 Jun 2023 01:33:06 ", "Title": "Inductive Linear Probing for Few-shot Node Classification", "Authors": ["Hirthik Mathavan", "Zhen Tan", "Nivedh Mudiam", "Huan Liu"], "Categories": "cs.LG cs.SI"}, "abstract": "Meta-learning has emerged as a powerful training strategy for few-shot node classification, demonstrating its effectiveness in the transductive setting. However, the existing literature predominantly focuses on transductive few-shot node classification, neglecting the widely studied inductive setting in the broader few-shot learning community. This oversight limits our comprehensive understanding of the performance of meta-learning based methods on graph data. In this work, we conduct an empirical study to highlight the limitations of current frameworks in the inductive few-shot node classification setting. Additionally, we propose a simple yet competitive baseline approach specifically tailored for inductive few-shot node classification tasks. We hope our work can provide a new path forward to better understand how the meta-learning paradigm works in the graph domain.", "url": "https://arxiv.org/abs/2306.08192"}, {"metadata": {"arXiv": "2306.08201", "Date": "Wed, 14 Jun 2023 02:09:52 ", "Title": "Graph Laplacian Learning with Exponential Family Noise", "Authors": ["Changhao Shi", "Gal Mishne"], "Categories": "cs.LG eess.SP"}, "abstract": "A common challenge in applying graph machine learning methods is that the underlying graph of a system is often unknown. Although different graph inference methods have been proposed for continuous graph signals, inferring the graph structure underlying other types of data, such as discrete counts, is under-explored. In this paper, we generalize a graph signal processing (GSP) framework for learning a graph from smooth graph signals to the exponential family noise distribution to model various data types. We propose an alternating algorithm that estimates the graph Laplacian as well as the unobserved smooth representation from the noisy signals. We demonstrate in synthetic and real-world data that our new algorithm outperforms competing Laplacian estimation methods under noise model mismatch.", "url": "https://arxiv.org/abs/2306.08201"}, {"metadata": {"arXiv": "2306.08210", "Date": "Wed, 14 Jun 2023 02:45:14 ", "Title": "Uncertainty-Aware Robust Learning on Noisy Graphs", "Authors": ["Shuyi Chen", "Kaize Ding", "Shixiang Zhu"], "Categories": "cs.LG"}, "abstract": "Graph neural networks have shown impressive capabilities in solving various graph learning tasks, particularly excelling in node classification. However, their effectiveness can be hindered by the challenges arising from the widespread existence of noisy measurements associated with the topological or nodal information present in real-world graphs. These inaccuracies in observations can corrupt the crucial patterns within the graph data, ultimately resulting in undesirable performance in practical applications. To address these issues, this paper proposes a novel uncertainty-aware graph learning framework motivated by distributionally robust optimization. Specifically, we use a graph neural network-based encoder to embed the node features and find the optimal node embeddings by minimizing the worst-case risk through a minimax formulation. Such an uncertainty-aware learning process leads to improved node representations and a more robust graph predictive model that effectively mitigates the impact of uncertainty arising from data noise. Our experimental result shows that the proposed framework achieves superior predictive performance compared to the state-of-the-art baselines under various noisy settings.", "url": "https://arxiv.org/abs/2306.08210"}, {"metadata": {"arXiv": "2306.08230", "Date": "Wed, 14 Jun 2023 03:59:21 ", "Title": "Unbiased Learning of Deep Generative Models with Structured Discrete Representations", "Authors": ["Harry Bendekgey", "Gabriel Hope and Erik B. Sudderth"], "Categories": "cs.LG stat.ML", "Comments": ["35 pages", "7 figures"]}, "abstract": "By composing graphical models with deep learning architectures, we learn generative models with the strengths of both frameworks. The structured variational autoencoder (SVAE) inherits structure and interpretability from graphical models, and flexible likelihoods for high-dimensional data from deep learning, but poses substantial optimization challenges. We propose novel algorithms for learning SVAEs, and are the first to demonstrate the SVAE's ability to handle multimodal uncertainty when data is missing by incorporating discrete latent variables. Our memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent, while demonstrating robustness to incomplete optimization. To more rapidly learn accurate graphical model parameters, we derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work. These optimization innovations enable the first comparisons of the SVAE to state-of-the-art time series models, where the SVAE performs competitively while learning interpretable and structured discrete data representations.", "url": "https://arxiv.org/abs/2306.08230"}, {"metadata": {"arXiv": "2306.08259", "Date": "Wed, 14 Jun 2023 05:48:36 ", "Title": "LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting", "Authors": ["Xu Liu", "Yutong Xia", "Yuxuan Liang", "Junfeng Hu", "Yiwei Wang", "Lei Bai", "Chao Huang", "Zhenguang Liu", "Bryan Hooi", "Roger Zimmermann"], "Categories": "cs.LG"}, "abstract": "Traffic forecasting plays a critical role in smart city initiatives and has experienced significant advancements thanks to the power of deep learning in capturing non-linear patterns of traffic data. However, the promising results achieved on current public datasets may not be applicable to practical scenarios due to limitations within these datasets. First, the limited sizes of them may not reflect the real-world scale of traffic networks. Second, the temporal coverage of these datasets is typically short, posing hurdles in studying long-term patterns and acquiring sufficient samples for training deep models. Third, these datasets often lack adequate metadata for sensors, which compromises the reliability and interpretability of the data. To mitigate these limitations, we introduce the LargeST benchmark dataset. It encompasses a total number of 8,600 sensors with a 5-year time coverage and includes comprehensive metadata. Using LargeST, we perform in-depth data analysis to extract data insights, benchmark well-known baselines in terms of their performance and efficiency, and identify challenges as well as opportunities for future research. We release the datasets and baseline implementations at: https://github.com/liuxu77/LargeST.", "url": "https://arxiv.org/abs/2306.08259"}, {"metadata": {"arXiv": "2306.08274", "Date": "Wed, 14 Jun 2023 06:24:58 ", "Title": "Why Using Either Aggregated Features or Adjacency Lists in Directed or Undirected Graph? Empirical Study and Simple Classification Method", "Authors": ["Seiji Maekawa", "Yuya Sasaki", "Makoto Onizuka"], "Categories": "cs.LG cs.SI"}, "abstract": "Node classification is one of the hottest tasks in graph analysis. In this paper, we focus on the choices of node representations (aggregated features vs. adjacency lists) and the edge direction of an input graph (directed vs. undirected), which have a large influence on classification results. We address the first empirical study to benchmark the performance of various GNNs that use either combination of node representations and edge directions. Our experiments demonstrate that no single combination stably achieves state-of-the-art results across datasets, which indicates that we need to select appropriate combinations depending on the characteristics of datasets. In response, we propose a simple yet holistic classification method A2DUG which leverages all combinations of node representation variants in directed and undirected graphs. We demonstrate that A2DUG stably performs well on various datasets. Surprisingly, it largely outperforms the current state-of-the-art methods in several datasets. This result validates the importance of the adaptive effect control on the combinations of node representations and edge directions.", "url": "https://arxiv.org/abs/2306.08274"}, {"metadata": {"arXiv": "2306.08289", "Date": "Wed, 14 Jun 2023 06:52:07 ", "Title": "$\\textbf{A}^2\\textbf{CiD}^2$: Accelerating Asynchronous Communication in Decentralized Deep Learning", "Authors": ["Adel Nabli (MLIA", "Mila)", "Eugene Belilovsky (Mila)", "Edouard Oyallon (MLIA)"], "Categories": "cs.LG cs.DC"}, "abstract": "Distributed training of Deep Learning models has been critical to many recent successes in the field. Current standard methods primarily rely on synchronous centralized algorithms which induce major communication bottlenecks and limit their usability to High-Performance Computing (HPC) environments with strong connectivity. Decentralized asynchronous algorithms are emerging as a potential alternative but their practical applicability still lags. In this work, we focus on peerto-peer asynchronous methods due to their flexibility and parallelization potentials. In order to mitigate the increase in bandwidth they require at large scale and in poorly connected contexts, we introduce a principled asynchronous, randomized, gossip-based algorithm which works thanks to a continuous momentum named $\\textbf{A}^2\\textbf{CiD}^2$. In addition to inducing a significant communication acceleration at no cost other than doubling the parameters, minimal adaptation is required to incorporate $\\textbf{A}^2\\textbf{CiD}^2$ to other asynchronous approaches. We demonstrate its efficiency theoretically and numerically. Empirically on the ring graph, adding $\\textbf{A}^2\\textbf{CiD}^2$ has the same effect as doubling the communication rate. In particular, we show consistent improvement on the ImageNet dataset using up to 64 asynchronous workers (A100 GPUs) and various communication network topologies.", "url": "https://arxiv.org/abs/2306.08289"}, {"metadata": {"arXiv": "2306.08292", "Date": "Wed, 14 Jun 2023 07:01:31 ", "Title": "A reinforcement learning strategy for p-adaptation in high order solvers", "Authors": ["David Huergo", "Gonzalo Rubio", "Esteban Ferrer"], "Categories": "cs.LG physics.flu-dyn"}, "abstract": "Reinforcement learning (RL) has emerged as a promising approach to automating decision processes. This paper explores the application of RL techniques to optimise the polynomial order in the computational mesh when using high-order solvers. Mesh adaptation plays a crucial role in improving the efficiency of numerical simulations by improving accuracy while reducing the cost. Here, actor-critic RL models based on Proximal Policy Optimization offer a data-driven approach for agents to learn optimal mesh modifications based on evolving conditions. The paper provides a strategy for p-adaptation in high-order solvers and includes insights into the main aspects of RL-based mesh adaptation, including the formulation of appropriate reward structures and the interaction between the RL agent and the simulation environment. We discuss the impact of RL-based mesh p-adaptation on computational efficiency and accuracy. We test the RL p-adaptation strategy on a 1D inviscid Burgers' equation to demonstrate the effectiveness of the strategy. The RL strategy reduces the computational cost and improves accuracy over uniform adaptation, while minimising human intervention.", "url": "https://arxiv.org/abs/2306.08292"}, {"metadata": {"arXiv": "2306.08299", "Date": "Wed, 14 Jun 2023 07:11:30 ", "Title": "SaDI: A Self-adaptive Decomposed Interpretable Framework for Electric Load Forecasting under Extreme Events", "Authors": ["Hengbo Liu", "Ziqing Ma", "Linxiao Yang", "Tian Zhou", "Rui Xia", "Yi Wang", "Qingsong Wen", "Liang Sun"], "Categories": "cs.LG"}, "abstract": "Accurate prediction of electric load is crucial in power grid planning and management. In this paper, we solve the electric load forecasting problem under extreme events such as scorching heats. One challenge for accurate forecasting is the lack of training samples under extreme conditions. Also load usually changes dramatically in these extreme conditions, which calls for interpretable model to make better decisions. In this paper, we propose a novel forecasting framework, named Self-adaptive Decomposed Interpretable framework~(SaDI), which ensembles long-term trend, short-term trend, and period modelings to capture temporal characteristics in different components. The external variable triggered loss is proposed for the imbalanced learning under extreme events. Furthermore, Generalized Additive Model (GAM) is employed in the framework for desirable interpretability. The experiments on both Central China electric load and public energy meters from buildings show that the proposed SaDI framework achieves average 22.14% improvement compared with the current state-of-the-art algorithms in forecasting under extreme events in terms of daily mean of normalized RMSE. Code, Public datasets, and Appendix are available at: https://doi.org/10.24433/CO.9696980.v1 .", "url": "https://arxiv.org/abs/2306.08299"}, {"metadata": {"arXiv": "2306.08318", "Date": "Wed, 14 Jun 2023 07:38:01 ", "Title": "Identification of Energy Management Configuration Concepts from a Set of Pareto-optimal Solutions", "Authors": ["Felix Lanfermann and Qiqi Liu and Yaochu Jin and Sebastian Schmitt"], "Categories": "cs.LG cs.SY eess.SY", "Comments": ["16 pages", "7 figures", "submitted to Applied Energy"]}, "abstract": "Optimizing building configurations for an efficient use of energy is increasingly receiving attention by current research and several methods have been developed to address this task. Selecting a suitable configuration based on multiple conflicting objectives, such as initial investment cost, recurring cost, robustness with respect to uncertainty of grid operation is, however, a difficult multi-criteria decision making problem. Concept identification can facilitate a decision maker by sorting configuration options into semantically meaningful groups (concepts), further introducing constraints to meet trade-off expectations for a selection of objectives. In this study, for a set of 20000 Pareto-optimal building energy management configurations, resulting from a many-objective evolutionary optimization, multiple concept identification iterations are conducted to provide a basis for making an informed investment decision. In a series of subsequent analysis steps, it is shown how the choice of description spaces, i.e., the partitioning of the features into sets for which consistent and non-overlapping concepts are required, impacts the type of information that can be extracted and that different setups of description spaces illuminate several different aspects of the configuration data - an important aspect that has not been addressed in previous work.", "url": "https://arxiv.org/abs/2306.08318"}, {"metadata": {"arXiv": "2306.08320", "Date": "Wed, 14 Jun 2023 07:39:09 ", "Title": "Nearly Optimal Algorithms with Sublinear Computational Complexity for Online Kernel Regression", "Authors": ["Junfan Li and Shizhong Liao"], "Categories": "cs.LG stat.ML"}, "abstract": "The trade-off between regret and computational cost is a fundamental problem for online kernel regression, and previous algorithms worked on the trade-off can not keep optimal regret bounds at a sublinear computational complexity. In this paper, we propose two new algorithms, AOGD-ALD and NONS-ALD, which can keep nearly optimal regret bounds at a sublinear computational complexity, and give sufficient conditions under which our algorithms work. Both algorithms dynamically maintain a group of nearly orthogonal basis used to approximate the kernel mapping, and keep nearly optimal regret bounds by controlling the approximate error. The number of basis depends on the approximate error and the decay rate of eigenvalues of the kernel matrix. If the eigenvalues decay exponentially, then AOGD-ALD and NONS-ALD separately achieves a regret of $O(\\sqrt{L(f)})$ and $O(\\mathrm{d}_{\\mathrm{eff}}(\\mu)\\ln{T})$ at a computational complexity in $O(\\ln^2{T})$. If the eigenvalues decay polynomially with degree $p\\geq 1$, then our algorithms keep the same regret bounds at a computational complexity in $o(T)$ in the case of $p>4$ and $p\\geq 10$, respectively. $L(f)$ is the cumulative losses of $f$ and $\\mathrm{d}_{\\mathrm{eff}}(\\mu)$ is the effective dimension of the problem. The two regret bounds are nearly optimal and are not comparable.", "url": "https://arxiv.org/abs/2306.08320"}, {"metadata": {"arXiv": "2306.08325", "Date": "Wed, 14 Jun 2023 07:54:53 ", "Title": "GCformer: An Efficient Framework for Accurate and Scalable Long-Term Multivariate Time Series Forecasting", "Authors": ["YanJun Zhao", "Ziqing Ma", "Tian Zhou", "Liang Sun", "Mengni Ye", "Yi Qian"], "Categories": "cs.LG"}, "abstract": "Transformer-based models have emerged as promising tools for time series forecasting. However, these model cannot make accurate prediction for long input time series. On the one hand, they failed to capture global dependencies within time series data. On the other hand, the long input sequence usually leads to large model size and high time complexity. To address these limitations, we present GCformer, which combines a structured global convolutional branch for processing long input sequences with a local Transformer-based branch for capturing short, recent signals. A cohesive framework for a global convolution kernel has been introduced, utilizing three distinct parameterization methods. The selected structured convolutional kernel in the global branch has been specifically crafted with sublinear complexity, thereby allowing for the efficient and effective processing of lengthy and noisy input signals. Empirical studies on six benchmark datasets demonstrate that GCformer outperforms state-of-the-art methods, reducing MSE error in multivariate time series benchmarks by 4.38% and model parameters by 61.92%. In particular, the global convolutional branch can serve as a plug-in block to enhance the performance of other models, with an average improvement of 31.93\\%, including various recently published Transformer-based models. Our code is publicly available at https://github.com/zyj-111/GCformer.", "url": "https://arxiv.org/abs/2306.08325"}, {"metadata": {"arXiv": "2306.08328", "Date": "Wed, 14 Jun 2023 08:00:49 ", "Title": "Distribution Shift Inversion for Out-of-Distribution Prediction", "Authors": ["Runpeng Yu", "Songhua Liu", "Xingyi Yang", "Xinchao Wang"], "Categories": "cs.LG cs.CV"}, "abstract": "Machine learning society has witnessed the emergence of a myriad of Out-of-Distribution (OoD) algorithms, which address the distribution shift between the training and the testing distribution by searching for a unified predictor or invariant feature representation. However, the task of directly mitigating the distribution shift in the unseen testing set is rarely investigated, due to the unavailability of the testing distribution during the training phase and thus the impossibility of training a distribution translator mapping between the training and testing distribution. In this paper, we explore how to bypass the requirement of testing distribution for distribution translator training and make the distribution translation useful for OoD prediction. We propose a portable Distribution Shift Inversion algorithm, in which, before being fed into the prediction model, the OoD testing samples are first linearly combined with additional Gaussian noise and then transferred back towards the training distribution using a diffusion model trained only on the source distribution. Theoretical analysis reveals the feasibility of our method. Experimental results, on both multiple-domain generalization datasets and single-domain generalization datasets, show that our method provides a general performance gain when plugged into a wide range of commonly used OoD algorithms.", "url": "https://arxiv.org/abs/2306.08328"}, {"metadata": {"arXiv": "2306.08393", "Date": "Wed, 14 Jun 2023 09:37:39 ", "Title": "Provably Personalized and Robust Federated Learning", "Authors": ["Mariel Werner", "Lie He", "Sai Praneeth Karimireddy", "Michael Jordan", "Martin Jaggi"], "Categories": "cs.LG cs.DC"}, "abstract": "Clustering clients with similar objectives and learning a model per cluster is an intuitive and interpretable approach to personalization in federated learning. However, doing so with provable and optimal guarantees has remained an open challenge. In this work, we formalize personalized federated learning as a stochastic optimization problem where the stochastic gradients on a client may correspond to one of $K$ distributions. In such a setting, we show that using i) a simple thresholding-based clustering algorithm, and ii) local client gradients obtains optimal convergence guarantees. In fact, our rates asymptotically match those obtained if we knew the true underlying clustering of the clients. Furthermore, our algorithms are provably robust in the Byzantine setting where some fraction of the gradients are corrupted.", "url": "https://arxiv.org/abs/2306.08393"}, {"metadata": {"arXiv": "2306.08432", "Date": "Wed, 14 Jun 2023 11:02:08 ", "Title": "Batches Stabilize the Minimum Norm Risk in High Dimensional Overparameterized Linear Regression", "Authors": ["Shahar Stein Ioushua", "Inbar Hasidim", "Ofer Shayevitz and Meir Feder"], "Categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "Comments": ["51 pages"]}, "abstract": "Learning algorithms that divide the data into batches are prevalent in many machine-learning applications, typically offering useful trade-offs between computational efficiency and performance. In this paper, we examine the benefits of batch-partitioning through the lens of a minimum-norm overparameterized linear regression model with isotropic Gaussian features. We suggest a natural small-batch version of the minimum-norm estimator, and derive an upper bound on its quadratic risk, showing it is inversely proportional to the noise level as well as to the overparameterization ratio, for the optimal choice of batch size. In contrast to minimum-norm, our estimator admits a stable risk behavior that is monotonically increasing in the overparameterization ratio, eliminating both the blowup at the interpolation point and the double-descent phenomenon. Interestingly, we observe that this implicit regularization offered by the batch partition is partially explained by feature overlap between the batches. Our bound is derived via a novel combination of techniques, in particular normal approximation in the Wasserstein metric of noisy projections over random subspaces.", "url": "https://arxiv.org/abs/2306.08432"}, {"metadata": {"arXiv": "2306.08445", "Date": "Wed, 14 Jun 2023 11:37:12 ", "Title": "Deep Gaussian Markov Random Fields for Graph-Structured Dynamical Systems", "Authors": ["Fiona Lippert", "Bart Kranstauber", "E. Emiel van Loon", "Patrick Forr\\'e"], "Categories": "cs.LG"}, "abstract": "Probabilistic inference in high-dimensional state-space models is computationally challenging. For many spatiotemporal systems, however, prior knowledge about the dependency structure of state variables is available. We leverage this structure to develop a computationally efficient approach to state estimation and learning in graph-structured state-space models with (partially) unknown dynamics and limited historical data. Building on recent methods that combine ideas from deep learning with principled inference in Gaussian Markov random fields (GMRF), we reformulate graph-structured state-space models as Deep GMRFs defined by simple spatial and temporal graph layers. This results in a flexible spatiotemporal prior that can be learned efficiently from a single time sequence via variational inference. Under linear Gaussian assumptions, we retain a closed-form posterior, which can be sampled efficiently using the conjugate gradient method, scaling favourably compared to classical Kalman filter based approaches", "url": "https://arxiv.org/abs/2306.08445"}, {"metadata": {"arXiv": "2306.08447", "Date": "Wed, 14 Jun 2023 11:38:36 ", "Title": "Towards Rigorous Design of OoD Detectors", "Authors": ["Chih-Hong Cheng", "Changshun Wu", "Harald Ruess", "Saddek Bensalem"], "Categories": "cs.LG"}, "abstract": "Out-of-distribution (OoD) detection techniques are instrumental for safety-related neural networks. We are arguing, however, that current performance-oriented OoD detection techniques geared towards matching metrics such as expected calibration error, are not sufficient for establishing safety claims. What is missing is a rigorous design approach for developing, verifying, and validating OoD detectors. These design principles need to be aligned with the intended functionality and the operational domain. Here, we formulate some of the key technical challenges, together with a possible way forward, for developing a rigorous and safety-related design methodology for OoD detectors.", "url": "https://arxiv.org/abs/2306.08447"}, {"metadata": {"arXiv": "2306.08469", "Date": "Wed, 14 Jun 2023 12:32:38 ", "Title": "Self-supervised Learning and Graph Classification under Heterophily", "Authors": ["Yilin Ding", "Zhen Liu", "Hao Hao"], "Categories": "cs.LG cs.SI"}, "abstract": "Self-supervised learning has shown its promising capability in graph representation learning in recent work. Most existing pre-training strategies usually choose the popular Graph neural networks (GNNs), which can be seen as a special form of low-pass filter, fail to effectively capture heterophily. In this paper, we first present an experimental investigation exploring the performance of low-pass and high-pass filters in heterophily graph classification, where the results clearly show that high-frequency signal is important for learning heterophily graph representation. On the other hand, it is still unclear how to effectively capture the structural pattern of graphs and how to measure the capability of the self-supervised pre-training strategy in capturing graph structure. To address the problem, we first design a quantitative metric to Measure Graph Structure (MGS), which analyzes correlation between structural similarity and embedding similarity of graph pairs. Then, to enhance the graph structural information captured by self-supervised learning, we propose a novel self-supervised strategy for Pre-training GNNs based on the Metric (PGM). Extensive experiments validate our pre-training strategy achieves state-of-the-art performance for molecular property prediction and protein function prediction. In addition, we find choosing the suitable filter sometimes may be better than designing good pre-training strategies for heterophily graph classification.", "url": "https://arxiv.org/abs/2306.08469"}, {"metadata": {"arXiv": "2306.08470", "Date": "Wed, 14 Jun 2023 12:34:00 ", "Title": "Bandits with Replenishable Knapsacks: the Best of both Worlds", "Authors": ["Martino Bernasconi", "Matteo Castiglioni", "Andrea Celli", "Federico Fusco"], "Categories": "cs.LG stat.ML"}, "abstract": "The bandits with knapsack (BwK) framework models online decision-making problems in which an agent makes a sequence of decisions subject to resource consumption constraints. The traditional model assumes that each action consumes a non-negative amount of resources and the process ends when the initial budgets are fully depleted. We study a natural generalization of the BwK framework which allows non-monotonic resource utilization, i.e., resources can be replenished by a positive amount. We propose a best-of-both-worlds primal-dual template that can handle any online learning problem with replenishment for which a suitable primal regret minimizer exists. In particular, we provide the first positive results for the case of adversarial inputs by showing that our framework guarantees a constant competitive ratio $\\alpha$ when $B=\\Omega(T)$ or when the possible per-round replenishment is a positive constant. Moreover, under a stochastic input model, our algorithm yields an instance-independent $\\tilde{O}(T^{1/2})$ regret bound which complements existing instance-dependent bounds for the same setting. Finally, we provide applications of our framework to some economic problems of practical relevance.", "url": "https://arxiv.org/abs/2306.08470"}, {"metadata": {"arXiv": "2306.08501", "Date": "Wed, 14 Jun 2023 13:31:55 ", "Title": "Adaptive Modeling of Satellite-Derived Nighttime Lights Time-Series for Tracking Urban Change Processes Using Machine Learning", "Authors": ["Srija Chakraborty and Eleanor C. Stokes"], "Categories": "cs.LG"}, "abstract": "Remotely sensed nighttime lights (NTL) uniquely capture urban change processes that are important to human and ecological well-being, such as urbanization, socio-political conflicts and displacement, impacts from disasters, holidays, and changes in daily human patterns of movement. Though several NTL products are global in extent, intrinsic city-specific factors that affect lighting, such as development levels, and social, economic, and cultural characteristics, are unique to each city, making the urban processes embedded in NTL signatures difficult to characterize, and limiting the scalability of urban change analyses. In this study, we propose a data-driven approach to detect urban changes from daily satellite-derived NTL data records that is adaptive across cities and effective at learning city-specific temporal patterns. The proposed method learns to forecast NTL signatures from past data records using neural networks and allows the use of large volumes of unlabeled data, eliminating annotation effort. Urban changes are detected based on deviations of observed NTL from model forecasts using an anomaly detection approach. Comparing model forecasts with observed NTL also allows identifying the direction of change (positive or negative) and monitoring change severity for tracking recovery. In operationalizing the model, we consider ten urban areas from diverse geographic regions with dynamic NTL time-series and demonstrate the generalizability of the approach for detecting the change processes with different drivers and rates occurring within these urban areas based on NTL deviation. This scalable approach for monitoring changes from daily remote sensing observations efficiently utilizes large data volumes to support continuous monitoring and decision making.", "url": "https://arxiv.org/abs/2306.08501"}, {"metadata": {"arXiv": "2306.08537", "Date": "Wed, 14 Jun 2023 14:37:34 ", "Title": "VIBR: Learning View-Invariant Value Functions for Robust Visual Control", "Authors": ["Tom Dupuis", "Jaonary Rabarisoa", "Quoc-Cuong Pham and David Filliat"], "Categories": "cs.LG cs.CV cs.SY eess.SY"}, "abstract": "End-to-end reinforcement learning on images showed significant progress in the recent years. Data-based approach leverage data augmentation and domain randomization while representation learning methods use auxiliary losses to learn task-relevant features. Yet, reinforcement still struggles in visually diverse environments full of distractions and spurious noise. In this work, we tackle the problem of robust visual control at its core and present VIBR (View-Invariant Bellman Residuals), a method that combines multi-view training and invariant prediction to reduce out-of-distribution (OOD) generalization gap for RL based visuomotor control. Our model-free approach improve baselines performances without the need of additional representation learning objectives and with limited additional computational cost. We show that VIBR outperforms existing methods on complex visuo-motor control environment with high visual perturbation. Our approach achieves state-of the-art results on the Distracting Control Suite benchmark, a challenging benchmark still not solved by current methods, where we evaluate the robustness to a number of visual perturbators, as well as OOD generalization and extrapolation capabilities.", "url": "https://arxiv.org/abs/2306.08537"}, {"metadata": {"arXiv": "2306.08553", "Date": "Wed, 14 Jun 2023 14:58:36 ", "Title": "Noise Stability Optimization for Flat Minima with Optimal Convergence Rates", "Authors": ["Haotian Ju", "Dongyue Li", "and Hongyang R. Zhang"], "Categories": "cs.LG cs.DS math.OC stat.ML", "Comments": ["34 pages", "3 figures", "7 tables"]}, "abstract": "We consider finding flat, local minimizers by adding average weight perturbations. Given a nonconvex function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ and a $d$-dimensional distribution $\\mathcal{P}$ which is symmetric at zero, we perturb the weight of $f$ and define $F(W) = \\mathbb{E}[f({W + U})]$, where $U$ is a random sample from $\\mathcal{P}$. This injection induces regularization through the Hessian trace of $f$ for small, isotropic Gaussian perturbations. Thus, the weight-perturbed function biases to minimizers with low Hessian trace. Several prior works have studied settings related to this weight-perturbed function by designing algorithms to improve generalization. Still, convergence rates are not known for finding minima under the average perturbations of the function $F$. This paper considers an SGD-like algorithm that injects random noise before computing gradients while leveraging the symmetry of $\\mathcal{P}$ to reduce variance. We then provide a rigorous analysis, showing matching upper and lower bounds of our algorithm for finding an approximate first-order stationary point of $F$ when the gradient of $f$ is Lipschitz-continuous. We empirically validate our algorithm for several image classification tasks with various architectures. Compared to sharpness-aware minimization, we note a 12.6% and 7.8% drop in the Hessian trace and top eigenvalue of the found minima, respectively, averaged over eight datasets. Ablation studies validate the benefit of the design of our algorithm.", "url": "https://arxiv.org/abs/2306.08553"}, {"metadata": {"arXiv": "2306.08590", "Date": "Wed, 14 Jun 2023 15:53:48 ", "Title": "Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning", "Authors": ["Nikhil Vyas", "Depen Morwani", "Rosie Zhao", "Gal Kaplun", "Sham Kakade", "Boaz Barak"], "Categories": "cs.LG stat.ML"}, "abstract": "The success of SGD in deep learning has been ascribed by prior works to the implicit bias induced by high learning rate or small batch size (\"SGD noise\"). While prior works that focused on offline learning (i.e., multiple-epoch training), we study the impact of SGD noise on online (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that large learning rate and small batch size do not confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating larger or more cost-effective gradient steps. Our work suggests that SGD in the online regime can be construed as taking noisy steps along the \"golden path\" of the noiseless gradient flow algorithm. We provide evidence to support this hypothesis by conducting experiments that reduce SGD noise during training and by measuring the pointwise functional distance between models trained with varying SGD noise levels, but at equivalent loss values. Our findings challenge the prevailing understanding of SGD and offer novel insights into its role in online learning.", "url": "https://arxiv.org/abs/2306.08590"}, {"metadata": {"arXiv": "2306.08595", "Date": "Wed, 14 Jun 2023 15:55:19 ", "Title": "TensorKrowch: Smooth integration of tensor networks in machine learning", "Authors": ["Jos\\'e Ram\\'on Pareja Monturiol", "David P\\'erez-Garc\\'ia", "Alejandro Pozas-Kerstjens"], "Categories": "cs.LG cond-mat.stat-mech cond-mat.str-el quant-ph", "Comments": ["17 pages", "2 figures", "RevTex4.2. The TensorKrowch GitHub repository is in https://github.com/joserapa98/tensorkrowch and the TensorKrowch documentation is in https://joserapa98.github.io/tensorkrowch"]}, "abstract": "Tensor networks are factorizations of high-dimensional tensors into networks of smaller tensors. They have applications in physics and mathematics, and recently have been proposed as promising machine learning architectures. To ease the integration of tensor networks in machine learning pipelines, we introduce TensorKrowch, an open source Python library built on top of PyTorch. Providing a user-friendly interface, TensorKrowch allows users to construct any tensor network, train it, and integrate it as a layer in more intricate deep learning models. In this paper, we describe the main functionality and basic usage of TensorKrowch, and provide technical details on its building blocks and the optimizations performed to achieve efficient operation.", "url": "https://arxiv.org/abs/2306.08595"}, {"metadata": {"arXiv": "2306.08617", "Date": "Wed, 14 Jun 2023 16:23:42 ", "Title": "Multi-class Graph Clustering via Approximated Effective $p$-Resistance", "Authors": ["Shota Saito and Mark Herbster"], "Categories": "cs.LG stat.ML", "Comments": ["To appear at ICML 2023"]}, "abstract": "This paper develops an approximation to the (effective) $p$-resistance and applies it to multi-class clustering. Spectral methods based on the graph Laplacian and its generalization to the graph $p$-Laplacian have been a backbone of non-euclidean clustering techniques. The advantage of the $p$-Laplacian is that the parameter $p$ induces a controllable bias on cluster structure. The drawback of $p$-Laplacian eigenvector based methods is that the third and higher eigenvectors are difficult to compute. Thus, instead, we are motivated to use the $p$-resistance induced by the $p$-Laplacian for clustering. For $p$-resistance, small $p$ biases towards clusters with high internal connectivity while large $p$ biases towards clusters of small ``extent,'' that is a preference for smaller shortest-path distances between vertices in the cluster. However, the $p$-resistance is expensive to compute. We overcome this by developing an approximation to the $p$-resistance. We prove upper and lower bounds on this approximation and observe that it is exact when the graph is a tree. We also provide theoretical justification for the use of $p$-resistance for clustering. Finally, we provide experiments comparing our approximated $p$-resistance clustering to other $p$-Laplacian based methods.", "url": "https://arxiv.org/abs/2306.08617"}, {"metadata": {"arXiv": "2306.08627", "Date": "Wed, 14 Jun 2023 16:41:55 ", "Title": "Graph-Based Matrix Completion Applied to Weather Data", "Authors": ["Beno\\^it Loucheur", "P.-A. Absil", "Michel Journ\\'ee"], "Categories": "cs.LG cs.CE physics.ao-ph", "Comments": ["Accepted by EUSIPCO2023"]}, "abstract": "Low-rank matrix completion is the task of recovering unknown entries of a matrix by assuming that the true matrix admits a good low-rank approximation. Sometimes additional information about the variables is known, and incorporating this information into a matrix completion model can lead to a better completion quality. We consider the situation where information between the column/row entities of the matrix is available as a weighted graph. In this framework, we address the problem of completing missing entries in air temperature data recorded by weather stations. We construct test sets by holding back data at locations that mimic real-life gaps in weather data. On such test sets, we show that adequate spatial and temporal graphs can significantly improve the accuracy of the completion obtained by graph-regularized low-rank matrix completion methods.", "url": "https://arxiv.org/abs/2306.08627"}, {"metadata": {"arXiv": "2306.08656", "Date": "Wed, 14 Jun 2023 17:52:02 ", "Title": "Augment then Smooth: Reconciling Differential Privacy with Certified Robustness", "Authors": ["Jiapeng Wu", "Atiyeh Ashari Ghomi", "David Glukhov", "Jesse C. Cresswell", "Franziska Boenisch", "Nicolas Papernot"], "Categories": "cs.LG cs.CR", "Comments": ["25 pages", "19 figures"]}, "abstract": "Machine learning models are susceptible to a variety of attacks that can erode trust in their deployment. These threats include attacks against the privacy of training data and adversarial examples that jeopardize model accuracy. Differential privacy and randomized smoothing are effective defenses that provide certifiable guarantees for each of these threats, however, it is not well understood how implementing either defense impacts the other. In this work, we argue that it is possible to achieve both privacy guarantees and certified robustness simultaneously. We provide a framework called DP-CERT for integrating certified robustness through randomized smoothing into differentially private model training. For instance, compared to differentially private stochastic gradient descent on CIFAR10, DP-CERT leads to a 12-fold increase in certified accuracy and a 10-fold increase in the average certified radius at the expense of a drop in accuracy of 1.2%. Through in-depth per-sample metric analysis, we show that the certified radius correlates with the local Lipschitz constant and smoothness of the loss surface. This provides a new way to diagnose when private models will fail to be robust.", "url": "https://arxiv.org/abs/2306.08656"}, {"metadata": {"arXiv": "2306.08670", "Date": "Wed, 14 Jun 2023 17:59:15 ", "Title": "Decentralized Learning Dynamics in the Gossip Model", "Authors": ["John Lazarsfeld", "Dan Alistarh"], "Categories": "cs.LG cs.DC cs.DS"}, "abstract": "We study a distributed multi-armed bandit setting among a population of $n$ memory-constrained nodes in the gossip model: at each round, every node locally adopts one of $m$ arms, observes a reward drawn from the arm's (adversarially chosen) distribution, and then communicates with a randomly sampled neighbor, exchanging information to determine its policy in the next round. We introduce and analyze several families of dynamics for this task that are decentralized: each node's decision is entirely local and depends only on its most recently obtained reward and that of the neighbor it sampled. We show a connection between the global evolution of these decentralized dynamics with a certain class of \"zero-sum\" multiplicative weight update algorithms, and we develop a general framework for analyzing the population-level regret of these natural protocols. Using this framework, we derive sublinear regret bounds under a wide range of parameter regimes (i.e., the size of the population and number of arms) for both the stationary reward setting (where the mean of each arm's distribution is fixed over time) and the adversarial reward setting (where means can vary over time). Further, we show that these protocols can approximately optimize convex functions over the simplex when the reward distributions are generated from a stochastic gradient oracle.", "url": "https://arxiv.org/abs/2306.08670"}, {"metadata": {"arXiv": "2306.08700", "Date": "Wed, 14 Jun 2023 18:48:04 ", "Title": "Iterative self-transfer learning: A general methodology for response time-history prediction based on small dataset", "Authors": ["Yongjia Xu", "Xinzheng Lu", "Yifan Fei and Yuli Huang"], "Categories": "cs.LG", "Comments": ["14 pages", "8 figures; Published on Journal of Computational Design and Engineering", "9(5)", "2089-2102"], "Journal-ref": "Journal of Computational Design and Engineering, 9(5), 2089-2102 (2022)", "DOI": "10.1093/jcde/qwac098"}, "abstract": "There are numerous advantages of deep neural network surrogate modeling for response time-history prediction. However, due to the high cost of refined numerical simulations and actual experiments, the lack of data has become an unavoidable bottleneck in practical applications. An iterative self-transfer learningmethod for training neural networks based on small datasets is proposed in this study. A new mapping-based transfer learning network, named as deep adaptation network with three branches for regression (DAN-TR), is proposed. A general iterative network training strategy is developed by coupling DAN-TR and the pseudo-label strategy, and the establishment of corresponding datasets is also discussed. Finally, a complex component is selected as a case study. The results show that the proposed method can improve the model performance by near an order of magnitude on small datasets without the need of external labeled samples,well behaved pre-trainedmodels, additional artificial labeling, and complex physical/mathematical analysis.", "url": "https://arxiv.org/abs/2306.08700"}, {"metadata": {"arXiv": "2306.08734", "Date": "Wed, 14 Jun 2023 20:35:01 ", "Title": "WavPool: A New Block for Deep Neural Networks", "Authors": ["Samuel D. McDermott", "M. Voetberg", "Brian Nord"], "Categories": "cs.LG stat.ML", "Comments": ["8+8 pages", "3+3 figures"], "Report-no": "FERMILAB-CONF-23-278-CSAID"}, "abstract": "Modern deep neural networks comprise many operational layers, such as dense or convolutional layers, which are often collected into blocks. In this work, we introduce a new, wavelet-transform-based network architecture that we call the multi-resolution perceptron: by adding a pooling layer, we create a new network block, the WavPool. The first step of the multi-resolution perceptron is transforming the data into its multi-resolution decomposition form by convolving the input data with filters of fixed coefficients but increasing size. Following image processing techniques, we are able to make scale and spatial information simultaneously accessible to the network without increasing the size of the data vector. WavPool outperforms a similar multilayer perceptron while using fewer parameters, and outperforms a comparable convolutional neural network by ~ 10% on relative accuracy on CIFAR-10.", "url": "https://arxiv.org/abs/2306.08734"}, {"metadata": {"arXiv": "2306.08746", "Date": "Wed, 14 Jun 2023 21:06:07 ", "Title": "MetaML: Automating Customizable Cross-Stage Design-Flow for Deep Learning Acceleration", "Authors": ["Zhiqiang Que", "Shuo Liu", "Markus Rognlien", "Ce Guo", "Jose G. F. Coutinho", "Wayne Luk"], "Categories": "cs.LG cs.AR", "Comments": ["5 pages", "Accepted at FPL'23"]}, "abstract": "This paper introduces a novel optimization framework for deep neural network (DNN) hardware accelerators, enabling the rapid development of customized and automated design flows. More specifically, our approach aims to automate the selection and configuration of low-level optimization techniques, encompassing DNN and FPGA low-level optimizations. We introduce novel optimization and transformation tasks for building design-flow architectures, which are highly customizable and flexible, thereby enhancing the performance and efficiency of DNN accelerators. Our results demonstrate considerable reductions of up to 92\\% in DSP usage and 89\\% in LUT usage for two networks, while maintaining accuracy and eliminating the need for human effort or domain expertise. In comparison to state-of-the-art approaches, our design achieves higher accuracy and utilizes three times fewer DSP resources, underscoring the advantages of our proposed framework.", "url": "https://arxiv.org/abs/2306.08746"}, {"metadata": {"arXiv": "2306.08754", "Date": "Wed, 14 Jun 2023 21:26:31 ", "Title": "ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators", "Authors": ["Sungduk Yu", "Walter M. Hannah", "Liran Peng", "Mohamed Aziz Bhouri", "Ritwik Gupta", "Jerry Lin", "Bj\\\"orn L\\\"utjens", "Justus C. Will", "Tom Beucler", "Bryce E. Harrop", "Benjamin R. Hillman", "Andrea M. Jenney", "Savannah L. Ferretti", "Nana Liu", "Anima Anandkumar", "Noah D. Brenowitz", "Veronika Eyring", "Pierre Gentine", "Stephan Mandt", "Jaideep Pathak", "Carl Vondrick", "Rose Yu", "Laure Zanna", "Ryan P. Abernathey", "Fiaz Ahmed", "David C. Bader", "Pierre Baldi", "Elizabeth A. Barnes", "Gunnar Behrens", "Christopher S. Bretherton", "Julius J. M. Busecke", "Peter M. Caldwell", "Wayne Chuang", "Yilun Han", "Yu Huang", "Fernando Iglesias-Suarez", "Sanket Jantre", "Karthik Kashinath", "Marat Khairoutdinov", "Thorsten Kurth", "Nicholas J. Lutsko", "Po-Lun Ma", "Griffin Mooers", "J. David Neelin", "David A. Randall", "Sara Shamekh", "Akshay Subramaniam", "Mark A. Taylor", "Nathan M. Urban", "Janni Yuval", "Guang J. Zhang", "Tian Zheng", "Michael S. Pritchard"], "Categories": "cs.LG physics.ao-ph"}, "abstract": "Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise prediction of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics on a host climate simulator's macro-scale physical state. The dataset is global in coverage, spans multiple years at high sampling frequency, and is designed such that resulting emulators are compatible with downstream coupling into operational climate simulators. We implement a range of deterministic and stochastic regression baselines to highlight the ML challenges and their scoring. The data (https://huggingface.co/datasets/LEAP/ClimSim_high-res) and code (https://leap-stc.github.io/ClimSim) are released openly to support the development of hybrid ML-physics and high-fidelity climate simulations for the benefit of science and society.", "url": "https://arxiv.org/abs/2306.08754"}, {"metadata": {"arXiv": "2306.08757", "Date": "Wed, 14 Jun 2023 21:48:38 ", "Title": "InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models", "Authors": ["Yingheng Wang", "Yair Schiff", "Aaron Gokaslan", "Weishen Pan", "Fei Wang", "Christopher De Sa", "Volodymyr Kuleshov"], "Categories": "cs.LG cs.CV", "Comments": ["ICML 2023"]}, "abstract": "While diffusion models excel at generating high-quality samples, their latent variables typically lack semantic meaning and are not suitable for representation learning. Here, we propose InfoDiffusion, an algorithm that augments diffusion models with low-dimensional latent variables that capture high-level factors of variation in the data. InfoDiffusion relies on a learning objective regularized with the mutual information between observed and hidden variables, which improves latent space quality and prevents the latents from being ignored by expressive diffusion-based decoders. Empirically, we find that InfoDiffusion learns disentangled and human-interpretable latent representations that are competitive with state-of-the-art generative and contrastive methods, while retaining the high sample quality of diffusion models. Our method enables manipulating the attributes of generated images and has the potential to assist tasks that require exploring a learned latent space to generate quality samples, e.g., generative design.", "url": "https://arxiv.org/abs/2306.08757"}, {"metadata": {"arXiv": "2306.08827", "Date": "Thu, 15 Jun 2023 02:49:05 ", "Title": "PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs", "Authors": ["Zhongkai Hao", "Jiachen Yao", "Chang Su", "Hang Su", "Ziao Wang", "Fanzhi Lu", "Zeyu Xia", "Yichi Zhang", "Songming Liu", "Lu Lu", "Jun Zhu"], "Categories": "cs.LG cs.NA math.NA physics.comp-ph"}, "abstract": "While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future research, particularly in areas such as domain decomposition methods and loss reweighting for handling multi-scale problems and complex geometry. While PINNacle does not guarantee success in all real-world scenarios, it represents a significant contribution to the field by offering a robust, diverse, and comprehensive benchmark suite that will undoubtedly foster further research and development in PINNs.", "url": "https://arxiv.org/abs/2306.08827"}, {"metadata": {"arXiv": "2306.08838", "Date": "Thu, 15 Jun 2023 04:03:06 ", "Title": "Differentially Private Domain Adaptation with Theoretical Guarantees", "Authors": ["Raef Bassily", "Corinna Cortes", "Anqi Mao", "Mehryar Mohri"], "Categories": "cs.LG cs.CR stat.ML"}, "abstract": "In many applications, the labeled data at the learner's disposal is subject to privacy constraints and is relatively limited. To derive a more accurate predictor for the target domain, it is often beneficial to leverage publicly available labeled data from an alternative domain, somewhat close to the target domain. This is the modern problem of supervised domain adaptation from a public source to a private target domain. We present two $(\\epsilon, \\delta)$-differentially private adaptation algorithms for supervised adaptation, for which we make use of a general optimization problem, recently shown to benefit from favorable theoretical learning guarantees. Our first algorithm is designed for regression with linear predictors and shown to solve a convex optimization problem. Our second algorithm is a more general solution for loss functions that may be non-convex but Lipschitz and smooth. While our main objective is a theoretical analysis, we also report the results of several experiments first demonstrating that the non-private versions of our algorithms outperform adaptation baselines and next showing that, for larger values of the target sample size or $\\epsilon$, the performance of our private algorithms remains close to that of the non-private formulation.", "url": "https://arxiv.org/abs/2306.08838"}, {"metadata": {"arXiv": "2306.08848", "Date": "Thu, 15 Jun 2023 04:24:13 ", "Title": "Datasheets for Machine Learning Sensors", "Authors": ["Matthew Stewart", "Pete Warden", "Yasmine Omri", "Shvetank Prakash", "Joao Santos", "Shawn Hymel", "Benjamin Brown", "Jim MacArthur", "Nat Jeffries", "Brian Plancher", "Vijay Janapa Reddi"], "Categories": "cs.LG cs.CY cs.HC"}, "abstract": "Machine learning (ML) sensors offer a new paradigm for sensing that enables intelligence at the edge while empowering end-users with greater control of their data. As these ML sensors play a crucial role in the development of intelligent devices, clear documentation of their specifications, functionalities, and limitations is pivotal. This paper introduces a standard datasheet template for ML sensors and discusses its essential components including: the system's hardware, ML model and dataset attributes, end-to-end performance metrics, and environmental impact. We provide an example datasheet for our own ML sensor and discuss each section in detail. We highlight how these datasheets can facilitate better understanding and utilization of sensor data in ML applications, and we provide objective measures upon which system performance can be evaluated and compared. Together, ML sensors and their datasheets provide greater privacy, security, transparency, explainability, auditability, and user-friendliness for ML-enabled embedded systems. We conclude by emphasizing the need for standardization of datasheets across the broader ML community to ensure the responsible and effective use of sensor data.", "url": "https://arxiv.org/abs/2306.08848"}, {"metadata": {"arXiv": "2306.08860", "Date": "Thu, 15 Jun 2023 05:04:39 ", "Title": "OMS-DPM: Optimizing the Model Schedule for Diffusion Probabilistic Models", "Authors": ["Enshu Liu", "Xuefei Ning", "Zinan Lin", "Huazhong Yang and Yu Wang"], "Categories": "cs.LG", "Comments": ["Accepted by ICML2023"]}, "abstract": "Diffusion probabilistic models (DPMs) are a new class of generative models that have achieved state-of-the-art generation quality in various domains. Despite the promise, one major drawback of DPMs is the slow generation speed due to the large number of neural network evaluations required in the generation process. In this paper, we reveal an overlooked dimension -- model schedule -- for optimizing the trade-off between generation quality and speed. More specifically, we observe that small models, though having worse generation quality when used alone, could outperform large models in certain generation steps. Therefore, unlike the traditional way of using a single model, using different models in different generation steps in a carefully designed \\emph{model schedule} could potentially improve generation quality and speed \\emph{simultaneously}. We design OMS-DPM, a predictor-based search algorithm, to optimize the model schedule given an arbitrary generation time budget and a set of pre-trained models. We demonstrate that OMS-DPM can find model schedules that improve generation quality and speed than prior state-of-the-art methods across CIFAR-10, CelebA, ImageNet, and LSUN datasets. When applied to the public checkpoints of the Stable Diffusion model, we are able to accelerate the sampling by 2$\\times$ while maintaining the generation quality.", "url": "https://arxiv.org/abs/2306.08860"}, {"metadata": {"arXiv": "2306.08862", "Date": "Thu, 15 Jun 2023 05:15:13 ", "Title": "Hyperbolic Convolution via Kernel Point Aggregation", "Authors": ["Eric Qu", "Dongmian Zou"], "Categories": "cs.LG stat.ML"}, "abstract": "Learning representations according to the underlying geometry is of vital importance for non-Euclidean data. Studies have revealed that the hyperbolic space can effectively embed hierarchical or tree-like data. In particular, the few past years have witnessed a rapid development of hyperbolic neural networks. However, it is challenging to learn good hyperbolic representations since common Euclidean neural operations, such as convolution, do not extend to the hyperbolic space. Most hyperbolic neural networks do not embrace the convolution operation and ignore local patterns. Others either only use non-hyperbolic convolution, or miss essential properties such as equivariance to permutation. We propose HKConv, a novel trainable hyperbolic convolution which first correlates trainable local hyperbolic features with fixed kernel points placed in the hyperbolic space, then aggregates the output features within a local neighborhood. HKConv not only expressively learns local features according to the hyperbolic geometry, but also enjoys equivariance to permutation of hyperbolic points and invariance to parallel transport of a local neighborhood. We show that neural networks with HKConv layers advance state-of-the-art in various tasks.", "url": "https://arxiv.org/abs/2306.08862"}, {"metadata": {"arXiv": "2306.08881", "Date": "Thu, 15 Jun 2023 06:35:24 ", "Title": "Evaluation and Optimization of Gradient Compression for Distributed Deep Learning", "Authors": ["Lin Zhang", "Longteng Zhang", "Shaohuai Shi", "Xiaowen Chu", "Bo Li"], "Categories": "cs.LG cs.DC", "Comments": ["Accepted at ICDCS 2023"]}, "abstract": "To accelerate distributed training, many gradient compression methods have been proposed to alleviate the communication bottleneck in synchronous stochastic gradient descent (S-SGD), but their efficacy in real-world applications still remains unclear. In this work, we first evaluate the efficiency of three representative compression methods (quantization with Sign-SGD, sparsification with Top-k SGD, and low-rank with Power-SGD) on a 32-GPU cluster. The results show that they cannot always outperform well-optimized S-SGD or even worse due to their incompatibility with three key system optimization techniques (all-reduce, pipelining, and tensor fusion) in S-SGD. To this end, we propose a novel gradient compression method, called alternate compressed Power-SGD (ACP-SGD), which alternately compresses and communicates low-rank matrices. ACP-SGD not only significantly reduces the communication volume, but also enjoys the three system optimizations like S-SGD. Compared with Power-SGD, the optimized ACP-SGD can largely reduce the compression and communication overheads, while achieving similar model accuracy. In our experiments, ACP-SGD achieves an average of 4.06x and 1.43x speedups over S-SGD and Power-SGD, respectively, and it consistently outperforms other baselines across different setups (from 8 GPUs to 64 GPUs and from 1Gb/s Ethernet to 100Gb/s InfiniBand).", "url": "https://arxiv.org/abs/2306.08881"}, {"metadata": {"arXiv": "2306.08900", "Date": "Thu, 15 Jun 2023 07:08:41 ", "Title": "Offline Multi-Agent Reinforcement Learning with Coupled Value Factorization", "Authors": ["Xiangsen Wang", "Xianyuan Zhan"], "Categories": "cs.LG cs.MA", "Comments": ["Accepted by the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023)"]}, "abstract": "Offline reinforcement learning (RL) that learns policies from offline datasets without environment interaction has received considerable attention in recent years. Compared with the rich literature in the single-agent case, offline multi-agent RL is still a relatively underexplored area. Most existing methods directly apply offline RL ingredients in the multi-agent setting without fully leveraging the decomposable problem structure, leading to less satisfactory performance in complex tasks. We present OMAC, a new offline multi-agent RL algorithm with coupled value factorization. OMAC adopts a coupled value factorization scheme that decomposes the global value function into local and shared components, and also maintains the credit assignment consistency between the state-value and Q-value functions. Moreover, OMAC performs in-sample learning on the decomposed local state-value functions, which implicitly conducts max-Q operation at the local level while avoiding distributional shift caused by evaluating out-of-distribution actions. Based on the comprehensive evaluations of the offline multi-agent StarCraft II micro-management tasks, we demonstrate the superior performance of OMAC over the state-of-the-art offline multi-agent RL methods.", "url": "https://arxiv.org/abs/2306.08900"}, {"metadata": {"arXiv": "2306.08933", "Date": "Thu, 15 Jun 2023 08:16:01 ", "Title": "Towards Interpretability in Audio and Visual Affective Machine Learning: A Review", "Authors": ["David S. Johnson", "Olya Hakobyan", "and Hanna Drimalla"], "Categories": "cs.LG", "Comments": ["8 pages", "2 tables"]}, "abstract": "Machine learning is frequently used in affective computing, but presents challenges due the opacity of state-of-the-art machine learning methods. Because of the impact affective machine learning systems may have on an individual's life, it is important that models be made transparent to detect and mitigate biased decision making. In this regard, affective machine learning could benefit from the recent advancements in explainable artificial intelligence (XAI) research. We perform a structured literature review to examine the use of interpretability in the context of affective machine learning. We focus on studies using audio, visual, or audiovisual data for model training and identified 29 research articles. Our findings show an emergence of the use of interpretability methods in the last five years. However, their use is currently limited regarding the range of methods used, the depth of evaluations, and the consideration of use-cases. We outline the main gaps in the research and provide recommendations for researchers that aim to implement interpretable methods for affective machine learning.", "url": "https://arxiv.org/abs/2306.08933"}, {"metadata": {"arXiv": "2306.08942", "Date": "Thu, 15 Jun 2023 08:27:50 ", "Title": "Active Representation Learning for General Task Space with Applications in Robotics", "Authors": ["Yifang Chen", "Yingbing Huang", "Simon S. Du", "Kevin Jamieson", "Guanya Shi"], "Categories": "cs.LG cs.RO"}, "abstract": "Representation learning based on multi-task pretraining has become a powerful approach in many domains. In particular, task-aware representation learning aims to learn an optimal representation for a specific target task by sampling data from a set of source tasks, while task-agnostic representation learning seeks to learn a universal representation for a class of tasks. In this paper, we propose a general and versatile algorithmic and theoretic framework for \\textit{active representation learning}, where the learner optimally chooses which source tasks to sample from. This framework, along with a tractable meta algorithm, allows most arbitrary target and source task spaces (from discrete to continuous), covers both task-aware and task-agnostic settings, and is compatible with deep representation learning practices. We provide several instantiations under this framework, from bilinear and feature-based nonlinear to general nonlinear cases. In the bilinear case, by leveraging the non-uniform spectrum of the task representation and the calibrated source-target relevance, we prove that the sample complexity to achieve $\\varepsilon$-excess risk on target scales with $ (k^*)^2 \\|v^*\\|_2^2 \\varepsilon^{-2}$ where $k^*$ is the effective dimension of the target and $\\|v^*\\|_2^2 \\in (0,1]$ represents the connection between source and target space. Compared to the passive one, this can save up to $\\frac{1}{d_W}$ of sample complexity, where $d_W$ is the task space dimension. Finally, we demonstrate different instantiations of our meta algorithm in synthetic datasets and robotics problems, from pendulum simulations to real-world drone flight datasets. On average, our algorithms outperform baselines by $20\\%-70\\%$.", "url": "https://arxiv.org/abs/2306.08942"}, {"metadata": {"arXiv": "2306.08943", "Date": "Thu, 15 Jun 2023 08:33:52 ", "Title": "Neural Fields with Hard Constraints of Arbitrary Differential Order", "Authors": ["Fangcheng Zhong", "Kyle Fogarty", "Param Hanji", "Tianhao Wu", "Alejandro Sztrajman", "Andrew Spielberg", "Andrea Tagliasacchi", "Petra Bosilj", "Cengiz Oztireli"], "Categories": "cs.LG cs.NA math.NA"}, "abstract": "While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as \\emph{Constrained Neural Fields} (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization.", "url": "https://arxiv.org/abs/2306.08943"}, {"metadata": {"arXiv": "2306.08951", "Date": "Thu, 15 Jun 2023 08:44:35 ", "Title": "MLonMCU: TinyML Benchmarking with Fast Retargeting", "Authors": ["Philipp van Kempen", "Rafael Stahl", "Daniel Mueller-Gritschneder", "Ulf Schlichtmann"], "Categories": "cs.LG", "Comments": ["CODAI 2022 Workshop - Embedded System Week (ESWeek)"]}, "abstract": "While there exist many ways to deploy machine learning models on microcontrollers, it is non-trivial to choose the optimal combination of frameworks and targets for a given application. Thus, automating the end-to-end benchmarking flow is of high relevance nowadays. A tool called MLonMCU is proposed in this paper and demonstrated by benchmarking the state-of-the-art TinyML frameworks TFLite for Microcontrollers and TVM effortlessly with a large number of configurations in a low amount of time.", "url": "https://arxiv.org/abs/2306.08951"}, {"metadata": {"arXiv": "2306.08954", "Date": "Thu, 15 Jun 2023 08:47:50 ", "Title": "Re-Benchmarking Pool-Based Active Learning for Binary Classification", "Authors": ["Po-Yi Lu", "Chun-Liang Li", "Hsuan-Tien Lin"], "Categories": "cs.LG"}, "abstract": "Active learning is a paradigm that significantly enhances the performance of machine learning models when acquiring labeled data is expensive. While several benchmarks exist for evaluating active learning strategies, their findings exhibit some misalignment. This discrepancy motivates us to develop a transparent and reproducible benchmark for the community. Our efforts result in an open-sourced implementation (https://github.com/ariapoy/active-learning-benchmark) that is reliable and extensible for future research. By conducting thorough re-benchmarking experiments, we have not only rectified misconfigurations in existing benchmark but also shed light on the under-explored issue of model compatibility, which directly causes the observed discrepancy. Resolving the discrepancy reassures that the uncertainty sampling strategy of active learning remains an effective and preferred choice for most datasets. Our experience highlights the importance of dedicating research efforts towards re-benchmarking existing benchmarks to produce more credible results and gain deeper insights.", "url": "https://arxiv.org/abs/2306.08954"}, {"metadata": {"arXiv": "2306.08968", "Date": "Thu, 15 Jun 2023 09:02:24 ", "Title": "Partial-Label Regression", "Authors": ["Xin Cheng and Deng-Bao Wang and Lei Feng and Min-Ling Zhang and Bo An"], "Categories": "cs.LG", "Comments": ["Accepted by AAAI 2023"]}, "abstract": "Partial-label learning is a popular weakly supervised learning setting that allows each training example to be annotated with a set of candidate labels. Previous studies on partial-label learning only focused on the classification setting where candidate labels are all discrete, which cannot handle continuous labels with real values. In this paper, we provide the first attempt to investigate partial-label regression, where each training example is annotated with a set of real-valued candidate labels. To solve this problem, we first propose a simple baseline method that takes the average loss incurred by candidate labels as the predictive loss. The drawback of this method lies in that the loss incurred by the true label may be overwhelmed by other false labels. To overcome this drawback, we propose an identification method that takes the least loss incurred by candidate labels as the predictive loss. We further improve it by proposing a progressive identification method to differentiate candidate labels using progressively updated weights for incurred losses. We prove that the latter two methods are model-consistent and provide convergence analyses. Our proposed methods are theoretically grounded and can be compatible with any models, optimizers, and losses. Experiments validate the effectiveness of our proposed methods.", "url": "https://arxiv.org/abs/2306.08968"}, {"metadata": {"arXiv": "2306.08984", "Date": "Thu, 15 Jun 2023 09:25:04 ", "Title": "Tree Variational Autoencoders", "Authors": ["Laura Manduchi", "Moritz Vandenhirtz", "Alain Ryser", "Julia Vogt"], "Categories": "cs.LG cs.CV"}, "abstract": "We propose a new generative hierarchical clustering model that learns a flexible tree-based posterior distribution over latent variables. The proposed Tree Variational Autoencoder (TreeVAE) hierarchically divides samples according to their intrinsic characteristics, shedding light on hidden structure in the data. It adapts its architecture to discover the optimal tree for encoding dependencies between latent variables. The proposed tree-based generative architecture permits lightweight conditional inference and improves generative performance by utilizing specialized leaf decoders. We show that TreeVAE uncovers underlying clusters in the data and finds meaningful hierarchical relations between the different groups on a variety of datasets, including real-world imaging data. We present empirically that TreeVAE provides a more competitive log-likelihood lower bound than the sequential counterparts. Finally, due to its generative nature, TreeVAE is able to generate new samples from the discovered clusters via conditional sampling.", "url": "https://arxiv.org/abs/2306.08984"}, {"metadata": {"arXiv": "2306.09000", "Date": "Thu, 15 Jun 2023 09:54:21 ", "Title": "When and Why Momentum Accelerates SGD:An Empirical Study", "Authors": ["Jingwen Fu", "Bohan Wang", "Huishuai Zhang", "Zhizheng Zhang", "Wei Chen", "Nanning Zheng"], "Categories": "cs.LG cs.CV"}, "abstract": "Momentum has become a crucial component in deep learning optimizers, necessitating a comprehensive understanding of when and why it accelerates stochastic gradient descent (SGD). To address the question of ''when'', we establish a meaningful comparison framework that examines the performance of SGD with Momentum (SGDM) under the \\emph{effective learning rates} $\\eta_{ef}$, a notion unifying the influence of momentum coefficient $\\mu$ and batch size $b$ over learning rate $\\eta$. In the comparison of SGDM and SGD with the same effective learning rate and the same batch size, we observe a consistent pattern: when $\\eta_{ef}$ is small, SGDM and SGD experience almost the same empirical training losses; when $\\eta_{ef}$ surpasses a certain threshold, SGDM begins to perform better than SGD. Furthermore, we observe that the advantage of SGDM over SGD becomes more pronounced with a larger batch size. For the question of ``why'', we find that the momentum acceleration is closely related to \\emph{abrupt sharpening} which is to describe a sudden jump of the directional Hessian along the update direction. Specifically, the misalignment between SGD and SGDM happens at the same moment that SGD experiences abrupt sharpening and converges slower. Momentum improves the performance of SGDM by preventing or deferring the occurrence of abrupt sharpening. Together, this study unveils the interplay between momentum, learning rates, and batch sizes, thus improving our understanding of momentum acceleration.", "url": "https://arxiv.org/abs/2306.09000"}, {"metadata": {"arXiv": "2306.09044", "Date": "Thu, 15 Jun 2023 11:07:17 ", "Title": "Hands-on detection for steering wheels with neural networks", "Authors": ["Michael Hollmer and Andreas Fischer"], "Categories": "cs.LG", "Comments": ["Proc. of the Interdisciplinary Conference on Mechanics", "Computers and Electrics (ICMECE 2022)"]}, "abstract": "In this paper the concept of a machine learning based hands-on detection algorithm is proposed. The hand detection is implemented on the hardware side using a capacitive method. A sensor mat in the steering wheel detects a change in capacity as soon as the driver's hands come closer. The evaluation and final decision about hands-on or hands-off situations is done using machine learning. In order to find a suitable machine learning model, different models are implemented and evaluated. Based on accuracy, memory consumption and computational effort the most promising one is selected and ported on a micro controller. The entire system is then evaluated in terms of reliability and response time.", "url": "https://arxiv.org/abs/2306.09044"}, {"metadata": {"arXiv": "2306.09048", "Date": "Thu, 15 Jun 2023 11:12:35 ", "Title": "Optimal Best-Arm Identification in Bandits with Access to Offline Data", "Authors": ["Shubhada Agrawal", "Sandeep Juneja", "Karthikeyan Shanmugam", "Arun Sai Suggala"], "Categories": "cs.LG stat.ML", "Comments": ["45 pages", "5 figures"]}, "abstract": "Learning paradigms based purely on offline data as well as those based solely on sequential online learning have been well-studied in the literature. In this paper, we consider combining offline data with online learning, an area less studied but of obvious practical importance. We consider the stochastic $K$-armed bandit problem, where our goal is to identify the arm with the highest mean in the presence of relevant offline data, with confidence $1-\\delta$. We conduct a lower bound analysis on policies that provide such $1-\\delta$ probabilistic correctness guarantees. We develop algorithms that match the lower bound on sample complexity when $\\delta$ is small. Our algorithms are computationally efficient with an average per-sample acquisition cost of $\\tilde{O}(K)$, and rely on a careful characterization of the optimality conditions of the lower bound problem.", "url": "https://arxiv.org/abs/2306.09048"}, {"metadata": {"arXiv": "2306.09087", "Date": "Thu, 15 Jun 2023 12:33:39 ", "Title": "Deep learning based Meta-modeling for Multi-objective Technology Optimization of Electrical Machines", "Authors": ["Vivek Parekh", "Dominik Flore", "Sebastian Sch\\\"ops"], "Categories": "cs.LG", "Comments": ["12 pages", "15 figures"]}, "abstract": "Optimization of rotating electrical machines is both time- and computationally expensive. Because of the different parametrization, design optimization is commonly executed separately for each machine technology. In this paper, we present the application of a variational auto-encoder (VAE) to optimize two different machine technologies simultaneously, namely an asynchronous machine and a permanent magnet synchronous machine. After training, we employ a deep neural network and a decoder as meta-models to predict global key performance indicators (KPIs) and generate associated new designs, respectively, through unified latent space in the optimization loop. Numerical results demonstrate concurrent parametric multi-objective technology optimization in the high-dimensional design space. The VAE-based approach is quantitatively compared to a classical deep learning-based direct approach for KPIs prediction.", "url": "https://arxiv.org/abs/2306.09087"}, {"metadata": {"arXiv": "2306.09096", "Date": "Thu, 15 Jun 2023 12:47:56 ", "Title": "Multi-Objective Optimization of Electrical Machines using a Hybrid Data-and Physics-Driven Approach", "Authors": ["Vivek Parekh", "Dominik Flore", "Sebastian Sch\\\"ops", "Peter Theisinger"], "Categories": "cs.LG", "Comments": ["This work was presented at 11th International Conference on Computation in Electromagnetics as poster presentation"]}, "abstract": "Magneto-static finite element (FE) simulations make numerical optimization of electrical machines very time-consuming and computationally intensive during the design stage. In this paper, we present the application of a hybrid data-and physics-driven model for numerical optimization of permanent magnet synchronous machines (PMSM). Following the data-driven supervised training, deep neural network (DNN) will act as a meta-model to characterize the electromagnetic behavior of PMSM by predicting intermediate FE measures. These intermediate measures are then post-processed with various physical models to compute the required key performance indicators (KPIs), e.g., torque, shaft power, and material costs. We perform multi-objective optimization with both classical FE and a hybrid approach using a nature-inspired evolutionary algorithm. We show quantitatively that the hybrid approach maintains the quality of Pareto results better or close to conventional FE simulation-based optimization while being computationally very cheap.", "url": "https://arxiv.org/abs/2306.09096"}, {"metadata": {"arXiv": "2306.09099", "Date": "Thu, 15 Jun 2023 12:51:56 ", "Title": "Unbalanced Diffusion Schr\\\"odinger Bridge", "Authors": ["Matteo Pariset", "Ya-Ping Hsieh", "Charlotte Bunne", "Andreas Krause", "Valentin De Bortoli"], "Categories": "cs.LG"}, "abstract": "Schr\\\"odinger bridges (SBs) provide an elegant framework for modeling the temporal evolution of populations in physical, chemical, or biological systems. Such natural processes are commonly subject to changes in population size over time due to the emergence of new species or birth and death events. However, existing neural parameterizations of SBs such as diffusion Schr\\\"odinger bridges (DSBs) are restricted to settings in which the endpoints of the stochastic process are both probability measures and assume conservation of mass constraints. To address this limitation, we introduce unbalanced DSBs which model the temporal evolution of marginals with arbitrary finite mass. This is achieved by deriving the time reversal of stochastic differential equations with killing and birth terms. We present two novel algorithmic schemes that comprise a scalable objective function for training unbalanced DSBs and provide a theoretical analysis alongside challenging applications on predicting heterogeneous molecular single-cell responses to various cancer drugs and simulating the emergence and spread of new viral variants.", "url": "https://arxiv.org/abs/2306.09099"}, {"metadata": {"arXiv": "2306.09104", "Date": "Thu, 15 Jun 2023 13:00:56 ", "Title": "On Strengthening and Defending Graph Reconstruction Attack with Markov Chain Approximation", "Authors": ["Zhanke Zhou", "Chenyu Zhou", "Xuan Li", "Jiangchao Yao", "Quanming Yao", "Bo Han"], "Categories": "cs.LG cs.CR", "Comments": ["Accepted by ICML 2023"]}, "abstract": "Although powerful graph neural networks (GNNs) have boosted numerous real-world applications, the potential privacy risk is still underexplored. To close this gap, we perform the first comprehensive study of graph reconstruction attack that aims to reconstruct the adjacency of nodes. We show that a range of factors in GNNs can lead to the surprising leakage of private links. Especially by taking GNNs as a Markov chain and attacking GNNs via a flexible chain approximation, we systematically explore the underneath principles of graph reconstruction attack, and propose two information theory-guided mechanisms: (1) the chain-based attack method with adaptive designs for extracting more private information; (2) the chain-based defense method that sharply reduces the attack fidelity with moderate accuracy loss. Such two objectives disclose a critical belief that to recover better in attack, you must extract more multi-aspect knowledge from the trained GNN; while to learn safer for defense, you must forget more link-sensitive information in training GNNs. Empirically, we achieve state-of-the-art results on six datasets and three common GNNs. The code is publicly available at: https://github.com/tmlr-group/MC-GRA.", "url": "https://arxiv.org/abs/2306.09104"}, {"metadata": {"arXiv": "2306.09105", "Date": "Thu, 15 Jun 2023 13:01:16 ", "Title": "Performance Evaluation and Comparison of a New Regression Algorithm", "Authors": ["Sabina Gooljar", "Kris Manohar and Patrick Hosein"], "Categories": "cs.LG", "Comments": ["8 pages", "2 figures", "In Proceedings of the 12th International Conference on Data Science", "Technology and Applications July 2023"]}, "abstract": "In recent years, Machine Learning algorithms, in particular supervised learning techniques, have been shown to be very effective in solving regression problems. We compare the performance of a newly proposed regression algorithm against four conventional machine learning algorithms namely, Decision Trees, Random Forest, k-Nearest Neighbours and XG Boost. The proposed algorithm was presented in detail in a previous paper but detailed comparisons were not included. We do an in-depth comparison, using the Mean Absolute Error (MAE) as the performance metric, on a diverse set of datasets to illustrate the great potential and robustness of the proposed approach. The reader is free to replicate our results since we have provided the source code in a GitHub repository while the datasets are publicly available.", "url": "https://arxiv.org/abs/2306.09105"}, {"metadata": {"arXiv": "2306.09121", "Date": "Thu, 15 Jun 2023 13:29:09 ", "Title": "The Split Matters: Flat Minima Methods for Improving the Performance of GNNs", "Authors": ["Nicolas Lell and Ansgar Scherp"], "Categories": "cs.LG"}, "abstract": "When training a Neural Network, it is optimized using the available training data with the hope that it generalizes well to new or unseen testing data. At the same absolute value, a flat minimum in the loss landscape is presumed to generalize better than a sharp minimum. Methods for determining flat minima have been mostly researched for independent and identically distributed (i. i. d.) data such as images. Graphs are inherently non-i. i. d. since the vertices are edge-connected. We investigate flat minima methods and combinations of those methods for training graph neural networks (GNNs). We use GCN and GAT as well as extend Graph-MLP to work with more layers and larger graphs. We conduct experiments on small and large citation, co-purchase, and protein datasets with different train-test splits in both the transductive and inductive training procedure. Results show that flat minima methods can improve the performance of GNN models by over 2 points, if the train-test split is randomized. Following Shchur et al., randomized splits are essential for a fair evaluation of GNNs, as other (fixed) splits like 'Planetoid' are biased. Overall, we provide important insights for improving and fairly evaluating flat minima methods on GNNs. We recommend practitioners to always use weight averaging techniques, in particular EWA when using early stopping. While weight averaging techniques are only sometimes the best performing method, they are less sensitive to hyperparameters, need no additional training, and keep the original model unchanged. All source code is available in https://github.com/Foisunt/FMMs-in-GNNs.", "url": "https://arxiv.org/abs/2306.09121"}, {"metadata": {"arXiv": "2306.09129", "Date": "Thu, 15 Jun 2023 13:41:59 ", "Title": "Deep Learning for Energy Time-Series Analysis and Forecasting", "Authors": ["Maria Tzelepi", "Charalampos Symeonidis", "Paraskevi Nousi", "Efstratios Kakaletsis", "Theodoros Manousis", "Pavlos Tosidis", "Nikos Nikolaidis and Anastasios Tefas"], "Categories": "cs.LG", "Comments": ["13 papges", "4 figures"]}, "abstract": "Energy time-series analysis describes the process of analyzing past energy observations and possibly external factors so as to predict the future. Different tasks are involved in the general field of energy time-series analysis and forecasting, with electric load demand forecasting, personalized energy consumption forecasting, as well as renewable energy generation forecasting being among the most common ones. Following the exceptional performance of Deep Learning (DL) in a broad area of vision tasks, DL models have successfully been utilized in time-series forecasting tasks. This paper aims to provide insight into various DL methods geared towards improving the performance in energy time-series forecasting tasks, with special emphasis in Greek Energy Market, and equip the reader with the necessary knowledge to apply these methods in practice.", "url": "https://arxiv.org/abs/2306.09129"}, {"metadata": {"arXiv": "2306.09136", "Date": "Thu, 15 Jun 2023 13:49:30 ", "Title": "Logarithmic Bayes Regret Bounds", "Authors": ["Alexia Atsidakou", "Branislav Kveton", "Sumeet Katariya", "Constantine Caramanis", "Sujay Sanghavi"], "Categories": "cs.LG stat.ML"}, "abstract": "We derive the first finite-time logarithmic regret bounds for Bayesian bandits. For Gaussian bandits, we obtain a $O(c_h \\log^2 n)$ bound, where $c_h$ is a prior-dependent constant. This matches the asymptotic lower bound of Lai (1987). Our proofs mark a technical departure from prior works, and are simple and general. To show generality, we apply our technique to linear bandits. Our bounds shed light on the value of the prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve the $\\tilde{O}(\\sqrt{n})$ bounds, that despite the existing lower bounds, have become standard in the literature.", "url": "https://arxiv.org/abs/2306.09136"}, {"metadata": {"arXiv": "2306.09158", "Date": "Thu, 15 Jun 2023 14:32:35 ", "Title": "Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution Generalization and Detection", "Authors": ["Haoyue Bai", "Gregory Canal", "Xuefeng Du", "Jeongyeol Kwon", "Robert Nowak", "Yixuan Li"], "Categories": "cs.LG", "Comments": ["ICML 2023"]}, "abstract": "Modern machine learning models deployed in the wild can encounter both covariate and semantic shifts, giving rise to the problems of out-of-distribution (OOD) generalization and OOD detection respectively. While both problems have received significant research attention lately, they have been pursued independently. This may not be surprising, since the two tasks have seemingly conflicting goals. This paper provides a new unified approach that is capable of simultaneously generalizing to covariate shifts while robustly detecting semantic shifts. We propose a margin-based learning framework that exploits freely available unlabeled data in the wild that captures the environmental test-time OOD distributions under both covariate and semantic shifts. We show both empirically and theoretically that the proposed margin constraint is the key to achieving both OOD generalization and detection. Extensive experiments show the superiority of our framework, outperforming competitive baselines that specialize in either OOD generalization or OOD detection. Code is publicly available at https://github.com/deeplearning-wisc/scone.", "url": "https://arxiv.org/abs/2306.09158"}, {"metadata": {"arXiv": "2306.09177", "Date": "Thu, 15 Jun 2023 14:56:37 ", "Title": "Dis-AE: Multi-domain & Multi-task Generalisation on Real-World Clinical Data", "Authors": ["Daniel Kreuter", "Samuel Tull", "Julian Gilbey", "Jacobus Preller", "BloodCounts! Consortium", "John A.D. Aston", "James H.F. Rudd", "Suthesh Sivapalaratnam", "Carola-Bibiane Sch\\\"onlieb", "Nicholas Gleadall", "Michael Roberts"], "Categories": "cs.LG", "Comments": ["17 pages main body", "5 figures", "18 pages of appendix"]}, "abstract": "Clinical data is often affected by clinically irrelevant factors such as discrepancies between measurement devices or differing processing methods between sites. In the field of machine learning (ML), these factors are known as domains and the distribution differences they cause in the data are known as domain shifts. ML models trained using data from one domain often perform poorly when applied to data from another domain, potentially leading to wrong predictions. As such, developing machine learning models that can generalise well across multiple domains is a challenging yet essential task in the successful application of ML in clinical practice. In this paper, we propose a novel disentangled autoencoder (Dis-AE) neural network architecture that can learn domain-invariant data representations for multi-label classification of medical measurements even when the data is influenced by multiple interacting domain shifts at once. The model utilises adversarial training to produce data representations from which the domain can no longer be determined. We evaluate the model's domain generalisation capabilities on synthetic datasets and full blood count (FBC) data from blood donors as well as primary and secondary care patients, showing that Dis-AE improves model generalisation on multiple domains simultaneously while preserving clinically relevant information.", "url": "https://arxiv.org/abs/2306.09177"}, {"metadata": {"arXiv": "2306.09202", "Date": "Thu, 15 Jun 2023 15:37:31 ", "Title": "Combinatorial Pure Exploration of Multi-Armed Bandit with a Real Number Action Class", "Authors": ["Shintaro Nakamura and Masashi Sugiyama"], "Categories": "cs.LG"}, "abstract": "The combinatorial pure exploration (CPE) in the stochastic multi-armed bandit setting (MAB) is a well-studied online decision-making problem: A player wants to find the optimal \\emph{action} $\\boldsymbol{\\pi}^*$ from \\emph{action class} $\\mathcal{A}$, which is a collection of subsets of arms with certain combinatorial structures. Though CPE can represent many combinatorial structures such as paths, matching, and spanning trees, most existing works focus only on binary action class $\\mathcal{A}\\subseteq\\{0, 1\\}^d$ for some positive integer $d$. This binary formulation excludes important problems such as the optimal transport, knapsack, and production planning problems. To overcome this limitation, we extend the binary formulation to real, $\\mathcal{A}\\subseteq\\mathbb{R}^d$, and propose a new algorithm. The only assumption we make is that the number of actions in $\\mathcal{A}$ is polynomial in $d$. We show an upper bound of the sample complexity for our algorithm and the action class-dependent lower bound for R-CPE-MAB, by introducing a quantity that characterizes the problem's difficulty, which is a generalization of the notion \\emph{width} introduced in Chen et al.[2014].", "url": "https://arxiv.org/abs/2306.09202"}, {"metadata": {"arXiv": "2306.09205", "Date": "Thu, 15 Jun 2023 15:40:04 ", "Title": "Reward-Free Curricula for Training Robust World Models", "Authors": ["Marc Rigter", "Minqi Jiang", "Ingmar Posner"], "Categories": "cs.LG"}, "abstract": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. Achieving a general agent requires robustness across different environments. However, different environments may require different amounts of data to learn a suitable world model. In this work, we address the problem of efficiently learning robust world models in the reward-free setting. As a measure of robustness, we consider the minimax regret objective. We show that the minimax regret objective can be connected to minimising the maximum error in the world model across environments. This informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms naive domain randomisation, resulting in improved robustness, efficiency, and generalisation.", "url": "https://arxiv.org/abs/2306.09205"}, {"metadata": {"arXiv": "2306.09210", "Date": "Thu, 15 Jun 2023 15:47:50 ", "Title": "Optimal Exploration for Model-Based RL in Nonlinear Systems", "Authors": ["Andrew Wagenmaker", "Guanya Shi", "Kevin Jamieson"], "Categories": "cs.LG cs.RO cs.SY eess.SY math.OC stat.ML"}, "abstract": "Learning to control unknown nonlinear dynamical systems is a fundamental problem in reinforcement learning and control theory. A commonly applied approach is to first explore the environment (exploration), learn an accurate model of it (system identification), and then compute an optimal controller with the minimum cost on this estimated system (policy optimization). While existing work has shown that it is possible to learn a uniformly good model of the system~\\citep{mania2020active}, in practice, if we aim to learn a good controller with a low cost on the actual system, certain system parameters may be significantly more critical than others, and we therefore ought to focus our exploration on learning such parameters. In this work, we consider the setting of nonlinear dynamical systems and seek to formally quantify, in such settings, (a) which parameters are most relevant to learning a good controller, and (b) how we can best explore so as to minimize uncertainty in such parameters. Inspired by recent work in linear systems~\\citep{wagenmaker2021task}, we show that minimizing the controller loss in nonlinear systems translates to estimating the system parameters in a particular, task-dependent metric. Motivated by this, we develop an algorithm able to efficiently explore the system to reduce uncertainty in this metric, and prove a lower bound showing that our approach learns a controller at a near-instance-optimal rate. Our algorithm relies on a general reduction from policy optimization to optimal experiment design in arbitrary systems, and may be of independent interest. We conclude with experiments demonstrating the effectiveness of our method in realistic nonlinear robotic systems.", "url": "https://arxiv.org/abs/2306.09210"}, {"metadata": {"arXiv": "2306.09211", "Date": "Thu, 15 Jun 2023 15:49:37 ", "Title": "A Framework for Learning from Demonstration with Minimal Human Effort", "Authors": ["Marc Rigter", "Bruno Lacerda", "Nick Hawes"], "Categories": "cs.LG cs.RO", "Comments": ["Preprint version of IEEE Robotics and Automation Letters paper"]}, "abstract": "We consider robot learning in the context of shared autonomy, where control of the system can switch between a human teleoperator and autonomous control. In this setting we address reinforcement learning, and learning from demonstration, where there is a cost associated with human time. This cost represents the human time required to teleoperate the robot, or recover the robot from failures. For each episode, the agent must choose between requesting human teleoperation, or using one of its autonomous controllers. In our approach, we learn to predict the success probability for each controller, given the initial state of an episode. This is used in a contextual multi-armed bandit algorithm to choose the controller for the episode. A controller is learnt online from demonstrations and reinforcement learning so that autonomous performance improves, and the system becomes less reliant on the teleoperator with more experience. We show that our approach to controller selection reduces the human cost to perform two simulated tasks and a single real-world task.", "url": "https://arxiv.org/abs/2306.09211"}, {"metadata": {"arXiv": "2306.09253", "Date": "Thu, 15 Jun 2023 16:30:33 ", "Title": "MinMax Networks", "Authors": ["Winfried Lohmiller", "Philipp Gassert", "Jean-Jacques Slotine"], "Categories": "cs.LG math.DS"}, "abstract": "While much progress has been achieved over the last decades in neuro-inspired machine learning, there are still fundamental theoretical problems in gradient-based learning using combinations of neurons. These problems, such as saddle points and suboptimal plateaus of the cost function, can lead in theory and practice to failures of learning. In addition, the discrete step size selection of the gradient is problematic since too large steps can lead to instability and too small steps slow down the learning. This paper describes an alternative discrete MinMax learning approach for continuous piece-wise linear functions. Global exponential convergence of the algorithm is established using Contraction Theory with Inequality Constraints, which is extended from the continuous to the discrete case in this paper: The parametrization of each linear function piece is, in contrast to deep learning, linear in the proposed MinMax network. This allows a linear regression stability proof as long as measurements do not transit from one linear region to its neighbouring linear region. The step size of the discrete gradient descent is Lagrangian limited orthogonal to the edge of two neighbouring linear functions. It will be shown that this Lagrangian step limitation does not decrease the convergence of the unconstrained system dynamics in contrast to a step size limitation in the direction of the gradient. We show that the convergence rate of a constrained piece-wise linear function learning is equivalent to the exponential convergence rates of the individual local linear regions.", "url": "https://arxiv.org/abs/2306.09253"}, {"metadata": {"arXiv": "2306.09256", "Date": "Thu, 15 Jun 2023 16:32:08 ", "Title": "A Survey of Some Density Based Clustering Techniques", "Authors": ["Rupanka Bhuyan and Samarjeet Borah"], "Categories": "cs.LG cs.DB", "Comments": ["4 pages", "1 figure", "conference paper"], "DOI": "10.13140/2.1.4554.6887"}, "abstract": "Density Based Clustering are a type of Clustering methods using in data mining for extracting previously unknown patterns from data sets. There are a number of density based clustering methods such as DBSCAN, OPTICS, DENCLUE, VDBSCAN, DVBSCAN, DBCLASD and ST-DBSCAN. In this paper, a study of these methods is done along with their characteristics, advantages and disadvantages and most importantly, their applicability to different types of data sets to mine useful and appropriate patterns.", "url": "https://arxiv.org/abs/2306.09256"}, {"metadata": {"arXiv": "2306.09261", "Date": "Thu, 15 Jun 2023 16:36:34 ", "Title": "Mitigating Cold-start Forecasting using Cold Causal Demand Forecasting Model", "Authors": ["Zahra Fatemi", "Minh Huynh", "Elena Zheleva", "Zamir Syed", "Xiaojun Di"], "Categories": "cs.LG"}, "abstract": "Forecasting multivariate time series data, which involves predicting future values of variables over time using historical data, has significant practical applications. Although deep learning-based models have shown promise in this field, they often fail to capture the causal relationship between dependent variables, leading to less accurate forecasts. Additionally, these models cannot handle the cold-start problem in time series data, where certain variables lack historical data, posing challenges in identifying dependencies among variables. To address these limitations, we introduce the Cold Causal Demand Forecasting (CDF-cold) framework that integrates causal inference with deep learning-based models to enhance the forecasting accuracy of multivariate time series data affected by the cold-start problem. To validate the effectiveness of the proposed approach, we collect 15 multivariate time-series datasets containing the network traffic of different Google data centers. Our experiments demonstrate that the CDF-cold framework outperforms state-of-the-art forecasting models in predicting future values of multivariate time series data.", "url": "https://arxiv.org/abs/2306.09261"}, {"metadata": {"arXiv": "2306.09301", "Date": "Thu, 15 Jun 2023 17:28:00 ", "Title": "OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection", "Authors": ["Jingyang Zhang", "Jingkang Yang", "Pengyun Wang", "Haoqi Wang", "Yueqian Lin", "Haoran Zhang", "Yiyou Sun", "Xuefeng Du", "Kaiyang Zhou", "Wayne Zhang", "Yixuan Li", "Ziwei Liu", "Yiran Chen", "Hai Li"], "Categories": "cs.LG cs.CV", "Comments": ["Submitted to NeurIPS Datasets & Benchmarks Track. See code at https://github.com/Jingkang50/OpenOOD/ and leaderboard at https://zjysteven.github.io/OpenOOD/"]}, "abstract": "Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and usability. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate, standardized, and user-friendly evaluation of OOD detection methodologies. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale datasets such as ImageNet, investigates full-spectrum OOD detection which is important yet underexplored, and introduces new features including an online leaderboard and an easy-to-use evaluator. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool of OOD detection methodologies. With these enhancements, OpenOOD v1.5 aims to drive advancements and offer a more robust and comprehensive evaluation benchmark for OOD detection research.", "url": "https://arxiv.org/abs/2306.09301"}, {"metadata": {"arXiv": "2306.09328", "Date": "Thu, 15 Jun 2023 17:58:04 ", "Title": "WizMap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings", "Authors": ["Zijie J. Wang", "Fred Hohman", "Duen Horng Chau"], "Categories": "cs.LG cs.CL cs.CV cs.HC", "Comments": ["8 pages", "8 figures", "Accepted to ACL 2023. For a demo video", "see https://youtu.be/8fJG87QVceQ. For a live demo", "see https://poloclub.github.io/wizmap. Code is available at https://github.com/poloclub/wizmap"]}, "abstract": "Machine learning models often learn latent embedding representations that capture the domain semantics of their training data. These embedding representations are valuable for interpreting trained models, building new models, and analyzing new datasets. However, interpreting and using embeddings can be challenging due to their opaqueness, high dimensionality, and the large size of modern datasets. To tackle these challenges, we present WizMap, an interactive visualization tool to help researchers and practitioners easily explore large embeddings. With a novel multi-resolution embedding summarization method and a familiar map-like interaction design, WizMap enables users to navigate and interpret embedding spaces with ease. Leveraging modern web technologies such as WebGL and Web Workers, WizMap scales to millions of embedding points directly in users' web browsers and computational notebooks without the need for dedicated backend servers. WizMap is open-source and available at the following public demo link: https://poloclub.github.io/wizmap.", "url": "https://arxiv.org/abs/2306.09328"}, {"metadata": {"arXiv": "2306.09338", "Date": "Thu, 15 Jun 2023 17:59:27 ", "Title": "Understanding Optimization of Deep Learning", "Authors": ["Xianbiao Qi", "Jianan Wang and Lei Zhang"], "Categories": "cs.LG cs.CV math.OC stat.ML", "Comments": ["International Digital Economy Academy (IDEA)"]}, "abstract": "This article provides a comprehensive understanding of optimization in deep learning, with a primary focus on the challenges of gradient vanishing and gradient exploding, which normally lead to diminished model representational ability and training instability, respectively. We analyze these two challenges through several strategic measures, including the improvement of gradient flow and the imposition of constraints on a network's Lipschitz constant. To help understand the current optimization methodologies, we categorize them into two classes: explicit optimization and implicit optimization. Explicit optimization methods involve direct manipulation of optimizer parameters, including weight, gradient, learning rate, and weight decay. Implicit optimization methods, by contrast, focus on improving the overall landscape of a network by enhancing its modules, such as residual shortcuts, normalization methods, attention mechanisms, and activations. In this article, we provide an in-depth analysis of these two optimization classes and undertake a thorough examination of the Jacobian matrices and the Lipschitz constants of many widely used deep learning modules, highlighting existing issues as well as potential improvements. Moreover, we also conduct a series of analytical experiments to substantiate our theoretical discussions. This article does not aim to propose a new optimizer or network. Rather, our intention is to present a comprehensive understanding of optimization in deep learning. We hope that this article will assist readers in gaining a deeper insight in this field and encourages the development of more robust, efficient, and high-performing models.", "url": "https://arxiv.org/abs/2306.09338"}, {"metadata": {"arXiv": "2306.09342", "Date": "Thu, 15 Jun 2023 17:59:32 ", "Title": "PaReprop: Fast Parallelized Reversible Backpropagation", "Authors": ["Tyler Zhu and Karttikeya Mangalam"], "Categories": "cs.LG cs.CL cs.CV", "Comments": ["Spotlight paper", "T4V Workshop @ CVPR 2023"]}, "abstract": "The growing size of datasets and deep learning models has made faster and memory-efficient training crucial. Reversible transformers have recently been introduced as an exciting new method for extremely memory-efficient training, but they come with an additional computation overhead of activation re-computation in the backpropagation phase. We present PaReprop, a fast Parallelized Reversible Backpropagation algorithm that parallelizes the additional activation re-computation overhead in reversible training with the gradient computation itself in backpropagation phase. We demonstrate the effectiveness of the proposed PaReprop algorithm through extensive benchmarking across model families (ViT, MViT, Swin and RoBERTa), data modalities (Vision & NLP), model sizes (from small to giant), and training batch sizes. Our empirical results show that PaReprop achieves up to 20% higher training throughput than vanilla reversible training, largely mitigating the theoretical overhead of 25% lower throughput from activation recomputation in reversible training. Project page: https://tylerzhu.com/pareprop.", "url": "https://arxiv.org/abs/2306.09342"}, {"metadata": {"arXiv": "2306.08419", "Date": "Wed, 14 Jun 2023 10:31:37 ", "Title": "Mediated Multi-Agent Reinforcement Learning", "Authors": ["Dmitry Ivanov", "Ilya Zisman", "Kirill Chernyshev"], "Categories": "cs.MA cs.GT cs.LG", "Journal-ref": "AAMAS '23, Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (May 2023) Pages 49-57", "DOI": "10.5555/3545946.3598618"}, "abstract": "The majority of Multi-Agent Reinforcement Learning (MARL) literature equates the cooperation of self-interested agents in mixed environments to the problem of social welfare maximization, allowing agents to arbitrarily share rewards and private information. This results in agents that forgo their individual goals in favour of social good, which can potentially be exploited by selfish defectors. We argue that cooperation also requires agents' identities and boundaries to be respected by making sure that the emergent behaviour is an equilibrium, i.e., a convention that no agent can deviate from and receive higher individual payoffs. Inspired by advances in mechanism design, we propose to solve the problem of cooperation, defined as finding socially beneficial equilibrium, by using mediators. A mediator is a benevolent entity that may act on behalf of agents, but only for the agents that agree to it. We show how a mediator can be trained alongside agents with policy gradient to maximize social welfare subject to constraints that encourage agents to cooperate through the mediator. Our experiments in matrix and iterative games highlight the potential power of applying mediators in MARL.", "url": "https://arxiv.org/abs/2306.08419"}, {"metadata": {"arXiv": "2306.08224", "Date": "Wed, 14 Jun 2023 03:30:04 ", "Title": "Expanding Versatility of Agile Locomotion through Policy Transitions Using Latent State Representation", "Authors": ["Guilherme Christmann", "Ying-Sheng Luo", "Jonathan Hans Soeseno", "Wei-Chao Chen"], "Categories": "cs.RO cs.LG", "Comments": ["Presented at ICRA 2023"]}, "abstract": "This paper proposes the transition-net, a robust transition strategy that expands the versatility of robot locomotion in the real-world setting. To this end, we start by distributing the complexity of different gaits into dedicated locomotion policies applicable to real-world robots. Next, we expand the versatility of the robot by unifying the policies with robust transitions into a single coherent meta-controller by examining the latent state representations. Our approach enables the robot to iteratively expand its skill repertoire and robustly transition between any policy pair in a library. In our framework, adding new skills does not introduce any process that alters the previously learned skills. Moreover, training of a locomotion policy takes less than an hour with a single consumer GPU. Our approach is effective in the real-world and achieves a 19% higher average success rate for the most challenging transition pairs in our experiments compared to existing approaches.", "url": "https://arxiv.org/abs/2306.08224"}, {"metadata": {"arXiv": "2306.08281", "Date": "Wed, 14 Jun 2023 06:39:34 ", "Title": "3-Dimensional Sonic Phase-invariant Echo Localization", "Authors": ["Christopher Hahne"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["Accepted at ICRA 2023"]}, "abstract": "Parallax and Time-of-Flight (ToF) are often regarded as complementary in robotic vision where various light and weather conditions remain challenges for advanced camera-based 3-Dimensional (3-D) reconstruction. To this end, this paper establishes Parallax among Corresponding Echoes (PaCE) to triangulate acoustic ToF pulses from arbitrary sensor positions in 3-D space for the first time. This is achieved through a novel round-trip reflection model that pinpoints targets at the intersection of ellipsoids, which are spanned by sensor locations and detected arrival times. Inter-channel echo association becomes a crucial prerequisite for target detection and is learned from feature similarity obtained by a stack of Siamese Multi-Layer Perceptrons (MLPs). The PaCE algorithm enables phase-invariant 3-D object localization from only 1 isotropic emitter and at least 3 ToF receivers with relaxed sensor position constraints. Experiments are conducted with airborne ultrasound sensor hardware and back this hypothesis with quantitative results.", "url": "https://arxiv.org/abs/2306.08281"}, {"metadata": {"arXiv": "2306.09010", "Date": "Thu, 15 Jun 2023 10:11:38 ", "Title": "DiAReL: Reinforcement Learning with Disturbance Awareness for Robust Sim2Real Policy Transfer in Robot Control", "Authors": ["Mohammadhossein Malmir (1)", "Josip Josifovski (1)", "Noah Klarmann (2)", "Alois Knoll (1) ((1) Department of Computer Engineering", "School of Computation", "Information and Technology", "Technical University of Munich", "(2) Rosenheim University of Applied Sciences)"], "Categories": "cs.RO cs.LG cs.SY eess.SY", "Comments": ["Submitted to the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)"]}, "abstract": "Delayed Markov decision processes fulfill the Markov property by augmenting the state space of agents with a finite time window of recently committed actions. In reliance with these state augmentations, delay-resolved reinforcement learning algorithms train policies to learn optimal interactions with environments featured with observation or action delays. Although such methods can directly be trained on the real robots, due to sample inefficiency, limited resources or safety constraints, a common approach is to transfer models trained in simulation to the physical robot. However, robotic simulations rely on approximated models of the physical systems, which hinders the sim2real transfer. In this work, we consider various uncertainties in the modelling of the robot's dynamics as unknown intrinsic disturbances applied on the system input. We introduce a disturbance-augmented Markov decision process in delayed settings as a novel representation to incorporate disturbance estimation in training on-policy reinforcement learning algorithms. The proposed method is validated across several metrics on learning a robotic reaching task and compared with disturbance-unaware baselines. The results show that the disturbance-augmented models can achieve higher stabilization and robustness in the control response, which in turn improves the prospects of successful sim2real transfer.", "url": "https://arxiv.org/abs/2306.09010"}, {"metadata": {"arXiv": "2306.09273", "Date": "Thu, 15 Jun 2023 16:53:26 ", "Title": "Your Room is not Private: Gradient Inversion Attack for Deep Q-Learning", "Authors": ["Miao Li", "Wenhao Ding", "Ding Zhao"], "Categories": "cs.RO cs.CR cs.CV cs.LG", "Comments": ["15 pages", "9 figures"]}, "abstract": "The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot access substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly in relation to decision-making algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the Deep Q-Learning algorithm, utilizing gradient inversion to reconstruct states, actions, and Q-values. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or transmitting the data to public servers. Nevertheless, these gradients contain sufficient information to potentially expose private data. To validate our approach, we conduct experiments on the AI2THOR simulator and evaluate our algorithm on active perception, a prevalent task in embodied AI. The experimental results convincingly demonstrate the effectiveness of our method in successfully recovering all information from the data across all 120 room layouts.", "url": "https://arxiv.org/abs/2306.09273"}, {"metadata": {"arXiv": "2306.08938", "Date": "Thu, 15 Jun 2023 08:21:41 ", "Title": "Scalable Resource Management for Dynamic MEC: An Unsupervised Link-Output Graph Neural Network Approach", "Authors": ["Xiucheng Wang and Nan Cheng and Lianhao Fu and Wei Quan and Ruijin Sun and Yilong Hui and Tom Luan and Xuemin (Sherman) Shen"], "Categories": "eess.SY cs.LG cs.SY"}, "abstract": "Deep learning has been successfully adopted in mobile edge computing (MEC) to optimize task offloading and resource allocation. However, the dynamics of edge networks raise two challenges in neural network (NN)-based optimization methods: low scalability and high training costs. Although conventional node-output graph neural networks (GNN) can extract features of edge nodes when the network scales, they fail to handle a new scalability issue whereas the dimension of the decision space may change as the network scales. To address the issue, in this paper, a novel link-output GNN (LOGNN)-based resource management approach is proposed to flexibly optimize the resource allocation in MEC for an arbitrary number of edge nodes with extremely low algorithm inference delay. Moreover, a label-free unsupervised method is applied to train the LOGNN efficiently, where the gradient of edge tasks processing delay with respect to the LOGNN parameters is derived explicitly. In addition, a theoretical analysis of the scalability of the node-output GNN and link-output GNN is performed. Simulation results show that the proposed LOGNN can efficiently optimize the MEC resource allocation problem in a scalable way, with an arbitrary number of servers and users. In addition, the proposed unsupervised training method has better convergence performance and speed than supervised learning and reinforcement learning-based training methods. The code is available at \\url{https://github.com/UNIC-Lab/LOGNN}.", "url": "https://arxiv.org/abs/2306.08938"}, {"metadata": {"arXiv": "2306.08000 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 06:26:54 ", "Title": "Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models", "Authors": ["Aakash Mishra", "Rajat Mittal", "Christy Jestin", "Kostas Tingos", "Pranav Rajpurkar"], "Categories": "physics.med-ph cs.CL cs.CV cs.LG eess.IV", "Comments": ["3 pages", "1 table", "Medical Imaging with Deep Learning", "Short Paper"], "Report-no": "Short-Paper-120"}, "abstract": "Recent advances in zero-shot learning have enabled the use of paired image-text data to replace structured labels, replacing the need for expert annotated datasets. Models such as CLIP-based CheXzero utilize these advancements in the domain of chest X-ray interpretation. We hypothesize that domain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer the potential to improve the performance of CLIP-like models with specific domain knowledge by replacing BERT weights at the cost of breaking the original model's alignment. We evaluate the performance of zero-shot classification models with domain-specific pre-training for detecting low-prevalence pathologies. Even though replacing the weights of the original CLIP-BERT degrades model performance on commonly found pathologies, we show that pre-trained text towers perform exceptionally better on low-prevalence diseases. This motivates future ensemble models with a combination of differently trained language models for maximal performance.", "url": "https://arxiv.org/abs/2306.08000"}, {"metadata": {"arXiv": "2306.08102 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 19:46:40 ", "Title": "Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise Reduction", "Authors": ["Deborah Pereg"], "Categories": "eess.IV cs.CV cs.LG"}, "abstract": "Speckle noise has long been an extensively studied problem in medical imaging. In recent years, there have been significant advances in leveraging deep learning methods for noise reduction. Nevertheless, adaptation of supervised learning models to unseen domains remains a challenging problem. Specifically, deep neural networks (DNNs) trained for computational imaging tasks are vulnerable to changes in the acquisition system's physical parameters, such as: sampling space, resolution, and contrast. Even within the same acquisition system, performance degrades across datasets of different biological tissues. In this work, we propose a few-shot supervised learning framework for optical coherence tomography (OCT) noise reduction, that offers a dramatic increase in training speed and requires only a single image, or part of an image, and a corresponding speckle suppressed ground truth, for training. Furthermore, we formulate the domain shift problem for OCT diverse imaging systems, and prove that the output resolution of a despeckling trained model is determined by the source domain resolution. We also provide possible remedies. We propose different practical implementations of our approach, verify and compare their applicability, robustness, and computational efficiency. Our results demonstrate significant potential for generally improving sample complexity, generalization, and time efficiency, for coherent and non-coherent noise reduction via supervised learning models, that can also be leveraged for other real-time computer vision applications.", "url": "https://arxiv.org/abs/2306.08102"}, {"metadata": {"arXiv": "2306.08128 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 20:49:02 ", "Title": "Self-supervised Deep Hyperspectral Inpainting with the Sparsity and Low-Rank Considerations", "Authors": ["Shuo Li", "Mehrdad Yaghoobi"], "Categories": "eess.IV cs.CV cs.LG", "Comments": ["10 pages including appendix", "5 figures 2 tables. submitted to IEEE transactions on Signal Processing journal"]}, "abstract": "Hyperspectral images are typically composed of hundreds of narrow and contiguous spectral bands, each containing information about the material composition of the imaged scene. However, these images can be affected by various sources of noise, distortions, or data losses, which can significantly degrade their quality and usefulness. To address these problems, we introduce two novel self-supervised Hyperspectral Images (HSI) inpainting algorithms: Low Rank and Sparsity Constraint Plug-and-Play (LRS-PnP), and its extension LRS-PnP-DIP, which features the strong learning capability, but is still free of external training data. We conduct the stability analysis under some mild assumptions which guarantees the algorithm to converge. It is specifically very helpful for the practical applications. Extensive experiments demonstrate that the proposed solution is able to produce visually and qualitatively superior inpainting results, achieving state-of-the-art performance. The code for reproducing the results is available at \\url{https://github.com/shuoli0708/LRS-PnP-DIP}.", "url": "https://arxiv.org/abs/2306.08128"}, {"metadata": {"arXiv": "2306.08167 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 22:44:53 ", "Title": "Where Does My Model Underperform? A Human Evaluation of Slice Discovery Algorithms", "Authors": ["Nari Johnson", "\\'Angel Alexander Cabrera", "Gregory Plumb", "Ameet Talwalkar"], "Categories": "cs.HC cs.CV cs.LG", "Comments": ["preprint", "10 pages (excluding Appendix)"]}, "abstract": "Machine learning (ML) models that achieve high average accuracy can still underperform on semantically coherent subsets (i.e. \"slices\") of data. This behavior can have significant societal consequences for the safety or bias of the model in deployment, but identifying these underperforming slices can be difficult in practice, especially in domains where practitioners lack access to group annotations to define coherent subsets of their data. Motivated by these challenges, ML researchers have developed new slice discovery algorithms that aim to group together coherent and high-error subsets of data. However, there has been little evaluation focused on whether these tools help humans form correct hypotheses about where (for which groups) their model underperforms. We conduct a controlled user study (N = 15) where we show 40 slices output by two state-of-the-art slice discovery algorithms to users, and ask them to form hypotheses about where an object detection model underperforms. Our results provide positive evidence that these tools provide some benefit over a naive baseline, and also shed light on challenges faced by users during the hypothesis formation step. We conclude by discussing design opportunities for ML and HCI researchers. Our findings point to the importance of centering users when designing and evaluating new tools for slice discovery.", "url": "https://arxiv.org/abs/2306.08167"}, {"metadata": {"arXiv": "2306.08198 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 01:53:17 ", "Title": "Explainable and Position-Aware Learning in Digital Pathology", "Authors": ["Milan Aryal and Nasim Yahyasoltani"], "Categories": "eess.IV cs.CV cs.LG"}, "abstract": "Encoding whole slide images (WSI) as graphs is well motivated since it makes it possible for the gigapixel resolution WSI to be represented in its entirety for the purpose of graph learning. To this end, WSIs can be broken into smaller patches that represent the nodes of the graph. Then, graph-based learning methods can be utilized for the grading and classification of cancer. Message passing among neighboring nodes is the foundation of graph-based learning methods. However, they do not take into consideration any positional information for any of the patches, and if two patches are found in topologically isomorphic neighborhoods, their embeddings are nearly similar to one another. In this work, classification of cancer from WSIs is performed with positional embedding and graph attention. In order to represent the positional embedding of the nodes in graph classification, the proposed method makes use of spline convolutional neural networks (CNN). The algorithm is then tested with the WSI dataset for grading prostate cancer and kidney cancer. A comparison of the proposed method with leading approaches in cancer diagnosis and grading verify improved performance. The identification of cancerous regions in WSIs is another critical task in cancer diagnosis. In this work, the explainability of the proposed model is also addressed. A gradient-based explainbility approach is used to generate the saliency mapping for the WSIs. This can be used to look into regions of WSI that are responsible for cancer diagnosis thus rendering the proposed model explainable.", "url": "https://arxiv.org/abs/2306.08198"}, {"metadata": {"arXiv": "2306.08303 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 07:19:13 ", "Title": "Pedestrian Recognition with Radar Data-Enhanced Deep Learning Approach Based on Micro-Doppler Signatures", "Authors": ["Haoming Li", "Yu Xiang", "Haodong Xu", "Wenyong Wang"], "Categories": "eess.SP cs.CV cs.LG", "Comments": ["6 pages,17 figures"]}, "abstract": "As a hot topic in recent years, the ability of pedestrians identification based on radar micro-Doppler signatures is limited by the lack of adequate training data. In this paper, we propose a data-enhanced multi-characteristic learning (DEMCL) model with data enhancement (DE) module and multi-characteristic learning (MCL) module to learn more complementary pedestrian micro-Doppler (m-D) signatures. In DE module, a range-Doppler generative adversarial network (RDGAN) is proposed to enhance free walking datasets, and MCL module with multi-scale convolution neural network (MCNN) and radial basis function neural network (RBFNN) is trained to learn m-D signatures extracted from enhanced datasets. Experimental results show that our model is 3.33% to 10.24% more accurate than other studies and has a short run time of 0.9324 seconds on a 25-minute walking dataset.", "url": "https://arxiv.org/abs/2306.08303"}, {"metadata": {"arXiv": "2306.08955 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 08:48:14 ", "Title": "A Comparison of Self-Supervised Pretraining Approaches for Predicting Disease Risk from Chest Radiograph Images", "Authors": ["Yanru Chen", "Michael T Lu", "Vineet K Raghu"], "Categories": "eess.IV cs.CV cs.LG", "Comments": ["33 pages", "22 figures", "Accepted for publication at MIDL 2023"]}, "abstract": "Deep learning is the state-of-the-art for medical imaging tasks, but requires large, labeled datasets. For risk prediction, large datasets are rare since they require both imaging and follow-up (e.g., diagnosis codes). However, the release of publicly available imaging data with diagnostic labels presents an opportunity for self and semi-supervised approaches to improve label efficiency for risk prediction. Though several studies have compared self-supervised approaches in natural image classification, object detection, and medical image interpretation, there is limited data on which approaches learn robust representations for risk prediction. We present a comparison of semi- and self-supervised learning to predict mortality risk using chest x-ray images. We find that a semi-supervised autoencoder outperforms contrastive and transfer learning in internal and external validation.", "url": "https://arxiv.org/abs/2306.08955"}, {"metadata": {"arXiv": "2306.09335 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 17:59:02 ", "Title": "Class-Conditional Conformal Prediction With Many Classes", "Authors": ["Tiffany Ding", "Anastasios N. Angelopoulos", "Stephen Bates", "Michael I. Jordan", "Ryan J. Tibshirani"], "Categories": "stat.ML cs.CV cs.LG stat.ME"}, "abstract": "Standard conformal prediction methods provide a marginal coverage guarantee, which means that for a random test point, the conformal prediction set contains the true label with a user-chosen probability. In many classification problems, we would like to obtain a stronger guarantee -- that for test points of a specific class, the prediction set contains the true label with the same user-chosen probability. Existing conformal prediction methods do not work well when there is a limited amount of labeled data per class, as is often the case in real applications where the number of classes is large. We propose a method called clustered conformal prediction, which clusters together classes that have \"similar\" conformal scores and then performs conformal prediction at the cluster level. Based on empirical evaluation across four image data sets with many (up to 1000) classes, we find that clustered conformal typically outperforms existing methods in terms of class-conditional coverage and set size metrics.", "url": "https://arxiv.org/abs/2306.09335"}, {"metadata": {"arXiv": "2306.08785 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 23:43:18 ", "Title": "Density-Aware Reinforcement Learning to Optimise Energy Efficiency in UAV-Assisted Networks", "Authors": ["Babatunji Omoniwa", "Boris Galkin", "Ivana Dusparic"], "Categories": "cs.NI cs.DC cs.LG cs.MA", "Comments": ["7 pages", "To appear in the conference proceedings of IEEE WiMob 2023", "Montreal", "Canada"]}, "abstract": "Unmanned aerial vehicles (UAVs) serving as aerial base stations can be deployed to provide wireless connectivity to mobile users, such as vehicles. However, the density of vehicles on roads often varies spatially and temporally primarily due to mobility and traffic situations in a geographical area, making it difficult to provide ubiquitous service. Moreover, as energy-constrained UAVs hover in the sky while serving mobile users, they may be faced with interference from nearby UAV cells or other access points sharing the same frequency band, thereby impacting the system's energy efficiency (EE). Recent multi-agent reinforcement learning (MARL) approaches applied to optimise the users' coverage worked well in reasonably even densities but might not perform as well in uneven users' distribution, i.e., in urban road networks with uneven concentration of vehicles. In this work, we propose a density-aware communication-enabled multi-agent decentralised double deep Q-network (DACEMAD-DDQN) approach that maximises the total system's EE by jointly optimising the trajectory of each UAV, the number of connected users, and the UAVs' energy consumption while keeping track of dense and uneven users' distribution. Our result outperforms state-of-the-art MARL approaches in terms of EE by as much as 65% - 85%.", "url": "https://arxiv.org/abs/2306.08785"}, {"metadata": {"arXiv": "2306.07972 (*cross-listing*)", "Date": "Wed, 17 May 2023 15:48:21 ", "Title": "Leveraging Machine Learning for Multichain DeFi Fraud Detection", "Authors": ["Georgios Palaiokrassas and Sandro Scherrers and Iason Ofeidis and Leandros Tassiulas"], "Categories": "q-fin.GN cs.CR cs.LG"}, "abstract": "Since the inception of permissionless blockchains with Bitcoin in 2008, it became apparent that their most well-suited use case is related to making the financial system and its advantages available to everyone seamlessly without depending on any trusted intermediaries. Smart contracts across chains provide an ecosystem of decentralized finance (DeFi), where users can interact with lending pools, Automated Market Maker (AMM) exchanges, stablecoins, derivatives, etc. with a cumulative locked value which had exceeded 160B USD. While DeFi comes with high rewards, it also carries plenty of risks. Many financial crimes have occurred over the years making the early detection of malicious activity an issue of high priority. The proposed framework introduces an effective method for extracting a set of features from different chains, including the largest one, Ethereum and it is evaluated over an extensive dataset we gathered with the transactions of the most widely used DeFi protocols (23 in total, including Aave, Compound, Curve, Lido, and Yearn) based on a novel dataset in collaboration with Covalent. Different Machine Learning methods were employed, such as XGBoost and a Neural Network for identifying fraud accounts detection interacting with DeFi and we demonstrate that the introduction of novel DeFi-related features, significantly improves the evaluation results, where Accuracy, Precision, Recall, F1-score and F2-score where utilized.", "url": "https://arxiv.org/abs/2306.07972"}, {"metadata": {"arXiv": "2306.07973 (*cross-listing*)", "Date": "Wed, 17 May 2023 20:15:26 ", "Title": "PrivaScissors: Enhance the Privacy of Collaborative Inference through the Lens of Mutual Information", "Authors": ["Lin Duan", "Jingwei Sun", "Yiran Chen", "Maria Gorlatova"], "Categories": "cs.CR cs.LG"}, "abstract": "Edge-cloud collaborative inference empowers resource-limited IoT devices to support deep learning applications without disclosing their raw data to the cloud server, thus preserving privacy. Nevertheless, prior research has shown that collaborative inference still results in the exposure of data and predictions from edge devices. To enhance the privacy of collaborative inference, we introduce a defense strategy called PrivaScissors, which is designed to reduce the mutual information between a model's intermediate outcomes and the device's data and predictions. We evaluate PrivaScissors's performance on several datasets in the context of diverse attacks and offer a theoretical robustness guarantee.", "url": "https://arxiv.org/abs/2306.07973"}, {"metadata": {"arXiv": "2306.07974 (*cross-listing*)", "Date": "Thu, 18 May 2023 21:16:59 ", "Title": "Chainlet Orbits: Topological Address Embedding for the Bitcoin Blockchain", "Authors": ["Poupak Azad", "Baris Coskunuzer", "Murat Kantarcioglu", "Cuneyt Gurcan Akcora"], "Categories": "cs.CR cs.LG"}, "abstract": "The rise of cryptocurrencies like Bitcoin, which enable transactions with a degree of pseudonymity, has led to a surge in various illicit activities, including ransomware payments and transactions on darknet markets. These illegal activities often utilize Bitcoin as the preferred payment method. However, current tools for detecting illicit behavior either rely on a few heuristics and laborious data collection processes or employ computationally inefficient graph neural network (GNN) models that are challenging to interpret. To overcome the computational and interpretability limitations of existing techniques, we introduce an effective solution called Chainlet Orbits. This approach embeds Bitcoin addresses by leveraging their topological characteristics in transactions. By employing our innovative address embedding, we investigate e-crime in Bitcoin networks by focusing on distinctive substructures that arise from illicit behavior. The results of our node classification experiments demonstrate superior performance compared to state-of-the-art methods, including both topological and GNN-based approaches. Moreover, our approach enables the use of interpretable and explainable machine learning models in as little as 15 minutes for most days on the Bitcoin transaction network.", "url": "https://arxiv.org/abs/2306.07974"}, {"metadata": {"arXiv": "2306.07981 (*cross-listing*)", "Date": "Thu, 01 Jun 2023 01:44:49 ", "Title": "Feature Engineering-Based Detection of Buffer Overflow Vulnerability in Source Code Using Neural Networks", "Authors": ["Mst Shapna Akter", "Hossain Shahriar", "Juan Rodriguez Cardenas", "Sheikh Iqbal Ahamed", "and Alfredo Cuzzocrea"], "Categories": "cs.CR cs.LG cs.SE"}, "abstract": "One of the most significant challenges in the field of software code auditing is the presence of vulnerabilities in software source code. Every year, more and more software flaws are discovered, either internally in proprietary code or publicly disclosed. These flaws are highly likely to be exploited and can lead to system compromise, data leakage, or denial of service. To create a large-scale machine learning system for function level vulnerability identification, we utilized a sizable dataset of C and C++ open-source code containing millions of functions with potential buffer overflow exploits. We have developed an efficient and scalable vulnerability detection method based on neural network models that learn features extracted from the source codes. The source code is first converted into an intermediate representation to remove unnecessary components and shorten dependencies. We maintain the semantic and syntactic information using state of the art word embedding algorithms such as GloVe and fastText. The embedded vectors are subsequently fed into neural networks such as LSTM, BiLSTM, LSTM Autoencoder, word2vec, BERT, and GPT2 to classify the possible vulnerabilities. We maintain the semantic and syntactic information using state of the art word embedding algorithms such as GloVe and fastText. The embedded vectors are subsequently fed into neural networks such as LSTM, BiLSTM, LSTM Autoencoder, word2vec, BERT, and GPT2 to classify the possible vulnerabilities. Furthermore, we have proposed a neural network model that can overcome issues associated with traditional neural networks. We have used evaluation metrics such as F1 score, precision, recall, accuracy, and total execution time to measure the performance. We have conducted a comparative analysis between results derived from features containing a minimal text representation and semantic and syntactic information.", "url": "https://arxiv.org/abs/2306.07981"}, {"metadata": {"arXiv": "2306.07989 (*cross-listing*)", "Date": "Fri, 09 Jun 2023 19:01:32 ", "Title": "A Survey on Cross-Architectural IoT Malware Threat Hunting", "Authors": ["Anandharaju Durai Raju", "Ibrahim Abualhaol", "Ronnie Salvador Giagone", "Yang Zhou", "and Shengqiang Huang"], "Categories": "cs.CR cs.LG", "Comments": ["https://ieeexplore.ieee.org/abstract/document/9462110"], "Journal-ref": "IEEE Access 2021", "DOI": "10.1109/ACCESS.2021.3091427"}, "abstract": "In recent years, the increase in non-Windows malware threats had turned the focus of the cybersecurity community. Research works on hunting Windows PE-based malwares are maturing, whereas the developments on Linux malware threat hunting are relatively scarce. With the advent of the Internet of Things (IoT) era, smart devices that are getting integrated into human life have become a hackers highway for their malicious activities. The IoT devices employ various Unix-based architectures that follow ELF (Executable and Linkable Format) as their standard binary file specification. This study aims at providing a comprehensive survey on the latest developments in cross-architectural IoT malware detection and classification approaches. Aided by a modern taxonomy, we discuss the feature representations, feature extraction techniques, and machine learning models employed in the surveyed works. We further provide more insights on the practical challenges involved in cross-architectural IoT malware threat hunting and discuss various avenues to instill potential future research.", "url": "https://arxiv.org/abs/2306.07989"}, {"metadata": {"arXiv": "2306.07997 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 19:04:07 ", "Title": "Machine Learning Approach on Multiclass Classification of Internet Firewall Log Files", "Authors": ["Md Habibur Rahman", "Taminul Islam", "Md Masum Rana", "Rehnuma Tasnim", "Tanzina Rahman Mona", "Md. Mamun Sakib"], "Categories": "cs.CR cs.LG cs.NI", "Comments": ["Accepted and presented in International Conference on Computational Intelligence and Sustainable Engineering (CISES-2023)", "2022", "7 pages", "13 figures"]}, "abstract": "Firewalls are critical components in securing communication networks by screening all incoming (and occasionally exiting) data packets. Filtering is carried out by comparing incoming data packets to a set of rules designed to prevent malicious code from entering the network. To regulate the flow of data packets entering and leaving a network, an Internet firewall keeps a track of all activity. While the primary function of log files is to aid in troubleshooting and diagnostics, the information they contain is also very relevant to system audits and forensics. Firewalls primary function is to prevent malicious data packets from being sent. In order to better defend against cyberattacks and understand when and how malicious actions are influencing the internet, it is necessary to examine log files. As a result, the firewall decides whether to 'allow,' 'deny,' 'drop,' or 'reset-both' the incoming and outgoing packets. In this research, we apply various categorization algorithms to make sense of data logged by a firewall device. Harmonic mean F1 score, recall, and sensitivity measurement data with a 99% accuracy score in the random forest technique are used to compare the classifier's performance. To be sure, the proposed characteristics did significantly contribute to enhancing the firewall classification rate, as seen by the high accuracy rates generated by the other methods.", "url": "https://arxiv.org/abs/2306.07997"}, {"metadata": {"arXiv": "2306.08007 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 09:07:49 ", "Title": "Leveraging dendritic properties to advance machine learning and neuro-inspired computing", "Authors": ["Michalis Pagkalos", "Roman Makarov and Panayiota Poirazi"], "Categories": "cs.NE cs.LG q-bio.NC", "Comments": ["11 pages", "2 figures"]}, "abstract": "The brain is a remarkably capable and efficient system. It can process and store huge amounts of noisy and unstructured information using minimal energy. In contrast, current artificial intelligence (AI) systems require vast resources for training while still struggling to compete in tasks that are trivial for biological agents. Thus, brain-inspired engineering has emerged as a promising new avenue for designing sustainable, next-generation AI systems. Here, we describe how dendritic mechanisms of biological neurons have inspired innovative solutions for significant AI problems, including credit assignment in multilayer networks, catastrophic forgetting, and high energy consumption. These findings provide exciting alternatives to existing architectures, showing how dendritic research can pave the way for building more powerful and energy-efficient artificial learning systems.", "url": "https://arxiv.org/abs/2306.08007"}, {"metadata": {"arXiv": "2306.08060 (*cross-listing*)", "Date": "Wed, 31 May 2023 06:06:28 ", "Title": "Software Supply Chain Vulnerabilities Detection in Source Code: Performance Comparison between Traditional and Quantum Machine Learning Algorithms", "Authors": ["Mst Shapna Akter", "Md Jobair Hossain Faruk", "Nafisa Anjum", "Mohammad Masum", "Hossain Shahriar", "Akond Rahman", "Fan Wu", "Alfredo Cuzzocrea"], "Categories": "cs.CR cs.LG quant-ph", "DOI": "10.1109/BigData55660.2022.10020813"}, "abstract": "The software supply chain (SSC) attack has become one of the crucial issues that are being increased rapidly with the advancement of the software development domain. In general, SSC attacks execute during the software development processes lead to vulnerabilities in software products targeting downstream customers and even involved stakeholders. Machine Learning approaches are proven in detecting and preventing software security vulnerabilities. Besides, emerging quantum machine learning can be promising in addressing SSC attacks. Considering the distinction between traditional and quantum machine learning, performance could be varies based on the proportions of the experimenting dataset. In this paper, we conduct a comparative analysis between quantum neural networks (QNN) and conventional neural networks (NN) with a software supply chain attack dataset known as ClaMP. Our goal is to distinguish the performance between QNN and NN and to conduct the experiment, we develop two different models for QNN and NN by utilizing Pennylane for quantum and TensorFlow and Keras for traditional respectively. We evaluated the performance of both models with different proportions of the ClaMP dataset to identify the f1 score, recall, precision, and accuracy. We also measure the execution time to check the efficiency of both models. The demonstration result indicates that execution time for QNN is slower than NN with a higher percentage of datasets. Due to recent advancements in QNN, a large level of experiments shall be carried out to understand both models accurately in our future research.", "url": "https://arxiv.org/abs/2306.08060"}, {"metadata": {"arXiv": "2306.08086 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 19:07:14 ", "Title": "Safe Use of Neural Networks", "Authors": ["George Redinbo"], "Categories": "eess.SP cs.LG", "Comments": ["13 pages"]}, "abstract": "Neural networks in modern communication systems can be susceptible to internal numerical errors that can drastically effect decision results. Such structures are composed of many sections each of which generally contain weighting operations and activation function evaluations. The safe use comes from methods employing number based codes that can detect arithmetic errors in the network's processing steps. Each set of operations generates parity values dictated by a code in two ways. One set of parities is obtained from a section's outputs while a second comparable set is developed directly from the original inputs. The parity values protecting the activation functions involve a Taylor series approximation to the activation functions. We focus on using long numerically based convolutional codes because of the large size of data sets. The codes are based on Discrete Fourier Transform kernels and there are many design options available. Mathematical program simulations show our error-detecting techniques are effective and efficient.", "url": "https://arxiv.org/abs/2306.08086"}, {"metadata": {"arXiv": "2306.08105 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 19:50:03 ", "Title": "Model-Free Market Risk Hedging Using Crowding Networks", "Authors": ["Vadim Zlotnikov", "Jiayu Liu", "Igor Halperin", "Fei He", "Lisa Huang"], "Categories": "q-fin.PM cs.LG q-fin.GN q-fin.RM", "Comments": ["8 pages", "6 figures"]}, "abstract": "Crowding is widely regarded as one of the most important risk factors in designing portfolio strategies. In this paper, we analyze stock crowding using network analysis of fund holdings, which is used to compute crowding scores for stocks. These scores are used to construct costless long-short portfolios, computed in a distribution-free (model-free) way and without using any numerical optimization, with desirable properties of hedge portfolios. More specifically, these long-short portfolios provide protection for both small and large market price fluctuations, due to their negative correlation with the market and positive convexity as a function of market returns. By adding our long-short portfolio to a baseline portfolio such as a traditional 60/40 portfolio, our method provides an alternative way to hedge portfolio risk including tail risk, which does not require costly option-based strategies or complex numerical optimization. The total cost of such hedging amounts to the total cost of rebalancing the hedge portfolio.", "url": "https://arxiv.org/abs/2306.08105"}, {"metadata": {"arXiv": "2306.08116 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 20:18:24 ", "Title": "CipherSniffer: Classifying Cipher Types", "Authors": ["Brendan Artley", "Greg Mehdiyev"], "Categories": "cs.CL cs.CR cs.LG"}, "abstract": "Ciphers are a powerful tool for encrypting communication. There are many different cipher types, which makes it computationally expensive to solve a cipher using brute force. In this paper, we frame the decryption task as a classification problem. We first create a dataset of transpositions, substitutions, text reversals, word reversals, sentence shifts, and unencrypted text. Then, we evaluate the performance of various tokenizer-model combinations on this task.", "url": "https://arxiv.org/abs/2306.08116"}, {"metadata": {"arXiv": "2306.08121 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 20:34:15 ", "Title": "Better Generalization with Semantic IDs: A case study in Ranking for Recommendations", "Authors": ["Anima Singh", "Trung Vu", "Raghunandan Keshavan", "Nikhil Mehta", "Xinyang Yi", "Lichan Hong", "Lukasz Heldt", "Li Wei", "Ed Chi", "Maheswaran Sathiamoorthy"], "Categories": "cs.IR cs.LG"}, "abstract": "Training good representations for items is critical in recommender models. Typically, an item is assigned a unique randomly generated ID, and is commonly represented by learning an embedding corresponding to the value of the random ID. Although widely used, this approach have limitations when the number of items are large and items are power-law distributed -- typical characteristics of real-world recommendation systems. This leads to the item cold-start problem, where the model is unable to make reliable inferences for tail and previously unseen items. Removing these ID features and their learned embeddings altogether to combat cold-start issue severely degrades the recommendation quality. Content-based item embeddings are more reliable, but they are expensive to store and use, particularly for users' past item interaction sequence. In this paper, we use Semantic IDs, a compact discrete item representations learned from content embeddings using RQ-VAE that captures hierarchy of concepts in items. We showcase how we use them as a replacement of item IDs in a resource-constrained ranking model used in an industrial-scale video sharing platform. Moreover, we show how Semantic IDs improves the generalization ability of our system, without sacrificing top-level metrics.", "url": "https://arxiv.org/abs/2306.08121"}, {"metadata": {"arXiv": "2306.08125 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 20:37:02 ", "Title": "Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD", "Authors": ["Yijun Wan", "Abdellatif Zaidi", "Umut Simsekli"], "Categories": "stat.ML cs.LG math.PR", "Comments": ["28 pages", "1 figure"]}, "abstract": "Neural network compression has been an increasingly important subject, due to its practical implications in terms of reducing the computational requirements and its theoretical implications, as there is an explicit connection between compressibility and the generalization error. Recent studies have shown that the choice of the hyperparameters of stochastic gradient descent (SGD) can have an effect on the compressibility of the learned parameter vector. Even though these results have shed some light on the role of the training dynamics over compressibility, they relied on unverifiable assumptions and the resulting theory does not provide a practical guideline due to its implicitness. In this study, we propose a simple modification for SGD, such that the outputs of the algorithm will be provably compressible without making any nontrivial assumptions. We consider a one-hidden-layer neural network trained with SGD and we inject additive heavy-tailed noise to the iterates at each iteration. We then show that, for any compression rate, there exists a level of overparametrization (i.e., the number of hidden units), such that the output of the algorithm will be compressible with high probability. To achieve this result, we make two main technical contributions: (i) we build on a recent study on stochastic analysis and prove a 'propagation of chaos' result with improved rates for a class of heavy-tailed stochastic differential equations, and (ii) we derive strong-error estimates for their Euler discretization. We finally illustrate our approach on experiments, where the results suggest that the proposed approach achieves compressibility with a slight compromise from the training and test error.", "url": "https://arxiv.org/abs/2306.08125"}, {"metadata": {"arXiv": "2306.08188 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 01:15:55 ", "Title": "Contextual Font Recommendations based on User Intent", "Authors": ["Sanat Sharma", "Jayant Kumar", "Jing Zheng", "Tracy Holloway King"], "Categories": "cs.HC cs.IR cs.LG", "Comments": ["In Proceedings of ACM SIGIR Workshop on eCommerce (SIGIR eCom'23)"]}, "abstract": "Adobe Fonts has a rich library of over 20,000 unique fonts that Adobe users utilize for creating graphics, posters, composites etc. Due to the nature of the large library, knowing what font to select can be a daunting task that requires a lot of experience. For most users in Adobe products, especially casual users of Adobe Express, this often means choosing the default font instead of utilizing the rich and diverse fonts available. In this work, we create an intent-driven system to provide contextual font recommendations to users to aid in their creative journey. Our system takes in multilingual text input and recommends suitable fonts based on the user's intent. Based on user entitlements, the mix of free and paid fonts is adjusted. The feature is currently used by millions of Adobe Express users with a CTR of >25%.", "url": "https://arxiv.org/abs/2306.08188"}, {"metadata": {"arXiv": "2306.08256 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 05:44:53 ", "Title": "Data Augmentation for Seizure Prediction with Generative Diffusion Model", "Authors": ["Kai Shu", "Yuchang Zhao", "Le Wu", "Aiping Liu", "Ruobing Qian", "and Xun Chen"], "Categories": "eess.SP cs.LG", "Comments": ["12 pages", "6 figures"]}, "abstract": "Objective: Seizure prediction is of great importance to improve the life of patients. The focal point is to distinguish preictal states from interictal ones. With the development of machine learning, seizure prediction methods have achieved significant progress. However, the severe imbalance problem between preictal and interictal data still poses a great challenge, restricting the performance of classifiers. Data augmentation is an intuitive way to solve this problem. Existing data augmentation methods generate samples by overlapping or recombining data. The distribution of generated samples is limited by original data, because such transformations cannot fully explore the feature space and offer new information. As the epileptic EEG representation varies among seizures, these generated samples cannot provide enough diversity to achieve high performance on a new seizure. As a consequence, we propose a novel data augmentation method with diffusion model called DiffEEG. Methods: Diffusion models are a class of generative models that consist of two processes. Specifically, in the diffusion process, the model adds noise to the input EEG sample step by step and converts the noisy sample into output random noise, exploring the distribution of data by minimizing the loss between the output and the noise added. In the denoised process, the model samples the synthetic data by removing the noise gradually, diffusing the data distribution to outward areas and narrowing the distance between different clusters. Results: We compared DiffEEG with existing methods, and integrated them into three representative classifiers. The experiments indicate that DiffEEG could further improve the performance and shows superiority to existing methods. Conclusion: This paper proposes a novel and effective method to solve the imbalanced problem and demonstrates the effectiveness and generality of our method.", "url": "https://arxiv.org/abs/2306.08256"}, {"metadata": {"arXiv": "2306.08270 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 06:13:50 ", "Title": "Imagery Tracking of Sun Activity Using 2D Circular Kernel Time Series Transformation, Entropy Measures and Machine Learning Approaches", "Authors": ["Irewola Aaron Oludehinwa", "Andrei Velichko", "Maksim Belyaev and Olasunkanmi I. Olusola"], "Categories": "astro-ph.SR astro-ph.IM cs.LG", "Comments": ["21 pages", "10 figures", "3 tables"]}, "abstract": "The sun is highly complex in nature and its observatory imagery features is one of the most important sources of information about the sun activity, space and Earth's weather conditions. The NASA, solar Dynamics Observatory captures approximately 70,000 images of the sun activity in a day and the continuous visual inspection of this solar observatory images is challenging. In this study, we developed a technique of tracking the sun's activity using 2D circular kernel time series transformation, statistical and entropy measures, with machine learning approaches. The technique involves transforming the solar observatory image section into 1-Dimensional time series (1-DTS) while the statistical and entropy measures (Approach 1) and direct classification (Approach 2) is used to capture the extraction features from the 1-DTS for machine learning classification into 'solar storm' and 'no storm'. We found that the potential accuracy of the model in tracking the activity of the sun is approximately 0.981 for Approach 1 and 0.999 for Approach 2. The stability of the developed approach to rotational transformation of the solar observatory image is evident. When training on the original dataset for Approach 1, the match index (T90) of the distribution of solar storm areas reaches T90 ~ 0.993, and T90 ~ 0.951 for Approach 2. In addition, when using the extended training base, the match indices increased to T90 ~ 0.994 and T90 ~ 1, respectively. This model consistently classifies areas with swirling magnetic lines associated with solar storms and is robust to image rotation, glare, and optical artifacts.", "url": "https://arxiv.org/abs/2306.08270"}, {"metadata": {"arXiv": "2306.08280 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 06:35:10 ", "Title": "Differentially Private Wireless Federated Learning Using Orthogonal Sequences", "Authors": ["Xizixiang Wei", "Tianhao Wang", "Ruiquan Huang", "Cong Shen", "Jing Yang", "H. Vincent Poor"], "Categories": "cs.IT cs.CR cs.LG eess.SP math.IT stat.ML", "Comments": ["33 pages", "5 figures", "submitted to IEEE TSP"]}, "abstract": "We propose a novel privacy-preserving uplink over-the-air computation (AirComp) method, termed FLORAS, for single-input single-output (SISO) wireless federated learning (FL) systems. From the communication design perspective, FLORAS eliminates the requirement of channel state information at the transmitters (CSIT) by leveraging the properties of orthogonal sequences. From the privacy perspective, we prove that FLORAS can offer both item-level and client-level differential privacy (DP) guarantees. Moreover, by adjusting the system parameters, FLORAS can flexibly achieve different DP levels at no additional cost. A novel FL convergence bound is derived which, combined with the privacy guarantees, allows for a smooth tradeoff between convergence rate and differential privacy levels. Numerical results demonstrate the advantages of FLORAS compared with the baseline AirComp method, and validate that our analytical results can guide the design of privacy-preserving FL with different tradeoff requirements on the model convergence and privacy levels.", "url": "https://arxiv.org/abs/2306.08280"}, {"metadata": {"arXiv": "2306.08306 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 07:23:36 ", "Title": "Towards Balanced Active Learning for Multimodal Classification", "Authors": ["Meng Shen", "Yizheng Huang", "Jianxiong Yin", "Heqing Zou", "Deepu Rajan", "Simon See"], "Categories": "cs.MM cs.LG", "Comments": ["11 pages"]}, "abstract": "Training multimodal networks requires a vast amount of data due to their larger parameter space compared to unimodal networks. Active learning is a widely used technique for reducing data annotation costs by selecting only those samples that could contribute to improving model performance. However, current active learning strategies are mostly designed for unimodal tasks, and when applied to multimodal data, they often result in biased sample selection from the dominant modality. This unfairness hinders balanced multimodal learning, which is crucial for achieving optimal performance. To address this issue, we propose three guidelines for designing a more balanced multimodal active learning strategy. Following these guidelines, a novel approach is proposed to achieve more fair data selection by modulating the gradient embedding with the dominance degree among modalities. Our studies demonstrate that the proposed method achieves more balanced multimodal learning by avoiding greedy sample selection from the dominant modality. Our approach outperforms existing active learning strategies on a variety of multimodal classification tasks. Overall, our work highlights the importance of balancing sample selection in multimodal active learning and provides a practical solution for achieving more balanced active learning for multimodal classification.", "url": "https://arxiv.org/abs/2306.08306"}, {"metadata": {"arXiv": "2306.08321 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 07:42:37 ", "Title": "Nonparametric regression using over-parameterized shallow ReLU neural networks", "Authors": ["Yunfei Yang", "Ding-Xuan Zhou"], "Categories": "stat.ML cs.LG math.ST stat.TH"}, "abstract": "It is shown that over-parameterized neural networks can achieve minimax optimal rates of convergence (up to logarithmic factors) for learning functions from certain smooth function classes, if the weights are suitably constrained or regularized. Specifically, we consider the nonparametric regression of estimating an unknown $d$-variate function by using shallow ReLU neural networks. It is assumed that the regression function is from the H\\\"older space with smoothness $\\alpha<(d+3)/2$ or a variation space corresponding to shallow neural networks, which can be viewed as an infinitely wide neural network. In this setting, we prove that least squares estimators based on shallow neural networks with certain norm constraints on the weights are minimax optimal, if the network width is sufficiently large. As a byproduct, we derive a new size-independent bound for the local Rademacher complexity of shallow ReLU neural networks, which may be of independent interest.", "url": "https://arxiv.org/abs/2306.08321"}, {"metadata": {"arXiv": "2306.08364 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 08:53:20 ", "Title": "Provably Efficient Offline Reinforcement Learning with Perturbed Data Sources", "Authors": ["Chengshuai Shi", "Wei Xiong", "Cong Shen", "Jing Yang"], "Categories": "stat.ML cs.IT cs.LG math.IT", "Comments": ["ICML 2023"]}, "abstract": "Existing theoretical studies on offline reinforcement learning (RL) mostly consider a dataset sampled directly from the target task. In practice, however, data often come from several heterogeneous but related sources. Motivated by this gap, this work aims at rigorously understanding offline RL with multiple datasets that are collected from randomly perturbed versions of the target task instead of from itself. An information-theoretic lower bound is derived, which reveals a necessary requirement on the number of involved sources in addition to that on the number of data samples. Then, a novel HetPEVI algorithm is proposed, which simultaneously considers the sample uncertainties from a finite number of data samples per data source and the source uncertainties due to a finite number of available data sources. Theoretical analyses demonstrate that HetPEVI can solve the target task as long as the data sources collectively provide a good data coverage. Moreover, HetPEVI is demonstrated to be optimal up to a polynomial factor of the horizon length. Finally, the study is extended to offline Markov games and offline robust RL, which demonstrates the generality of the proposed designs and theoretical analyses.", "url": "https://arxiv.org/abs/2306.08364"}, {"metadata": {"arXiv": "2306.08394 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 09:38:05 ", "Title": "Compatibility of Fairness Metrics with EU Non-Discrimination Laws: Demographic Parity & Conditional Demographic Disparity", "Authors": ["Lisa Koutsoviti Koumeri", "Magali Legast", "Yasaman Yousefi", "Koen Vanhoof", "Axel Legay", "Christoph Schommer"], "Categories": "cs.CY cs.LG", "Comments": ["Submitted at the 19th International Conference on Artificial Intelligence and Law - ICAIL 2023. Expected decision date is July 18", "2023"]}, "abstract": "Empirical evidence suggests that algorithmic decisions driven by Machine Learning (ML) techniques threaten to discriminate against legally protected groups or create new sources of unfairness. This work supports the contextual approach to fairness in EU non-discrimination legal framework and aims at assessing up to what point we can assure legal fairness through fairness metrics and under fairness constraints. For that, we analyze the legal notion of non-discrimination and differential treatment with the fairness definition Demographic Parity (DP) through Conditional Demographic Disparity (CDD). We train and compare different classifiers with fairness constraints to assess whether it is possible to reduce bias in the prediction while enabling the contextual approach to judicial interpretation practiced under EU non-discrimination laws. Our experimental results on three scenarios show that the in-processing bias mitigation algorithm leads to different performances in each of them. Our experiments and analysis suggest that AI-assisted decision-making can be fair from a legal perspective depending on the case at hand and the legal justification. These preliminary results encourage future work which will involve further case studies, metrics, and fairness notions.", "url": "https://arxiv.org/abs/2306.08394"}, {"metadata": {"arXiv": "2306.08406 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 10:03:33 ", "Title": "Feature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement", "Authors": ["Hejung Yang", "Hong-Goo Kang"], "Categories": "eess.AS cs.LG cs.SD", "Comments": ["INTERSPEECH 2023 accepted"]}, "abstract": "Large, pre-trained representation models trained using self-supervised learning have gained popularity in various fields of machine learning because they are able to extract high-quality salient features from input data. As such, they have been frequently used as base networks for various pattern classification tasks such as speech recognition. However, not much research has been conducted on applying these types of models to the field of speech signal generation. In this paper, we investigate the feasibility of using pre-trained speech representation models for a downstream speech enhancement task. To alleviate mismatches between the input features of the pre-trained model and the target enhancement model, we adopt a novel feature normalization technique to smoothly link these modules together. Our proposed method enables significant improvements in speech quality compared to baselines when combined with various types of pre-trained speech models.", "url": "https://arxiv.org/abs/2306.08406"}, {"metadata": {"arXiv": "2306.08451 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 11:51:11 ", "Title": "A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias", "Authors": ["Seyedeh Somayyeh Mousavi and Reza Sameni"], "Categories": "physics.med-ph cs.LG q-bio.QM"}, "abstract": "Blood pressure is a vital sign that offers important insights into overall health, particularly cardiovascular well-being. It plays a critical role in medical settings and homes for disease prevention, diagnosis, treatment, and management. Physicians heavily rely on blood pressure values for making crucial decisions. Most commercial devices utilize cuffs for blood pressure measurement, and automatic devices have gained popularity due to the high prevalence of hypertension. Self-measurement and home monitoring of blood pressure are also recommended. However, concerns arise regarding the accuracy of blood pressure measurement technologies and the alignment of reported values with actual values. People often adjust their medication based on these reported values, making accuracy vital. This study focuses on the concept of ``bias'' to highlight potential discrepancies between reported and actual blood pressure values. Previous research has identified biases originating from three categories: (1) blood pressure measurement devices, (2) subject-specific factors, and (3) measurement sessions. Specifically, this study examines biases associated with cuff-based blood pressure technologies due to their widespread use in medical applications and the growing trend of home monitoring. Identifying and addressing the primary sources of biases is crucial to prevent their propagation and mitigate potential consequences. Additionally, the study explores the future prospects of blood pressure monitoring using machine learning methods.", "url": "https://arxiv.org/abs/2306.08451"}, {"metadata": {"arXiv": "2306.08489 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 13:09:38 ", "Title": "Analysis and Approximate Inference of Large and Dense Random Kronecker Graphs", "Authors": ["Zhenyu Liao", "Yuanqian Xia", "Chengmei Niu", "Yong Xiao"], "Categories": "stat.ML cs.LG math.SP", "Comments": ["27 pages and 3 figures"]}, "abstract": "Random graph models are playing an increasingly important role in science and industry, and finds their applications in a variety of fields ranging from social and traffic networks, to recommendation systems and molecular genetics. In this paper, we perform an in-depth analysis of the random Kronecker graph model proposed in \\cite{leskovec2010kronecker}, when the number of graph vertices $N$ is large. Built upon recent advances in random matrix theory, we show, in the dense regime, that the random Kronecker graph adjacency matrix follows approximately a signal-plus-noise model, with a small-rank (of order at most $\\log N$) signal matrix that is linear in the graph parameters and a random noise matrix having a quarter-circle-form singular value distribution. This observation allows us to propose a ``denoise-and-solve'' meta algorithm to approximately infer the graph parameters, with reduced computational complexity and (asymptotic) performance guarantee. Numerical experiments of graph inference and graph classification on both synthetic and realistic graphs are provided to support the advantageous performance of the proposed approach.", "url": "https://arxiv.org/abs/2306.08489"}, {"metadata": {"arXiv": "2306.08494 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 13:18:09 ", "Title": "Langevin Monte Carlo for strongly log-concave distributions: Randomized midpoint revisited", "Authors": ["Lu Yu", "Avetik Karagulyan", "Arnak Dalalyan"], "Categories": "math.ST cs.LG math.PR stat.TH"}, "abstract": "We revisit the problem of sampling from a target distribution that has a smooth strongly log-concave density everywhere in $\\mathbb R^p$. In this context, if no additional density information is available, the randomized midpoint discretization for the kinetic Langevin diffusion is known to be the most scalable method in high dimensions with large condition numbers. Our main result is a nonasymptotic and easy to compute upper bound on the Wasserstein-2 error of this method. To provide a more thorough explanation of our method for establishing the computable upper bound, we conduct an analysis of the midpoint discretization for the vanilla Langevin process. This analysis helps to clarify the underlying principles and provides valuable insights that we use to establish an improved upper bound for the kinetic Langevin process with the midpoint discretization. Furthermore, by applying these techniques we establish new guarantees for the kinetic Langevin process with Euler discretization, which have a better dependence on the condition number than existing upper bounds.", "url": "https://arxiv.org/abs/2306.08494"}, {"metadata": {"arXiv": "2306.08505 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 13:41:23 ", "Title": "DiffuDetox: A Mixed Diffusion Model for Text Detoxification", "Authors": ["Griffin Floto", "Mohammad Mahdi Abdollah Pour", "Parsa Farinneya", "Zhenwei Tang", "Ali Pesaranghader", "Manasa Bharadwaj", "Scott Sanner"], "Categories": "cs.CL cs.LG", "Comments": ["7 pages", "1 figure", "ACL findings 2023"]}, "abstract": "Text detoxification is a conditional text generation task aiming to remove offensive content from toxic text. It is highly useful for online forums and social media, where offensive content is frequently encountered. Intuitively, there are diverse ways to detoxify sentences while preserving their meanings, and we can select from detoxified sentences before displaying text to users. Conditional diffusion models are particularly suitable for this task given their demonstrated higher generative diversity than existing conditional text generation models based on language models. Nonetheless, text fluency declines when they are trained with insufficient data, which is the case for this task. In this work, we propose DiffuDetox, a mixed conditional and unconditional diffusion model for text detoxification. The conditional model takes toxic text as the condition and reduces its toxicity, yielding a diverse set of detoxified sentences. The unconditional model is trained to recover the input text, which allows the introduction of additional fluent text for training and thus ensures text fluency. Extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed DiffuDetox.", "url": "https://arxiv.org/abs/2306.08505"}, {"metadata": {"arXiv": "2306.08510 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 13:53:31 ", "Title": "Permutation Invariant Recurrent Neural Networks for Sound Source Tracking Applications", "Authors": ["David Diaz-Guerra", "Archontis Politis", "Antonio Miguel", "Jose R. Beltran", "Tuomas Virtanen"], "Categories": "eess.AS cs.LG cs.SD eess.SP", "Comments": ["Accepted for publication at Forum Acusticum 2023"]}, "abstract": "Many multi-source localization and tracking models based on neural networks use one or several recurrent layers at their final stages to track the movement of the sources. Conventional recurrent neural networks (RNNs), such as the long short-term memories (LSTMs) or the gated recurrent units (GRUs), take a vector as their input and use another vector to store their state. However, this approach results in the information from all the sources being contained in a single ordered vector, which is not optimal for permutation-invariant problems such as multi-source tracking. In this paper, we present a new recurrent architecture that uses unordered sets to represent both its input and its state and that is invariant to the permutations of the input set and equivariant to the permutations of the state set. Hence, the information of every sound source is represented in an individual embedding and the new estimates are assigned to the tracked trajectories regardless of their order.", "url": "https://arxiv.org/abs/2306.08510"}, {"metadata": {"arXiv": "2306.08529 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 14:23:19 ", "Title": "SQL2Circuits: Estimating Metrics for SQL Queries with A Quantum Natural Language Processing Method", "Authors": ["Valter Uotila"], "Categories": "cs.DB cs.LG quant-ph", "Comments": ["33 pages", "15 figures", "2 tables"]}, "abstract": "Quantum computing has developed significantly in recent years. Developing algorithms to estimate various metrics for SQL queries has been an important research question in database research since the estimations affect query optimization and database performance. This work represents a quantum natural language processing (QNLP) -inspired approach for constructing a quantum machine learning model which can classify SQL queries with respect to their execution times and cardinalities. From the quantum machine learning perspective, we compare our model and results to the previous research in QNLP and conclude that our model reaches similar accuracy as the QNLP model in the classification tasks. This indicates that the QNLP model is a promising method even when applied to problems that are not in QNLP. We study the developed quantum machine learning model by calculating its expressibility and entangling capability histograms. The results show that the model has favorable properties to be expressible but also not too complex to be executed on quantum hardware.", "url": "https://arxiv.org/abs/2306.08529"}, {"metadata": {"arXiv": "2306.08538 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 14:38:25 ", "Title": "Fast and Private Inference of Deep Neural Networks by Co-designing Activation Functions", "Authors": ["Abdulrahman Diaa", "Lucas Fenaux", "Thomas Humphries", "Marian Dietz", "Faezeh Ebrahimianghazani", "Bailey Kacsmar", "Xinda Li", "Nils Lukas", "Rasoul Akhavan Mahdavi", "Simon Oya", "Ehsan Amjadian", "Florian Kerschbaum"], "Categories": "cs.CR cs.LG"}, "abstract": "Machine Learning as a Service (MLaaS) is an increasingly popular design where a company with abundant computing resources trains a deep neural network and offers query access for tasks like image classification. The challenge with this design is that MLaaS requires the client to reveal their potentially sensitive queries to the company hosting the model. Multi-party computation (MPC) protects the client's data by allowing encrypted inferences. However, current approaches suffer prohibitively large inference times. The inference time bottleneck in MPC is the evaluation of non-linear layers such as ReLU activation functions. Motivated by the success of previous work co-designing machine learning and MPC aspects, we develop an activation function co-design. We replace all ReLUs with a polynomial approximation and evaluate them with single-round MPC protocols, which give state-of-the-art inference times in wide-area networks. Furthermore, to address the accuracy issues previously encountered with polynomial activations, we propose a novel training algorithm that gives accuracy competitive with plaintext models. Our evaluation shows between $4$ and $90\\times$ speedups in inference time on large models with up to $23$ million parameters while maintaining competitive inference accuracy.", "url": "https://arxiv.org/abs/2306.08538"}, {"metadata": {"arXiv": "2306.08566 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 15:17:58 ", "Title": "Federated Learning-based Vehicle Trajectory Prediction against Cyberattacks", "Authors": ["Zhe Wang", "Tingkai Yan"], "Categories": "cs.CR cs.LG cs.SY eess.SY"}, "abstract": "With the development of the Internet of Vehicles (IoV), vehicle wireless communication poses serious cybersecurity challenges. Faulty information, such as fake vehicle positions and speeds sent by surrounding vehicles, could cause vehicle collisions, traffic jams, and even casualties. Additionally, private vehicle data leakages, such as vehicle trajectory and user account information, may damage user property and security. Therefore, achieving a cyberattack-defense scheme in the IoV system with faulty data saturation is necessary. This paper proposes a Federated Learning-based Vehicle Trajectory Prediction Algorithm against Cyberattacks (FL-TP) to address the above problems. The FL-TP is intensively trained and tested using a publicly available Vehicular Reference Misbehavior (VeReMi) dataset with five types of cyberattacks: constant, constant offset, random, random offset, and eventual stop. The results show that the proposed FL-TP algorithm can improve cyberattack detection and trajectory prediction by up to 6.99% and 54.86%, respectively, under the maximum cyberattack permeability scenarios compared with benchmark methods.", "url": "https://arxiv.org/abs/2306.08566"}, {"metadata": {"arXiv": "2306.08620 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 16:27:53 ", "Title": "Anticipatory Music Transformer", "Authors": ["John Thickstun", "David Hall", "Chris Donahue", "Percy Liang"], "Categories": "cs.SD cs.LG eess.AS stat.ML", "Comments": ["33 pages", "6 figures"]}, "abstract": "We introduce anticipation: a method for constructing a controllable generative model of a temporal point process (the event process) conditioned asynchronously on realizations of a second, correlated process (the control process). We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence. This work is motivated by problems arising in the control of symbolic music generation. We focus on infilling control tasks, whereby the controls are a subset of the events themselves, and conditional generation completes a sequence of events given the fixed control events. We train anticipatory infilling models using the large and diverse Lakh MIDI music dataset. These models match the performance of autoregressive models for prompted music generation, with the additional capability to perform infilling control tasks, including accompaniment. Human evaluators report that an anticipatory model produces accompaniments with similar musicality to even music composed by humans over a 20-second clip.", "url": "https://arxiv.org/abs/2306.08620"}, {"metadata": {"arXiv": "2306.08634 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 16:55:24 ", "Title": "Predicting Wireless Channel Quality by means of Moving Averages and Regression Models", "Authors": ["Gabriele Formis", "Stefano Scanzio", "Gianluca Cena", "Adriano Valenzano"], "Categories": "cs.NI cs.LG cs.NE", "Comments": ["preprint", "8 pages"], "Journal-ref": "IEEE 19th International Conference on Factory Communication Systems (WFCS 2023)", "DOI": "10.1109/WFCS57264.2023.10144122"}, "abstract": "The ability to reliably predict the future quality of a wireless channel, as seen by the media access control layer, is a key enabler to improve performance of future industrial networks that do not rely on wires. Knowing in advance how much channel behavior may change can speed up procedures for adaptively selecting the best channel, making the network more deterministic, reliable, and less energy-hungry, possibly improving device roaming capabilities at the same time. To this aim, popular approaches based on moving averages and regression were compared, using multiple key performance indicators, on data captured from a real Wi-Fi setup. Moreover, a simple technique based on a linear combination of outcomes from different techniques was presented and analyzed, to further reduce the prediction error, and some considerations about lower bounds on achievable errors have been reported. We found that the best model is the exponential moving average, which managed to predict the frame delivery ratio with a 2.10\\% average error and, at the same time, has lower computational complexity and memory consumption than the other models we analyzed.", "url": "https://arxiv.org/abs/2306.08634"}, {"metadata": {"arXiv": "2306.08650 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 17:30:02 ", "Title": "Learning to Rank when Grades Matter", "Authors": ["Le Yan", "Zhen Qin", "Xuanhui Wang", "Gil Shamir", "Mike Bendersky"], "Categories": "cs.IR cs.LG"}, "abstract": "Graded labels are ubiquitous in real-world learning-to-rank applications, especially in human rated relevance data. Traditional learning-to-rank techniques aim to optimize the ranked order of documents. They typically, however, ignore predicting actual grades. This prevents them from being adopted in applications where grades matter, such as filtering out ``poor'' documents. Achieving both good ranking performance and good grade prediction performance is still an under-explored problem. Existing research either focuses only on ranking performance by not calibrating model outputs, or treats grades as numerical values, assuming labels are on a linear scale and failing to leverage the ordinal grade information. In this paper, we conduct a rigorous study of learning to rank with grades, where both ranking performance and grade prediction performance are important. We provide a formal discussion on how to perform ranking with non-scalar predictions for grades, and propose a multiobjective formulation to jointly optimize both ranking and grade predictions. In experiments, we verify on several public datasets that our methods are able to push the Pareto frontier of the tradeoff between ranking and grade prediction performance, showing the benefit of leveraging ordinal grade information.", "url": "https://arxiv.org/abs/2306.08650"}, {"metadata": {"arXiv": "2306.08682 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 18:04:07 ", "Title": "Predicting Real-time Crash Risks during Hurricane Evacuation Using Connected Vehicle Data", "Authors": ["Zaheen E Muktadi Syed and Samiul Hasan"], "Categories": "stat.ML cs.LG"}, "abstract": "Hurricane evacuation, ordered to save lives of people of coastal regions, generates high traffic demand with increased crash risk. To mitigate such risk, transportation agencies need to anticipate highway locations with high crash risks to deploy appropriate countermeasures. With ubiquitous sensors and communication technologies, it is now possible to retrieve micro-level vehicular data containing individual vehicle trajectory and speed information. Such high-resolution vehicle data, potentially available in real time, can be used to assess prevailing traffic safety conditions. Using vehicle speed and acceleration profiles, potential crash risks can be predicted in real time. Previous studies on real-time crash risk prediction mainly used data from infrastructure-based sensors which may not cover many road segments. In this paper, we present methods to determine potential crash risks during hurricane evacuation from an emerging alternative data source known as connected vehicle data. Such data contain vehicle location, speed, and acceleration information collected at a very high frequency (less than 30 seconds). To predict potential crash risks, we utilized a dataset collected during the evacuation period of Hurricane Ida on Interstate-10 (I-10) in the state of Louisiana. Multiple machine learning models were trained considering weather features and different traffic characteristics extracted from the connected vehicle data in 5-minute intervals. The results indicate that the Gaussian Process Boosting (GPBoost) and Extreme Gradient Boosting (XGBoost) models perform better (recall = 0.91) than other models. The real-time connected vehicle data for crash risks assessment will allow traffic managers to efficiently utilize resources to proactively take safety measures.", "url": "https://arxiv.org/abs/2306.08682"}, {"metadata": {"arXiv": "2306.08698 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 18:38:32 ", "Title": "Phase Transitions of Civil Unrest across Countries and Time", "Authors": ["Dan Braha"], "Categories": "physics.soc-ph cs.LG nlin.AO", "Comments": ["Main paper (38 pages) and Supporting Information (138 pages)"]}, "abstract": "Phase transitions, characterized by abrupt shifts between macroscopic patterns of organization, are ubiquitous in complex systems. Despite considerable research in the physical and natural sciences, the empirical study of this phenomenon in societal systems is relatively underdeveloped. The goal of this study is to explore whether the dynamics of collective civil unrest can be plausibly characterized as a sequence of recurrent phase shifts, with each phase having measurable and identifiable latent characteristics. We introduce a macro-level statistical model of civil unrest and evaluate its plausibility using a comprehensive dataset of civil unrest events in 170 countries from 1946 to 2017. Our findings demonstrate that the macro-level phase model effectively captures the characteristics of civil unrest data from diverse countries globally and that universal mechanisms may underlie certain aspects of the dynamics of civil unrest. We also introduce a new scale to quantify a country's long-term unrest per unit of time and show that civil unrest events tend to cluster geographically, with the magnitude of civil unrest concentrated in specific regions. Our approach has the potential to identify and measure phase transitions in various collective human phenomena beyond civil unrest, contributing to a better understanding of complex social systems.", "url": "https://arxiv.org/abs/2306.08698"}, {"metadata": {"arXiv": "2306.08719 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 19:48:30 ", "Title": "Off-policy Evaluation in Doubly Inhomogeneous Environments", "Authors": ["Zeyu Bian", "Chengchun Shi", "Zhengling Qi and Lan Wang"], "Categories": "stat.ME cs.LG"}, "abstract": "This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions -- temporal stationarity and individual homogeneity are both violated. To handle the ``double inhomogeneities\", we propose a class of latent factor models for the reward and observation transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first paper that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms competing methods that ignore either temporal nonstationarity or individual heterogeneity. Finally, we illustrate our method on a data set from the Medical Information Mart for Intensive Care.", "url": "https://arxiv.org/abs/2306.08719"}, {"metadata": {"arXiv": "2306.08744 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 21:01:35 ", "Title": "Are training trajectories of deep single-spike and deep ReLU network equivalent?", "Authors": ["Ana Stanojevic", "Stanis{\\l}aw Wo\\'zniak", "Guillaume Bellec", "Giovanni Cherubini", "Angeliki Pantazi and Wulfram Gerstner"], "Categories": "cs.NE cs.LG"}, "abstract": "Communication by binary and sparse spikes is a key factor for the energy efficiency of biological brains. However, training deep spiking neural networks (SNNs) with backpropagation is harder than with artificial neural networks (ANNs), which is puzzling given that recent theoretical results provide exact mapping algorithms from ReLU to time-to-first-spike (TTFS) SNNs. Building upon these results, we analyze in theory and in simulation the learning dynamics of TTFS-SNNs. Our analysis highlights that even when an SNN can be mapped exactly to a ReLU network, it cannot always be robustly trained by gradient descent. The reason for that is the emergence of a specific instance of the vanishing-or-exploding gradient problem leading to a bias in the gradient descent trajectory in comparison with the equivalent ANN. After identifying this issue we derive a generic solution for the network initialization and SNN parameterization which guarantees that the SNN can be trained as robustly as its ANN counterpart. Our theoretical findings are illustrated in practice on image classification datasets. Our method achieves the same accuracy as deep ConvNets on CIFAR10 and enables fine-tuning on the much larger PLACES365 dataset without loss of accuracy compared to the ANN. We argue that the combined perspective of conversion and fine-tuning with robust gradient descent in SNN will be decisive to optimize SNNs for hardware implementations needing low latency and resilience to noise and quantization.", "url": "https://arxiv.org/abs/2306.08744"}, {"metadata": {"arXiv": "2306.08745 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 21:04:50 ", "Title": "PLAN: Variance-Aware Private Mean Estimation", "Authors": ["Martin Aum\\\"uller", "Christian Janos Lebeda", "Boel Nelson", "Rasmus Pagh"], "Categories": "cs.CR cs.DS cs.LG"}, "abstract": "Differentially private mean estimation is an important building block in privacy-preserving algorithms for data analysis and machine learning. Though the trade-off between privacy and utility is well understood in the worst case, many datasets exhibit structure that could potentially be exploited to yield better algorithms. In this paper we present $\\textit{Private Limit Adapted Noise (PLAN)}$, a family of differentially private algorithms for mean estimation in the setting where inputs are independently sampled from a distribution $\\mathcal{D}$ over $\\mathbf{R}^d$, with coordinate-wise standard deviations $\\boldsymbol{\\sigma} \\in \\mathbf{R}^d$. Similar to mean estimation under Mahalanobis distance, PLAN tailors the shape of the noise to the shape of the data, but unlike previous algorithms the privacy budget is spent non-uniformly over the coordinates. Under a concentration assumption on $\\mathcal{D}$, we show how to exploit skew in the vector $\\boldsymbol{\\sigma}$, obtaining a (zero-concentrated) differentially private mean estimate with $\\ell_2$ error proportional to $\\|\\boldsymbol{\\sigma}\\|_1$. Previous work has either not taken $\\boldsymbol{\\sigma}$ into account, or measured error in Mahalanobis distance $\\unicode{x2013}$ in both cases resulting in $\\ell_2$ error proportional to $\\sqrt{d}\\|\\boldsymbol{\\sigma}\\|_2$, which can be up to a factor $\\sqrt{d}$ larger. To verify the effectiveness of \\algorithmname, we empirically evaluate accuracy on both synthetic and real world data.", "url": "https://arxiv.org/abs/2306.08745"}, {"metadata": {"arXiv": "2306.08749 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 21:17:31 ", "Title": "Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports", "Authors": ["Qingqing Zhu", "Tejas Sudharshan Mathai", "Pritam Mukherjee", "Yifan Peng", "Ronald M. Summers", "and Zhiyong Lu"], "Categories": "cs.CL cs.LG"}, "abstract": "Despite the reduction in turn-around times in radiology reports with the use of speech recognition software, persistent communication errors can significantly impact the interpretation of the radiology report. Pre-filling a radiology report holds promise in mitigating reporting errors, and despite efforts in the literature to generate medical reports, there exists a lack of approaches that exploit the longitudinal nature of patient visit records in the MIMIC-CXR dataset. To address this gap, we propose to use longitudinal multi-modal data, i.e., previous patient visit CXR, current visit CXR, and previous visit report, to pre-fill the 'findings' section of a current patient visit report. We first gathered the longitudinal visit information for 26,625 patients from the MIMIC-CXR dataset and created a new dataset called Longitudinal-MIMIC. With this new dataset, a transformer-based model was trained to capture the information from longitudinal patient visit records containing multi-modal data (CXR images + reports) via a cross-attention-based multi-modal fusion module and a hierarchical memory-driven decoder. In contrast to previous work that only uses current visit data as input to train a model, our work exploits the longitudinal information available to pre-fill the 'findings' section of radiology reports. Experiments show that our approach outperforms several recent approaches by >=3% on F1 score, and >=2% for BLEU-4, METEOR and ROUGE-L respectively. The dataset and code will be made publicly available.", "url": "https://arxiv.org/abs/2306.08749"}, {"metadata": {"arXiv": "2306.08777 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 23:13:03 ", "Title": "MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting", "Authors": ["Felix Biggs", "Antonin Schrab", "Arthur Gretton"], "Categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "Comments": ["42 pages", "7 figures", "1 table"]}, "abstract": "We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean Discrepancy (MMD), by adapting over the set of kernels used in defining it. For finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum. Exponential concentration bounds are proved for our proposed statistics under the null and alternative. We further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting. This technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders. We highlight the applicability of our MMD-FUSE test on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests.", "url": "https://arxiv.org/abs/2306.08777"}, {"metadata": {"arXiv": "2306.08783 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 23:39:37 ", "Title": "HOSSnet: an Efficient Physics-Guided Neural Network for Simulating Crack Propagation", "Authors": ["Shengyu Chen", "Shihang Feng", "Yao Huang", "Zhou Lei", "Xiaowei Jia", "Youzuo Lin", "Estaben Rougier"], "Categories": "cs.CE cs.LG", "Comments": ["12 pages"]}, "abstract": "Hybrid Optimization Software Suite (HOSS), which is a combined finite-discrete element method (FDEM), is one of the advanced approaches to simulating high-fidelity fracture and fragmentation processes but the application of pure HOSS simulation is computationally expensive. At the same time, machine learning methods, shown tremendous success in several scientific problems, are increasingly being considered promising alternatives to physics-based models in the scientific domains. Thus, our goal in this work is to build a new data-driven methodology to reconstruct the crack fracture accurately in the spatial and temporal fields. We leverage physical constraints to regularize the fracture propagation in the long-term reconstruction. In addition, we introduce perceptual loss and several extra pure machine learning optimization approaches to improve the reconstruction performance of fracture data further. We demonstrate the effectiveness of our proposed method through both extrapolation and interpolation experiments. The results confirm that our proposed method can reconstruct high-fidelity fracture data over space and time in terms of pixel-wise reconstruction error and structural similarity. Visual comparisons also show promising results in long-term", "url": "https://arxiv.org/abs/2306.08783"}, {"metadata": {"arXiv": "2306.08804 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 01:18:02 ", "Title": "PEACE: Cross-Platform Hate Speech Detection- A Causality-guided Framework", "Authors": ["Paras Sheth", "Tharindu Kumarage", "Raha Moraffah", "Aman Chadha", "and Huan Liu"], "Categories": "cs.CL cs.LG"}, "abstract": "Hate speech detection refers to the task of detecting hateful content that aims at denigrating an individual or a group based on their religion, gender, sexual orientation, or other characteristics. Due to the different policies of the platforms, different groups of people express hate in different ways. Furthermore, due to the lack of labeled data in some platforms it becomes challenging to build hate speech detection models. To this end, we revisit if we can learn a generalizable hate speech detection model for the cross platform setting, where we train the model on the data from one (source) platform and generalize the model across multiple (target) platforms. Existing generalization models rely on linguistic cues or auxiliary information, making them biased towards certain tags or certain kinds of words (e.g., abusive words) on the source platform and thus not applicable to the target platforms. Inspired by social and psychological theories, we endeavor to explore if there exist inherent causal cues that can be leveraged to learn generalizable representations for detecting hate speech across these distribution shifts. To this end, we propose a causality-guided framework, PEACE, that identifies and leverages two intrinsic causal cues omnipresent in hateful content: the overall sentiment and the aggression in the text. We conduct extensive experiments across multiple platforms (representing the distribution shift) showing if causal cues can help cross-platform generalization.", "url": "https://arxiv.org/abs/2306.08804"}, {"metadata": {"arXiv": "2306.08805 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 01:21:12 ", "Title": "Exact Count of Boundary Pieces of ReLU Classifiers: Towards the Proper Complexity Measure for Classification", "Authors": ["Pawe{\\l} Piwek", "Adam Klukowski", "Tianyang Hu"], "Categories": "stat.ML cs.LG", "Comments": ["Accepted to UAI 2023"]}, "abstract": "Classic learning theory suggests that proper regularization is the key to good generalization and robustness. In classification, current training schemes only target the complexity of the classifier itself, which can be misleading and ineffective. Instead, we advocate directly measuring the complexity of the decision boundary. Existing literature is limited in this area with few well-established definitions of boundary complexity. As a proof of concept, we start by analyzing ReLU neural networks, whose boundary complexity can be conveniently characterized by the number of affine pieces. With the help of tropical geometry, we develop a novel method that can explicitly count the exact number of boundary pieces, and as a by-product, the exact number of total affine pieces. Numerical experiments are conducted and distinctive properties of our boundary complexity are uncovered. First, the boundary piece count appears largely independent of other measures, e.g., total piece count, and $l_2$ norm of weights, during the training process. Second, the boundary piece count is negatively correlated with robustness, where popular robust training techniques, e.g., adversarial training or random noise injection, are found to reduce the number of boundary pieces.", "url": "https://arxiv.org/abs/2306.08805"}, {"metadata": {"arXiv": "2306.08847 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 04:23:25 ", "Title": "Improving Reading Comprehension Question Generation with Data Augmentation and Overgenerate-and-rank", "Authors": ["Nischal Ashok Kumar", "Nigel Fernandez", "Zichao Wang", "Andrew Lan"], "Categories": "cs.CL cs.CY cs.LG", "Comments": ["Oral presentation at ACL BEA workshop 2023. Code available at: https://github.com/umass-ml4ed/question-gen-aug-ranking"]}, "abstract": "Reading comprehension is a crucial skill in many aspects of education, including language learning, cognitive development, and fostering early literacy skills in children. Automated answer-aware reading comprehension question generation has significant potential to scale up learner support in educational activities. One key technical challenge in this setting is that there can be multiple questions, sometimes very different from each other, with the same answer; a trained question generation method may not necessarily know which question human educators would prefer. To address this challenge, we propose 1) a data augmentation method that enriches the training dataset with diverse questions given the same context and answer and 2) an overgenerate-and-rank method to select the best question from a pool of candidates. We evaluate our method on the FairytaleQA dataset, showing a 5% absolute improvement in ROUGE-L over the best existing method. We also demonstrate the effectiveness of our method in generating harder, \"implicit\" questions, where the answers are not contained in the context as text spans.", "url": "https://arxiv.org/abs/2306.08847"}, {"metadata": {"arXiv": "2306.08853 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 04:42:25 ", "Title": "In Search of netUnicorn: A Data-Collection Platform to Develop Generalizable ML Models for Network Security Problems", "Authors": ["Roman Beltiukov", "Wenbo Guo", "Arpit Gupta", "Walter Willinger"], "Categories": "cs.NI cs.CR cs.LG"}, "abstract": "The remarkable success of the use of machine learning-based solutions for network security problems has been impeded by the developed ML models' inability to maintain efficacy when used in different network environments exhibiting different network behaviors. This issue is commonly referred to as the generalizability problem of ML models. The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem. Unfortunately, these methods are generally ill-suited or even counterproductive in the network security domain, where they often result in unrealistic or poor-quality datasets. To address this issue, we propose an augmented ML pipeline that leverages explainable ML tools to guide the network data collection in an iterative fashion. To ensure the data's realism and quality, we require that the new datasets should be endogenously collected in this iterative process, thus advocating for a gradual removal of data-related problems to improve model generalizability. To realize this capability, we develop a data-collection platform, netUnicorn, that takes inspiration from the classic \"hourglass\" model and is implemented as its \"thin waist\" to simplify data collection for different learning problems from diverse network environments. The proposed system decouples data-collection intents from the deployment mechanisms and disaggregates these high-level intents into smaller reusable, self-contained tasks. We demonstrate how netUnicorn simplifies collecting data for different learning problems from multiple network environments and how the proposed iterative data collection improves a model's generalizability.", "url": "https://arxiv.org/abs/2306.08853"}, {"metadata": {"arXiv": "2306.08888 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 06:41:23 ", "Title": "ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design", "Authors": ["Srivatsan Krishnan", "Amir Yazdanbaksh", "Shvetank Prakash", "Jason Jabbour", "Ikechukwu Uchendu", "Susobhan Ghosh", "Behzad Boroujerdian", "Daniel Richins", "Devashree Tripathy", "Aleksandra Faust", "Vijay Janapa Reddi"], "Categories": "cs.AR cs.LG", "Comments": ["International Symposium on Computer Architecture (ISCA 2023)"]}, "abstract": "Machine learning is a prevalent approach to tame the complexity of design space exploration for domain-specific architectures. Using ML for design space exploration poses challenges. First, it's not straightforward to identify the suitable algorithm from an increasing pool of ML methods. Second, assessing the trade-offs between performance and sample efficiency across these methods is inconclusive. Finally, lack of a holistic framework for fair, reproducible, and objective comparison across these methods hinders progress of adopting ML-aided architecture design space exploration and impedes creating repeatable artifacts. To mitigate these challenges, we introduce ArchGym, an open-source gym and easy-to-extend framework that connects diverse search algorithms to architecture simulators. To demonstrate utility, we evaluate ArchGym across multiple vanilla and domain-specific search algorithms in designing custom memory controller, deep neural network accelerators, and custom SoC for AR/VR workloads, encompassing over 21K experiments. Results suggest that with unlimited samples, ML algorithms are equally favorable to meet user-defined target specification if hyperparameters are tuned; no solution is necessarily better than another (e.g., reinforcement learning vs. Bayesian methods). We coin the term hyperparameter lottery to describe the chance for a search algorithm to find an optimal design provided meticulously selected hyperparameters. The ease of data collection and aggregation in ArchGym facilitates research in ML-aided architecture design space exploration. As a case study, we show this advantage by developing a proxy cost model with an RMSE of 0.61% that offers a 2,000-fold reduction in simulation time. Code and data for ArchGym is available at https://bit.ly/ArchGym.", "url": "https://arxiv.org/abs/2306.08888"}, {"metadata": {"arXiv": "2306.08907 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 07:20:26 ", "Title": "MCPI: Integrating Multimodal Data for Enhanced Prediction of Compound Protein Interactions", "Authors": ["Li Zhang", "Wenhao Li", "Haotian Guan", "Zhiquan He", "Mingjun Cheng", "Han Wang"], "Categories": "q-bio.BM cs.LG", "Comments": ["12 pages", "9 figures"]}, "abstract": "The identification of compound-protein interactions (CPI) plays a critical role in drug screening, drug repurposing, and combination therapy studies. The effectiveness of CPI prediction relies heavily on the features extracted from both compounds and target proteins. While various prediction methods employ different feature combinations, both molecular-based and network-based models encounter the common obstacle of incomplete feature representations. Thus, a promising solution to this issue is to fully integrate all relevant CPI features. This study proposed a novel model named MCPI, which is designed to improve the prediction performance of CPI by integrating multiple sources of information, including the PPI network, CCI network, and structural features of CPI. The results of the study indicate that the MCPI model outperformed other existing methods for predicting CPI on public datasets. Furthermore, the study has practical implications for drug development, as the model was applied to search for potential inhibitors among FDA-approved drugs in response to the SARS-CoV-2 pandemic. The prediction results were then validated through the literature, suggesting that the MCPI model could be a useful tool for identifying potential drug candidates. Overall, this study has the potential to advance our understanding of CPI and guide drug development efforts.", "url": "https://arxiv.org/abs/2306.08907"}, {"metadata": {"arXiv": "2306.08929 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 08:02:07 ", "Title": "Community Detection Attack against Collaborative Learning-based Recommender Systems", "Authors": ["Yacine Belal", "Sonia Ben Mokhtar", "Mohamed Maouche and Anthony Simonet-Boulogne"], "Categories": "cs.IR cs.CR cs.LG cs.SI", "ACM-class": "H.3.3; I.2.6; I.2.11; K.6.5"}, "abstract": "Collaborative-learning based recommender systems emerged following the success of collaborative learning techniques such as Federated Learning (FL) and Gossip Learning (GL). In these systems, users participate in the training of a recommender system while keeping their history of consumed items on their devices. While these solutions seemed appealing for preserving the privacy of the participants at a first glance, recent studies have shown that collaborative learning can be vulnerable to a variety of privacy attacks. In this paper we propose a novel privacy attack called Community Detection Attack (CDA), which allows an adversary to discover the members of a community based on a set of items of her choice (e.g., discovering users interested in LGBT content). Through experiments on three real recommendation datasets and by using two state-of-the-art recommendation models, we assess the sensitivity of an FL-based recommender system as well as two flavors of Gossip Learning-based recommender systems to CDA. Results show that on all models and all datasets, the FL setting is more vulnerable to CDA than Gossip settings. We further evaluated two off-the-shelf mitigation strategies, namely differential privacy (DP) and a share less policy, which consists in sharing a subset of model parameters. Results show a better privacy-utility trade-off for the share less policy compared to DP especially in the Gossip setting.", "url": "https://arxiv.org/abs/2306.08929"}, {"metadata": {"arXiv": "2306.08947 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 08:39:24 ", "Title": "RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation", "Authors": ["Gabriel B\\'en\\'edict", "Olivier Jeunen", "Samuele Papa", "Samarth Bhargav", "Daan Odijk", "Maarten de Rijke"], "Categories": "cs.IR cs.LG", "Comments": ["code: https://github.com/gabriben/recfusion"], "Report-no": "12"}, "abstract": "In this paper we propose RecFusion, which comprise a set of diffusion models for recommendation. Unlike image data which contain spatial correlations, a user-item interaction matrix, commonly utilized in recommendation, lacks spatial relationships between users and items. We formulate diffusion on a 1D vector and propose binomial diffusion, which explicitly models binary user-item interactions with a Bernoulli process. We show that RecFusion approaches the performance of complex VAE baselines on the core recommendation setting (top-n recommendation for binary non-sequential feedback) and the most common datasets (MovieLens and Netflix). Our proposed diffusion models that are specialized for 1D and/or binary setups have implications beyond recommendation systems, such as in the medical domain with MRI and CT scans.", "url": "https://arxiv.org/abs/2306.08947"}, {"metadata": {"arXiv": "2306.08967 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 09:02:17 ", "Title": "Accelerating Dynamic Network Embedding with Billions of Parameter Updates to Milliseconds", "Authors": ["Haoran Deng", "Yang Yang", "Jiahe Li", "Haoyang Cai", "Shiliang Pu", "Weihao Jiang"], "Categories": "cs.SI cs.LG", "DOI": "10.1145/3580305.3599250"}, "abstract": "Network embedding, a graph representation learning method illustrating network topology by mapping nodes into lower-dimension vectors, is challenging to accommodate the ever-changing dynamic graphs in practice. Existing research is mainly based on node-by-node embedding modifications, which falls into the dilemma of efficient calculation and accuracy. Observing that the embedding dimensions are usually much smaller than the number of nodes, we break this dilemma with a novel dynamic network embedding paradigm that rotates and scales the axes of embedding space instead of a node-by-node update. Specifically, we propose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm, which achieves an efficient and accurate dynamic network embedding by rotating and scaling the coordinate system where the network embedding resides with no more than the number of edge modifications changes of node embeddings. Moreover, a dynamic Personalized PageRank is applied to the obtained network embeddings to enhance node embeddings and capture higher-order neighbor information dynamically. Experiments of node classification, link prediction, and graph reconstruction on different-sized dynamic graphs suggest that DAMF advances dynamic network embedding. Further, we unprecedentedly expand dynamic network embedding experiments to billion-edge graphs, where DAMF updates billion-level parameters in less than 10ms.", "url": "https://arxiv.org/abs/2306.08967"}, {"metadata": {"arXiv": "2306.09002 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 10:00:03 ", "Title": "ExoMDN: Rapid characterization of exoplanet interior structures with Mixture Density Networks", "Authors": ["Philipp Baumeister and Nicola Tosi"], "Categories": "astro-ph.EP cs.LG", "Comments": ["15 pages", "15 figures", "accepted for publication in Astronomy & Astrophysics. The ExoMDN model is freely accessible at https://github.com/philippbaumeister/ExoMDN"]}, "abstract": "Characterizing the interior structure of exoplanets is essential for understanding their diversity, formation, and evolution. As the interior of exoplanets is inaccessible to observations, an inverse problem must be solved, where numerical structure models need to conform to observable parameters such as mass and radius. This is a highly degenerate problem whose solution often relies on computationally-expensive and time-consuming inference methods such as Markov Chain Monte Carlo. We present ExoMDN, a machine-learning model for the interior characterization of exoplanets based on Mixture Density Networks (MDN). The model is trained on a large dataset of more than 5.6 million synthetic planets below 25 Earth masses consisting of an iron core, a silicate mantle, a water and high-pressure ice layer, and a H/He atmosphere. We employ log-ratio transformations to convert the interior structure data into a form that the MDN can easily handle. Given mass, radius, and equilibrium temperature, we show that ExoMDN can deliver a full posterior distribution of mass fractions and thicknesses of each planetary layer in under a second on a standard Intel i5 CPU. Observational uncertainties can be easily accounted for through repeated predictions from within the uncertainties. We use ExoMDN to characterize the interior of 22 confirmed exoplanets with mass and radius uncertainties below 10% and 5% respectively, including the well studied GJ 1214 b, GJ 486 b, and the TRAPPIST-1 planets. We discuss the inclusion of the fluid Love number $k_2$ as an additional (potential) observable, showing how it can significantly reduce the degeneracy of interior structures. Utilizing the fast predictions of ExoMDN, we show that measuring $k_2$ with an accuracy of 10% can constrain the thickness of core and mantle of an Earth analog to $\\approx13\\%$ of the true values.", "url": "https://arxiv.org/abs/2306.09002"}, {"metadata": {"arXiv": "2306.09025 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 10:34:20 ", "Title": "CoverHunter: Cover Song Identification with Refined Attention and Alignments", "Authors": ["Feng Liu", "Deyi Tuo", "Yinan Xu", "Xintong Han"], "Categories": "cs.SD cs.LG eess.AS", "Comments": ["6 pages", "3 figures"]}, "abstract": "Abstract: Cover song identification (CSI) focuses on finding the same music with different versions in reference anchors given a query track. In this paper, we propose a novel system named CoverHunter that overcomes the shortcomings of existing detection schemes by exploring richer features with refined attention and alignments. CoverHunter contains three key modules: 1) A convolution-augmented transformer (i.e., Conformer) structure that captures both local and global feature interactions in contrast to previous methods mainly relying on convolutional neural networks; 2) An attention-based time pooling module that further exploits the attention in the time dimension; 3) A novel coarse-to-fine training scheme that first trains a network to roughly align the song chunks and then refines the network by training on the aligned chunks. At the same time, we also summarize some important training tricks used in our system that help achieve better results. Experiments on several standard CSI datasets show that our method significantly improves over state-of-the-art methods with an embedding size of 128 (2.3% on SHS100K-TEST and 17.7% on DaTacos).", "url": "https://arxiv.org/abs/2306.09025"}, {"metadata": {"arXiv": "2306.09049 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 11:13:54 ", "Title": "Mapping Researcher Activity based on Publication Data by means of Transformers", "Authors": ["Zineddine Bettouche and Andreas Fischer"], "Categories": "cs.CL cs.DL cs.IR cs.LG", "Comments": ["Proc. of the Interdisciplinary Conference on Mechanics", "Computers and Electrics (ICMECE 2022)"]}, "abstract": "Modern performance on several natural language processing (NLP) tasks has been enhanced thanks to the Transformer-based pre-trained language model BERT. We employ this concept to investigate a local publication database. Research papers are encoded and clustered to form a landscape view of the scientific topics, in which research is active. Authors working on similar topics can be identified by calculating the similarity between their papers. Based on this, we define a similarity metric between authors. Additionally we introduce the concept of self-similarity to indicate the topical variety of authors.", "url": "https://arxiv.org/abs/2306.09049"}, {"metadata": {"arXiv": "2306.09066 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 11:48:50 ", "Title": "A Bayesian approach to uncertainty in word embedding bias estimation", "Authors": ["Alicja Dobrzeniecka and Rafal Urbaniak"], "Categories": "cs.CL cs.HC cs.LG stat.AP stat.ME", "Comments": ["52 pages", "39 figures"]}, "abstract": "Multiple measures, such as WEAT or MAC, attempt to quantify the magnitude of bias present in word embeddings in terms of a single-number metric. However, such metrics and the related statistical significance calculations rely on treating pre-averaged data as individual data points and employing bootstrapping techniques with low sample sizes. We show that similar results can be easily obtained using such methods even if the data are generated by a null model lacking the intended bias. Consequently, we argue that this approach generates false confidence. To address this issue, we propose a Bayesian alternative: hierarchical Bayesian modeling, which enables a more uncertainty-sensitive inspection of bias in word embeddings at different levels of granularity. To showcase our method, we apply it to Religion, Gender, and Race word lists from the original research, together with our control neutral word lists. We deploy the method using Google, Glove, and Reddit embeddings. Further, we utilize our approach to evaluate a debiasing technique applied to Reddit word embedding. Our findings reveal a more complex landscape than suggested by the proponents of single-number metrics. The datasets and source code for the paper are publicly available.", "url": "https://arxiv.org/abs/2306.09066"}, {"metadata": {"arXiv": "2306.09111 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 13:13:56 ", "Title": "Enhanced Sampling with Machine Learning: A Review", "Authors": ["Shams Mehdi", "Zachary Smith", "Lukas Herron", "Ziyue Zou and Pratyush Tiwary"], "Categories": "cond-mat.stat-mech cs.LG physics.chem-ph physics.comp-ph", "Comments": ["Submitted as invited article to Annual Review of Physical Chemistry vol 75"]}, "abstract": "Molecular dynamics (MD) enables the study of physical systems with excellent spatiotemporal resolution but suffers from severe time-scale limitations. To address this, enhanced sampling methods have been developed to improve exploration of configurational space. However, implementing these is challenging and requires domain expertise. In recent years, integration of machine learning (ML) techniques in different domains has shown promise, prompting their adoption in enhanced sampling as well. Although ML is often employed in various fields primarily due to its data-driven nature, its integration with enhanced sampling is more natural with many common underlying synergies. This review explores the merging of ML and enhanced MD by presenting different shared viewpoints. It offers a comprehensive overview of this rapidly evolving field, which can be difficult to stay updated on. We highlight successful strategies like dimensionality reduction, reinforcement learning, and flow-based methods. Finally, we discuss open problems at the exciting ML-enhanced MD interface.", "url": "https://arxiv.org/abs/2306.09111"}, {"metadata": {"arXiv": "2306.09112 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 13:15:26 ", "Title": "On Certified Generalization in Structured Prediction", "Authors": ["Bastian Boll", "Christoph Schn\\\"orr"], "Categories": "stat.ML cs.LG"}, "abstract": "In structured prediction, target objects have rich internal structure which does not factorize into independent components and violates common i.i.d. assumptions. This challenge becomes apparent through the exponentially large output space in applications such as image segmentation or scene graph generation. We present a novel PAC-Bayesian risk bound for structured prediction wherein the rate of generalization scales not only with the number of structured examples but also with their size. The underlying assumption, conforming to ongoing research on generative models, is that data are generated by the Knothe-Rosenblatt rearrangement of a factorizing reference measure. This allows to explicitly distill the structure between random output variables into a Wasserstein dependency matrix. Our work makes a preliminary step towards leveraging powerful generative models to establish generalization bounds for discriminative downstream tasks in the challenging setting of structured prediction.", "url": "https://arxiv.org/abs/2306.09112"}, {"metadata": {"arXiv": "2306.09128 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 13:41:17 ", "Title": "Fast Algorithms for Directed Graph Partitioning Using Flows and Reweighted Eigenvalues", "Authors": ["Lap Chi Lau", "Kam Chuen Tung", "Robert Wang"], "Categories": "cs.DS cs.DM cs.LG math.CO"}, "abstract": "We consider a new semidefinite programming relaxation for directed edge expansion, which is obtained by adding triangle inequalities to the reweighted eigenvalue formulation. Applying the matrix multiplicative weight update method to this relaxation, we derive almost linear-time algorithms to achieve $O(\\sqrt{\\log{n}})$-approximation and Cheeger-type guarantee for directed edge expansion, as well as an improved cut-matching game for directed graphs. This provides a primal-dual flow-based framework to obtain the best known algorithms for directed graph partitioning. The same approach also works for vertex expansion and for hypergraphs, providing a simple and unified approach to achieve the best known results for different expansion problems and different algorithmic techniques.", "url": "https://arxiv.org/abs/2306.09128"}, {"metadata": {"arXiv": "2306.09171 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 14:49:54 ", "Title": "How are the people in the photos judged? Analysis of brain activity when assessing levels of trust and attractiveness", "Authors": ["Bernadetta Bartosik", "Grzegorz M. Wojcik", "Andrzej Kawiak", "Aneta Brzezicka"], "Categories": "cs.HC cs.LG"}, "abstract": "Trust is the foundation of every area of life. Without it, it is difficult to build lasting relationships. Unfortunately, in recent years, trust has been severely damaged by the spread of fake news and disinformation, which has become a serious social problem. In addition to trust, the factor influencing interpersonal relationships is perceived attractiveness, which is currently created to a large extent by digital media. Understanding the principles of judging others can be helpful in fighting prejudice and rebuilding trust in society. One way to learn about people's choices is to record their brain activity as they make choices. The article presents an experiment in which the faces of different people were presented, and the participants' task was to assess how much they can trust a given person and how attractive they are. During the study, the EEG signal was recorded, which was used to build models of logistic regression classifiers. In addition, the most active areas of the brain that participate in the assessment of trust and attractiveness of the face were indicated.", "url": "https://arxiv.org/abs/2306.09171"}, {"metadata": {"arXiv": "2306.09189 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 15:16:16 ", "Title": "High-Resolution Convolutional Neural Networks on Homomorphically Encrypted Data via Sharding Ciphertexts", "Authors": ["Vivian Maloney", "Richard F. Obrecht", "Vikram Saraph", "Prathibha Rama", "Kate Tallaksen"], "Categories": "cs.CR cs.LG", "Comments": ["14 pages", "9 figures"]}, "abstract": "Recently, Deep Convolutional Neural Networks (DCNNs) including the ResNet-20 architecture have been privately evaluated on encrypted, low-resolution data with the Residue-Number-System Cheon-Kim-Kim-Song (RNS-CKKS) homomorphic encryption scheme. We extend methods for evaluating DCNNs on images with larger dimensions and many channels, beyond what can be stored in single ciphertexts. Additionally, we simplify and improve the efficiency of the recently introduced multiplexed image format, demonstrating that homomorphic evaluation can work with standard, row-major matrix packing and results in encrypted inference time speedups by $4.6-6.5\\times$. We also show how existing DCNN models can be regularized during the training process to further improve efficiency and accuracy. These techniques are applied to homomorphically evaluate a DCNN with high accuracy on the high-resolution ImageNet dataset for the first time, achieving $80.2\\%$ top-1 accuracy. We also achieve the highest reported accuracy of homomorphically evaluated CNNs on the CIFAR-10 dataset of $98.3\\%$.", "url": "https://arxiv.org/abs/2306.09189"}, {"metadata": {"arXiv": "2306.09194 (*cross-listing*)", "Date": "Thu, 25 May 2023 02:57:16 ", "Title": "Undetectable Watermarks for Language Models", "Authors": ["Miranda Christ", "Sam Gunn", "Or Zamir"], "Categories": "cs.CR cs.CL cs.LG"}, "abstract": "Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text. Prior works have suggested methods of embedding watermarks in model outputs, by noticeably altering the output distribution. We ask: Is it possible to introduce a watermark without incurring any detectable change to the output distribution? To this end we introduce a cryptographically-inspired notion of undetectable watermarks for language models. That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks should remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography.", "url": "https://arxiv.org/abs/2306.09194"}, {"metadata": {"arXiv": "2306.09223 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 15:59:26 ", "Title": "Few-shot bioacoustic event detection at the DCASE 2023 challenge", "Authors": ["Ines Nolasco", "Burooj Ghani", "Shubhr Singh", "Ester Vida\\~na-Vila", "Helen Whitehead", "Emily Grout", "Michael Emmerson", "Frants Jensen", "Ivan Kiskin", "Joe Morford", "Ariana Strandburg-Peshkin", "Lisa Gill", "Hanna Pamu{\\l}a", "Vincent Lostanlen", "Dan Stowell"], "Categories": "cs.SD cs.LG eess.AS", "Comments": ["submitted to DCASE 2023 workshop"]}, "abstract": "Few-shot bioacoustic event detection consists in detecting sound events of specified types, in varying soundscapes, while having access to only a few examples of the class of interest. This task ran as part of the DCASE challenge for the third time this year with an evaluation set expanded to include new animal species, and a new rule: ensemble models were no longer allowed. The 2023 few shot task received submissions from 6 different teams with F-scores reaching as high as 63% on the evaluation set. Here we describe the task, focusing on describing the elements that differed from previous years. We also take a look back at past editions to describe how the task has evolved. Not only have the F-score results steadily improved (40% to 60% to 63%), but the type of systems proposed have also become more complex. Sound event detection systems are no longer simple variations of the baselines provided: multiple few-shot learning methodologies are still strong contenders for the task.", "url": "https://arxiv.org/abs/2306.09223"}, {"metadata": {"arXiv": "2306.09239 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 16:22:57 ", "Title": "Exploiting the Brain's Network Structure for Automatic Identification of ADHD Subjects", "Authors": ["Soumyabrata Dey", "Ravishankar Rao", "Mubarak Shah"], "Categories": "q-bio.NC cs.LG eess.IV"}, "abstract": "Attention Deficit Hyperactive Disorder (ADHD) is a common behavioral problem affecting children. In this work, we investigate the automatic classification of ADHD subjects using the resting state Functional Magnetic Resonance Imaging (fMRI) sequences of the brain. We show that the brain can be modeled as a functional network, and certain properties of the networks differ in ADHD subjects from control subjects. We compute the pairwise correlation of brain voxels' activity over the time frame of the experimental protocol which helps to model the function of a brain as a network. Different network features are computed for each of the voxels constructing the network. The concatenation of the network features of all the voxels in a brain serves as the feature vector. Feature vectors from a set of subjects are then used to train a PCA-LDA (principal component analysis-linear discriminant analysis) based classifier. We hypothesized that ADHD-related differences lie in some specific regions of the brain and using features only from those regions is sufficient to discriminate ADHD and control subjects. We propose a method to create a brain mask that includes the useful regions only and demonstrate that using the feature from the masked regions improves classification accuracy on the test data set. We train our classifier with 776 subjects and test on 171 subjects provided by The Neuro Bureau for the ADHD-200 challenge. We demonstrate the utility of graph-motif features, specifically the maps that represent the frequency of participation of voxels in network cycles of length 3. The best classification performance (69.59%) is achieved using 3-cycle map features with masking. Our proposed approach holds promise in being able to diagnose and understand the disorder.", "url": "https://arxiv.org/abs/2306.09239"}, {"metadata": {"arXiv": "2306.09251 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 16:30:08 ", "Title": "Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models", "Authors": ["Gen Li", "Yuting Wei", "Yuxin Chen", "Yuejie Chi"], "Categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH"}, "abstract": "Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to reliable estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model (DDPM)), we derive a convergence rate proportional to $1/\\sqrt{T}$, matching the state-of-the-art theory. Our theory imposes only minimal assumptions on the target data distribution (e.g., no smoothness assumption is imposed), and is developed based on an elementary yet versatile non-asymptotic approach without resorting to toolboxes for SDEs and ODEs. Further, we design two accelerated variants, improving the convergence to $1/T^2$ for the ODE-based sampler and $1/T$ for the DDPM-type sampler, which might be of independent theoretical and empirical interest.", "url": "https://arxiv.org/abs/2306.09251"}, {"metadata": {"arXiv": "2306.09258 (*cross-listing*)", "Date": "Thu, 25 May 2023 19:13:31 ", "Title": "Coding for the Gaussian Channel in the Finite Blocklength Regime Using a CNN-Autoencoder", "Authors": ["Nourhan Hesham", "Mohamed Bouzid", "Ahmad Abdel-Qader", "and Anas Chaaban"], "Categories": "cs.IT cs.LG math.IT", "Comments": ["This paper is submitted in IEEE International Black Sea Conference on Communications and Networking 2023 (Status: Accepted). This is a 6-pages paper with 3 figures and two tables"]}, "abstract": "The development of delay-sensitive applications that require ultra high reliability created an additional challenge for wireless networks. This led to Ultra-Reliable Low-Latency Communications, as a use case that 5G and beyond 5G systems must support. However, supporting low latency communications requires the use of short codes, while attaining vanishing frame error probability (FEP) requires long codes. Thus, developing codes for the finite blocklength regime (FBR) achieving certain reliability requirements is necessary. This paper investigates the potential of Convolutional Neural Networks autoencoders (CNN-AE) in approaching the theoretical maximum achievable rate over a Gaussian channel for a range of signal-to-noise ratios at a fixed blocklength and target FEP, which is a different perspective compared to existing works that explore the use of CNNs from bit-error and symbol-error rate perspectives. We explain the studied CNN-AE architecture, evaluate it numerically, and compare it to the theoretical maximum achievable rate and the achievable rates of polar coded quadrature amplitude modulation (QAM), Reed-Muller coded QAM, multilevel polar coded modulation, and a TurboAE-MOD scheme from the literature. Numerical results show that the CNN-AE outperforms these benchmark schemes and approaches the theoretical maximum rate, demonstrating the capability of CNN-AEs in learning good codes for delay-constrained applications.", "url": "https://arxiv.org/abs/2306.09258"}, {"metadata": {"arXiv": "2306.09260 (*cross-listing*)", "Date": "Wed, 07 Jun 2023 14:22:41 ", "Title": "IsoEx: an explainable unsupervised approach to process event logs cyber investigation", "Authors": ["Pierre Lavieille and Ismail Alaoui Hassani Atlas"], "Categories": "cs.CR cs.LG"}, "abstract": "39 seconds. That is the timelapse between two consecutive cyber attacks as of 2023. Meaning that by the time you are done reading this abstract, about 1 or 2 additional cyber attacks would have occurred somewhere in the world. In this context of highly increased frequency of cyber threats, Security Operation Centers (SOC) and Computer Emergency Response Teams (CERT) can be overwhelmed. In order to relieve the cybersecurity teams in their investigative effort and help them focus on more added-value tasks, machine learning approaches and methods started to emerge. This paper introduces a novel method, IsoEx, for detecting anomalous and potentially problematic command lines during the investigation of contaminated devices. IsoEx is built around a set of features that leverages the log structure of the command line, as well as its parent/child relationship, to achieve a greater accuracy than traditional methods. To detect anomalies, IsoEx resorts to an unsupervised anomaly detection technique that is both highly sensitive and lightweight. A key contribution of the paper is its emphasis on interpretability, achieved through the features themselves and the application of eXplainable Artificial Intelligence (XAI) techniques and visualizations. This is critical to ensure the adoption of the method by SOC and CERT teams, as the paper argues that the current literature on machine learning for log investigation has not adequately addressed the issue of explainability. This method was proven efficient in a real-life environment as it was built to support a company\\'s SOC and CERT", "url": "https://arxiv.org/abs/2306.09260"}, {"metadata": {"arXiv": "2306.09262 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 16:37:36 ", "Title": "A Heavy-Tailed Algebra for Probabilistic Programming", "Authors": ["Feynman Liang", "Liam Hodgkinson", "Michael W. Mahoney"], "Categories": "stat.ML cs.LG cs.PL", "Comments": ["21 pages", "6 figures"]}, "abstract": "Despite the successes of probabilistic models based on passing noise through neural networks, recent work has identified that such methods often fail to capture tail behavior accurately, unless the tails of the base distribution are appropriately calibrated. To overcome this deficiency, we propose a systematic approach for analyzing the tails of random variables, and we illustrate how this approach can be used during the static analysis (before drawing samples) pass of a probabilistic programming language compiler. To characterize how the tails change under various operations, we develop an algebra which acts on a three-parameter family of tail asymptotics and which is based on the generalized Gamma distribution. Our algebraic operations are closed under addition and multiplication; they are capable of distinguishing sub-Gaussians with differing scales; and they handle ratios sufficiently well to reproduce the tails of most important statistical distributions directly from their definitions. Our empirical results confirm that inference algorithms that leverage our heavy-tailed algebra attain superior performance across a number of density modeling and variational inference tasks.", "url": "https://arxiv.org/abs/2306.09262"}, {"metadata": {"arXiv": "2306.09290 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 17:16:34 ", "Title": "Generalizable Resource Scaling of 5G Slices using Constrained Reinforcement Learning", "Authors": ["Muhammad Sulaiman", "Mahdieh Ahmadi", "Mohammad A. Salahuddin", "Raouf Boutaba", "Aladdin Saleh"], "Categories": "cs.NI cs.LG"}, "abstract": "Network slicing is a key enabler for 5G to support various applications. Slices requested by service providers (SPs) have heterogeneous quality of service (QoS) requirements, such as latency, throughput, and jitter. It is imperative that the 5G infrastructure provider (InP) allocates the right amount of resources depending on the slice's traffic, such that the specified QoS levels are maintained during the slice's lifetime while maximizing resource efficiency. However, there is a non-trivial relationship between the QoS and resource allocation. In this paper, this relationship is learned using a regression-based model. We also leverage a risk-constrained reinforcement learning agent that is trained offline using this model and domain randomization for dynamically scaling slice resources while maintaining the desired QoS level. Our novel approach reduces the effects of network modeling errors since it is model-free and does not require QoS metrics to be mathematically formulated in terms of traffic. In addition, it provides robustness against uncertain network conditions, generalizes to different real-world traffic patterns, and caters to various QoS metrics. The results show that the state-of-the-art approaches can lead to QoS degradation as high as 44.5% when tested on previously unseen traffic. On the other hand, our approach maintains the QoS degradation below a preset 10% threshold on such traffic, while minimizing the allocated resources. Additionally, we demonstrate that the proposed approach is robust against varying network conditions and inaccurate traffic predictions.", "url": "https://arxiv.org/abs/2306.09290"}, {"metadata": {"arXiv": "2306.09297 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 17:25:15 ", "Title": "Fix Fairness, Don't Ruin Accuracy: Performance Aware Fairness Repair using AutoML", "Authors": ["Giang Nguyen", "Sumon Biswas", "Hridesh Rajan"], "Categories": "cs.SE cs.LG", "Comments": ["In Proceedings of The 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2023)"]}, "abstract": "Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias in real-world scenarios. In order to demonstrate the effectiveness of our approach, we evaluated our approach on four fairness problems and 16 different ML models, and our results show a significant improvement over the baseline and existing bias mitigation techniques. Our approach, Fair-AutoML, successfully repaired 60 out of 64 buggy cases, while existing bias mitigation techniques only repaired up to 44 out of 64 cases.", "url": "https://arxiv.org/abs/2306.09297"}, {"metadata": {"arXiv": "2306.09318 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 17:53:14 ", "Title": "Inroads into Autonomous Network Defence using Explained Reinforcement Learning", "Authors": ["Myles Foley", "Mia Wang", "Zoe M", "Chris Hicks", "Vasilios Mavroudis"], "Categories": "cs.CR cs.LG"}, "abstract": "Computer network defence is a complicated task that has necessitated a high degree of human involvement. However, with recent advancements in machine learning, fully autonomous network defence is becoming increasingly plausible. This paper introduces an end-to-end methodology for studying attack strategies, designing defence agents and explaining their operation. First, using state diagrams, we visualise adversarial behaviour to gain insight about potential points of intervention and inform the design of our defensive models. We opt to use a set of deep reinforcement learning agents trained on different parts of the task and organised in a shallow hierarchy. Our evaluation shows that the resulting design achieves a substantial performance improvement compared to prior work. Finally, to better investigate the decision-making process of our agents, we complete our analysis with a feature ablation and importance study.", "url": "https://arxiv.org/abs/2306.09318"}, {"metadata": {"arXiv": "2306.09340 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 17:59:31 ", "Title": "Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking", "Authors": ["Bj\\\"orn Bebensee", "Haejun Lee"], "Categories": "cs.CL cs.LG", "Comments": ["Accepted to ACL 2023"]}, "abstract": "In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30$\\times$ larger D3ST-XXL model by 5.0 points.", "url": "https://arxiv.org/abs/2306.09340"}, {"metadata": {"arXiv": "2306.08397", "Date": "Wed, 14 Jun 2023 09:45:29 ", "Title": "Scalable Neural-Probabilistic Answer Set Programming", "Authors": ["Arseny Skryagin and Daniel Ochs and Devendra Singh Dhami and Kristian Kersting"], "Categories": "cs.AI", "Comments": ["37 pages", "14 figures"]}, "abstract": "The goal of combining the robustness of neural networks and the expressiveness of symbolic methods has rekindled the interest in Neuro-Symbolic AI. Deep Probabilistic Programming Languages (DPPLs) have been developed for probabilistic logic programming to be carried out via the probability estimations of deep neural networks. However, recent SOTA DPPL approaches allow only for limited conditional probabilistic queries and do not offer the power of true joint probability estimation. In our work, we propose an easy integration of tractable probabilistic inference within a DPPL. To this end, we introduce SLASH, a novel DPPL that consists of Neural-Probabilistic Predicates (NPPs) and a logic program, united via answer set programming (ASP). NPPs are a novel design principle allowing for combining all deep model types and combinations thereof to be represented as a single probabilistic predicate. In this context, we introduce a novel $+/-$ notation for answering various types of probabilistic queries by adjusting the atom notations of a predicate. To scale well, we show how to prune the stochastically insignificant parts of the (ground) program, speeding up reasoning without sacrificing the predictive performance. We evaluate SLASH on a variety of different tasks, including the benchmark task of MNIST addition and Visual Question Answering (VQA).", "url": "https://arxiv.org/abs/2306.08397"}, {"metadata": {"arXiv": "2306.08680", "Date": "Wed, 14 Jun 2023 18:02:00 ", "Title": "Temporally Extended Goal Recognition in Fully Observable Non-Deterministic Domain Models", "Authors": ["Ramon Fraga Pereira", "Francesco Fuggitti", "Felipe Meneguzzi", "Giuseppe De Giacomo"], "Categories": "cs.AI", "Comments": ["arXiv admin note: substantial text overlap with arXiv:2103.11692"]}, "abstract": "Goal Recognition is the task of discerning the correct intended goal that an agent aims to achieve, given a set of goal hypotheses, a domain model, and a sequence of observations (i.e., a sample of the plan executed in the environment). Existing approaches assume that goal hypotheses comprise a single conjunctive formula over a single final state and that the environment dynamics are deterministic, preventing the recognition of temporally extended goals in more complex settings. In this paper, we expand goal recognition to temporally extended goals in Fully Observable Non-Deterministic (FOND) planning domain models, focusing on goals on finite traces expressed in Linear Temporal Logic (LTLf) and Pure Past Linear Temporal Logic (PLTLf). We develop the first approach capable of recognizing goals in such settings and evaluate it using different LTLf and PLTLf goals over six FOND planning domain models. Empirical results show that our approach is accurate in recognizing temporally extended goals in different recognition settings.", "url": "https://arxiv.org/abs/2306.08680"}, {"metadata": {"arXiv": "2306.08708", "Date": "Wed, 14 Jun 2023 19:20:43 ", "Title": "AiXpand AI OS -- Decentralized ubiquitous computing MLOps execution engine", "Authors": ["Beatrice Milik", "Stefan Saraev", "Cristian Bleotiu", "Radu Lupaescu", "Bogdan Hobeanu", "Andrei Ionut Damian"], "Categories": "cs.AI cs.DC cs.NI", "Comments": ["preprint"], "ACM-class": "I.2.5; I.2.11"}, "abstract": "Over the past few years, ubiquitous, or pervasive computing has gained popularity as the primary approach for a wide range of applications, including enterprise-grade systems, consumer applications, and gaming systems. Ubiquitous computing refers to the integration of computing technologies into everyday objects and environments, creating a network of interconnected devices that can communicate with each other and with humans. By using ubiquitous computing technologies, communities can become more connected and efficient, with members able to communicate and collaborate more easily. This enabled interconnectedness and collaboration can lead to a more successful and sustainable community. The spread of ubiquitous computing, however, has emphasized the importance of automated learning and smart applications in general. Even though there have been significant strides in Artificial Intelligence and Deep Learning, large scale adoption has been hesitant due to mounting pressure on expensive and highly complex cloud numerical-compute infrastructures. Adopting, and even developing, practical machine learning systems can come with prohibitive costs, not only in terms of complex infrastructures but also of solid expertise in Data Science and Machine Learning. In this paper we present an innovative approach for low-code development and deployment of end-to-end AI cooperative application pipelines. We address infrastructure allocation, costs, and secure job distribution in a fully decentralized global cooperative community based on tokenized economics.", "url": "https://arxiv.org/abs/2306.08708"}, {"metadata": {"arXiv": "2306.08843", "Date": "Thu, 15 Jun 2023 04:08:09 ", "Title": "Real-Time Network-Level Traffic Signal Control: An Explicit Multiagent Coordination Method", "Authors": ["Wanyuan Wang", "Tianchi Qiao", "Jinming Ma", "Jiahui Jin", "Zhibin Li", "Weiwei Wu", "and Yichuan Jian"], "Categories": "cs.AI cs.MA"}, "abstract": "Efficient traffic signal control (TSC) has been one of the most useful ways for reducing urban road congestion. Key to the challenge of TSC includes 1) the essential of real-time signal decision, 2) the complexity in traffic dynamics, and 3) the network-level coordination. Recent efforts that applied reinforcement learning (RL) methods can query policies by mapping the traffic state to the signal decision in real-time, however, is inadequate for unexpected traffic flows. By observing real traffic information, online planning methods can compute the signal decisions in a responsive manner. We propose an explicit multiagent coordination (EMC)-based online planning methods that can satisfy adaptive, real-time and network-level TSC. By multiagent, we model each intersection as an autonomous agent, and the coordination efficiency is modeled by a cost (i.e., congestion index) function between neighbor intersections. By network-level coordination, each agent exchanges messages with respect to cost function with its neighbors in a fully decentralized manner. By real-time, the message passing procedure can interrupt at any time when the real time limit is reached and agents select the optimal signal decisions according to the current message. Moreover, we prove our EMC method can guarantee network stability by borrowing ideas from transportation domain. Finally, we test our EMC method in both synthetic and real road network datasets. Experimental results are encouraging: compared to RL and conventional transportation baselines, our EMC method performs reasonably well in terms of adapting to real-time traffic dynamics, minimizing vehicle travel time and scalability to city-scale road networks.", "url": "https://arxiv.org/abs/2306.08843"}, {"metadata": {"arXiv": "2306.09042", "Date": "Thu, 15 Jun 2023 11:04:30 ", "Title": "A Graphical Formalism for Commonsense Reasoning with Recipes", "Authors": ["Antonis Bikakis", "Aissatou Diallo", "Luke Dickens", "Anthony Hunter", "and Rob Miller"], "Categories": "cs.AI", "Comments": ["10 pages"]}, "abstract": "Whilst cooking is a very important human activity, there has been little consideration given to how we can formalize recipes for use in a reasoning framework. We address this need by proposing a graphical formalization that captures the comestibles (ingredients, intermediate food items, and final products), and the actions on comestibles in the form of a labelled bipartite graph. We then propose formal definitions for comparing recipes, for composing recipes from subrecipes, and for deconstructing recipes into subrecipes. We also introduce and compare two formal definitions for substitution into recipes which are required when there are missing ingredients, or some actions are not possible, or because there is a need to change the final product somehow.", "url": "https://arxiv.org/abs/2306.09042"}, {"metadata": {"arXiv": "2306.09082", "Date": "Thu, 15 Jun 2023 12:25:41 ", "Title": "Behavioral Cloning via Search in Embedded Demonstration Dataset", "Authors": ["Federico Malato", "Florian Leopold", "Ville Hautamaki", "Andrew Melnik"], "Categories": "cs.AI"}, "abstract": "Behavioural cloning uses a dataset of demonstrations to learn a behavioural policy. To overcome various learning and policy adaptation problems, we propose to use latent space to index a demonstration dataset, instantly access similar relevant experiences, and copy behavior from these situations. Actions from a selected similar situation can be performed by the agent until representations of the agent's current situation and the selected experience diverge in the latent space. Thus, we formulate our control problem as a search problem over a dataset of experts' demonstrations. We test our approach on BASALT MineRL-dataset in the latent representation of a Video PreTraining model. We compare our model to state-of-the-art Minecraft agents. Our approach can effectively recover meaningful demonstrations and show human-like behavior of an agent in the Minecraft environment in a wide variety of scenarios. Experimental results reveal that performance of our search-based approach is comparable to trained models, while allowing zero-shot task adaptation by changing the demonstration examples.", "url": "https://arxiv.org/abs/2306.09082"}, {"metadata": {"arXiv": "2306.09138", "Date": "Thu, 15 Jun 2023 13:50:46 ", "Title": "Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases", "Authors": ["Riccardo Zese", "Evelina Lamma", "Fabrizio Riguzzi"], "Categories": "cs.AI cs.LO"}, "abstract": "The necessity to manage inconsistency in Description Logics Knowledge Bases (KBs) has come to the fore with the increasing importance gained by the Semantic Web, where information comes from different sources that constantly change their content and may contain contradictory descriptions when considered either alone or together. Classical reasoning algorithms do not handle inconsistent KBs, forcing the debugging of the KB in order to remove the inconsistency. In this paper, we exploit an existing probabilistic semantics called DISPONTE to overcome this problem and allow queries also in case of inconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE and empirically tested the validity of our proposal. Moreover, we formally compare the presented approach to that of the repair semantics, one of the most established semantics when considering DL reasoning tasks.", "url": "https://arxiv.org/abs/2306.09138"}, {"metadata": {"arXiv": "2306.09309", "Date": "Thu, 15 Jun 2023 17:43:17 ", "Title": "Who Needs to Know? Minimal Knowledge for Optimal Coordination", "Authors": ["Niklas Lauffer", "Ameesh Shah", "Micah Carroll", "Michael Dennis", "Stuart Russell"], "Categories": "cs.AI cs.MA", "Comments": ["To be published at ICML 2023"], "ACM-class": "I.2.6; I.2.11"}, "abstract": "To optimally coordinate with others in cooperative games, it is often crucial to have information about one's collaborators: successful driving requires understanding which side of the road to drive on. However, not every feature of collaborators is strategically relevant: the fine-grained acceleration of drivers may be ignored while maintaining optimal coordination. We show that there is a well-defined dichotomy between strategically relevant and irrelevant information. Moreover, we show that, in dynamic games, this dichotomy has a compact representation that can be efficiently computed via a Bellman backup operator. We apply this algorithm to analyze the strategically relevant information for tasks in both a standard and a partially observable version of the Overcooked environment. Theoretical and empirical results show that our algorithms are significantly more efficient than baselines. Videos are available at https://minknowledge.github.io.", "url": "https://arxiv.org/abs/2306.09309"}, {"metadata": {"arXiv": "2306.07998", "Date": "Mon, 12 Jun 2023 19:57:11 ", "Title": "Contrastive Attention Networks for Attribution of Early Modern Print", "Authors": ["Nikolai Vogler", "Kartik Goyal", "Kishore PV Reddy", "Elizaveta Pertseva", "Samuel V. Lemley", "Christopher N. Warren", "Max G'Sell", "Taylor Berg-Kirkpatrick"], "Categories": "cs.CV cs.AI", "Comments": ["Proceedings of AAAI 2023"]}, "abstract": "In this paper, we develop machine learning techniques to identify unknown printers in early modern (c.~1500--1800) English printed books. Specifically, we focus on matching uniquely damaged character type-imprints in anonymously printed books to works with known printers in order to provide evidence of their origins. Until now, this work has been limited to manual investigations by analytical bibliographers. We present a Contrastive Attention-based Metric Learning approach to identify similar damage across character image pairs, which is sensitive to very subtle differences in glyph shapes, yet robust to various confounding sources of noise associated with digitized historical books. To overcome the scarce amount of supervised data, we design a random data synthesis procedure that aims to simulate bends, fractures, and inking variations induced by the early printing process. Our method successfully improves downstream damaged type-imprint matching among printed works from this period, as validated by in-domain human experts. The results of our approach on two important philosophical works from the Early Modern period demonstrate potential to extend the extant historical research about the origins and content of these books.", "url": "https://arxiv.org/abs/2306.07998"}, {"metadata": {"arXiv": "2306.08129", "Date": "Tue, 13 Jun 2023 20:50:22 ", "Title": "AVIS: Autonomous Visual Information Seeking with Large Language Models", "Authors": ["Ziniu Hu", "Ahmet Iscen", "Chen Sun", "Kai-Wei Chang", "Yizhou Sun", "David A Ross", "Cordelia Schmid", "Alireza Fathi"], "Categories": "cs.CV cs.AI cs.CL"}, "abstract": "In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as \"What event is commemorated by the building depicted in this image?\", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-intensive visual question answering benchmarks such as Infoseek and OK-VQA.", "url": "https://arxiv.org/abs/2306.08129"}, {"metadata": {"arXiv": "2306.08213", "Date": "Wed, 14 Jun 2023 02:57:23 ", "Title": "SMC-UDA: Structure-Modal Constraint for Unsupervised Cross-Domain Renal Segmentation", "Authors": ["Zhusi Zhong", "Jie Li", "Lulu Bi", "Li Yang", "Ihab Kamel", "Rama Chellappa", "Xinbo Gao", "Harrison Bai", "Zhicheng Jiao"], "Categories": "cs.CV cs.AI", "Comments": ["conference"]}, "abstract": "Medical image segmentation based on deep learning often fails when deployed on images from a different domain. The domain adaptation methods aim to solve domain-shift challenges, but still face some problems. The transfer learning methods require annotation on the target domain, and the generative unsupervised domain adaptation (UDA) models ignore domain-specific representations, whose generated quality highly restricts segmentation performance. In this study, we propose a novel Structure-Modal Constrained (SMC) UDA framework based on a discriminative paradigm and introduce edge structure as a bridge between domains. The proposed multi-modal learning backbone distills structure information from image texture to distinguish domain-invariant edge structure. With the structure-constrained self-learning and progressive ROI, our methods segment the kidney by locating the 3D spatial structure of the edge. We evaluated SMC-UDA on public renal segmentation datasets, adapting from the labeled source domain (CT) to the unlabeled target domain (CT/MRI). The experiments show that our proposed SMC-UDA has a strong generalization and outperforms generative UDA methods.", "url": "https://arxiv.org/abs/2306.08213"}, {"metadata": {"arXiv": "2306.08240", "Date": "Wed, 14 Jun 2023 04:56:31 ", "Title": "Semi-supervised Cell Recognition under Point Supervision", "Authors": ["Zhongyi Shui", "Yizhi Zhao", "Sunyi Zheng", "Yunlong Zhang", "Honglin Li", "Shichuan Zhang", "Xiaoxuan Yu", "Chenglu Zhu", "Lin Yang"], "Categories": "cs.CV cs.AI"}, "abstract": "Cell recognition is a fundamental task in digital histopathology image analysis. Point-based cell recognition (PCR) methods normally require a vast number of annotations, which is extremely costly, time-consuming and labor-intensive. Semi-supervised learning (SSL) can provide a shortcut to make full use of cell information in gigapixel whole slide images without exhaustive labeling. However, research into semi-supervised point-based cell recognition (SSPCR) remains largely overlooked. Previous SSPCR works are all built on density map-based PCR models, which suffer from unsatisfactory accuracy, slow inference speed and high sensitivity to hyper-parameters. To address these issues, end-to-end PCR models are proposed recently. In this paper, we develop a SSPCR framework suitable for the end-to-end PCR models for the first time. Overall, we use the current models to generate pseudo labels for unlabeled images, which are in turn utilized to supervise the models training. Besides, we introduce a co-teaching strategy to overcome the confirmation bias problem that generally exists in self-training. A distribution alignment technique is also incorporated to produce high-quality, unbiased pseudo labels for unlabeled data. Experimental results on four histopathology datasets concerning different types of staining styles show the effectiveness and versatility of the proposed framework. Code is available at \\textcolor{magenta}{\\url{https://github.com/windygooo/SSPCR}", "url": "https://arxiv.org/abs/2306.08240"}, {"metadata": {"arXiv": "2306.08251", "Date": "Wed, 14 Jun 2023 05:34:02 ", "Title": "GBSD: Generative Bokeh with Stage Diffusion", "Authors": ["Jieren Deng", "Xin Zhou", "Hao Tian", "Zhihong Pan", "and Derek Aguiar"], "Categories": "cs.CV cs.AI"}, "abstract": "The bokeh effect is an artistic technique that blurs out-of-focus areas in a photograph and has gained interest due to recent developments in text-to-image synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior work on rendering bokeh effects have focused on post hoc image manipulation to produce similar blurring effects in existing photographs using classical computer graphics or neural rendering techniques, but have either depth discontinuity artifacts or are restricted to reproducing bokeh effects that are present in the training data. More recent diffusion based models can synthesize images with an artistic style, but either require the generation of high-dimensional masks, expensive fine-tuning, or affect global image characteristics. In this paper, we present GBSD, the first generative text-to-image model that synthesizes photorealistic images with a bokeh style. Motivated by how image synthesis occurs progressively in diffusion models, our approach combines latent diffusion models with a 2-stage conditioning algorithm to render bokeh effects on semantically defined objects. Since we can focus the effect on objects, this semantic bokeh effect is more versatile than classical rendering techniques. We evaluate GBSD both quantitatively and qualitatively and demonstrate its ability to be applied in both text-to-image and image-to-image settings.", "url": "https://arxiv.org/abs/2306.08251"}, {"metadata": {"arXiv": "2306.08314", "Date": "Wed, 14 Jun 2023 07:33:43 ", "Title": "Automated Speaker Independent Visual Speech Recognition: A Comprehensive Survey", "Authors": ["Praneeth Nemani", "G. Sai Krishna", "Supriya Kundrapu"], "Categories": "cs.CV cs.AI"}, "abstract": "Speaker-independent VSR is a complex task that involves identifying spoken words or phrases from video recordings of a speaker's facial movements. Over the years, there has been a considerable amount of research in the field of VSR involving different algorithms and datasets to evaluate system performance. These efforts have resulted in significant progress in developing effective VSR models, creating new opportunities for further research in this area. This survey provides a detailed examination of the progression of VSR over the past three decades, with a particular emphasis on the transition from speaker-dependent to speaker-independent systems. We also provide a comprehensive overview of the various datasets used in VSR research and the preprocessing techniques employed to achieve speaker independence. The survey covers the works published from 1990 to 2023, thoroughly analyzing each work and comparing them on various parameters. This survey provides an in-depth analysis of speaker-independent VSR systems evolution from 1990 to 2023. It outlines the development of VSR systems over time and highlights the need to develop end-to-end pipelines for speaker-independent VSR. The pictorial representation offers a clear and concise overview of the techniques used in speaker-independent VSR, thereby aiding in the comprehension and analysis of the various methodologies. The survey also highlights the strengths and limitations of each technique and provides insights into developing novel approaches for analyzing visual speech cues. Overall, This comprehensive review provides insights into the current state-of-the-art speaker-independent VSR and highlights potential areas for future research.", "url": "https://arxiv.org/abs/2306.08314"}, {"metadata": {"arXiv": "2306.08336", "Date": "Wed, 14 Jun 2023 08:08:08 ", "Title": "Global-Local Processing in Convolutional Neural Networks", "Authors": ["Zahra Rezvani", "Soroor Shekarizeh", "Mohammad Sabokrou"], "Categories": "cs.CV cs.AI"}, "abstract": "Convolutional Neural Networks (CNNs) have achieved outstanding performance on image processing challenges. Actually, CNNs imitate the typically developed human brain structures at the micro-level (Artificial neurons). At the same time, they distance themselves from imitating natural visual perception in humans at the macro architectures (high-level cognition). Recently it has been investigated that CNNs are highly biased toward local features and fail to detect the global aspects of their input. Nevertheless, the literature offers limited clues on this problem. To this end, we propose a simple yet effective solution inspired by the unconscious behavior of the human pupil. We devise a simple module called Global Advantage Stream (GAS) to learn and capture the holistic features of input samples (i.e., the global features). Then, the GAS features were combined with a CNN network as a plug-and-play component called the Global/Local Processing (GLP) model. The experimental results confirm that this stream improves the accuracy with an insignificant additional computational/temporal load and makes the network more robust to adversarial attacks. Furthermore, investigating the interpretation of the model shows that it learns a more holistic representation similar to the perceptual system of healthy humans", "url": "https://arxiv.org/abs/2306.08336"}, {"metadata": {"arXiv": "2306.08487", "Date": "Wed, 14 Jun 2023 13:07:48 ", "Title": "Recognizing Unseen Objects via Multimodal Intensive Knowledge Graph Propagation", "Authors": ["Likang Wu", "Zhi Li", "Hongke Zhao", "Zhefeng Wang", "Qi Liu", "Baoxing Huai", "Nicholas Jing Yuan", "Enhong Chen"], "Categories": "cs.CV cs.AI", "Comments": ["arXiv admin note: text overlap with arXiv:1805.11724 by other authors"]}, "abstract": "Zero-Shot Learning (ZSL), which aims at automatically recognizing unseen objects, is a promising learning paradigm to understand new real-world knowledge for machines continuously. Recently, the Knowledge Graph (KG) has been proven as an effective scheme for handling the zero-shot task with large-scale and non-attribute data. Prior studies always embed relationships of seen and unseen objects into visual information from existing knowledge graphs to promote the cognitive ability of the unseen data. Actually, real-world knowledge is naturally formed by multimodal facts. Compared with ordinary structural knowledge from a graph perspective, multimodal KG can provide cognitive systems with fine-grained knowledge. For example, the text description and visual content can depict more critical details of a fact than only depending on knowledge triplets. Unfortunately, this multimodal fine-grained knowledge is largely unexploited due to the bottleneck of feature alignment between different modalities. To that end, we propose a multimodal intensive ZSL framework that matches regions of images with corresponding semantic embeddings via a designed dense attention module and self-calibration loss. It makes the semantic transfer process of our ZSL framework learns more differentiated knowledge between entities. Our model also gets rid of the performance limitation of only using rough global features. We conduct extensive experiments and evaluate our model on large-scale real-world data. The experimental results clearly demonstrate the effectiveness of the proposed model in standard zero-shot classification tasks.", "url": "https://arxiv.org/abs/2306.08487"}, {"metadata": {"arXiv": "2306.08541", "Date": "Wed, 14 Jun 2023 14:40:50 ", "Title": "Zero-Shot 3D Shape Sketch View Similarity and Retrieval", "Authors": ["Gianluca Berardi and Yulia Gryaditskaya"], "Categories": "cs.CV cs.AI"}, "abstract": "We conduct a detailed study of the ability of pretrained on pretext tasks ViT and ResNet feature layers to quantify the similarity between pairs of 2D sketch views of individual 3D shapes. We assess the performance in terms of the models' abilities to retrieve similar views and ground-truth 3D shapes. Going beyond naive zero-shot performance study, we investigate alternative fine-tuning strategies on one or several shape classes, and their generalization to other shape classes. Leveraging progress in NPR (Non-Photo Realistic) rendering, we generate synthetic sketch views in several styles which we use to fine-tune pretrained foundation models using contrastive learning. We study how the scale of an object in a sketch affects the similarity of features at different network layers. We observe that depending on the scale, different feature layers can be more indicative of shape similarities in sketch views. However, we find that similar object scales result in the best performance of ViT and ResNet. In summary, we show that careful selection of a fine-tuning strategy allows us to obtain consistent improvement in zero-shot shape retrieval accuracy. We believe that our work will have a significant impact on research in the sketch domain, providing insights and guidance on how to adopt large pretrained models as perceptual losses.", "url": "https://arxiv.org/abs/2306.08541"}, {"metadata": {"arXiv": "2306.08641", "Date": "Wed, 14 Jun 2023 17:15:01 ", "Title": "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models", "Authors": ["Lingxi Xie", "Longhui Wei", "Xiaopeng Zhang", "Kaifeng Bi", "Xiaotao Gu", "Jianlong Chang", "Qi Tian"], "Categories": "cs.CV cs.AI", "Comments": ["17 pages", "14 figures", "technical report", "expected to be updated in the near future"]}, "abstract": "The AI community has been pursuing algorithms known as artificial general intelligence (AGI) that apply to any kind of real-world problem. Recently, chat systems powered by large language models (LLMs) emerge and rapidly become a promising direction to achieve AGI in natural language processing (NLP), but the path towards AGI in computer vision (CV) remains unclear. One may owe the dilemma to the fact that visual signals are more complex than language signals, yet we are interested in finding concrete reasons, as well as absorbing experiences from GPT and LLMs to solve the problem. In this paper, we start with a conceptual definition of AGI and briefly review how NLP solves a wide range of tasks via a chat system. The analysis inspires us that unification is the next important goal of CV. But, despite various efforts in this direction, CV is still far from a system like GPT that naturally integrates all tasks. We point out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world. We then imagine a pipeline that puts a CV algorithm (i.e., an agent) in world-scale, interactable environments, pre-trains it to predict future frames with respect to its action, and then fine-tunes it with instruction to accomplish various tasks. We expect substantial research and engineering efforts to push the idea forward and scale it up, for which we share our perspectives on future research directions.", "url": "https://arxiv.org/abs/2306.08641"}, {"metadata": {"arXiv": "2306.08687", "Date": "Wed, 14 Jun 2023 18:12:15 ", "Title": "Norm-guided latent space exploration for text-to-image generation", "Authors": ["Dvir Samuel", "Rami Ben-Ari", "Nir Darshan", "Haggai Maron", "Gal Chechik"], "Categories": "cs.CV cs.AI"}, "abstract": "Text-to-image diffusion models show great potential in synthesizing a large variety of concepts in new compositions and scenarios. However, their latent seed space is still not well understood and has been shown to have an impact in generating new and rare concepts. Specifically, simple operations like interpolation and centroid finding work poorly with the standard Euclidean and spherical metrics in the latent space. This paper makes the observation that current training procedures make diffusion models biased toward inputs with a narrow range of norm values. This has strong implications for methods that rely on seed manipulation for image generation that can be further applied to few-shot and long-tail learning tasks. To address this issue, we propose a novel method for interpolating between two seeds and demonstrate that it defines a new non-Euclidean metric that takes into account a norm-based prior on seeds. We describe a simple yet efficient algorithm for approximating this metric and use it to further define centroids in the latent seed space. We show that our new interpolation and centroid evaluation techniques significantly enhance the generation of rare concept images. This further leads to state-of-the-art performance on few-shot and long-tail benchmarks, improving prior approach in terms of generation speed, image quality, and semantic content.", "url": "https://arxiv.org/abs/2306.08687"}, {"metadata": {"arXiv": "2306.08814", "Date": "Thu, 15 Jun 2023 02:12:45 ", "Title": "A Self-Supervised Miniature One-Shot Texture Segmentation (MOSTS) Model for Real-Time Robot Navigation and Embedded Applications", "Authors": ["Yu Chen", "Chirag Rastogi", "Zheyu Zhou", "and William R. Norris"], "Categories": "cs.CV cs.AI cs.RO", "Comments": ["10 pages", "12 figures"]}, "abstract": "Determining the drivable area, or free space segmentation, is critical for mobile robots to navigate indoor environments safely. However, the lack of coherent markings and structures (e.g., lanes, curbs, etc.) in indoor spaces places the burden of traversability estimation heavily on the mobile robot. This paper explores the use of a self-supervised one-shot texture segmentation framework and an RGB-D camera to achieve robust drivable area segmentation. With a fast inference speed and compact size, the developed model, MOSTS is ideal for real-time robot navigation and various embedded applications. A benchmark study was conducted to compare MOSTS's performance with existing one-shot texture segmentation models to evaluate its performance. Additionally, a validation dataset was built to assess MOSTS's ability to perform texture segmentation in the wild, where it effectively identified small low-lying objects that were previously undetectable by depth measurements. Further, the study also compared MOSTS's performance with two State-Of-The-Art (SOTA) indoor semantic segmentation models, both quantitatively and qualitatively. The results showed that MOSTS offers comparable accuracy with up to eight times faster inference speed in indoor drivable area segmentation.", "url": "https://arxiv.org/abs/2306.08814"}, {"metadata": {"arXiv": "2306.08861", "Date": "Thu, 15 Jun 2023 05:12:54 ", "Title": "Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization", "Authors": ["Makito Kobayashi", "Chen-Chieh Liao", "Keito Inoue", "Sentaro Yojima", "Masafumi Takahashi"], "Categories": "cs.CV cs.AI"}, "abstract": "In this work, we proposed a new style-diverse dataset for the domain of motion style transfer. The motion dataset uses an industrial-standard human bone structure and thus is industry-ready to be plugged into 3D characters for many projects. We claim the challenges in motion style transfer and encourage future work in this domain by releasing the proposed motion dataset to the public. We conduct a comprehensive study on motion style transfer in the experiment using the state-of-the-art method, and the results show the proposed dataset's validity for the motion style transfer task.", "url": "https://arxiv.org/abs/2306.08861"}, {"metadata": {"arXiv": "2306.08889", "Date": "Thu, 15 Jun 2023 06:45:46 ", "Title": "Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models", "Authors": ["Ishaan Singh Rawal", "Shantanu Jaiswal", "Basura Fernando", "Cheston Tan"], "Categories": "cs.CV cs.AI"}, "abstract": "While VideoQA Transformer models demonstrate competitive performance on standard benchmarks, the reasons behind their success remain unclear. Do these models jointly capture and leverage the rich multimodal structures and dynamics from video and text? Or are they merely exploiting shortcuts to achieve high scores? We analyze this with $\\textit{QUAG}$ (QUadrant AveraGe), a lightweight and non-parametric probe that systematically ablates the model's coupled multimodal understanding during inference. Surprisingly, QUAG reveals that the models manage to maintain high performance even when injected with multimodal sub-optimality. Additionally, even after replacing self-attention in multimodal fusion blocks with \"QUAG-attention\", a simplistic and less-expressive variant of self-attention, the models maintain high performance. This means that current VideoQA benchmarks and their metrics do not penalize shortcuts that discount joint multimodal understanding. Motivated by this, we propose the $\\textit{CLAVI}$ (Counterfactual in LAnguage and VIdeo) benchmark, a diagnostic dataset for benchmarking coupled multimodal understanding in VideoQA through counterfactuals. CLAVI consists of temporal questions and videos that are augmented to curate balanced counterfactuals in language and video domains. Hence, it incentivizes, and identifies the reliability of learnt multimodal representations. We evaluate CLAVI and find that models achieve high performance on multimodal shortcut instances, but have very poor performance on the counterfactuals. Hence, we position CLAVI as a litmus test to identify, diagnose and improve the sub-optimality of learnt multimodal VideoQA representations which the current benchmarks are unable to assess.", "url": "https://arxiv.org/abs/2306.08889"}, {"metadata": {"arXiv": "2306.08913", "Date": "Thu, 15 Jun 2023 07:32:10 ", "Title": "Advancing Volumetric Medical Image Segmentation via Global-Local Masked Autoencoder", "Authors": ["Jia-Xin Zhuang", "Luyang Luo", "Hao Chen"], "Categories": "cs.CV cs.AI"}, "abstract": "Masked autoencoder (MAE) has emerged as a promising self-supervised pretraining technique to enhance the representation learning of a neural network without human intervention. To adapt MAE onto volumetric medical images, existing methods exhibit two challenges: first, the global information crucial for understanding the clinical context of the holistic data is lacked; second, there was no guarantee of stabilizing the representations learned from the randomly masked inputs. To tackle these limitations, we proposed Global-Local Masked AutoEncoder (GL-MAE), a simple yet effective self-supervised pre-training strategy. GL-MAE reconstructs both the masked global and masked local volumes, which enables learning the essential local details as well as the global context. We further introduced global-to-global consistency and local-to-global correspondence via global-guided consistency learning to enhance and stabilize the representation learning of the masked volumes. Finetuning results on multiple datasets illustrate the superiority of our method over other state-of-the-art self-supervised algorithms, demonstrating its effectiveness on versatile volumetric medical image segmentation tasks, even when annotations are scarce. Codes and models will be released upon acceptance.", "url": "https://arxiv.org/abs/2306.08913"}, {"metadata": {"arXiv": "2306.09117", "Date": "Thu, 15 Jun 2023 13:23:57 ", "Title": "UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering", "Authors": ["Mingjie Pan", "Li Liu", "Jiaming Liu", "Peixiang Huang", "Longlong Wang", "Shanghang Zhang", "Shaoqing Xu", "Zhiyi Lai", "Kuiyuan Yang"], "Categories": "cs.CV cs.AI"}, "abstract": "In this technical report, we present our solution, named UniOCC, for the Vision-Centric 3D occupancy prediction track in the nuScenes Open Dataset Challenge at CVPR 2023. Existing methods for occupancy prediction primarily focus on optimizing projected features on 3D volume space using 3D occupancy labels. However, the generation process of these labels is complex and expensive (relying on 3D semantic annotations), and limited by voxel resolution, they cannot provide fine-grained spatial semantics. To address this limitation, we propose a novel Unifying Occupancy (UniOcc) prediction method, explicitly imposing spatial geometry constraint and complementing fine-grained semantic supervision through volume ray rendering. Our method significantly enhances model performance and demonstrates promising potential in reducing human annotation costs. Given the laborious nature of annotating 3D occupancy, we further introduce a Depth-aware Teacher Student (DTS) framework to enhance prediction accuracy using unlabeled data. Our solution achieves 51.27\\% mIoU on the official leaderboard with single model, placing 3rd in this challenge.", "url": "https://arxiv.org/abs/2306.09117"}, {"metadata": {"arXiv": "2306.09179", "Date": "Thu, 15 Jun 2023 14:58:21 ", "Title": "Neural World Models for Computer Vision", "Authors": ["Anthony Hu"], "Categories": "cs.CV cs.AI cs.RO", "Comments": ["PhD thesis"]}, "abstract": "Humans navigate in their environment by learning a mental model of the world through passive observation and active interaction. Their world model allows them to anticipate what might happen next and act accordingly with respect to an underlying objective. Such world models hold strong promises for planning in complex environments like in autonomous driving. A human driver, or a self-driving system, perceives their surroundings with their eyes or their cameras. They infer an internal representation of the world which should: (i) have spatial memory (e.g. occlusions), (ii) fill partially observable or noisy inputs (e.g. when blinded by sunlight), and (iii) be able to reason about unobservable events probabilistically (e.g. predict different possible futures). They are embodied intelligent agents that can predict, plan, and act in the physical world through their world model. In this thesis we present a general framework to train a world model and a policy, parameterised by deep neural networks, from camera observations and expert demonstrations. We leverage important computer vision concepts such as geometry, semantics, and motion to scale world models to complex urban driving scenes. First, we propose a model that predicts important quantities in computer vision: depth, semantic segmentation, and optical flow. We then use 3D geometry as an inductive bias to operate in the bird's-eye view space. We present for the first time a model that can predict probabilistic future trajectories of dynamic agents in bird's-eye view from 360{\\deg} surround monocular cameras only. Finally, we demonstrate the benefits of learning a world model in closed-loop driving. Our model can jointly predict static scene, dynamic scene, and ego-behaviour in an urban driving environment.", "url": "https://arxiv.org/abs/2306.09179"}, {"metadata": {"arXiv": "2306.09265", "Date": "Thu, 15 Jun 2023 16:39:24 ", "Title": "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models", "Authors": ["Peng Xu", "Wenqi Shao", "Kaipeng Zhang", "Peng Gao", "Shuo Liu", "Meng Lei", "Fanqing Meng", "Siyuan Huang", "Yu Qiao", "Ping Luo"], "Categories": "cs.CV cs.AI", "Comments": ["28 pages", "10 figures", "a comprehensive evaluation of large vision-language models"]}, "abstract": "Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena", "url": "https://arxiv.org/abs/2306.09265"}, {"metadata": {"arXiv": "2306.09341", "Date": "Thu, 15 Jun 2023 17:59:31 ", "Title": "Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis", "Authors": ["Xiaoshi Wu", "Yiming Hao", "Keqiang Sun", "Yixiong Chen", "Feng Zhu", "Rui Zhao", "Hongsheng Li"], "Categories": "cs.CV cs.AI cs.DB", "Comments": ["19 pages", "10 figures"]}, "abstract": "Recent text-to-image generative models can generate high-fidelity images from text inputs, but the quality of these generated images cannot be accurately evaluated by existing evaluation metrics. To address this issue, we introduce Human Preference Dataset v2 (HPD v2), a large-scale dataset that captures human preferences on images from a wide range of sources. HPD v2 comprises 798,090 human preference choices on 430,060 pairs of images, making it the largest dataset of its kind. The text prompts and images are deliberately collected to eliminate potential bias, which is a common issue in previous datasets. By fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a scoring model that can more accurately predict text-generated images' human preferences. Our experiments demonstrate that HPS v2 generalizes better than previous metrics across various image distributions and is responsive to algorithmic improvements of text-to-image generative models, making it a preferable evaluation metric for these models. We also investigate the design of the evaluation prompts for text-to-image generative models, to make the evaluation stable, fair and easy-to-use. Finally, we establish a benchmark for text-to-image generative models using HPS v2, which includes a set of recent text-to-image models from the academia, community and industry. The code and dataset is / will be available at https://github.com/tgxs002/HPSv2.", "url": "https://arxiv.org/abs/2306.09341"}, {"metadata": {"arXiv": "2306.08206", "Date": "Wed, 14 Jun 2023 02:19:59 ", "Title": "Ball Trajectory Inference from Multi-Agent Sports Contexts Using Set Transformer and Hierarchical Bi-LSTM", "Authors": ["Hyunsung Kim", "Han-Jun Choi", "Chang Jo Kim", "Jinsung Yoon", "Sang-Ki Ko"], "Categories": "cs.MA cs.AI", "MSC-class": "68T20 (Primary) 68U35, 68T30 (Secondary)", "DOI": "10.1145/3580305.3599779"}, "abstract": "As artificial intelligence spreads out to numerous fields, the application of AI to sports analytics is also in the spotlight. However, one of the major challenges is the difficulty of automated acquisition of continuous movement data during sports matches. In particular, it is a conundrum to reliably track a tiny ball on a wide soccer pitch with obstacles such as occlusion and imitations. Tackling the problem, this paper proposes an inference framework of ball trajectory from player trajectories as a cost-efficient alternative to ball tracking. We combine Set Transformers to get permutation-invariant and equivariant representations of the multi-agent contexts with a hierarchical architecture that intermediately predicts the player ball possession to support the final trajectory inference. Also, we introduce the reality loss term and postprocessing to secure the estimated trajectories to be physically realistic. The experimental results show that our model provides natural and accurate trajectories as well as admissible player ball possession at the same time. Lastly, we suggest several practical applications of our framework including missing trajectory imputation, semi-automated pass annotation, automated zoom-in for match broadcasting, and calculating possession-wise running performance metrics.", "url": "https://arxiv.org/abs/2306.08206"}, {"metadata": {"arXiv": "2306.08511", "Date": "Wed, 14 Jun 2023 13:55:25 ", "Title": "Measuring and Controlling Divisiveness in Rank Aggregation", "Authors": ["Rachael Colley", "Umberto Grandi", "C\\'esar Hidalgo", "Mariana Macedo and Carlos Navarrete"], "Categories": "cs.MA cs.AI cs.CY", "Comments": ["25 pages", "8 figures"]}, "abstract": "In rank aggregation, members of a population rank issues to decide which are collectively preferred. We focus instead on identifying divisive issues that express disagreements among the preferences of individuals. We analyse the properties of our divisiveness measures and their relation to existing notions of polarisation. We also study their robustness under incomplete preferences and algorithms for control and manipulation of divisiveness. Our results advance our understanding of how to quantify disagreements in collective decision-making.", "url": "https://arxiv.org/abs/2306.08511"}, {"metadata": {"arXiv": "2306.08651", "Date": "Wed, 14 Jun 2023 17:30:57 ", "Title": "Toward Grounded Social Reasoning", "Authors": ["Minae Kwon", "Hengyuan Hu", "Vivek Myers", "Siddharth Karamcheti", "Anca Dragan", "Dorsa Sadigh"], "Categories": "cs.RO cs.AI"}, "abstract": "Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not socially appropriate to disassemble the sports car and put it away as part of the \"tidying\". How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable social reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and *actively gather information from the environment* that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded social reasoning. To evaluate our framework at scale, we release the MessySurfaces dataset which contains images of 70 real-world surfaces that need to be cleaned. We additionally illustrate our approach with a robot on 2 carefully designed surfaces. We find an average 12.9% improvement on the MessySurfaces benchmark and an average 15% improvement on the robot experiments over baselines that do not use active perception. The dataset, code, and videos of our approach can be found at https://minaek.github.io/groundedsocialreasoning.", "url": "https://arxiv.org/abs/2306.08651"}, {"metadata": {"arXiv": "2306.08815", "Date": "Thu, 15 Jun 2023 02:18:21 ", "Title": "Decentralized Social Navigation with Non-Cooperative Robots via Bi-Level Optimization", "Authors": ["Rohan Chandra", "Rahul Menon", "Zayne Sprague", "Arya Anantula", "Joydeep Biswas"], "Categories": "cs.RO cs.AI cs.MA", "Comments": ["Submitted to IROS 2023"]}, "abstract": "This paper presents a fully decentralized approach for realtime non-cooperative multi-robot navigation in social mini-games, such as navigating through a narrow doorway or negotiating right of way at a corridor intersection. Our contribution is a new realtime bi-level optimization algorithm, in which the top-level optimization consists of computing a fair and collision-free ordering followed by the bottom-level optimization which plans optimal trajectories conditioned on the ordering. We show that, given such a priority order, we can impose simple kinodynamic constraints on each robot that are sufficient for it to plan collision-free trajectories with minimal deviation from their preferred velocities, similar to how humans navigate in these scenarios. We successfully deploy the proposed algorithm in the real world using F$1/10$ robots, a Clearpath Jackal, and a Boston Dynamics Spot as well as in simulation using the SocialGym 2.0 multi-agent social navigation simulator, in the doorway and corridor intersection scenarios. We compare with state-of-the-art social navigation methods using multi-agent reinforcement learning, collision avoidance algorithms, and crowd simulation models. We show that $(i)$ classical navigation performs $44\\%$ better than the state-of-the-art learning-based social navigation algorithms, $(ii)$ without a scheduling protocol, our approach results in collisions in social mini-games $(iii)$ our approach yields $2\\times$ and $5\\times$ fewer velocity changes than CADRL in doorways and intersections, and finally $(iv)$ bi-level navigation in doorways at a flow rate of $2.8 - 3.3$ (ms)$^{-1}$ is comparable to flow rate in human navigation at a flow rate of $4$ (ms)$^{-1}$.", "url": "https://arxiv.org/abs/2306.08815"}, {"metadata": {"arXiv": "2306.07983 (*cross-listing*)", "Date": "Fri, 02 Jun 2023 14:52:32 ", "Title": "A Hybrid Approach for Smart Alert Generation", "Authors": ["Yao Zhao", "Sophine Zhang", "Zhiyuan Yao"], "Categories": "cs.NI cs.AI"}, "abstract": "Anomaly detection is an important task in network management. However, deploying intelligent alert systems in real-world large-scale networking systems is challenging when we take into account (i) scalability, (ii) data heterogeneity, and (iii) generalizability and maintainability. In this paper, we propose a hybrid model for an alert system that combines statistical models with a whitelist mechanism to tackle these challenges and reduce false positive alerts. The statistical models take advantage of a large database to detect anomalies in time-series data, while the whitelist filters out persistently alerted nodes to further reduce false positives. Our model is validated using qualitative data from customer support cases. Future work includes more feature engineering and input data, as well as including human feedback in the model development process.", "url": "https://arxiv.org/abs/2306.07983"}, {"metadata": {"arXiv": "2306.07999 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 00:45:54 ", "Title": "Diagnostic test accuracy (DTA) of artificial intelligence in digital pathology: a systematic review, meta-analysis and quality assessment", "Authors": ["Clare McGenity", "Emily Clarke", "Charlotte Jennings", "Gillian Matthews", "Caroline Cartlidge", "Henschel Freduah-Agyemang", "Deborah Stocken", "Darren Treanor"], "Categories": "physics.med-ph cs.AI cs.CV eess.IV q-bio.QM", "Comments": ["22 pages", "5 figures", "8 tables + Supplementary materials"], "ACM-class": "I.2.1"}, "abstract": "Ensuring diagnostic performance of AI models before clinical use is key to the safe and successful adoption of these technologies. Studies reporting AI applied to digital pathology images for diagnostic purposes have rapidly increased in number in recent years. The aim of this work is to provide an overview of the diagnostic accuracy of AI in digital pathology images from all areas of pathology. This systematic review and meta-analysis included diagnostic accuracy studies using any type of artificial intelligence applied to whole slide images (WSIs) in any disease type. The reference standard was diagnosis through histopathological assessment and / or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We identified 2976 studies, of which 100 were included in the review and 48 in the full meta-analysis. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analysis was performed using a bivariate random effects model. 100 studies were identified for inclusion, equating to over 152,000 whole slide images (WSIs) and representing many disease types. Of these, 48 studies were included in the meta-analysis. These studies reported a mean sensitivity of 96.3% (CI 94.1-97.7) and mean specificity of 93.3% (CI 90.5-95.4) for AI. There was substantial heterogeneity in study design and all 100 studies identified for inclusion had at least one area at high or unclear risk of bias. This review provides a broad overview of AI performance across applications in whole slide imaging. However, there is huge variability in study design and available performance data, with details around the conduct of the study and make up of the datasets frequently missing. Overall, AI offers good accuracy when applied to WSIs but requires more rigorous evaluation of its performance.", "url": "https://arxiv.org/abs/2306.07999"}, {"metadata": {"arXiv": "2306.08010 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 10:58:05 ", "Title": "Domain Information Control at Inference Time for Acoustic Scene Classification", "Authors": ["Shahed Masoudian", "Khaled Koutini", "Markus Schedl", "Gerhard Widmer", "Navid Rekabsaz"], "Categories": "cs.SD cs.AI eess.AS"}, "abstract": "Domain shift is considered a challenge in machine learning as it causes significant degradation of model performance. In the Acoustic Scene Classification task (ASC), domain shift is mainly caused by different recording devices. Several studies have already targeted domain generalization to improve the performance of ASC models on unseen domains, such as new devices. Recently, the Controllable Gate Adapter ConGater has been proposed in Natural Language Processing to address the biased training data problem. ConGater allows controlling the debiasing process at inference time. ConGater's main advantage is the continuous and selective debiasing of a trained model, during inference. In this work, we adapt ConGater to the audio spectrogram transformer for an acoustic scene classification task. We show that ConGater can be used to selectively adapt the learned representations to be invariant to device domain shifts such as recording devices. Our analysis shows that ConGater can progressively remove device information from the learned representations and improve the model generalization, especially under domain shift conditions (e.g. unseen devices). We show that information removal can be extended to both device and location domain. Finally, we demonstrate ConGater's ability to enhance specific device performance without further training.", "url": "https://arxiv.org/abs/2306.08010"}, {"metadata": {"arXiv": "2306.08020 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 15:15:31 ", "Title": "Curatr: A Platform for Semantic Analysis and Curation of Historical Literary Texts", "Authors": ["Susan Leavy", "Gerardine Meaney", "Karen Wade and Derek Greene"], "Categories": "cs.CL cs.AI", "Comments": ["12 pages"], "Journal-ref": "Metadata and Semantic Research (MTSR 2019), Communications in Computer and Information Science, vol 1057. Springer, Cham", "DOI": "10.1007/978-3-030-36599-8_31"}, "abstract": "The increasing availability of digital collections of historical and contemporary literature presents a wealth of possibilities for new research in the humanities. The scale and diversity of such collections however, presents particular challenges in identifying and extracting relevant content. This paper presents Curatr, an online platform for the exploration and curation of literature with machine learning-supported semantic search, designed within the context of digital humanities scholarship. The platform provides a text mining workflow that combines neural word embeddings with expert domain knowledge to enable the generation of thematic lexicons, allowing researches to curate relevant sub-corpora from a large corpus of 18th and 19th century digitised texts.", "url": "https://arxiv.org/abs/2306.08020"}, {"metadata": {"arXiv": "2306.08042 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 18:01:46 ", "Title": "FLamE: Few-shot Learning from Natural Language Explanations", "Authors": ["Yangqiaoyu Zhou", "Yiming Zhang", "and Chenhao Tan"], "Categories": "cs.CL cs.AI", "Comments": ["Accepted at ACL 2023 main conference"]}, "abstract": "Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., \"not know\" for the neutral label) in generated explanations.", "url": "https://arxiv.org/abs/2306.08042"}, {"metadata": {"arXiv": "2306.08056 (*cross-listing*)", "Date": "Thu, 25 May 2023 06:53:18 ", "Title": "Distributed Trust Through the Lens of Software Architecture", "Authors": ["Sin Kit Lo", "Yue Liu", "Guangsheng Yu", "Qinghua Lu", "Xiwei Xu", "and Liming Zhu"], "Categories": "cs.CR cs.AI cs.SE"}, "abstract": "Distributed trust is a nebulous concept that has evolved from different perspectives in recent years. While one can attribute its current prominence to blockchain and cryptocurrency, the distributed trust concept has been cultivating progress in federated learning, trustworthy and responsible AI in an ecosystem setting, data sharing, privacy issues across organizational boundaries, and zero trust cybersecurity. This paper will survey the concept of distributed trust in multiple disciplines. It will take a system/software architecture point of view to look at trust redistribution/shift and the associated tradeoffs in systems and applications enabled by distributed trust technologies.", "url": "https://arxiv.org/abs/2306.08056"}, {"metadata": {"arXiv": "2306.08126 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 20:47:29 ", "Title": "PersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer", "Authors": ["Xu Han", "Bin Guo", "Yoon Jung", "Benjamin Yao", "Yu Zhang", "Xiaohu Liu", "Chenlei Guo"], "Categories": "cs.CL cs.AI", "Comments": ["10 pages", "3 figures", "accepted to SustaiNLP 2023"]}, "abstract": "Personalized dialogue agents (DAs) powered by large pre-trained language models (PLMs) often rely on explicit persona descriptions to maintain personality consistency. However, such descriptions may not always be available or may pose privacy concerns. To tackle this bottleneck, we introduce PersonaPKT, a lightweight transfer learning approach that can build persona-consistent dialogue models without explicit persona descriptions. By representing each persona as a continuous vector, PersonaPKT learns implicit persona-specific features directly from a small number of dialogue samples produced by the same persona, adding less than 0.1% trainable parameters for each persona on top of the PLM backbone. Empirical results demonstrate that PersonaPKT effectively builds personalized DAs with high storage efficiency, outperforming various baselines in terms of persona consistency while maintaining good response generation quality. In addition, it enhances privacy protection by avoiding explicit persona descriptions. Overall, PersonaPKT is an effective solution for creating personalized DAs that respect user privacy.", "url": "https://arxiv.org/abs/2306.08126"}, {"metadata": {"arXiv": "2306.08238 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 04:35:56 ", "Title": "Maestro: A Gamified Platform for Teaching AI Robustness", "Authors": ["Margarita Geleta and Jiacen Xu and Manikanta Loya and Junlin Wang and Sameer Singh and Zhou Li and Sergio Gago-Masague"], "Categories": "cs.HC cs.AI", "Comments": ["9 pages", "6 figures", "published at the Thirteenth Symposium on Educational Advances in Artificial Intelligence (EAAI-23) in the Association for the Advancement of Artificial Intelligence Conference (AAAI)", "2023"], "MSC-class": "68U35", "ACM-class": "H.5.2; I.2.m; J.m"}, "abstract": "Although the prevention of AI vulnerabilities is critical to preserve the safety and privacy of users and businesses, educational tools for robust AI are still underdeveloped worldwide. We present the design, implementation, and assessment of Maestro. Maestro is an effective open-source game-based platform that contributes to the advancement of robust AI education. Maestro provides goal-based scenarios where college students are exposed to challenging life-inspired assignments in a competitive programming environment. We assessed Maestro's influence on students' engagement, motivation, and learning success in robust AI. This work also provides insights into the design features of online learning tools that promote active learning opportunities in the robust AI domain. We analyzed the reflection responses (measured with Likert scales) of 147 undergraduate students using Maestro in two quarterly college courses in AI. According to the results, students who felt the acquisition of new skills in robust AI tended to appreciate highly Maestro and scored highly on material consolidation, curiosity, and mastery in robust AI. Moreover, the leaderboard, our key gamification element in Maestro, has effectively contributed to students' engagement and learning. Results also indicate that Maestro can be effectively adapted to any course length and depth without losing its educational quality.", "url": "https://arxiv.org/abs/2306.08238"}, {"metadata": {"arXiv": "2306.08302 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 07:15:26 ", "Title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap", "Authors": ["Shirui Pan", "Linhao Luo", "Yufei Wang", "Chen Chen", "Jiapu Wang", "Xindong Wu"], "Categories": "cs.CL cs.AI", "Comments": ["29 pages", "25 figures"]}, "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.", "url": "https://arxiv.org/abs/2306.08302"}, {"metadata": {"arXiv": "2306.08315 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 07:34:27 ", "Title": "Research on Named Entity Recognition in Improved transformer with R-Drop structure", "Authors": ["Weidong Ji", "Yousheng Zhang", "Guohui Zhou", "Xu Wang"], "Categories": "cs.CL cs.AI", "Comments": ["16 pages,7 figure"]}, "abstract": "To enhance the generalization ability of the model and improve the effectiveness of the transformer for named entity recognition tasks, the XLNet-Transformer-R model is proposed in this paper. The XLNet pre-trained model and the Transformer encoder with relative positional encodings are combined to enhance the model's ability to process long text and learn contextual information to improve robustness. To prevent overfitting, the R-Drop structure is used to improve the generalization capability and enhance the accuracy of the model in named entity recognition tasks. The model in this paper performs ablation experiments on the MSRA dataset and comparison experiments with other models on four datasets with excellent performance, demonstrating the strategic effectiveness of the XLNet-Transformer-R model.", "url": "https://arxiv.org/abs/2306.08315"}, {"metadata": {"arXiv": "2306.08350 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 08:38:51 ", "Title": "Multi-target Backdoor Attacks for Code Pre-trained Models", "Authors": ["Yanzhou Li", "Shangqing Liu", "Kangjie Chen", "Xiaofei Xie", "Tianwei Zhang and Yang Liu"], "Categories": "cs.CR cs.AI cs.CL", "Comments": ["ACL 2023 main conference"]}, "abstract": "Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for pre-trained models are designed for understanding tasks. In this paper, we propose task-agnostic backdoor attacks for code pre-trained models. Our backdoored model is pre-trained with two learning strategies (i.e., Poisoned Seq2Seq learning and token representation learning) to support the multi-target attack of downstream code understanding and generation tasks. During the deployment phase, the implanted backdoors in the victim models can be activated by the designed triggers to achieve the targeted attack. We evaluate our approach on two code understanding tasks and three code generation tasks over seven datasets. Extensive experiments demonstrate that our approach can effectively and stealthily attack code-related downstream tasks.", "url": "https://arxiv.org/abs/2306.08350"}, {"metadata": {"arXiv": "2306.08363 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 08:53:07 ", "Title": "Perceptions and Realities of Text-to-Image Generation", "Authors": ["Jonas Oppenlaender", "Johanna Silvennoinen", "Ville Paananen", "Aku Visuri"], "Categories": "cs.HC cs.AI cs.CY", "Comments": ["16 pages", "5 figures"], "ACM-class": "H.5.m; I.2.m"}, "abstract": "Generative artificial intelligence (AI) is a widely popular technology that will have a profound impact on society and individuals. Less than a decade ago, it was thought that creative work would be among the last to be automated - yet today, we see AI encroaching on many creative domains. In this paper, we present the findings of a survey study on people's perceptions of text-to-image generation. We touch on participants' technical understanding of the emerging technology, their fears and concerns, and thoughts about risks and dangers of text-to-image generation to the individual and society. We find that while participants were aware of the risks and dangers associated with the technology, only few participants considered the technology to be a personal risk. The risks for others were more easy to recognize for participants. Artists were particularly seen at risk. Participants who had tried the technology rated its future importance lower than those who had not tried it. This result shows that many people are still oblivious of the potential personal risks of generative artificial intelligence and the impending societal changes associated with this technology.", "url": "https://arxiv.org/abs/2306.08363"}, {"metadata": {"arXiv": "2306.08368 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 08:57:13 ", "Title": "T5-SR: A Unified Seq-to-Seq Decoding Strategy for Semantic Parsing", "Authors": ["Yuntao Li and Zhenpeng Su and Yutian Li and Hanchu Zhang and Sirui Wang and Wei Wu and Yan Zhang"], "Categories": "cs.CL cs.AI", "Comments": ["This paper has been accepted by ICASSP2023"]}, "abstract": "Translating natural language queries into SQLs in a seq2seq manner has attracted much attention recently. However, compared with abstract-syntactic-tree-based SQL generation, seq2seq semantic parsers face much more challenges, including poor quality on schematical information prediction and poor semantic coherence between natural language queries and SQLs. This paper analyses the above difficulties and proposes a seq2seq-oriented decoding strategy called SR, which includes a new intermediate representation SSQL and a reranking method with score re-estimator to solve the above obstacles respectively. Experimental results demonstrate the effectiveness of our proposed techniques and T5-SR-3b achieves new state-of-the-art results on the Spider dataset.", "url": "https://arxiv.org/abs/2306.08368"}, {"metadata": {"arXiv": "2306.08373 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 09:04:14 ", "Title": "A semantically enhanced dual encoder for aspect sentiment triplet extraction", "Authors": ["Baoxing Jiang", "Shehui Liang", "Peiyu Liu", "Kaifang Dong", "Hongye Li"], "Categories": "cs.CL cs.AI", "Comments": ["25 pages", "4 figures"]}, "abstract": "Aspect sentiment triplet extraction (ASTE) is a crucial subtask of aspect-based sentiment analysis (ABSA) that aims to comprehensively identify sentiment triplets. Previous research has focused on enhancing ASTE through innovative table-filling strategies. However, these approaches often overlook the multi-perspective nature of language expressions, resulting in a loss of valuable interaction information between aspects and opinions. To address this limitation, we propose a framework that leverages both a basic encoder, primarily based on BERT, and a particular encoder comprising a Bi-LSTM network and graph convolutional network (GCN ). The basic encoder captures the surface-level semantics of linguistic expressions, while the particular encoder extracts deeper semantics, including syntactic and lexical information. By modeling the dependency tree of comments and considering the part-of-speech and positional information of words, we aim to capture semantics that are more relevant to the underlying intentions of the sentences. An interaction strategy combines the semantics learned by the two encoders, enabling the fusion of multiple perspectives and facilitating a more comprehensive understanding of aspect--opinion relationships. Experiments conducted on benchmark datasets demonstrate the state-of-the-art performance of our proposed framework.", "url": "https://arxiv.org/abs/2306.08373"}, {"metadata": {"arXiv": "2306.08527 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 14:22:22 ", "Title": "Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement", "Authors": ["Zilu Guo", "Jun Du", "Chin-Hui Lee", "Yu Gao", "Wenbin Zhang"], "Categories": "eess.AS cs.AI cs.SD"}, "abstract": "The goal of this study is to implement diffusion models for speech enhancement (SE). The first step is to emphasize the theoretical foundation of variance-preserving (VP)-based interpolation diffusion under continuous conditions. Subsequently, we present a more concise framework that encapsulates both the VP- and variance-exploding (VE)-based interpolation diffusion methods. We demonstrate that these two methods are special cases of the proposed framework. Additionally, we provide a practical example of VP-based interpolation diffusion for the SE task. To improve performance and ease model training, we analyze the common difficulties encountered in diffusion models and suggest amenable hyper-parameters. Finally, we evaluate our model against several methods using a public benchmark to showcase the effectiveness of our approach", "url": "https://arxiv.org/abs/2306.08527"}, {"metadata": {"arXiv": "2306.08550 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 14:54:06 ", "Title": "User Simulation for Evaluating Information Access Systems", "Authors": ["Krisztian Balog and ChengXiang Zhai"], "Categories": "cs.HC cs.AI cs.IR", "Comments": ["Draft version 1.0", "currently under review"]}, "abstract": "Information access systems, such as search engines, recommender systems, and conversational assistants, have become integral to our daily lives as they help us satisfy our information needs. However, evaluating the effectiveness of these systems presents a long-standing and complex scientific challenge. This challenge is rooted in the difficulty of assessing a system's overall effectiveness in assisting users to complete tasks through interactive support, and further exacerbated by the substantial variation in user behaviour and preferences. To address this challenge, user simulation emerges as a promising solution. This book focuses on providing a thorough understanding of user simulation techniques designed specifically for evaluation purposes. We begin with a background of information access system evaluation and explore the diverse applications of user simulation. Subsequently, we systematically review the major research progress in user simulation, covering both general frameworks for designing user simulators, utilizing user simulation for evaluation, and specific models and algorithms for simulating user interactions with search engines, recommender systems, and conversational assistants. Realizing that user simulation is an interdisciplinary research topic, whenever possible, we attempt to establish connections with related fields, including machine learning, dialogue systems, user modeling, and economics. We end the book with a detailed discussion of important future research directions, many of which extend beyond the evaluation of information access systems and are expected to have broader impact on how to evaluate interactive intelligent systems in general.", "url": "https://arxiv.org/abs/2306.08550"}, {"metadata": {"arXiv": "2306.08564 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 15:17:48 ", "Title": "The Universal Law of Generalization Holds for Naturalistic Stimuli", "Authors": ["Raja Marjieh", "Nori Jacoby", "Joshua C. Peterson", "Thomas L. Griffiths"], "Categories": "q-bio.NC cs.AI stat.AP", "Comments": ["36 pages", "6 figures"]}, "abstract": "Shepard's universal law of generalization is a remarkable hypothesis about how intelligent organisms should perceive similarity. In its broadest form, the universal law states that the level of perceived similarity between a pair of stimuli should decay as a concave function of their distance when embedded in an appropriate psychological space. While extensively studied, evidence in support of the universal law has relied on low-dimensional stimuli and small stimulus sets that are very different from their real-world counterparts. This is largely because pairwise comparisons -- as required for similarity judgments -- scale quadratically in the number of stimuli. We provide direct evidence for the universal law in a naturalistic high-dimensional regime by analyzing an existing dataset of 214,200 human similarity judgments and a newly collected dataset of 390,819 human generalization judgments (N=2406 US participants) across three sets of natural images.", "url": "https://arxiv.org/abs/2306.08564"}, {"metadata": {"arXiv": "2306.08568 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 15:18:48 ", "Title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "Authors": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "Categories": "cs.CL cs.AI", "Comments": ["Large Language model", "Code Generation", "Code LLMs"]}, "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM", "url": "https://arxiv.org/abs/2306.08568"}, {"metadata": {"arXiv": "2306.08631 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 16:48:06 ", "Title": "Vulnerability Assessment of Industrial Control System with an Improved CVSS", "Authors": ["He Wen"], "Categories": "cs.CR cs.AI"}, "abstract": "Cyberattacks on industrial control systems (ICS) have been drawing attention in academia. However, this has not raised adequate concerns among some industrial practitioners. Therefore, it is necessary to identify the vulnerable locations and components in the ICS and investigate the attack scenarios and techniques. This study proposes a method to assess the risk of cyberattacks on ICS with an improved Common Vulnerability Scoring System (CVSS) and applies it to a continuous stirred tank reactor (CSTR) model. The results show the physical system levels of ICS have the highest severity once cyberattacked, and controllers, workstations, and human-machine interface are the crucial components in the cyberattack and defense.", "url": "https://arxiv.org/abs/2306.08631"}, {"metadata": {"arXiv": "2306.08666 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 17:57:24 ", "Title": "Radiology-GPT: A Large Language Model for Radiology", "Authors": ["Zhengliang Liu", "Aoxiao Zhong", "Yiwei Li", "Longtao Yang", "Chao Ju", "Zihao Wu", "Chong Ma", "Peng Shu", "Cheng Chen", "Sekeun Kim", "Haixing Dai", "Lin Zhao", "Dajiang Zhu", "Jun Liu", "Wei Liu", "Dinggang Shen", "Xiang Li", "Quanzheng Li", "Tianming Liu"], "Categories": "cs.CL cs.AI"}, "abstract": "We introduce Radiology-GPT, a large language model for radiology. Using an instruction tuning approach on an extensive dataset of radiology domain knowledge, Radiology-GPT demonstrates superior performance compared to general language models such as StableLM, Dolly and LLaMA. It exhibits significant versatility in radiological diagnosis, research, and communication. This work serves as a catalyst for future developments in clinical NLP. The successful implementation of Radiology-GPT is indicative of the potential of localizing generative large language models, specifically tailored for distinctive medical specialties, while ensuring adherence to privacy standards such as HIPAA. The prospect of developing individualized, large-scale language models that cater to specific needs of various hospitals presents a promising direction. The fusion of conversational competence and domain-specific knowledge in these models is set to foster future development in healthcare AI. A demo of Radiology-GPT is available at https://huggingface.co/spaces/allen-eric/radiology-gpt.", "url": "https://arxiv.org/abs/2306.08666"}, {"metadata": {"arXiv": "2306.08685 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 18:10:05 ", "Title": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models", "Authors": ["Ziqiao Ma", "Jiayi Pan", "Joyce Chai"], "Categories": "cs.CL cs.AI cs.CV", "Comments": ["ACL 2023"]}, "abstract": "The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can truly represent language with their grounded meanings and how grounding may further bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in open-world language learning. As an initial attempt, we propose object-oriented BERT (OctoBERT), a novel visually-grounded language model by pre-training on image-text pairs highlighting grounding as an objective. Through extensive experiments and analysis, we demonstrate that OctoBERT is a more coherent and fast grounded word learner, and that the grounding ability acquired during pre-training helps the model to learn unseen words more rapidly and robustly. Our code is available at https://github.com/sled-group/world-to-words", "url": "https://arxiv.org/abs/2306.08685"}, {"metadata": {"arXiv": "2306.08695 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 18:32:26 ", "Title": "GHP-MOFassemble: Diffusion modeling, high throughput screening, and molecular dynamics for rational discovery of novel metal-organic frameworks for carbon capture at scale", "Authors": ["Hyun Park", "Xiaoli Yan", "Ruijie Zhu", "E. A. Huerta", "Santanu Chaudhuri", "Donny Cooper", "Ian Foster", "Emad Tajkhorshid"], "Categories": "cond-mat.mtrl-sci cs.AI", "Comments": ["30 pages", "13 figures", "7 tables", "7 appendices"], "ACM-class": "I.2"}, "abstract": "We introduce GHP-MOFassemble, a Generative artificial intelligence (AI), High Performance framework to accelerate the rational design of metal-organic frameworks (MOFs) with high CO2 capacity and synthesizable linkers. Our framework combines a diffusion model, a class of generative AI, to generate novel linkers that are assembled with one of three pre-selected nodes into MOFs in a primitive cubic (pcu) topology. The CO2 capacities of these AI-generated MOFs are predicted using a modified version of the crystal graph convolutional neural network model. We then use the LAMMPS code to perform molecular dynamics simulations to relax the AI-generated MOF structures, and identify those that converge to stable structures, and maintain their porous properties throughout the simulations. Among 120,000 pcu MOF candidates generated by the GHP-MOFassemble framework, with three distinct metal nodes (Cu paddlewheel, Zn paddlewheel, Zn tetramer), a total of 102 structures completed molecular dynamics simulations at 1 bar with predicted CO2 capacity higher than 2 mmol/g at 0.1 bar, which corresponds to the top 5% of hMOFs in the hypothetical MOF (hMOF) dataset in the MOFX-DB database. Among these candidates, 18 have change in density lower than 1% during molecular dynamics simulations, indicating their stability. We also found that the top five GHP-MOFassemble's MOF structures have CO2 capacities higher than 96.9% of hMOF structures. This new approach combines generative AI, graph modeling, large-scale molecular dynamics simulations, and extreme scale computing to open up new pathways for the accelerated discovery of novel MOF structures at scale.", "url": "https://arxiv.org/abs/2306.08695"}, {"metadata": {"arXiv": "2306.08845 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 04:18:30 ", "Title": "Unsupervised speech intelligibility assessment with utterance level alignment distance between teacher and learner Wav2Vec-2.0 representations", "Authors": ["Nayan Anand", "Meenakshi Sirigiraju", "Chiranjeevi Yarra"], "Categories": "cs.SD cs.AI eess.AS"}, "abstract": "Speech intelligibility is crucial in language learning for effective communication. Thus, to develop computer-assisted language learning systems, automatic speech intelligibility detection (SID) is necessary. Most of the works have assessed the intelligibility in a supervised manner considering manual annotations, which requires cost and time; hence scalability is limited. To overcome these, this work proposes an unsupervised approach for SID. The proposed approach considers alignment distance computed with dynamic-time warping (DTW) between teacher and learner representation sequence as a measure to separate intelligible versus non-intelligible speech. We obtain the feature sequence using current state-of-the-art self-supervised representations from Wav2Vec-2.0. We found the detection accuracies as 90.37\\%, 92.57\\% and 96.58\\%, respectively, with three alignment distance measures -- mean absolute error, mean squared error and cosine distance (equal to one minus cosine similarity).", "url": "https://arxiv.org/abs/2306.08845"}, {"metadata": {"arXiv": "2306.08872 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 06:06:50 ", "Title": "Neural models for Factual Inconsistency Classification with Explanations", "Authors": ["Tathagata Raha", "Mukund Choudhary", "Abhinav Menon", "Harshit Gupta", "KV Aditya Srivatsa", "Manish Gupta", "Vasudeva Varma"], "Categories": "cs.CL cs.AI", "Comments": ["ECML-PKDD 2023"]}, "abstract": "Factual consistency is one of the most important requirements when editing high quality documents. It is extremely important for automatic text generation systems like summarization, question answering, dialog modeling, and language modeling. Still, automated factual inconsistency detection is rather under-studied. Existing work has focused on (a) finding fake news keeping a knowledge base in context, or (b) detecting broad contradiction (as part of natural language inference literature). However, there has been no work on detecting and explaining types of factual inconsistencies in text, without any knowledge base in context. In this paper, we leverage existing work in linguistics to formally define five types of factual inconsistencies. Based on this categorization, we contribute a novel dataset, FICLE (Factual Inconsistency CLassification with Explanation), with ~8K samples where each sample consists of two sentences (claim and context) annotated with type and span of inconsistency. When the inconsistency relates to an entity type, it is labeled as well at two levels (coarse and fine-grained). Further, we leverage this dataset to train a pipeline of four neural models to predict inconsistency type with explanations, given a (claim, context) sentence pair. Explanations include inconsistent claim fact triple, inconsistent context span, inconsistent claim component, coarse and fine-grained inconsistent entity types. The proposed system first predicts inconsistent spans from claim and context; and then uses them to predict inconsistency types and inconsistent entity types (when inconsistency is due to entities). We experiment with multiple Transformer-based natural language classification as well as generative models, and find that DeBERTa performs the best. Our proposed methods provide a weighted F1 of ~87% for inconsistency type classification across the five classes.", "url": "https://arxiv.org/abs/2306.08872"}, {"metadata": {"arXiv": "2306.08916 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 07:40:36 ", "Title": "Counterfactuals Modulo Temporal Logics", "Authors": ["Bernd Finkbeiner and Julian Siber"], "Categories": "cs.LO cs.AI", "Comments": ["24th International Conference on Logic for Programming", "Artificial Intelligence and Reasoning (LPAR-23)"], "DOI": "10.29007/qtw7"}, "abstract": "Lewis' theory of counterfactuals is the foundation of many contemporary notions of causality. In this paper, we extend this theory in the temporal direction to enable symbolic counterfactual reasoning on infinite sequences, such as counterexamples found by a model checker and trajectories produced by a reinforcement learning agent. In particular, our extension considers a more relaxed notion of similarity between worlds and proposes two additional counterfactual operators that close a semantic gap between the previous two in this more general setting. Further, we consider versions of counterfactuals that minimize the distance to the witnessing counterfactual worlds, a common requirement in causal analysis. To automate counterfactual reasoning in the temporal domain, we introduce a logic that combines temporal and counterfactual operators, and outline decision procedures for the satisfiability and trace-checking problems of this logic.", "url": "https://arxiv.org/abs/2306.08916"}, {"metadata": {"arXiv": "2306.08925 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 07:53:14 ", "Title": "Opinion Tree Parsing for Aspect-based Sentiment Analysis", "Authors": ["Xiaoyi Bao", "Xiaotong Jiang", "Zhongqing Wang", "Yue Zhang", "and Guodong Zhou"], "Categories": "cs.CL cs.AI"}, "abstract": "Extracting sentiment elements using pre-trained generative models has recently led to large improvements in aspect-based sentiment analysis benchmarks. However, these models always need large-scale computing resources, and they also ignore explicit modeling of structure between sentiment elements. To address these challenges, we propose an opinion tree parsing model, aiming to parse all the sentiment elements from an opinion tree, which is much faster, and can explicitly reveal a more comprehensive and complete aspect-level sentiment structure. In particular, we first introduce a novel context-free opinion grammar to normalize the opinion tree structure. We then employ a neural chart-based opinion tree parser to fully explore the correlations among sentiment elements and parse them into an opinion tree structure. Extensive experiments show the superiority of our proposed model and the capacity of the opinion tree parser with the proposed context-free opinion grammar. More importantly, the results also prove that our model is much faster than previous models.", "url": "https://arxiv.org/abs/2306.08925"}, {"metadata": {"arXiv": "2306.08952 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 08:44:41 ", "Title": "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models", "Authors": ["Qingyu Tan", "Hwee Tou Ng", "Lidong Bing"], "Categories": "cs.CL cs.AI", "Comments": ["ACL 2023"]}, "abstract": "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset \\tempreason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach. Our code and data are released on https://github.com/DAMO-NLP-SG/TempReason.", "url": "https://arxiv.org/abs/2306.08952"}, {"metadata": {"arXiv": "2306.08999 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 09:49:12 ", "Title": "Voting Booklet Bias: Stance Detection in Swiss Federal Communication", "Authors": ["Eric Egli", "Noah Mami\\'e", "Eyal Liron Dolev and Mathias M\\\"uller"], "Categories": "cs.CL cs.AI", "Comments": ["10 pages (including abstract and appendix)", "5 figures", "Keywords: stance detection", "natural language processing", "political analysis"]}, "abstract": "In this study, we use recent stance detection methods to study the stance (for, against or neutral) of statements in official information booklets for voters. Our main goal is to answer the fundamental question: are topics to be voted on presented in a neutral way? To this end, we first train and compare several models for stance detection on a large dataset about Swiss politics. We find that fine-tuning an M-BERT model leads to the best accuracy. We then use our best model to analyze the stance of utterances extracted from the Swiss federal voting booklet concerning the Swiss popular votes of September 2022, which is the main goal of this project. We evaluated the models in both a multilingual as well as a monolingual context for German, French, and Italian. Our analysis shows that some issues are heavily favored while others are more balanced, and that the results are largely consistent across languages. Our findings have implications for the editorial process of future voting booklets and the design of better automated systems for analyzing political discourse. The data and code accompanying this paper are available at https://github.com/ZurichNLP/voting-booklet-bias.", "url": "https://arxiv.org/abs/2306.08999"}, {"metadata": {"arXiv": "2306.09093 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 12:45:25 ", "Title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration", "Authors": ["Chenyang Lyu", "Minghao Wu", "Longyue Wang", "Xinting Huang", "Bingshuai Liu", "Zefeng Du", "Shuming Shi", "Zhaopeng Tu"], "Categories": "cs.CL cs.AI cs.CV", "Comments": ["Longyue Wang is the corresponding author. Our project page is at https://github.com/lyuchenyang/Macaw-LLM"]}, "abstract": "Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied. In this work, we propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information. Macaw-LLM consists of three main components: a modality module for encoding multi-modal data, a cognitive module for harnessing pretrained LLMs, and an alignment module for harmonizing diverse representations. Our novel alignment module seamlessly bridges multi-modal features to textual features, simplifying the adaptation process from the modality modules to the cognitive module. In addition, we construct a large-scale multi-modal instruction dataset in terms of multi-turn dialogue, including 69K image instances and 50K video instances. We have made our data, code and model publicly available, which we hope can pave the way for future research in multi-modal LLMs and expand the capabilities of LLMs to handle diverse data modalities and address complex real-world scenarios.", "url": "https://arxiv.org/abs/2306.09093"}, {"metadata": {"arXiv": "2306.09095 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 12:47:22 ", "Title": "Analogue and Physical Reservoir Computing Using Water Waves", "Authors": ["Ivan S. Maksymov"], "Categories": "physics.flu-dyn cs.AI nlin.CD nlin.PS", "Comments": ["Review article"]}, "abstract": "More than 3.5 billion people live in rural areas, where water and water energy resources play an important role in ensuring sustainable and productive rural economies. This article reviews and critically analyses the recent advances in the field of analogue and reservoir computing that have been driven by unique physical properties and energy of water waves. It also demonstrates that analogue and reservoir computing hold the potential to bring artificial intelligence closer to people living outside large cities, thus enabling them to enjoy the benefits of novel technologies that already work in large cities but are not readily available and suitable for regional communities.", "url": "https://arxiv.org/abs/2306.09095"}, {"metadata": {"arXiv": "2306.09106 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 13:02:41 ", "Title": "Audio Tagging on an Embedded Hardware Platform", "Authors": ["Gabriel Bibbo", "Arshdeep Singh", "Mark D. Plumbley"], "Categories": "cs.SD cs.AI cs.SY eess.AS eess.SY", "Comments": ["Submitted to DCASE 2023 Workshop"]}, "abstract": "Convolutional neural networks (CNNs) have exhibited state-of-the-art performance in various audio classification tasks. However, their real-time deployment remains a challenge on resource-constrained devices like embedded systems. In this paper, we analyze how the performance of large-scale pretrained audio neural networks designed for audio pattern recognition changes when deployed on a hardware such as Raspberry Pi. We empirically study the role of CPU temperature, microphone quality and audio signal volume on performance. Our experiments reveal that the continuous CPU usage results in an increased temperature that can trigger an automated slowdown mechanism in the Raspberry Pi, impacting inference latency. The quality of a microphone, specifically with affordable devices like the Google AIY Voice Kit, and audio signal volume, all affect the system performance. In the course of our investigation, we encounter substantial complications linked to library compatibility and the unique processor architecture requirements of the Raspberry Pi, making the process less straightforward compared to conventional computers (PCs). Our observations, while presenting challenges, pave the way for future researchers to develop more compact machine learning models, design heat-dissipative hardware, and select appropriate microphones when AI models are deployed for real-time applications on edge devices. All related assets and an interactive demo can be found on GitHub", "url": "https://arxiv.org/abs/2306.09106"}, {"metadata": {"arXiv": "2306.09114 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 13:19:08 ", "Title": "Relational Temporal Graph Reasoning for Dual-task Dialogue Language Understanding", "Authors": ["Bowen Xing and Ivor W. Tsang"], "Categories": "cs.CL cs.AI", "Comments": ["Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI). arXiv admin note: substantial text overlap with arXiv:2203.03856"]}, "abstract": "Dual-task dialog language understanding aims to tackle two correlative dialog language understanding tasks simultaneously via leveraging their inherent correlations. In this paper, we put forward a new framework, whose core is relational temporal graph reasoning.We propose a speaker-aware temporal graph (SATG) and a dual-task relational temporal graph (DRTG) to facilitate relational temporal modeling in dialog understanding and dual-task reasoning. Besides, different from previous works that only achieve implicit semantics-level interactions, we propose to model the explicit dependencies via integrating prediction-level interactions. To implement our framework, we first propose a novel model Dual-tAsk temporal Relational rEcurrent Reasoning network (DARER), which first generates the context-, speaker- and temporal-sensitive utterance representations through relational temporal modeling of SATG, then conducts recurrent dual-task relational temporal graph reasoning on DRTG, in which process the estimated label distributions act as key clues in prediction-level interactions. And the relational temporal modeling in DARER is achieved by relational convolutional networks (RGCNs). Then we further propose Relational Temporal Transformer (ReTeFormer), which achieves fine-grained relational temporal modeling via Relation- and Structure-aware Disentangled Multi-head Attention. Accordingly, we propose DARER with ReTeFormer (DARER2), which adopts two variants of ReTeFormer to achieve the relational temporal modeling of SATG and DTRG, respectively. The extensive experiments on different scenarios verify that our models outperform state-of-the-art models by a large margin. Remarkably, on the dialog sentiment classification task in the Mastodon dataset, DARER and DARER2 gain relative improvements of about 28% and 34% over the previous best model in terms of F1.", "url": "https://arxiv.org/abs/2306.09114"}, {"metadata": {"arXiv": "2306.09169 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 14:46:44 ", "Title": "Opportunities for Large Language Models and Discourse in Engineering Design", "Authors": ["Jan G\\\"opfert", "Jann M. Weinand", "Patrick Kuckertz", "Detlef Stolten"], "Categories": "cs.CL cs.AI cs.CE"}, "abstract": "In recent years, large language models have achieved breakthroughs on a wide range of benchmarks in natural language processing and continue to increase in performance. Recently, the advances of large language models have raised interest outside the natural language processing community and could have a large impact on daily life. In this paper, we pose the question: How will large language models and other foundation models shape the future product development process? We provide the reader with an overview of the subject by summarizing both recent advances in natural language processing and the use of information technology in the engineering design process. We argue that discourse should be regarded as the core of engineering design processes, and therefore should be represented in a digital artifact. On this basis, we describe how foundation models such as large language models could contribute to the design discourse by automating parts thereof that involve creativity and reasoning, and were previously reserved for humans. We describe how simulations, experiments, topology optimizations, and other process steps can be integrated into a machine-actionable, discourse-centric design process. Finally, we outline the future research that will be necessary for the implementation of the conceptualized framework.", "url": "https://arxiv.org/abs/2306.09169"}, {"metadata": {"arXiv": "2306.09255 (*cross-listing*)", "Date": "Mon, 29 May 2023 12:26:44 ", "Title": "Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations", "Authors": ["Attia Qammar", "Hongmei Wang", "Jianguo Ding", "Abdenacer Naouri", "Mahmoud Daneshmand", "Huansheng Ning"], "Categories": "cs.CR cs.AI cs.CY"}, "abstract": "Chatbots shifted from rule-based to artificial intelligence techniques and gained traction in medicine, shopping, customer services, food delivery, education, and research. OpenAI developed ChatGPT blizzard on the Internet as it crossed one million users within five days of its launch. However, with the enhanced popularity, chatbots experienced cybersecurity threats and vulnerabilities. This paper discussed the relevant literature, reports, and explanatory incident attacks generated against chatbots. Our initial point is to explore the timeline of chatbots from ELIZA (an early natural language processing computer program) to GPT-4 and provide the working mechanism of ChatGPT. Subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and LOLBINs. Furthermore, the history of cyberattacks and vulnerabilities exploited by cybercriminals are discussed, particularly considering the risk and vulnerabilities in ChatGPT. Addressing these threats and vulnerabilities requires specific strategies and measures to reduce the harmful consequences. Therefore, the future directions to address the challenges were presented.", "url": "https://arxiv.org/abs/2306.09255"}, {"metadata": {"arXiv": "2306.09308 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 17:42:48 ", "Title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models", "Authors": ["Myles Foley", "Ambrish Rawat", "Taesung Lee", "Yufang Hou", "Gabriele Picco", "Giulio Zizzo"], "Categories": "cs.CL cs.AI cs.CR"}, "abstract": "The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. Thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. In this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned LLM to its corresponding pre-trained base model. We consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method.", "url": "https://arxiv.org/abs/2306.09308"}, {"metadata": {"arXiv": "2306.09343 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 17:59:47 ", "Title": "SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts", "Authors": ["Rose E. Wang", "Pawan Wirawarn", "Noah Goodman", "Dorottya Demszky"], "Categories": "cs.CL cs.AI", "Comments": ["First two authors contributed equally. In the Proceedings of Innovative Use of NLP for Building Educational Applications 2023. The code and data are open-sourced here: https://github.com/rosewang2008/sight"]}, "abstract": "Lectures are a learning experience for both students and teachers. Students learn from teachers about the subject material, while teachers learn from students about how to refine their instruction. However, online student feedback is unstructured and abundant, making it challenging for teachers to learn and improve. We take a step towards tackling this challenge. First, we contribute a dataset for studying this problem: SIGHT is a large dataset of 288 math lecture transcripts and 15,784 comments collected from the Massachusetts Institute of Technology OpenCourseWare (MIT OCW) YouTube channel. Second, we develop a rubric for categorizing feedback types using qualitative analysis. Qualitative analysis methods are powerful in uncovering domain-specific insights, however they are costly to apply to large data sources. To overcome this challenge, we propose a set of best practices for using large language models (LLMs) to cheaply classify the comments at scale. We observe a striking correlation between the model's and humans' annotation: Categories with consistent human annotations (>$0.9$ inter-rater reliability, IRR) also display higher human-model agreement (>$0.7$), while categories with less consistent human annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lower human-model agreement ($0.3$-$0.5$). These techniques uncover useful student feedback from thousands of comments, costing around $\\$0.002$ per comment. We conclude by discussing exciting future directions on using online student feedback and improving automated annotation techniques for qualitative research.", "url": "https://arxiv.org/abs/2306.09343"}, {"metadata": {"arXiv": "2306.08014", "Date": "Tue, 13 Jun 2023 11:57:52 ", "Title": "Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language", "Authors": ["Magnus Koudahl", "Thijs van de Laar", "Bert de Vries"], "Categories": "cs.AI cs.LG", "Comments": ["49 pages", "31 figures"]}, "abstract": "The Free Energy Principle (FEP) is a theoretical framework for describing how (intelligent) systems self-organise into coherent, stable structures by minimising a free energy functional. Active Inference (AIF) is a corollary of the FEP that specifically details how systems that are able to plan for the future (agents) function by minimising particular free energy functionals that incorporate information seeking components. This paper is the first in a series of two where we derive a synthetic version of AIF on free form factor graphs. The present paper focuses on deriving a local version of the free energy functionals used for AIF. This enables us to construct a version of AIF which applies to arbitrary graphical models and interfaces with prior work on message passing algorithms. The resulting messages are derived in our companion paper. We also identify a gap in the graphical notation used for factor graphs. While factor graphs are great at expressing a generative model, they have so far been unable to specify the full optimisation problem including constraints. To solve this problem we develop Constrained Forney-style Factor Graph (CFFG) notation which permits a fully graphical description of variational inference objectives. We then proceed to show how CFFG's can be used to reconstruct prior algorithms for AIF as well as derive new ones. The latter is demonstrated by deriving an algorithm that permits direct policy inference for AIF agents, circumventing a long standing scaling issue that has so far hindered the application of AIF in industrial settings. We demonstrate our algorithm on the classic T-maze task and show that it reproduces the information seeking behaviour that is a hallmark feature of AIF.", "url": "https://arxiv.org/abs/2306.08014"}, {"metadata": {"arXiv": "2306.08094", "Date": "Tue, 13 Jun 2023 19:27:18 ", "Title": "Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning", "Authors": ["Michael Villarreal", "Bibek Poudel", "Weizi Li"], "Categories": "cs.AI cs.HC cs.LG cs.RO"}, "abstract": "The surge in Reinforcement Learning (RL) applications in Intelligent Transportation Systems (ITS) has contributed to its growth as well as highlighted key challenges. However, defining objectives of RL agents in traffic control and management tasks, as well as aligning policies with these goals through an effective formulation of Markov Decision Process (MDP), can be challenging and often require domain experts in both RL and ITS. Recent advancements in Large Language Models (LLMs) such as GPT-4 highlight their broad general knowledge, reasoning capabilities, and commonsense priors across various domains. In this work, we conduct a large-scale user study involving 70 participants to investigate whether novices can leverage ChatGPT to solve complex mixed traffic control problems. Three environments are tested, including ring road, bottleneck, and intersection. We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 136% compared to solely beginner capabilities, with some of them even outperforming experts. However, ChatGPT does not provide consistent improvements across all scenarios.", "url": "https://arxiv.org/abs/2306.08094"}, {"metadata": {"arXiv": "2306.08141", "Date": "Tue, 13 Jun 2023 21:10:45 ", "Title": "ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations", "Authors": ["Kailas Vodrahalli and James Zou"], "Categories": "cs.AI cs.CV cs.HC cs.LG", "Comments": ["20 pages", "13 figures"]}, "abstract": "As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate similar images. Interestingly, prompt diversity does not decrease as users find better prompts. We further propose to a new metric the study the steerability of AI using our dataset. We define steerability as the expected number of interactions required to adequately complete a task. We estimate this value by fitting a Markov chain for each target task and calculating the expected time to reach an adequate score in the Markov chain. We quantify and compare AI steerability across different types of target images and two different models, finding that images of cities and natural world images are more steerable than artistic and fantasy images. These findings provide insights into human-AI interaction behavior, present a concrete method of assessing AI steerability, and demonstrate the general utility of the ArtWhisperer dataset.", "url": "https://arxiv.org/abs/2306.08141"}, {"metadata": {"arXiv": "2306.08204", "Date": "Wed, 14 Jun 2023 02:12:49 ", "Title": "Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer", "Authors": ["Jaehyun Park", "Jaegyun Im", "Sanha Hwang", "Mintaek Lim", "Sabina Ualibekova", "Sejin Kim", "Sundong Kim"], "Categories": "cs.AI cs.LG"}, "abstract": "In the pursuit of artificial general intelligence (AGI), we tackle Abstraction and Reasoning Corpus (ARC) tasks using a novel two-pronged approach. We employ the Decision Transformer in an imitation learning paradigm to model human problem-solving, and introduce an object detection algorithm, the Push and Pull clustering method. This dual strategy enhances AI's ARC problem-solving skills and provides insights for AGI progression. Yet, our work reveals the need for advanced data collection tools, robust training datasets, and refined model structures. This study highlights potential improvements for Decision Transformers and propels future AGI research.", "url": "https://arxiv.org/abs/2306.08204"}, {"metadata": {"arXiv": "2306.08359", "Date": "Wed, 14 Jun 2023 08:51:43 ", "Title": "Hierarchical Task Network Planning for Facilitating Cooperative Multi-Agent Reinforcement Learning", "Authors": ["Xuechen Mu", "Hankz Hankui Zhuo", "Chen Chen", "Kai Zhang", "Chao Yu and Jianye Hao"], "Categories": "cs.AI cs.LG"}, "abstract": "Exploring sparse reward multi-agent reinforcement learning (MARL) environments with traps in a collaborative manner is a complex task. Agents typically fail to reach the goal state and fall into traps, which affects the overall performance of the system. To overcome this issue, we present SOMARL, a framework that uses prior knowledge to reduce the exploration space and assist learning. In SOMARL, agents are treated as part of the MARL environment, and symbolic knowledge is embedded using a tree structure to build a knowledge hierarchy. The framework has a two-layer hierarchical structure, comprising a hybrid module with a Hierarchical Task Network (HTN) planning and meta-controller at the higher level, and a MARL-based interactive module at the lower level. The HTN module and meta-controller use Hierarchical Domain Definition Language (HDDL) and the option framework to formalize symbolic knowledge and obtain domain knowledge and a symbolic option set, respectively. Moreover, the HTN module leverages domain knowledge to guide low-level agent exploration by assisting the meta-controller in selecting symbolic options. The meta-controller further computes intrinsic rewards of symbolic options to limit exploration behavior and adjust HTN planning solutions as needed. We evaluate SOMARL on two benchmarks, FindTreasure and MoveBox, and report superior performance over state-of-the-art MARL and subgoal-based baselines for MARL environments significantly.", "url": "https://arxiv.org/abs/2306.08359"}, {"metadata": {"arXiv": "2306.08765", "Date": "Wed, 14 Jun 2023 22:27:26 ", "Title": "Hybrids of Constraint-based and Noise-based Algorithms for Causal Discovery from Time Series", "Authors": ["Charles K. Assaad", "Daria Bystrova", "Julyan Arbel", "Emilie Devijver", "Eric Gaussier", "Wilfried Thuiller"], "Categories": "cs.AI cs.LG"}, "abstract": "Constraint-based and noise-based methods have been proposed to discover summary causal graphs from observational time series under strong assumptions which can be violated or impossible to verify in real applications. Recently, a hybrid method (Assaad et al, 2021) that combines these two approaches, proved to be robust to assumption violation. However, this method assumes that the summary causal graph is acyclic, but cycles are common in many applications. For example, in ecological communities, there may be cyclic relationships between predator and prey populations, creating feedback loops. Therefore, this paper presents two new frameworks for hybrids of constraint-based and noise-based methods that can discover summary causal graphs that may or may not contain cycles. For each framework, we provide two hybrid algorithms which are experimentally tested on simulated data, realistic ecological data, and real data from various applications. Experiments show that our hybrid approaches are robust and yield good results over most datasets.", "url": "https://arxiv.org/abs/2306.08765"}, {"metadata": {"arXiv": "2306.07992", "Date": "Sun, 11 Jun 2023 19:59:35 ", "Title": "Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework", "Authors": ["Minglei Yin", "Bin Liu", "Neil Zhenqiang Gong", "Xin Li"], "Categories": "cs.CV cs.AI cs.CR cs.LG"}, "abstract": "With rich visual data, such as images, becoming readily associated with items, visually-aware recommendation systems (VARS) have been widely used in different applications. Recent studies have shown that VARS are vulnerable to item-image adversarial attacks, which add human-imperceptible perturbations to the clean images associated with those items. Attacks on VARS pose new security challenges to a wide range of applications such as e-Commerce and social networks where VARS are widely used. How to secure VARS from such adversarial attacks becomes a critical problem. Currently, there is still a lack of systematic study on how to design secure defense strategies against visual attacks on VARS. In this paper, we attempt to fill this gap by proposing an adversarial image reconstruction and detection framework to secure VARS. Our proposed method can simultaneously (1) secure VARS from adversarial attacks characterized by local perturbations by image reconstruction based on global vision transformers; and (2) accurately detect adversarial examples using a novel contrastive learning approach. Meanwhile, our framework is designed to be used as both a filter and a detector so that they can be jointly trained to improve the flexibility of our defense strategy to a variety of attacks and VARS models. We have conducted extensive experimental studies with two popular attack methods (FGSM and PGD). Our experimental results on two real-world datasets show that our defense strategy against visual attacks is effective and outperforms existing methods on different attacks. Moreover, our method can detect adversarial examples with high accuracy.", "url": "https://arxiv.org/abs/2306.07992"}, {"metadata": {"arXiv": "2306.08068", "Date": "Tue, 13 Jun 2023 18:32:35 ", "Title": "DORSal: Diffusion for Object-centric Representations of Scenes $\\textit{et al.}$", "Authors": ["Allan Jabri", "Sjoerd van Steenkiste", "Emiel Hoogeboom", "Mehdi S. M. Sajjadi", "Thomas Kipf"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Project page: https://www.sjoerdvansteenkiste.com/dorsal"]}, "abstract": "Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View dataset, we show that DORSal enables scalable neural rendering of 3D scenes with object-level editing and improves upon existing approaches.", "url": "https://arxiv.org/abs/2306.08068"}, {"metadata": {"arXiv": "2306.08549", "Date": "Wed, 14 Jun 2023 14:50:23 ", "Title": "An Exploratory Study of Masked Face Recognition with Machine Learning Algorithms", "Authors": ["Megh Pudyel and Mustafa Atay"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["6 pages published in IEEE SoutheastCon 2023"], "ACM-class": "I.4.9; I.2.6", "Journal-ref": "SoutheastCon 2023 (2023) 877-882", "DOI": "10.1109/SoutheastCon51012.2023.10115205"}, "abstract": "Automated face recognition is a widely adopted machine learning technology for contactless identification of people in various processes such as automated border control, secure login to electronic devices, community surveillance, tracking school attendance, workplace clock in and clock out. Using face masks have become crucial in our daily life with the recent world-wide COVID-19 pandemic. The use of face masks causes the performance of conventional face recognition technologies to degrade considerably. The effect of mask-wearing in face recognition is yet an understudied issue. In this paper, we address this issue by evaluating the performance of a number of face recognition models which are tested by identifying masked and unmasked face images. We use six conventional machine learning algorithms, which are SVC, KNN, LDA, DT, LR and NB, to find out the ones which perform best, besides the ones which poorly perform, in the presence of masked face images. Local Binary Pattern (LBP) is utilized as the feature extraction operator. We generated and used synthesized masked face images. We prepared unmasked, masked, and half-masked training datasets and evaluated the face recognition performance against both masked and unmasked images to present a broad view of this crucial problem. We believe that our study is unique in elaborating the mask-aware facial recognition with almost all possible scenarios including half_masked-to-masked and half_masked-to-unmasked besides evaluating a larger number of conventional machine learning algorithms compared the other studies in the literature.", "url": "https://arxiv.org/abs/2306.08549"}, {"metadata": {"arXiv": "2306.08609", "Date": "Wed, 14 Jun 2023 16:13:27 ", "Title": "TomoSAM: a 3D Slicer extension using SAM for tomography segmentation", "Authors": ["Federico Semeraro", "Alexandre Quintart", "Sergio Fraile Izquierdo", "Joseph C. Ferguson"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "TomoSAM has been developed to integrate the cutting-edge Segment Anything Model (SAM) into 3D Slicer, a highly capable software platform used for 3D image processing and visualization. SAM is a promptable deep learning model that is able to identify objects and create image masks in a zero-shot manner, based only on a few user clicks. The synergy between these tools aids in the segmentation of complex 3D datasets from tomography or other imaging techniques, which would otherwise require a laborious manual segmentation process. The source code associated with this article can be found at https://github.com/fsemerar/SlicerTomoSAM", "url": "https://arxiv.org/abs/2306.08609"}, {"metadata": {"arXiv": "2306.08893", "Date": "Thu, 15 Jun 2023 06:53:05 ", "Title": "LOVM: Language-Only Vision Model Selection", "Authors": ["Orr Zohar", "Shih-Cheng Huang", "Kuan-Chieh Wang", "Serena Yeung"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Pre-trained multi-modal vision-language models (VLMs) are becoming increasingly popular due to their exceptional performance on downstream vision applications, particularly in the few- and zero-shot settings. However, selecting the best-performing VLM for some downstream applications is non-trivial, as it is dataset and task-dependent. Meanwhile, the exhaustive evaluation of all available VLMs on a novel application is not only time and computationally demanding but also necessitates the collection of a labeled dataset for evaluation. As the number of open-source VLM variants increases, there is a need for an efficient model selection strategy that does not require access to a curated evaluation dataset. This paper proposes a novel task and benchmark for efficiently evaluating VLMs' zero-shot performance on downstream applications without access to the downstream task dataset. Specifically, we introduce a new task LOVM: Language-Only Vision Model Selection, where methods are expected to perform both model selection and performance prediction based solely on a text description of the desired downstream application. We then introduced an extensive LOVM benchmark consisting of ground-truth evaluations of 35 pre-trained VLMs and 23 datasets, where methods are expected to rank the pre-trained VLMs and predict their zero-shot performance.", "url": "https://arxiv.org/abs/2306.08893"}, {"metadata": {"arXiv": "2306.08935", "Date": "Thu, 15 Jun 2023 08:17:49 ", "Title": "Context-Aware Change Detection With Semi-Supervised Learning", "Authors": ["Ritu Yadav", "Andrea Nascetti", "Yifang Ban"], "Categories": "cs.CV cs.AI cs.LG eess.IV", "Comments": ["Paper Accepted in IGARSS 2023"]}, "abstract": "Change detection using earth observation data plays a vital role in quantifying the impact of disasters in affected areas. While data sources like Sentinel-2 provide rich optical information, they are often hindered by cloud cover, limiting their usage in disaster scenarios. However, leveraging pre-disaster optical data can offer valuable contextual information about the area such as landcover type, vegetation cover, soil types, enabling a better understanding of the disaster's impact. In this study, we develop a model to assess the contribution of pre-disaster Sentinel-2 data in change detection tasks, focusing on disaster-affected areas. The proposed Context-Aware Change Detection Network (CACDN) utilizes a combination of pre-disaster Sentinel-2 data, pre and post-disaster Sentinel-1 data and ancillary Digital Elevation Models (DEM) data. The model is validated on flood and landslide detection and evaluated using three metrics: Area Under the Precision-Recall Curve (AUPRC), Intersection over Union (IoU), and mean IoU. The preliminary results show significant improvement (4\\%, AUPRC, 3-7\\% IoU, 3-6\\% mean IoU) in model's change detection capabilities when incorporated with pre-disaster optical data reflecting the effectiveness of using contextual information for accurate flood and landslide detection.", "url": "https://arxiv.org/abs/2306.08935"}, {"metadata": {"arXiv": "2306.08958", "Date": "Thu, 15 Jun 2023 08:51:24 ", "Title": "Temporally-Extended Prompts Optimization for SAM in Interactive Medical Image Segmentation", "Authors": ["Chuyun Shen", "Wenhao Li", "Ya Zhang", "Xiangfeng Wang"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["17 pages", "5 figures"]}, "abstract": "The Segmentation Anything Model (SAM) has recently emerged as a foundation model for addressing image segmentation. Owing to the intrinsic complexity of medical images and the high annotation cost, the medical image segmentation (MIS) community has been encouraged to investigate SAM's zero-shot capabilities to facilitate automatic annotation. Inspired by the extraordinary accomplishments of interactive medical image segmentation (IMIS) paradigm, this paper focuses on assessing the potential of SAM's zero-shot capabilities within the IMIS paradigm to amplify its benefits in the MIS domain. Regrettably, we observe that SAM's vulnerability to prompt forms (e.g., points, bounding boxes) becomes notably pronounced in IMIS. This leads us to develop a framework that adaptively offers suitable prompt forms for human experts. We refer to the framework above as temporally-extended prompts optimization (TEPO) and model it as a Markov decision process, solvable through reinforcement learning. Numerical experiments on the standardized benchmark BraTS2020 demonstrate that the learned TEPO agent can further enhance SAM's zero-shot capability in the MIS context.", "url": "https://arxiv.org/abs/2306.08958"}, {"metadata": {"arXiv": "2306.09085", "Date": "Thu, 15 Jun 2023 12:29:42 ", "Title": "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model", "Authors": ["Sihan Chen", "Xingjian He", "Handong Li", "Xiaojie Jin", "Jiashi Feng", "Jing Liu"], "Categories": "cs.CV cs.AI cs.CL cs.LG cs.MM"}, "abstract": "Due to the limited scale and quality of video-text training corpus, most vision-language foundation models employ image-text datasets for pretraining and primarily focus on modeling visually semantic representations while disregarding temporal semantic representations and correlations. To address this issue, we propose COSA, a COncatenated SAmple pretrained vision-language foundation model. COSA jointly models visual contents and event-level temporal cues using only image-text corpora. We achieve this by sequentially concatenating multiple image-text pairs as inputs for pretraining. This transformation effectively converts existing image-text corpora into a pseudo long-form video-paragraph corpus, enabling richer scene transformations and explicit event-description correspondence. Extensive experiments demonstrate that COSA consistently improves performance across a broad range of downstream tasks, including long-form/short-form video-text tasks and image-text tasks such as retrieval, captioning, and question answering. Notably, COSA achieves state-of-the-art results on various competitive benchmarks. Code and model are released at https://github.com/TXH-mercury/COSA.", "url": "https://arxiv.org/abs/2306.09085"}, {"metadata": {"arXiv": "2306.09124", "Date": "Thu, 15 Jun 2023 13:33:27 ", "Title": "DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks in the Physical World", "Authors": ["Caixin Kang", "Yinpeng Dong", "Zhengyi Wang", "Shouwei Ruan", "Hang Su", "Xingxing Wei"], "Categories": "cs.CV cs.AI cs.CR cs.LG"}, "abstract": "Adversarial attacks in the physical world, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is severely lacking. In this paper, we propose DIFFender, a novel defense method that leverages the pre-trained diffusion model to perform both localization and defense against potential adversarial patch attacks. DIFFender is designed as a pipeline consisting of two main stages: patch localization and restoration. In the localization stage, we exploit the intriguing properties of a diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ a text-guided diffusion model to eliminate adversarial regions in the image while preserving the integrity of the visual content. Additionally, we design a few-shot prompt-tuning algorithm to facilitate simple and efficient tuning, enabling the learned representations to easily transfer to downstream tasks, which optimize two stages jointly. We conduct extensive experiments on image classification and face recognition to demonstrate that DIFFender exhibits superior robustness under strong adaptive attacks and generalizes well across various scenarios, diverse classifiers, and multiple attack methods.", "url": "https://arxiv.org/abs/2306.09124"}, {"metadata": {"arXiv": "2306.09305", "Date": "Thu, 15 Jun 2023 17:38:48 ", "Title": "Fast Training of Diffusion Models with Masked Transformers", "Authors": ["Hongkai Zheng", "Weili Nie", "Arash Vahdat", "Anima Anandkumar"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (\\emph{e.g.}, 50\\%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256$\\times$256 show that our approach achieves the same performance as the state-of-the-art Diffusion Transformer (DiT) model, using only 31\\% of its original training time. Thus, our method allows for efficient training of diffusion models without sacrificing the generative performance.", "url": "https://arxiv.org/abs/2306.09305"}, {"metadata": {"arXiv": "2306.07995", "Date": "Mon, 12 Jun 2023 16:18:32 ", "Title": "Semantic-Based Neural Network Repair", "Authors": ["Richard Schumi", "Jun Sun"], "Categories": "cs.LG cs.AI cs.SE", "DOI": "10.1145/3597926.3598045"}, "abstract": "Recently, neural networks have spread into numerous fields including many safety-critical systems. Neural networks are built (and trained) by programming in frameworks such as TensorFlow and PyTorch. Developers apply a rich set of pre-defined layers to manually program neural networks or to automatically generate them (e.g., through AutoML). Composing neural networks with different layers is error-prone due to the non-trivial constraints that must be satisfied in order to use those layers. In this work, we propose an approach to automatically repair erroneous neural networks. The challenge is in identifying a minimal modification to the network so that it becomes valid. Modifying a layer might have cascading effects on subsequent layers and thus our approach must search recursively to identify a \"globally\" minimal modification. Our approach is based on an executable semantics of deep learning layers and focuses on four kinds of errors which are common in practice. We evaluate our approach for two usage scenarios, i.e., repairing automatically generated neural networks and manually written ones suffering from common model bugs. The results show that we are able to repair 100% of a set of randomly generated neural networks (which are produced with an existing AI framework testing approach) effectively and efficiently (with an average repair time of 21.08s) and 93.75% of a collection of real neural network bugs (with an average time of 3min 40s).", "url": "https://arxiv.org/abs/2306.07995"}, {"metadata": {"arXiv": "2306.08001", "Date": "Tue, 13 Jun 2023 06:49:50 ", "Title": "A Markovian Formalism for Active Querying", "Authors": ["Sid Ijju"], "Categories": "cs.LG cs.AI", "Comments": ["Active Learning", "Markov", "Inverse Reinforcement Learning", "Query"]}, "abstract": "Active learning algorithms have been an integral part of recent advances in artificial intelligence. However, the research in the field is widely varying and lacks an overall organizing leans. We outline a Markovian formalism for the field of active learning and survey the literature to demonstrate the organizing capability of our proposed formalism. Our formalism takes a partially observable Markovian system approach to the active learning process as a whole. We specifically outline how querying, dataset augmentation, reward updates, and other aspects of active learning can be viewed as a transition between meta-states in a Markovian system, and give direction into how other aspects of active learning can fit into our formalism.", "url": "https://arxiv.org/abs/2306.08001"}, {"metadata": {"arXiv": "2306.08003", "Date": "Tue, 13 Jun 2023 07:42:35 ", "Title": "DTW k-means clustering for fault detection in photovoltaic modules", "Authors": ["Edgar Hernando Sep\\'ulveda Oviedo (LAAS-DISCO", "LAAS-ISGE)", "Louise Trav\\'e-Massuy\\`es", "Audine Subias", "Marko Pavlov", "Corinne Alonso"], "Categories": "cs.LG cs.AI", "Journal-ref": "XI Congreso Internacional de Ingenier{\\'i}a Mec{\\'a}nica, Mecatr{\\'o}nica y Automatizaci{\\'o}n 2023, Apr 2023, Carthag{\\`e}ne, Colombia"}, "abstract": "The increase in the use of photovoltaic (PV) energy in the world has shown that the useful life and maintenance of a PV plant directly depend on theability to quickly detect severe faults on a PV plant. To solve this problem of detection, data based approaches have been proposed in the literature.However, these previous solutions consider only specific behavior of one or few faults. Most of these approaches can be qualified as supervised, requiring an enormous labelling effort (fault types clearly identified in each technology). In addition, most of them are validated in PV cells or one PV module. That is hardly applicable in large-scale PV plants considering their complexity. Alternatively, some unsupervised well-known approaches based on data try to detect anomalies but are not able to identify precisely the type of fault. The most performant of these methods do manage to efficiently group healthy panels and separate them from faulty panels. In that way, this article presents an unsupervised approach called DTW K-means. This approach takes advantages of both the dynamic time warping (DWT) metric and the Kmeans clustering algorithm as a data-driven approach. The results of this mixed method in a PV string are compared to diagnostic labels established by visual inspection of the panels.", "url": "https://arxiv.org/abs/2306.08003"}, {"metadata": {"arXiv": "2306.08004", "Date": "Tue, 13 Jun 2023 07:44:47 ", "Title": "Detection and classification of faults aimed at preventive maintenance of PV systems", "Authors": ["Edgar Hernando Sep\\'ulveda Oviedo (LAAS-DISCO", "LAAS-ISGE)", "Louise Trav\\'e-Massuy\\`es", "Audine Subias", "Marko Pavlov", "Corinne Alonso"], "Categories": "cs.LG cs.AI", "Journal-ref": "XI Congreso Internacional de Ingenier{\\'i}a Mec{\\'a}nica, Mecatr{\\'o}nica y Automatizaci{\\'o}n 2023, Universidad Nacional de Colombia, Apr 2023, Carthag{\\`e}ne, Colombia"}, "abstract": "Diagnosis in PV systems aims to detect, locate and identify faults. Diagnosing these faults is vital to guarantee energy production and extend the useful life of PV power plants. In the literature, multiple machine learning approaches have been proposed for this purpose. However, few of these works have paid special attention to the detection of fine faults and the specialized process of extraction and selection of features for their classification. A fine fault is one whose characteristic signature is difficult to distinguish to that of a healthy panel. As a contribution to the detection of fine faults (especially of the snail trail type), this article proposes an innovative approach based on the Random Forest (RF) algorithm. This approach uses a complex feature extraction and selection method that improves the computational time of fault classification while maintaining high accuracy.", "url": "https://arxiv.org/abs/2306.08004"}, {"metadata": {"arXiv": "2306.08009", "Date": "Tue, 13 Jun 2023 09:29:05 ", "Title": "DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation", "Authors": ["Zhicong Yan", "Shenghong Li", "Ruijie Zhao", "Yuan Tian", "Yuanyuan Zhao"], "Categories": "cs.LG cs.AI cs.CR", "Comments": ["It has been accepted by asiaccs"]}, "abstract": "Backdoor attacks have emerged as an urgent threat to Deep Neural Networks (DNNs), where victim DNNs are furtively implanted with malicious neurons that could be triggered by the adversary. To defend against backdoor attacks, many works establish a staged pipeline to remove backdoors from victim DNNs: inspecting, locating, and erasing. However, in a scenario where a few clean data can be accessible, such pipeline is fragile and cannot erase backdoors completely without sacrificing model accuracy. To address this issue, in this paper, we propose a novel data-free holistic backdoor erasing (DHBE) framework. Instead of the staged pipeline, the DHBE treats the backdoor erasing task as a unified adversarial procedure, which seeks equilibrium between two different competing processes: distillation and backdoor regularization. In distillation, the backdoored DNN is distilled into a proxy model, transferring its knowledge about clean data, yet backdoors are simultaneously transferred. In backdoor regularization, the proxy model is holistically regularized to prevent from infecting any possible backdoor transferred from distillation. These two processes jointly proceed with data-free adversarial optimization until a clean, high-accuracy proxy model is obtained. With the novel adversarial design, our framework demonstrates its superiority in three aspects: 1) minimal detriment to model accuracy, 2) high tolerance for hyperparameters, and 3) no demand for clean data. Extensive experiments on various backdoor attacks and datasets are performed to verify the effectiveness of the proposed framework. Code is available at \\url{https://github.com/yanzhicong/DHBE}", "url": "https://arxiv.org/abs/2306.08009"}, {"metadata": {"arXiv": "2306.08011", "Date": "Tue, 13 Jun 2023 11:08:30 ", "Title": "Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios", "Authors": ["Haochen Mei", "Gaolei Li", "Jun Wu", "Longfei Zheng"], "Categories": "cs.LG cs.AI cs.CR", "Comments": ["It can be accepted IJCNN"]}, "abstract": "Federated learning (FL) naturally faces the problem of data heterogeneity in real-world scenarios, but this is often overlooked by studies on FL security and privacy. On the one hand, the effectiveness of backdoor attacks on FL may drop significantly under non-IID scenarios. On the other hand, malicious clients may steal private data through privacy inference attacks. Therefore, it is necessary to have a comprehensive perspective of data heterogeneity, backdoor, and privacy inference. In this paper, we propose a novel privacy inference-empowered stealthy backdoor attack (PI-SBA) scheme for FL under non-IID scenarios. Firstly, a diverse data reconstruction mechanism based on generative adversarial networks (GANs) is proposed to produce a supplementary dataset, which can improve the attacker's local data distribution and support more sophisticated strategies for backdoor attacks. Based on this, we design a source-specified backdoor learning (SSBL) strategy as a demonstration, allowing the adversary to arbitrarily specify which classes are susceptible to the backdoor trigger. Since the PI-SBA has an independent poisoned data synthesis process, it can be integrated into existing backdoor attacks to improve their effectiveness and stealthiness in non-IID scenarios. Extensive experiments based on MNIST, CIFAR10 and Youtube Aligned Face datasets demonstrate that the proposed PI-SBA scheme is effective in non-IID FL and stealthy against state-of-the-art defense methods.", "url": "https://arxiv.org/abs/2306.08011"}, {"metadata": {"arXiv": "2306.08013", "Date": "Tue, 13 Jun 2023 11:46:00 ", "Title": "TopP\\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models", "Authors": ["Pum Jun Kim", "Yoojin Jang", "Jisu Kim", "Jaejun Yoo"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["8 pages", "6 figures"]}, "abstract": "We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\\'echet Inception Distance (FID), and the variants of Precision and Recall (P\\&R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\\&R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\\&R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\\&R is robust to outliers and non-independent and identically distributed (Non-IID) perturbations, while accurately capturing the true trend of change in samples. To the best of our knowledge, this is the first evaluation metric focused on the robust estimation of the support and provides its statistical consistency under noise.", "url": "https://arxiv.org/abs/2306.08013"}, {"metadata": {"arXiv": "2306.08021", "Date": "Tue, 13 Jun 2023 15:21:38 ", "Title": "Flexible Channel Dimensions for Differentiable Architecture Search", "Authors": ["Ahmet Caner Y\\\"uz\\\"ug\\\"uler and Nikolaos Dimitriadis and Pascal Frossard"], "Categories": "cs.LG cs.AI"}, "abstract": "Finding optimal channel dimensions (i.e., the number of filters in DNN layers) is essential to design DNNs that perform well under computational resource constraints. Recent work in neural architecture search aims at automating the optimization of the DNN model implementation. However, existing neural architecture search methods for channel dimensions rely on fixed search spaces, which prevents achieving an efficient and fully automated solution. In this work, we propose a novel differentiable neural architecture search method with an efficient dynamic channel allocation algorithm to enable a flexible search space for channel dimensions. We show that the proposed framework is able to find DNN architectures that are equivalent to previous methods in task accuracy and inference latency for the CIFAR-10 dataset with an improvement of $1.3-1.7\\times$ in GPU-hours and $1.5-1.7\\times$ in the memory requirements during the architecture search stage. Moreover, the proposed frameworks do not require a well-engineered search space a priori, which is an important step towards fully automated design of DNN architectures.", "url": "https://arxiv.org/abs/2306.08021"}, {"metadata": {"arXiv": "2306.08044", "Date": "Tue, 13 Jun 2023 18:02:57 ", "Title": "Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care", "Authors": ["Ali Shirali", "Alexander Schubert", "Ahmed Alaa"], "Categories": "cs.LG cs.AI"}, "abstract": "Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all while enabling the extraction of valuable information from intermediate signals that can guide the learning process. We evaluate our method in both off-policy and offline settings using simulated environments and real health records of patients in intensive care units. Our empirical results indicate that pruning significantly reduces the size of the action space while staying mostly consistent with the actions taken by physicians, outperforming the current state-of-the-art offline reinforcement learning method conservative Q-learning. Our work is a step towards developing reliable policies by effectively harnessing the wealth of available information in data-intensive critical care environments.", "url": "https://arxiv.org/abs/2306.08044"}, {"metadata": {"arXiv": "2306.08055", "Date": "Tue, 13 Jun 2023 18:22:24 ", "Title": "Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training", "Authors": ["Abraham J. Fetterman", "Ellie Kitanidis", "Joshua Albrecht", "Zachary Polizzi", "Bryden Fogelman", "Maksis Knutins", "Bartosz Wr\\'oblewski", "James B. Simon", "Kanjun Qiu"], "Categories": "cs.LG cs.AI"}, "abstract": "Hyperparameter tuning of deep learning models can lead to order-of-magnitude performance gains for the same amount of compute. Despite this, systematic tuning is uncommon, particularly for large models, which are expensive to evaluate and tend to have many hyperparameters, necessitating difficult judgment calls about tradeoffs, budgets, and search bounds. To address these issues and propose a practical method for robustly tuning large models, we present Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian optimization algorithm that performs local search around the performance-cost Pareto frontier. CARBS does well even in unbounded search spaces with many hyperparameters, learns scaling relationships so that it can tune models even as they are scaled up, and automates much of the \"black magic\" of tuning. Among our results, we effectively solve the entire ProcGen benchmark just by tuning a simple baseline (PPO, as provided in the original ProcGen paper). We also reproduce the model size vs. training tokens scaling result from the Chinchilla project (Hoffmann et al. 2022), while simultaneously discovering scaling laws for every other hyperparameter, via an easy automated process that uses significantly less compute and is applicable to any deep learning problem (not just language models).", "url": "https://arxiv.org/abs/2306.08055"}, {"metadata": {"arXiv": "2306.08149", "Date": "Tue, 13 Jun 2023 21:47:30 ", "Title": "Neural Mixed Effects for Nonlinear Personalized Predictions", "Authors": ["Torsten W\\\"ortwein", "Nicholas Allen", "Lisa B. Sheeber", "Randy P. Auerbach", "Jeffrey F. Cohn", "Louis-Philippe Morency"], "Categories": "cs.LG cs.AI"}, "abstract": "Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a neural network in a scalable manner. NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling. Empirically, we observe that NME improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent dataset to predict affective state sequences where half the mothers experience at least moderate symptoms of depression. Furthermore, we evaluate NME for two model architectures, including for neural conditional random fields (CRF) to predict affective state sequences where the CRF learns nonlinear person-specific temporal transitions between affective states. Analysis of these person-specific transitions on the mother-adolescent dataset shows interpretable trends related to the mother's depression symptoms.", "url": "https://arxiv.org/abs/2306.08149"}, {"metadata": {"arXiv": "2306.08157", "Date": "Tue, 13 Jun 2023 22:07:51 ", "Title": "Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks", "Authors": ["Rasoul Amirzadeh", "Asef Nazari", "Dhananjay Thiruvady", "and Mong Shan Ee"], "Categories": "cs.LG cs.AI q-fin.ST", "Comments": ["28 pages", "8 figures", "6 tables"]}, "abstract": "Cryptocurrencies have gained popularity across various sectors, especially in finance and investment. The popularity is partly due to their unique specifications originating from blockchain-related characteristics such as privacy, decentralisation, and untraceability. Despite their growing popularity, cryptocurrencies remain a high-risk investment due to their price volatility and uncertainty. The inherent volatility in cryptocurrency prices, coupled with internal cryptocurrency-related factors and external influential global economic factors makes predicting their prices and price movement directions challenging. Nevertheless, the knowledge obtained from predicting the direction of cryptocurrency prices can provide valuable guidance for investors in making informed investment decisions. To address this issue, this paper proposes a dynamic Bayesian network (DBN) approach, which can model complex systems in multivariate settings, to predict the price movement direction of five popular altcoins (cryptocurrencies other than Bitcoin) in the next trading day. The efficacy of the proposed model in predicting cryptocurrency price directions is evaluated from two perspectives. Firstly, our proposed approach is compared to two baseline models, namely an auto-regressive integrated moving average and support vector regression. Secondly, from a feature engineering point of view, the impact of twenty-three different features, grouped into four categories, on the DBN's prediction performance is investigated. The experimental results demonstrate that the DBN significantly outperforms the baseline models. In addition, among the groups of features, technical indicators are found to be the most effective predictors of cryptocurrency price directions.", "url": "https://arxiv.org/abs/2306.08157"}, {"metadata": {"arXiv": "2306.08166", "Date": "Tue, 13 Jun 2023 22:36:04 ", "Title": "Reinforcement Learning-Driven Linker Design via Fast Attention-based Point Cloud Alignment", "Authors": ["Rebecca M. Neeser", "Mehmet Akdel", "Daniel Kovtun", "Luca Naef"], "Categories": "cs.LG cs.AI q-bio.BM"}, "abstract": "Proteolysis-Targeting Chimeras (PROTACs) represent a novel class of small molecules which are designed to act as a bridge between an E3 ligase and a disease-relevant protein, thereby promoting its subsequent degradation. PROTACs are composed of two protein binding \"active\" domains, linked by a \"linker\" domain. The design of the linker domain is challenging due to geometric and chemical constraints given by its interactions, and the need to maximize drug-likeness. To tackle these challenges, we introduce ShapeLinker, a method for de novo design of linkers. It performs fragment-linking using reinforcement learning on an autoregressive SMILES generator. The method optimizes for a composite score combining relevant physicochemical properties and a novel, attention-based point cloud alignment score. This new method successfully generates linkers that satisfy both relevant 2D and 3D requirements, and achieves state-of-the-art results in producing novel linkers assuming a target linker conformation. This allows for more rational and efficient PROTAC design and optimization. Code and data are available at https://github.com/aivant/ShapeLinker.", "url": "https://arxiv.org/abs/2306.08166"}, {"metadata": {"arXiv": "2306.08194", "Date": "Wed, 14 Jun 2023 01:38:01 ", "Title": "Learning on Graphs under Label Noise", "Authors": ["Jingyang Yuan", "Xiao Luo", "Yifang Qin", "Yusheng Zhao", "Wei Ju", "Ming Zhang"], "Categories": "cs.LG cs.AI cs.IR cs.SI", "Comments": ["Accepted by IEEE International Conference on Acoustics", "Speech and Signal Processing (ICASSP 2023)"]}, "abstract": "Node classification on graphs is a significant task with a wide range of applications, including social analysis and anomaly detection. Even though graph neural networks (GNNs) have produced promising results on this task, current techniques often presume that label information of nodes is accurate, which may not be the case in real-world applications. To tackle this issue, we investigate the problem of learning on graphs with label noise and develop a novel approach dubbed Consistent Graph Neural Network (CGNN) to solve it. Specifically, we employ graph contrastive learning as a regularization term, which promotes two views of augmented nodes to have consistent representations. Since this regularization term cannot utilize label information, it can enhance the robustness of node representations to label noise. Moreover, to detect noisy labels on the graph, we present a sample selection technique based on the homophily assumption, which identifies noisy nodes by measuring the consistency between the labels with their neighbors. Finally, we purify these confident noisy labels to permit efficient semantic graph learning. Extensive experiments on three well-known benchmark datasets demonstrate the superiority of our CGNN over competing approaches.", "url": "https://arxiv.org/abs/2306.08194"}, {"metadata": {"arXiv": "2306.08232", "Date": "Wed, 14 Jun 2023 04:06:41 ", "Title": "Curricular Subgoals for Inverse Reinforcement Learning", "Authors": ["Shunyu Liu", "Yunpeng Qing", "Shuqi Xu", "Hongyan Wu", "Jiangtao Zhang", "Jingyuan Cong", "Tianhao Chen", "Yunfu Liu", "Mingli Song"], "Categories": "cs.LG cs.AI"}, "abstract": "Inverse Reinforcement Learning (IRL) aims to reconstruct the reward function from expert demonstrations to facilitate policy learning, and has demonstrated its remarkable success in imitation learning. To promote expert-like behavior, existing IRL methods mainly focus on learning global reward functions to minimize the trajectory difference between the imitator and the expert. However, these global designs are still limited by the redundant noise and error propagation problems, leading to the unsuitable reward assignment and thus downgrading the agent capability in complex multi-stage tasks. In this paper, we propose a novel Curricular Subgoal-based Inverse Reinforcement Learning (CSIRL) framework, that explicitly disentangles one task with several local subgoals to guide agent imitation. Specifically, CSIRL firstly introduces decision uncertainty of the trained agent over expert trajectories to dynamically select subgoals, which directly determines the exploration boundary of different task stages. To further acquire local reward functions for each stage, we customize a meta-imitation objective based on these curricular subgoals to train an intrinsic reward generator. Experiments on the D4RL and autonomous driving benchmarks demonstrate that the proposed methods yields results superior to the state-of-the-art counterparts, as well as better interpretability. Our code is available at https://github.com/Plankson/CSIRL.", "url": "https://arxiv.org/abs/2306.08232"}, {"metadata": {"arXiv": "2306.08277", "Date": "Wed, 14 Jun 2023 06:28:26 ", "Title": "FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks", "Authors": ["Mridul Gupta", "Hariprasad Kodamana", "Sayan Ranu"], "Categories": "cs.LG cs.AI", "Journal-ref": "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 23), 2023", "DOI": "10.1145/3580305.3599357"}, "abstract": "Modelling spatio-temporal processes on road networks is a task of growing importance. While significant progress has been made on developing spatio-temporal graph neural networks (Gnns), existing works are built upon three assumptions that are not practical on real-world road networks. First, they assume sensing on every node of a road network. In reality, due to budget-constraints or sensor failures, all locations (nodes) may not be equipped with sensors. Second, they assume that sensing history is available at all installed sensors. This is unrealistic as well due to sensor failures, loss of packets during communication, etc. Finally, there is an assumption of static road networks. Connectivity within networks change due to road closures, constructions of new roads, etc. In this work, we develop FRIGATE to address all these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that integrates positional, topological, and temporal information into rich inductive node representations. The joint fusion of this diverse information is made feasible through a novel combination of gated Lipschitz embeddings with Lstms. We prove that the proposed Gnn architecture is provably more expressive than message-passing Gnns used in state-of-the-art algorithms. The higher expressivity of FRIGATE naturally translates to superior empirical performance conducted on real-world network-constrained traffic data. In addition, FRIGATE is robust to frugal sensor deployment, changes in road network connectivity, and temporal irregularity in sensing.", "url": "https://arxiv.org/abs/2306.08277"}, {"metadata": {"arXiv": "2306.08293", "Date": "Wed, 14 Jun 2023 07:04:02 ", "Title": "Efficient Training of Physics-Informed Neural Networks with Direct Grid Refinement Algorithm", "Authors": ["Shikhar Nilabh and Fidel Grandia"], "Categories": "cs.LG cs.AI", "Comments": ["Submitted to ICML 2023 workshop: The Synergy of Scientific and Machine Learning Modelling"]}, "abstract": "This research presents the development of an innovative algorithm tailored for the adaptive sampling of residual points within the framework of Physics-Informed Neural Networks (PINNs). By addressing the limitations inherent in existing adaptive sampling techniques, our proposed methodology introduces a direct mesh refinement approach that effectively ensures both computational efficiency and adaptive point placement. Verification studies were conducted to evaluate the performance of our algorithm, showcasing reasonable agreement between the model based on our novel approach and benchmark model results. Comparative analyses with established adaptive resampling techniques demonstrated the superior performance of our approach, particularly when implemented with higher refinement factor. Overall, our findings highlight the enhancement of simulation accuracy achievable through the application of our adaptive sampling algorithm for Physics-Informed Neural Networks.", "url": "https://arxiv.org/abs/2306.08293"}, {"metadata": {"arXiv": "2306.08323", "Date": "Wed, 14 Jun 2023 07:47:44 ", "Title": "How to estimate carbon footprint when training deep learning models? A guide and review", "Authors": ["Lucia Bouza Heguerte (MAP5)", "Aur\\'elie Bugeau (IUF", "LaBRI", "UB)", "Lo\\\"ic Lannelongue"], "Categories": "cs.LG cs.AI cs.CY"}, "abstract": "Machine learning and deep learning models have become essential in the recent fast development of artificial intelligence in many sectors of the society. It is now widely acknowledge that the development of these models has an environmental cost that has been analyzed in many studies. Several online and software tools have been developed to track energy consumption while training machine learning models. In this paper, we propose a comprehensive introduction and comparison of these tools for AI practitioners wishing to start estimating the environmental impact of their work. We review the specific vocabulary, the technical requirements for each tool, and provide some advice on how and when to use these tools.", "url": "https://arxiv.org/abs/2306.08323"}, {"metadata": {"arXiv": "2306.08385", "Date": "Wed, 14 Jun 2023 09:21:15 ", "Title": "NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification", "Authors": ["Qitian Wu", "Wentao Zhao", "Zenan Li", "David Wipf", "Junchi Yan"], "Categories": "cs.LG cs.AI", "Comments": ["Published in NeurIPS 2022. 26 pages in total with the appendix"]}, "abstract": "Graph neural networks have been extensively studied for learning with inter-connected data. Despite this, recent evidence has revealed GNNs' deficiencies related to over-squashing, heterophily, handling long-range dependencies, edge incompleteness and particularly, the absence of graphs altogether. While a plausible solution is to learn new adaptive topology for message passing, issues concerning quadratic complexity hinder simultaneous guarantees for scalability and precision in large networks. In this paper, we introduce a novel all-pair message passing scheme for efficiently propagating node signals between arbitrary nodes, as an important building block for a pioneering Transformer-style network for node classification on large graphs, dubbed as \\textsc{NodeFormer}. Specifically, the efficient computation is enabled by a kernerlized Gumbel-Softmax operator that reduces the algorithmic complexity to linearity w.r.t. node numbers for learning latent graph structures from large, potentially fully-connected graphs in a differentiable manner. We also provide accompanying theory as justification for our design. Extensive experiments demonstrate the promising efficacy of the method in various tasks including node classification on graphs (with up to 2M nodes) and graph-enhanced applications (e.g., image classification) where input graphs are missing.", "url": "https://arxiv.org/abs/2306.08385"}, {"metadata": {"arXiv": "2306.08388", "Date": "Wed, 14 Jun 2023 09:24:32 ", "Title": "Skill-Critic: Refining Learned Skills for Reinforcement Learning", "Authors": ["Ce Hao", "Catherine Weaver", "Chen Tang", "Kenta Kawamoto", "Masayoshi Tomizuka", "Wei Zhan"], "Categories": "cs.LG cs.AI", "Comments": ["Preprint"]}, "abstract": "Hierarchical reinforcement learning (RL) can accelerate long-horizon decision-making by temporally abstracting a policy into multiple levels. Promising results in sparse reward environments have been seen with skills, i.e. sequences of primitive actions. Typically, a skill latent space and policy are discovered from offline data, but the resulting low-level policy can be unreliable due to low-coverage demonstrations or distribution shifts. As a solution, we propose fine-tuning the low-level policy in conjunction with high-level skill selection. Our Skill-Critic algorithm optimizes both the low and high-level policies; these policies are also initialized and regularized by the latent space learned from offline demonstrations to guide the joint policy optimization. We validate our approach in multiple sparse RL environments, including a new sparse reward autonomous racing task in Gran Turismo Sport. The experiments show that Skill-Critic's low-level policy fine-tuning and demonstration-guided regularization are essential for optimal performance. Images and videos are available at https://sites.google.com/view/skill-critic. We plan to open source the code with the final version.", "url": "https://arxiv.org/abs/2306.08388"}, {"metadata": {"arXiv": "2306.08448", "Date": "Wed, 14 Jun 2023 11:41:42 ", "Title": "Kalman Filter for Online Classification of Non-Stationary Data", "Authors": ["Michalis K. Titsias", "Alexandre Galashov", "Amal Rannen-Triki", "Razvan Pascanu", "Yee Whye Teh", "Jorg Bornschein"], "Categories": "cs.LG cs.AI"}, "abstract": "In Online Continual Learning (OCL) a learning system receives a stream of data and sequentially performs prediction and training steps. Important challenges in OCL are concerned with automatic adaptation to the particular non-stationary structure of the data, and with quantification of predictive uncertainty. Motivated by these challenges we introduce a probabilistic Bayesian online learning model by using a (possibly pretrained) neural representation and a state space model over the linear predictor weights. Non-stationarity over the linear predictor weights is modelled using a parameter drift transition density, parametrized by a coefficient that quantifies forgetting. Inference in the model is implemented with efficient Kalman filter recursions which track the posterior distribution over the linear weights, while online SGD updates over the transition dynamics coefficient allows to adapt to the non-stationarity seen in data. While the framework is developed assuming a linear Gaussian model, we also extend it to deal with classification problems and for fine-tuning the deep learning representation. In a set of experiments in multi-class classification using data sets such as CIFAR-100 and CLOC we demonstrate the predictive ability of the model and its flexibility to capture non-stationarity.", "url": "https://arxiv.org/abs/2306.08448"}, {"metadata": {"arXiv": "2306.08460", "Date": "Wed, 14 Jun 2023 12:04:28 ", "Title": "Improving Generalization in Meta-Learning via Meta-Gradient Augmentation", "Authors": ["Ren Wang", "Haoliang Sun", "Qi Wei", "Xiushan Nie", "Yuling Ma", "Yilong Yin"], "Categories": "cs.LG cs.AI"}, "abstract": "Meta-learning methods typically follow a two-loop framework, where each loop potentially suffers from notorious overfitting, hindering rapid adaptation and generalization to new tasks. Existing schemes solve it by enhancing the mutual-exclusivity or diversity of training samples, but these data manipulation strategies are data-dependent and insufficiently flexible. This work alleviates overfitting in meta-learning from the perspective of gradient regularization and proposes a data-independent \\textbf{M}eta-\\textbf{G}radient \\textbf{Aug}mentation (\\textbf{MGAug}) method. The key idea is to first break the rote memories by network pruning to address memorization overfitting in the inner loop, and then the gradients of pruned sub-networks naturally form the high-quality augmentation of the meta-gradient to alleviate learner overfitting in the outer loop. Specifically, we explore three pruning strategies, including \\textit{random width pruning}, \\textit{random parameter pruning}, and a newly proposed \\textit{catfish pruning} that measures a Meta-Memorization Carrying Amount (MMCA) score for each parameter and prunes high-score ones to break rote memories as much as possible. The proposed MGAug is theoretically guaranteed by the generalization bound from the PAC-Bayes framework. In addition, we extend a lightweight version, called MGAug-MaxUp, as a trade-off between performance gains and resource overhead. Extensive experiments on multiple few-shot learning benchmarks validate MGAug's effectiveness and significant improvement over various meta-baselines. The code is publicly available at \\url{https://github.com/xxLifeLover/Meta-Gradient-Augmentation}.", "url": "https://arxiv.org/abs/2306.08460"}, {"metadata": {"arXiv": "2306.08506", "Date": "Wed, 14 Jun 2023 13:43:44 ", "Title": "Probabilistic Regular Tree Priors for Scientific Symbolic Reasoning", "Authors": ["Tim Schneider", "Amin Totounferoush", "Wolfgang Nowak", "Steffen Staab"], "Categories": "cs.LG cs.AI cs.FL"}, "abstract": "Symbolic Regression (SR) allows for the discovery of scientific equations from data. To limit the large search space of possible equations, prior knowledge has been expressed in terms of formal grammars that characterize subsets of arbitrary strings. However, there is a mismatch between context-free grammars required to express the set of syntactically correct equations, missing closure properties of the former, and a tree structure of the latter. Our contributions are to (i) compactly express experts' prior beliefs about which equations are more likely to be expected by probabilistic Regular Tree Expressions (pRTE), and (ii) adapt Bayesian inference to make such priors efficiently available for symbolic regression encoded as finite state machines. Our scientific case studies show its effectiveness in soil science to find sorption isotherms and for modeling hyper-elastic materials.", "url": "https://arxiv.org/abs/2306.08506"}, {"metadata": {"arXiv": "2306.08586", "Date": "Wed, 14 Jun 2023 15:47:52 ", "Title": "Fed-ZERO: Efficient Zero-shot Personalization with Federated Mixture of Experts", "Authors": ["Chen Dun", "Mirian Hipolito Garcia", "Guoqing Zheng", "Ahmed Hassan Awadallah", "Robert Sim", "Anastasios Kyrillidis", "Dimitrios Dimitriadis"], "Categories": "cs.LG cs.AI math.OC", "Comments": ["14 Pages"]}, "abstract": "One of the goals in Federated Learning (FL) is to create personalized models that can adapt to the context of each participating client, while utilizing knowledge from a shared global model. Yet, often, personalization requires a fine-tuning step using clients' labeled data in order to achieve good performance. This may not be feasible in scenarios where incoming clients are fresh and/or have privacy concerns. It, then, remains open how one can achieve zero-shot personalization in these scenarios. We propose a novel solution by using a Mixture-of-Experts (MoE) framework within a FL setup. Our method leverages the diversity of the clients to train specialized experts on different subsets of classes, and a gating function to route the input to the most relevant expert(s). Our gating function harnesses the knowledge of a pretrained model common expert to enhance its routing decisions on-the-fly. As a highlight, our approach can improve accuracy up to 18\\% in state of the art FL settings, while maintaining competitive zero-shot performance. In practice, our method can handle non-homogeneous data distributions, scale more efficiently, and improve the state-of-the-art performance on common FL benchmarks.", "url": "https://arxiv.org/abs/2306.08586"}, {"metadata": {"arXiv": "2306.08604", "Date": "Wed, 14 Jun 2023 16:11:00 ", "Title": "A Unified Framework of Graph Information Bottleneck for Robustness and Membership Privacy", "Authors": ["Enyan Dai", "Limeng Cui", "Zhengyang Wang", "Xianfeng Tang", "Yinghan Wang", "Monica Cheng", "Bing Yin", "Suhang Wang"], "Categories": "cs.LG cs.AI cs.CR"}, "abstract": "Graph Neural Networks (GNNs) have achieved great success in modeling graph-structured data. However, recent works show that GNNs are vulnerable to adversarial attacks which can fool the GNN model to make desired predictions of the attacker. In addition, training data of GNNs can be leaked under membership inference attacks. This largely hinders the adoption of GNNs in high-stake domains such as e-commerce, finance and bioinformatics. Though investigations have been made in conducting robust predictions and protecting membership privacy, they generally fail to simultaneously consider the robustness and membership privacy. Therefore, in this work, we study a novel problem of developing robust and membership privacy-preserving GNNs. Our analysis shows that Information Bottleneck (IB) can help filter out noisy information and regularize the predictions on labeled samples, which can benefit robustness and membership privacy. However, structural noises and lack of labels in node classification challenge the deployment of IB on graph-structured data. To mitigate these issues, we propose a novel graph information bottleneck framework that can alleviate structural noises with neighbor bottleneck. Pseudo labels are also incorporated in the optimization to minimize the gap between the predictions on the labeled set and unlabeled set for membership privacy. Extensive experiments on real-world datasets demonstrate that our method can give robust predictions and simultaneously preserve membership privacy.", "url": "https://arxiv.org/abs/2306.08604"}, {"metadata": {"arXiv": "2306.08649", "Date": "Wed, 14 Jun 2023 17:28:46 ", "Title": "OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments", "Authors": ["Quentin Delfosse", "Jannis Bl\\\"uml", "Bjarne Gregori", "Sebastian Sztwiertnia", "Kristian Kersting"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["26 pages", "9 main paper pages", "14 appendix pages. In main paper: 5 figures", "2 tables"]}, "abstract": "Cognitive science and psychology suggest that object-centric representations of complex scenes are a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep reinforcement learning approaches rely on only pixel-based representations that do not capture the compositional properties of natural scenes. For this, we need environments and datasets that allow us to work and evaluate object-centric approaches. We present OCAtari, a set of environment that provides object-centric state representations of Atari games, the most-used evaluation framework for deep RL approaches. OCAtari also allows for RAM state manipulations of the games to change and create specific or even novel situations. The code base for this work is available at github.com/k4ntz/OC_Atari.", "url": "https://arxiv.org/abs/2306.08649"}, {"metadata": {"arXiv": "2306.08728", "Date": "Wed, 14 Jun 2023 20:13:24 ", "Title": "Towards trustworthy seizure onset detection using workflow notes", "Authors": ["Khaled Saab", "Siyi Tang", "Mohamed Taha", "Christopher Lee-Messer", "Christopher R\\'e", "Daniel Rubin"], "Categories": "cs.LG cs.AI eess.SP"}, "abstract": "A major barrier to deploying healthcare AI models is their trustworthiness. One form of trustworthiness is a model's robustness across different subgroups: while existing models may exhibit expert-level performance on aggregate metrics, they often rely on non-causal features, leading to errors in hidden subgroups. To take a step closer towards trustworthy seizure onset detection from EEG, we propose to leverage annotations that are produced by healthcare personnel in routine clinical workflows -- which we refer to as workflow notes -- that include multiple event descriptions beyond seizures. Using workflow notes, we first show that by scaling training data to an unprecedented level of 68,920 EEG hours, seizure onset detection performance significantly improves (+12.3 AUROC points) compared to relying on smaller training sets with expensive manual gold-standard labels. Second, we reveal that our binary seizure onset detection model underperforms on clinically relevant subgroups (e.g., up to a margin of 6.5 AUROC points between pediatrics and adults), while having significantly higher false positives on EEG clips showing non-epileptiform abnormalities compared to any EEG clip (+19 FPR points). To improve model robustness to hidden subgroups, we train a multilabel model that classifies 26 attributes other than seizures, such as spikes, slowing, and movement artifacts. We find that our multilabel model significantly improves overall seizure onset detection performance (+5.9 AUROC points) while greatly improving performance among subgroups (up to +8.3 AUROC points), and decreases false positives on non-epileptiform abnormalities by 8 FPR points. Finally, we propose a clinical utility metric based on false positives per 24 EEG hours and find that our multilabel model improves this clinical utility metric by a factor of 2x across different clinical settings.", "url": "https://arxiv.org/abs/2306.08728"}, {"metadata": {"arXiv": "2306.08762", "Date": "Wed, 14 Jun 2023 22:20:46 ", "Title": "Theoretical Hardness and Tractability of POMDPs in RL with Partial Hindsight State Information", "Authors": ["Ming Shi", "Yingbin Liang", "and Ness Shroff"], "Categories": "cs.LG cs.AI", "Comments": ["Submitted for publication"]}, "abstract": "Partially observable Markov decision processes (POMDPs) have been widely applied to capture many real-world applications. However, existing theoretical results have shown that learning in general POMDPs could be intractable, where the main challenge lies in the lack of latent state information. A key fundamental question here is how much hindsight state information (HSI) is sufficient to achieve tractability. In this paper, we establish a lower bound that reveals a surprising hardness result: unless we have full HSI, we need an exponentially scaling sample complexity to obtain an $\\epsilon$-optimal policy solution for POMDPs. Nonetheless, from the key insights in our lower-bound construction, we find that there exist important tractable classes of POMDPs even with partial HSI. In particular, for two novel classes of POMDPs with partial HSI, we provide new algorithms that are shown to be near-optimal by establishing new upper and lower bounds.", "url": "https://arxiv.org/abs/2306.08762"}, {"metadata": {"arXiv": "2306.08772", "Date": "Wed, 14 Jun 2023 22:50:25 ", "Title": "Katakomba: Tools and Benchmarks for Data-Driven NetHack", "Authors": ["Vladislav Kurenkov", "Alexander Nikulin", "Denis Tarasov", "Sergey Kolesnikov"], "Categories": "cs.LG cs.AI cs.NE", "Comments": ["Source code at https://github.com/tinkoff-ai/katakomba"]}, "abstract": "NetHack is known as the frontier of reinforcement learning research where learning-based methods still need to catch up to rule-based solutions. One of the promising directions for a breakthrough is using pre-collected datasets similar to recent developments in robotics, recommender systems, and more under the umbrella of offline reinforcement learning (ORL). Recently, a large-scale NetHack dataset was released; while it was a necessary step forward, it has yet to gain wide adoption in the ORL community. In this work, we argue that there are three major obstacles for adoption: tool-wise, implementation-wise, and benchmark-wise. To address them, we develop an open-source library that provides workflow fundamentals familiar to the ORL community: pre-defined D4RL-style tasks, uncluttered baseline implementations, and reliable evaluation tools with accompanying configs and logs synced to the cloud.", "url": "https://arxiv.org/abs/2306.08772"}, {"metadata": {"arXiv": "2306.08780", "Date": "Wed, 14 Jun 2023 23:24:01 ", "Title": "Explaining Explainability: Towards Deeper Actionable Insights into Deep Learning through Second-order Explainability", "Authors": ["E. Zhixuan Zeng", "Hayden Gunraj", "Sheldon Fernandez", "Alexander Wong"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Explainability plays a crucial role in providing a more comprehensive understanding of deep learning models' behaviour. This allows for thorough validation of the model's performance, ensuring that its decisions are based on relevant visual indicators and not biased toward irrelevant patterns existing in training data. However, existing methods provide only instance-level explainability, which requires manual analysis of each sample. Such manual review is time-consuming and prone to human biases. To address this issue, the concept of second-order explainable AI (SOXAI) was recently proposed to extend explainable AI (XAI) from the instance level to the dataset level. SOXAI automates the analysis of the connections between quantitative explanations and dataset biases by identifying prevalent concepts. In this work, we explore the use of this higher-level interpretation of a deep neural network's behaviour to allows us to \"explain the explainability\" for actionable insights. Specifically, we demonstrate for the first time, via example classification and segmentation cases, that eliminating irrelevant concepts from the training set based on actionable insights from SOXAI can enhance a model's performance.", "url": "https://arxiv.org/abs/2306.08780"}, {"metadata": {"arXiv": "2306.08803", "Date": "Thu, 15 Jun 2023 01:16:29 ", "Title": "Langevin Thompson Sampling with Logarithmic Communication: Bandits and Reinforcement Learning", "Authors": ["Amin Karbasi", "Nikki Lijing Kuang", "Yi-An Ma", "Siddharth Mitra"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["ICML 2023"], "ACM-class": "G.3; I.2.0"}, "abstract": "Thompson sampling (TS) is widely used in sequential decision making due to its ease of use and appealing empirical performance. However, many existing analytical and empirical results for TS rely on restrictive assumptions on reward distributions, such as belonging to conjugate families, which limits their applicability in realistic scenarios. Moreover, sequential decision making problems are often carried out in a batched manner, either due to the inherent nature of the problem or to serve the purpose of reducing communication and computation costs. In this work, we jointly study these problems in two popular settings, namely, stochastic multi-armed bandits (MABs) and infinite-horizon reinforcement learning (RL), where TS is used to learn the unknown reward distributions and transition dynamics, respectively. We propose batched $\\textit{Langevin Thompson Sampling}$ algorithms that leverage MCMC methods to sample from approximate posteriors with only logarithmic communication costs in terms of batches. Our algorithms are computationally efficient and maintain the same order-optimal regret guarantees of $\\mathcal{O}(\\log T)$ for stochastic MABs, and $\\mathcal{O}(\\sqrt{T})$ for RL. We complement our theoretical findings with experimental results.", "url": "https://arxiv.org/abs/2306.08803"}, {"metadata": {"arXiv": "2306.08810", "Date": "Thu, 15 Jun 2023 01:54:30 ", "Title": "Deep Generative Models for Decision-Making and Control", "Authors": ["Michael Janner"], "Categories": "cs.LG cs.AI", "Comments": ["UC Berkeley PhD thesis; supersedes arXiv:2010.14496", "arXiv:2106.02039", "and arXiv:2205.09991"]}, "abstract": "Deep model-based reinforcement learning methods offer a conceptually simple approach to the decision-making and control problem: use learning for the purpose of estimating an approximate dynamics model, and offload the rest of the work to classical trajectory optimization. However, this combination has a number of empirical shortcomings, limiting the usefulness of model-based methods in practice. The dual purpose of this thesis is to study the reasons for these shortcomings and to propose solutions for the uncovered problems. Along the way, we highlight how inference techniques from the contemporary generative modeling toolbox, including beam search, classifier-guided sampling, and image inpainting, can be reinterpreted as viable planning strategies for reinforcement learning problems.", "url": "https://arxiv.org/abs/2306.08810"}, {"metadata": {"arXiv": "2306.08854", "Date": "Thu, 15 Jun 2023 04:47:26 ", "Title": "A Gromov--Wasserstein Geometric View of Spectrum-Preserving Graph Coarsening", "Authors": ["Yifan Chen", "Rentian Yao", "Yun Yang", "Jie Chen"], "Categories": "cs.LG cs.AI stat.CO stat.ML", "Comments": ["To appear at ICML 2023. Code is available at https://github.com/ychen-stat-ml/GW-Graph-Coarsening"]}, "abstract": "Graph coarsening is a technique for solving large-scale graph problems by working on a smaller version of the original graph, and possibly interpolating the results back to the original graph. It has a long history in scientific computing and has recently gained popularity in machine learning, particularly in methods that preserve the graph spectrum. This work studies graph coarsening from a different perspective, developing a theory for preserving graph distances and proposing a method to achieve this. The geometric approach is useful when working with a collection of graphs, such as in graph classification and regression. In this study, we consider a graph as an element on a metric space equipped with the Gromov--Wasserstein (GW) distance, and bound the difference between the distance of two graphs and their coarsened versions. Minimizing this difference can be done using the popular weighted kernel $K$-means method, which improves existing spectrum-preserving methods with the proper choice of the kernel. The study includes a set of experiments to support the theory and method, including approximating the GW distance, preserving the graph spectrum, classifying graphs using spectral information, and performing regression using graph convolutional networks. Code is available at https://github.com/ychen-stat-ml/GW-Graph-Coarsening .", "url": "https://arxiv.org/abs/2306.08854"}, {"metadata": {"arXiv": "2306.08921", "Date": "Thu, 15 Jun 2023 07:48:32 ", "Title": "Multi-Temporal Relationship Inference in Urban Areas", "Authors": ["Shuangli Li", "Jingbo Zhou", "Ji Liu", "Tong Xu", "Enhong Chen", "Hui Xiong"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted by KDD 2023. Code and data: https://github.com/agave233/SEENet"]}, "abstract": "Finding multiple temporal relationships among locations can benefit a bunch of urban applications, such as dynamic offline advertising and smart public transport planning. While some efforts have been made on finding static relationships among locations, little attention is focused on studying time-aware location relationships. Indeed, abundant location-based human activities are time-varying and the availability of these data enables a new paradigm for understanding the dynamic relationships in a period among connective locations. To this end, we propose to study a new problem, namely multi-Temporal relationship inference among locations (Trial for short), where the major challenge is how to integrate dynamic and geographical influence under the relationship sparsity constraint. Specifically, we propose a solution to Trial with a graph learning scheme, which includes a spatially evolving graph neural network (SEENet) with two collaborative components: spatially evolving graph convolution module (SEConv) and spatially evolving self-supervised learning strategy (SE-SSL). SEConv performs the intra-time aggregation and inter-time propagation to capture the multifaceted spatially evolving contexts from the view of location message passing. In addition, SE-SSL designs time-aware self-supervised learning tasks in a global-local manner with additional evolving constraint to enhance the location representation learning and further handle the relationship sparsity. Finally, experiments on four real-world datasets demonstrate the superiority of our method over several state-of-the-art approaches.", "url": "https://arxiv.org/abs/2306.08921"}, {"metadata": {"arXiv": "2306.09005", "Date": "Thu, 15 Jun 2023 10:04:10 ", "Title": "Modularity Trumps Invariance for Compositional Robustness", "Authors": ["Ian Mason", "Anirban Sarkar", "Tomotake Sasaki", "Xavier Boix"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "By default neural networks are not robust to changes in data distribution. This has been demonstrated with simple image corruptions, such as blurring or adding noise, degrading image classification performance. Many methods have been proposed to mitigate these issues but for the most part models are evaluated on single corruptions. In reality, visual space is compositional in nature, that is, that as well as robustness to elemental corruptions, robustness to compositions of corruptions is also needed. In this work we develop a compositional image classification task where, given a few elemental corruptions, models are asked to generalize to compositions of these corruptions. That is, to achieve compositional robustness. We experimentally compare empirical risk minimization with an invariance building pairwise contrastive loss and, counter to common intuitions in domain generalization, achieve only marginal improvements in compositional robustness by encouraging invariance. To move beyond invariance, following previously proposed inductive biases that model architectures should reflect data structure, we introduce a modular architecture whose structure replicates the compositional nature of the task. We then show that this modular approach consistently achieves better compositional robustness than non-modular approaches. We additionally find empirical evidence that the degree of invariance between representations of 'in-distribution' elemental corruptions fails to correlate with robustness to 'out-of-distribution' compositions of corruptions.", "url": "https://arxiv.org/abs/2306.09005"}, {"metadata": {"arXiv": "2306.09118", "Date": "Thu, 15 Jun 2023 13:25:39 ", "Title": "Hyperbolic Representation Learning: Revisiting and Advancing", "Authors": ["Menglin Yang", "Min Zhou", "Rex Ying", "Yankai Chen", "Irwin King"], "Categories": "cs.LG cs.AI", "Comments": ["ICML 2023"]}, "abstract": "The non-Euclidean geometry of hyperbolic spaces has recently garnered considerable attention in the realm of representation learning. Current endeavors in hyperbolic representation largely presuppose that the underlying hierarchies can be automatically inferred and preserved through the adaptive optimization process. This assumption, however, is questionable and requires further validation. In this work, we first introduce a position-tracking mechanism to scrutinize existing prevalent \\hlms, revealing that the learned representations are sub-optimal and unsatisfactory. To address this, we propose a simple yet effective method, hyperbolic informed embedding (HIE), by incorporating cost-free hierarchical information deduced from the hyperbolic distance of the node to origin (i.e., induced hyperbolic norm) to advance existing \\hlms. The proposed method HIE is both task-agnostic and model-agnostic, enabling its seamless integration with a broad spectrum of models and tasks. Extensive experiments across various models and different tasks demonstrate the versatility and adaptability of the proposed method. Remarkably, our method achieves a remarkable improvement of up to 21.4\\% compared to the competing baselines.", "url": "https://arxiv.org/abs/2306.09118"}, {"metadata": {"arXiv": "2306.09147", "Date": "Thu, 15 Jun 2023 14:08:48 ", "Title": "Probabilistic Learning of Multivariate Time Series with Temporal Irregularity", "Authors": ["Yijun Li", "Cheuk Hang Leung", "Qi Wu"], "Categories": "cs.LG cs.AI"}, "abstract": "Multivariate sequential data collected in practice often exhibit temporal irregularities, including nonuniform time intervals and component misalignment. However, if uneven spacing and asynchrony are endogenous characteristics of the data rather than a result of insufficient observation, the information content of these irregularities plays a defining role in characterizing the multivariate dependence structure. Existing approaches for probabilistic forecasting either overlook the resulting statistical heterogeneities, are susceptible to imputation biases, or impose parametric assumptions on the data distribution. This paper proposes an end-to-end solution that overcomes these limitations by allowing the observation arrival times to play the central role of model construction, which is at the core of temporal irregularities. To acknowledge temporal irregularities, we first enable unique hidden states for components so that the arrival times can dictate when, how, and which hidden states to update. We then develop a conditional flow representation to non-parametrically represent the data distribution, which is typically non-Gaussian, and supervise this representation by carefully factorizing the log-likelihood objective to select conditional information that facilitates capturing time variation and path dependency. The broad applicability and superiority of the proposed solution are confirmed by comparing it with existing approaches through ablation studies and testing on real-world datasets.", "url": "https://arxiv.org/abs/2306.09147"}, {"metadata": {"arXiv": "2306.09200", "Date": "Thu, 15 Jun 2023 15:35:31 ", "Title": "ChessGPT: Bridging Policy Learning and Language Modeling", "Authors": ["Xidong Feng", "Yicheng Luo", "Ziyan Wang", "Hongrui Tang", "Mengyue Yang", "Kun Shao", "David Mguni", "Yali Du", "Jun Wang"], "Categories": "cs.LG cs.AI"}, "abstract": "When solving decision-making tasks, humans typically depend on information from two key sources: (1) Historical policy data, which provides interaction replay from the environment, and (2) Analytical insights in natural language form, exposing the invaluable thought process or strategic considerations. Despite this, the majority of preceding research focuses on only one source: they either use historical replay exclusively to directly learn policy or value functions, or engaged in language model training utilizing mere language corpus. In this paper, we argue that a powerful autonomous agent should cover both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning and language modeling by integrating data from these two sources in Chess games. Specifically, we build a large-scale game and language dataset related to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and ChessGPT, integrating policy learning and language modeling. Finally, we propose a full evaluation framework for evaluating language model's chess ability. Experimental results validate our model and dataset's effectiveness. We open source our code, model, and dataset at https://github.com/waterhorse1/ChessGPT.", "url": "https://arxiv.org/abs/2306.09200"}, {"metadata": {"arXiv": "2306.09222", "Date": "Thu, 15 Jun 2023 15:58:04 ", "Title": "Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization", "Authors": ["Ramnath Kumar and Kushal Majmundar and Dheeraj Nagaraj and Arun Sai Suggala"], "Categories": "cs.LG cs.AI"}, "abstract": "We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks. Our algorithm involves the importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with $f$-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with any popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate our approach's superiority on various tasks, including vanilla classification, classification with label imbalance, noisy labels, domain adaptation, and tabular representation learning. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +0.9%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in diverse domains.", "url": "https://arxiv.org/abs/2306.09222"}, {"metadata": {"arXiv": "2306.09293", "Date": "Thu, 15 Jun 2023 17:19:48 ", "Title": "Sampling-Based Techniques for Training Deep Neural Networks with Limited Computational Resources: A Scalability Evaluation", "Authors": ["Sana Ebrahimi", "Rishi Advani", "Abolfazl Asudeh"], "Categories": "cs.LG cs.AI"}, "abstract": "Deep neural networks are superior to shallow networks in learning complex representations. As such, there is a fast-growing interest in utilizing them in large-scale settings. The training process of neural networks is already known to be time-consuming, and having a deep architecture only aggravates the issue. This process consists mostly of matrix operations, among which matrix multiplication is the bottleneck. Several sampling-based techniques have been proposed for speeding up the training time of deep neural networks by approximating the matrix products. These techniques fall under two categories: (i) sampling a subset of nodes in every hidden layer as active at every iteration and (ii) sampling a subset of nodes from the previous layer to approximate the current layer's activations using the edges from the sampled nodes. In both cases, the matrix products are computed using only the selected samples. In this paper, we evaluate the scalability of these approaches on CPU machines with limited computational resources. Making a connection between the two research directions as special cases of approximating matrix multiplications in the context of neural networks, we provide a negative theoretical analysis that shows feedforward approximation is an obstacle against scalability. We conduct comprehensive experimental evaluations that demonstrate the most pressing challenges and limitations associated with the studied approaches. We observe that the hashing-based node selection method is not scalable to a large number of layers, confirming our theoretical analysis. Finally, we identify directions for future research.", "url": "https://arxiv.org/abs/2306.09293"}, {"metadata": {"arXiv": "2306.09302", "Date": "Thu, 15 Jun 2023 17:31:13 ", "Title": "Knowledge Guided Representation Learning and Causal Structure Learning in Soil Science", "Authors": ["Somya Sharma", "Swati Sharma", "Licheng Liu", "Rishabh Tushir", "Andy Neal", "Robert Ness", "John Crawford", "Emre Kiciman", "Ranveer Chandra"], "Categories": "cs.LG cs.AI"}, "abstract": "An improved understanding of soil can enable more sustainable land-use practices. Nevertheless, soil is called a complex, living medium due to the complex interaction of different soil processes that limit our understanding of soil. Process-based models and analyzing observed data provide two avenues for improving our understanding of soil processes. Collecting observed data is cost-prohibitive but reflects real-world behavior, while process-based models can be used to generate ample synthetic data which may not be representative of reality. We propose a framework, knowledge-guided representation learning, and causal structure learning (KGRCL), to accelerate scientific discoveries in soil science. The framework improves representation learning for simulated soil processes via conditional distribution matching with observed soil processes. Simultaneously, the framework leverages both observed and simulated data to learn a causal structure among the soil processes. The learned causal graph is more representative of ground truth than other graphs generated from other causal discovery methods. Furthermore, the learned causal graph is leveraged in a supervised learning setup to predict the impact of fertilizer use and changing weather on soil carbon. We present the results in five different locations to show the improvement in the prediction performance in out-of-sample and few-shots setting.", "url": "https://arxiv.org/abs/2306.09302"}, {"metadata": {"arXiv": "2306.09303", "Date": "Thu, 15 Jun 2023 17:31:26 ", "Title": "Datasets and Benchmarks for Offline Safe Reinforcement Learning", "Authors": ["Zuxin Liu", "Zijian Guo", "Haohong Lin", "Yihang Yao", "Jiacheng Zhu", "Zhepeng Cen", "Hanjiang Hu", "Wenhao Yu", "Tingnan Zhang", "Jie Tan", "Ding Zhao"], "Categories": "cs.LG cs.AI cs.RO", "Comments": ["22 pages.13 figures", "7 tables"]}, "abstract": "This paper presents a comprehensive benchmarking suite tailored to offline safe reinforcement learning (RL) challenges, aiming to foster progress in the development and evaluation of safe learning algorithms in both the training and deployment phases. Our benchmark suite contains three packages: 1) expertly crafted safe policies, 2) D4RL-styled datasets along with environment wrappers, and 3) high-quality offline safe RL baseline implementations. We feature a methodical data collection pipeline powered by advanced safe RL algorithms, which facilitates the generation of diverse datasets across 38 popular safe RL tasks, from robot control to autonomous driving. We further introduce an array of data post-processing filters, capable of modifying each dataset's diversity, thereby simulating various data collection conditions. Additionally, we provide elegant and extensible implementations of prevalent offline safe RL algorithms to accelerate research in this area. Through extensive experiments with over 50000 CPU and 800 GPU hours of computations, we evaluate and compare the performance of these baseline algorithms on the collected datasets, offering insights into their strengths, limitations, and potential areas of improvement. Our benchmarking framework serves as a valuable resource for researchers and practitioners, facilitating the development of more robust and reliable offline safe RL solutions in safety-critical applications. The benchmark website is available at \\url{www.offline-saferl.org}.", "url": "https://arxiv.org/abs/2306.09303"}, {"metadata": {"arXiv": "2306.09312", "Date": "Thu, 15 Jun 2023 17:47:31 ", "Title": "Semantic HELM: An Interpretable Memory for Reinforcement Learning", "Authors": ["Fabian Paischer", "Thomas Adler", "Markus Hofmarcher", "Sepp Hochreiter"], "Categories": "cs.LG cs.AI cs.CL stat.ML", "Comments": ["10 pages (+ references and appendix)", "Code: https://github.com/ml-jku/helm"]}, "abstract": "Reinforcement learning agents deployed in the real world often have to cope with partially observable environments. Therefore, most agents employ memory mechanisms to approximate the state of the environment. Recently, there have been impressive success stories in mastering partially observable environments, mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft. However, none of these methods are interpretable in the sense that it is not comprehensible for humans how the agent decides which actions to take based on its inputs. Yet, human understanding is necessary in order to deploy such methods in high-stake domains like autonomous driving or medical applications. We propose a novel memory mechanism that operates on human language to illuminate the decision-making process. First, we use CLIP to associate visual inputs with language tokens. Then we feed these tokens to a pretrained language model that serves the agent as memory and provides it with a coherent and interpretable representation of the past. Our memory mechanism achieves state-of-the-art performance in environments where memorizing the past is crucial to solve tasks. Further, we present situations where our memory component excels or fails to demonstrate strengths and weaknesses of our new approach.", "url": "https://arxiv.org/abs/2306.09312"}, {"metadata": {"arXiv": "2306.08041", "Date": "Tue, 13 Jun 2023 18:01:18 ", "Title": "On Faking a Nash Equilibrium", "Authors": ["Young Wu", "Jeremy McMahan", "Xiaojin Zhu", "Qiaomin Xie"], "Categories": "cs.MA cs.AI cs.CR cs.GT cs.LG"}, "abstract": "We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL algorithms.", "url": "https://arxiv.org/abs/2306.08041"}, {"metadata": {"arXiv": "2306.08647", "Date": "Wed, 14 Jun 2023 17:27:10 ", "Title": "Language to Rewards for Robotic Skill Synthesis", "Authors": ["Wenhao Yu", "Nimrod Gileadi", "Chuyuan Fu", "Sean Kirmani", "Kuang-Huei Lee", "Montse Gonzalez Arenas", "Hao-Tien Lewis Chiang", "Tom Erez", "Leonard Hasenclever", "Jan Humplik", "Brian Ichter", "Ted Xiao", "Peng Xu", "Andy Zeng", "Tingnan Zhang", "Nicolas Heess", "Dorsa Sadigh", "Jie Tan", "Yuval Tassa", "Fei Xia"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["https://language-to-reward.github.io/"]}, "abstract": "Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate interface generated by LLMs, we can effectively bridge the gap between high-level language instructions or corrections to low-level robot actions. Meanwhile, combining this with a real-time optimizer, MuJoCo MPC, empowers an interactive behavior creation experience where users can immediately observe the results and provide feedback to the system. To systematically evaluate the performance of our proposed method, we designed a total of 17 tasks for a simulated quadruped robot and a dexterous manipulator robot. We demonstrate that our proposed method reliably tackles 90% of the designed tasks, while a baseline using primitive skills as the interface with Code-as-policies achieves 50% of the tasks. We further validated our method on a real robot arm where complex manipulation skills such as non-prehensile pushing emerge through our interactive system.", "url": "https://arxiv.org/abs/2306.08647"}, {"metadata": {"arXiv": "2306.08748", "Date": "Wed, 14 Jun 2023 21:14:10 ", "Title": "Multi-Object Manipulation via Object-Centric Neural Scattering Functions", "Authors": ["Stephen Tian", "Yancheng Cai", "Hong-Xing Yu", "Sergey Zakharov", "Katherine Liu", "Adrien Gaidon", "Yunzhu Li", "Jiajun Wu"], "Categories": "cs.RO cs.AI cs.CV cs.LG", "Comments": ["First two authors contributed equally. Accepted at CVPR 2023. Project page: https://s-tian.github.io/projects/actionosf/"]}, "abstract": "Learned visual dynamics models have proven effective for robotic manipulation tasks. Yet, it remains unclear how best to represent scenes involving multi-object interactions. Current methods decompose a scene into discrete objects, but they struggle with precise modeling and manipulation amid challenging lighting conditions as they only encode appearance tied with specific illuminations. In this work, we propose using object-centric neural scattering functions (OSFs) as object representations in a model-predictive control framework. OSFs model per-object light transport, enabling compositional scene re-rendering under object rearrangement and varying lighting conditions. By combining this approach with inverse parameter estimation and graph-based neural dynamics models, we demonstrate improved model-predictive control performance and generalization in compositional multi-object environments, even in previously unseen scenarios and harsh lighting conditions.", "url": "https://arxiv.org/abs/2306.08748"}, {"metadata": {"arXiv": "2306.08870", "Date": "Thu, 15 Jun 2023 05:56:34 ", "Title": "Evolutionary Curriculum Training for DRL-Based Navigation Systems", "Authors": ["Max Asselmeier", "Zhaoyi Li", "Kelin Yu", "Danfei Xu"], "Categories": "cs.RO cs.AI cs.LG cs.MA", "Comments": ["Robotics: Science and Systems"]}, "abstract": "In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising method for robot collision avoidance. However, such DRL models often come with limitations, such as adapting effectively to structured environments containing various pedestrians. In order to solve this difficulty, previous research has attempted a few approaches, including training an end-to-end solution by integrating a waypoint planner with DRL and developing a multimodal solution to mitigate the drawbacks of the DRL model. However, these approaches have encountered several issues, including slow training times, scalability challenges, and poor coordination among different models. To address these challenges, this paper introduces a novel approach called evolutionary curriculum training to tackle these challenges. The primary goal of evolutionary curriculum training is to evaluate the collision avoidance model's competency in various scenarios and create curricula to enhance its insufficient skills. The paper introduces an innovative evaluation technique to assess the DRL model's performance in navigating structured maps and avoiding dynamic obstacles. Additionally, an evolutionary training environment generates all the curriculum to improve the DRL model's inadequate skills tested in the previous evaluation. We benchmark the performance of our model across five structured environments to validate the hypothesis that this evolutionary training environment leads to a higher success rate and a lower average number of collisions. Further details and results at our project website.", "url": "https://arxiv.org/abs/2306.08870"}, {"metadata": {"arXiv": "2306.09055", "Date": "Thu, 15 Jun 2023 11:27:30 ", "Title": "Predictive Maneuver Planning with Deep Reinforcement Learning (PMP-DRL) for comfortable and safe autonomous driving", "Authors": ["Jayabrata Chowdhury", "Vishruth Veerendranath", "Suresh Sundaram", "Narasimhan Sundararajan"], "Categories": "cs.RO cs.AI cs.LG"}, "abstract": "This paper presents a Predictive Maneuver Planning with Deep Reinforcement Learning (PMP-DRL) model for maneuver planning. Traditional rule-based maneuver planning approaches often have to improve their abilities to handle the variabilities of real-world driving scenarios. By learning from its experience, a Reinforcement Learning (RL)-based driving agent can adapt to changing driving conditions and improve its performance over time. Our proposed approach combines a predictive model and an RL agent to plan for comfortable and safe maneuvers. The predictive model is trained using historical driving data to predict the future positions of other surrounding vehicles. The surrounding vehicles' past and predicted future positions are embedded in context-aware grid maps. At the same time, the RL agent learns to make maneuvers based on this spatio-temporal context information. Performance evaluation of PMP-DRL has been carried out using simulated environments generated from publicly available NGSIM US101 and I80 datasets. The training sequence shows the continuous improvement in the driving experiences. It shows that proposed PMP-DRL can learn the trade-off between safety and comfortability. The decisions generated by the recent imitation learning-based model are compared with the proposed PMP-DRL for unseen scenarios. The results clearly show that PMP-DRL can handle complex real-world scenarios and make better comfortable and safe maneuver decisions than rule-based and imitative models.", "url": "https://arxiv.org/abs/2306.09055"}, {"metadata": {"arXiv": "2306.08715", "Date": "Wed, 14 Jun 2023 19:38:44 ", "Title": "Integrating machine learning paradigms and mixed-integer model predictive control for irrigation scheduling", "Authors": ["Bernard T. Agyeman", "Mohamed Naouri", "Willemijn Appels", "Jinfeng Liu (University of Alberta)", "Sirish L. Shah"], "Categories": "eess.SY cs.AI cs.LG cs.SY math.DS"}, "abstract": "The agricultural sector currently faces significant challenges in water resource conservation and crop yield optimization, primarily due to concerns over freshwater scarcity. Traditional irrigation scheduling methods often prove inadequate in meeting the needs of large-scale irrigation systems. To address this issue, this paper proposes a predictive irrigation scheduler that leverages the three paradigms of machine learning to optimize irrigation schedules. The proposed scheduler employs the k-means clustering approach to divide the field into distinct irrigation management zones based on soil hydraulic parameters and topology information. Furthermore, a long short-term memory network is employed to develop dynamic models for each management zone, enabling accurate predictions of soil moisture dynamics. Formulated as a mixed-integer model predictive control problem, the scheduler aims to maximize water uptake while minimizing overall water consumption and irrigation costs. To tackle the mixed-integer optimization challenge, the proximal policy optimization algorithm is utilized to train a reinforcement learning agent responsible for making daily irrigation decisions. To evaluate the performance of the proposed scheduler, a 26.4-hectare field in Lethbridge, Canada, was chosen as a case study for the 2015 and 2022 growing seasons. The results demonstrate the superiority of the proposed scheduler compared to a traditional irrigation scheduling method in terms of water use efficiency and crop yield improvement for both growing seasons. Notably, the proposed scheduler achieved water savings ranging from 6.4% to 22.8%, along with yield increases ranging from 2.3% to 4.3%.", "url": "https://arxiv.org/abs/2306.08715"}, {"metadata": {"arXiv": "2306.06815 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 01:22:39 ", "Title": "TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models", "Authors": ["Jiaqi Xue", "Yepeng Liu", "Mengxin Zheng", "Ting Hua", "Yilin Shen", "Ladislau Boloni and Qian Lou"], "Categories": "cs.CR cs.AI cs.CL cs.LG", "Comments": ["14 pages", "4 figures", "5 tables"]}, "abstract": "Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM APIs using few-shot data samples. Furthermore, we introduce a novel progressive trojan poisoning algorithm designed to generate poisoned prompts that retain efficacy and transferability across a diverse range of models. Our experiments and results demonstrate TrojPrompt's capacity to effectively insert Trojans into text prompts in real-world black-box PLM APIs, while maintaining exceptional performance on clean test sets and significantly outperforming baseline models. Our work sheds light on the potential security risks in current models and offers a potential defensive approach.", "url": "https://arxiv.org/abs/2306.06815"}, {"metadata": {"arXiv": "2306.07993 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 02:28:17 ", "Title": "Trustworthy Artificial Intelligence Framework for Proactive Detection and Risk Explanation of Cyber Attacks in Smart Grid", "Authors": ["Md. Shirajum Munir", "Sachin Shetty", "and Danda B. Rawat"], "Categories": "cs.CR cs.AI cs.LG", "Comments": ["Submitted for peer review"]}, "abstract": "The rapid growth of distributed energy resources (DERs), such as renewable energy sources, generators, consumers, and prosumers in the smart grid infrastructure, poses significant cybersecurity and trust challenges to the grid controller. Consequently, it is crucial to identify adversarial tactics and measure the strength of the attacker's DER. To enable a trustworthy smart grid controller, this work investigates a trustworthy artificial intelligence (AI) mechanism for proactive identification and explanation of the cyber risk caused by the control/status message of DERs. Thus, proposing and developing a trustworthy AI framework to facilitate the deployment of any AI algorithms for detecting potential cyber threats and analyzing root causes based on Shapley value interpretation while dynamically quantifying the risk of an attack based on Ward's minimum variance formula. The experiment with a state-of-the-art dataset establishes the proposed framework as a trustworthy AI by fulfilling the capabilities of reliability, fairness, explainability, transparency, reproducibility, and accountability.", "url": "https://arxiv.org/abs/2306.07993"}, {"metadata": {"arXiv": "2306.07994 (*cross-listing*)", "Date": "Mon, 12 Jun 2023 13:12:29 ", "Title": "MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer", "Authors": ["Yazheng Yang", "Zhou Zhao", "Qi Liu"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["Accepted by KDD23"], "DOI": "10.1145/3580305.3599438"}, "abstract": "Unsupervised text style transfer task aims to rewrite a text into target style while preserving its main content. Traditional methods rely on the use of a fixed-sized vector to regulate text style, which is difficult to accurately convey the style strength for each individual token. In fact, each token of a text contains different style intensity and makes different contribution to the overall style. Our proposed method addresses this issue by assigning individual style vector to each token in a text, allowing for fine-grained control and manipulation of the style strength. Additionally, an adversarial training framework integrated with teacher-student learning is introduced to enhance training stability and reduce the complexity of high-dimensional optimization. The results of our experiments demonstrate the efficacy of our method in terms of clearly improved style transfer accuracy and content preservation in both two-style transfer and multi-style transfer settings.", "url": "https://arxiv.org/abs/2306.07994"}, {"metadata": {"arXiv": "2306.08018 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 14:35:34 ", "Title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models", "Authors": ["Yin Fang", "Xiaozhuan Liang", "Ningyu Zhang", "Kangwei Liu", "Rui Huang", "Zhuo Chen", "Xiaohui Fan", "Huajun Chen"], "Categories": "q-bio.QM cs.AI cs.CE cs.CL cs.IR cs.LG", "Comments": ["Project homepage: https://github.com/zjunlp/Mol-Instructions"]}, "abstract": "Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomolecular research community. Mol-Instructions is made publicly accessible for future research endeavors and will be subjected to continual updates for enhanced applicability.", "url": "https://arxiv.org/abs/2306.08018"}, {"metadata": {"arXiv": "2306.08057 (*cross-listing*)", "Date": "Thu, 25 May 2023 04:11:14 ", "Title": "Symbolic Regression via Control Variable Genetic Programming", "Authors": ["Nan Jiang", "Yexiang Xue"], "Categories": "cs.NE cs.AI cs.LG"}, "abstract": "Learning symbolic expressions directly from experiment data is a vital step in AI-driven scientific discovery. Nevertheless, state-of-the-art approaches are limited to learning simple expressions. Regressing expressions involving many independent variables still remain out of reach. Motivated by the control variable experiments widely utilized in science, we propose Control Variable Genetic Programming (CVGP) for symbolic regression over many independent variables. CVGP expedites symbolic expression discovery via customized experiment design, rather than learning from a fixed dataset collected a priori. CVGP starts by fitting simple expressions involving a small set of independent variables using genetic programming, under controlled experiments where other variables are held as constants. It then extends expressions learned in previous generations by adding new independent variables, using new control variable experiments in which these variables are allowed to vary. Theoretically, we show CVGP as an incremental building approach can yield an exponential reduction in the search space when learning a class of expressions. Experimentally, CVGP outperforms several baselines in learning symbolic expressions involving multiple independent variables.", "url": "https://arxiv.org/abs/2306.08057"}, {"metadata": {"arXiv": "2306.08122 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 20:34:55 ", "Title": "Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level", "Authors": ["Mujahid Ali Quidwai", "Chunhui Li", "Parijat Dube"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["10 Pages", "4 Figures", "9 Tables", "to be published in 18th Workshop on Innovative Use of NLP for Building Educational Applications"]}, "abstract": "The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student's response. Our approach achieves up to 94% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of evaluating and detecting AI-generated text.", "url": "https://arxiv.org/abs/2306.08122"}, {"metadata": {"arXiv": "2306.08158 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 22:07:54 ", "Title": "Survey on Sociodemographic Bias in Natural Language Processing", "Authors": ["Vipul Gupta", "Pranav Narayanan Venkit", "Shomir Wilson", "Rebecca J. Passonneau"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["23 pages", "1 figure"]}, "abstract": "Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.", "url": "https://arxiv.org/abs/2306.08158"}, {"metadata": {"arXiv": "2306.08161 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 22:19:53 ", "Title": "h2oGPT: Democratizing Large Language Models", "Authors": ["Arno Candel", "Jon McKinney", "Philipp Singer", "Pascal Pfeiffer", "Maximilian Jeblick", "Prithvi Prabhu", "Jeff Gambera", "Mark Landry", "Shivam Bansal", "Ryan Chesler", "Chun Ming Lee", "Marcos V. Conde", "Pasha Stetsenko", "Olivier Grellier", "SriSatish Ambati"], "Categories": "cs.CL cs.AI cs.HC cs.IR cs.LG", "Comments": ["Work in progress by H2O.ai", "Inc"]}, "abstract": "Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material. We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language. Open-source language models help boost AI development and make it more accessible and trustworthy. They lower entry hurdles, allowing people and groups to tailor these models to their needs. This openness increases innovation, transparency, and fairness. An open-source strategy is needed to share AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.", "url": "https://arxiv.org/abs/2306.08161"}, {"metadata": {"arXiv": "2306.08162 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 22:25:35 ", "Title": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation", "Authors": ["Yuji Chai", "John Gkountouras", "Glenn G. Ko", "David Brooks", "Gu-Yeon Wei"], "Categories": "cs.CL cs.AI cs.LG"}, "abstract": "We introduce a method that dramatically reduces fine-tuning VRAM requirements and rectifies quantization errors in quantized Large Language Models. First, we develop an extremely memory-efficient fine-tuning (EMEF) method for quantized models using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an error-correcting algorithm designed to minimize errors induced by the quantization process. Our method reduces the memory requirements by up to 5.6 times, which enables fine-tuning a 7 billion parameter Large Language Model (LLM) on consumer laptops. At the same time, we propose a Low-Rank Error Correction (LREC) method that exploits the added LoRA layers to ameliorate the gap between the quantized model and its float point counterpart. Our error correction framework leads to a fully functional INT2 quantized LLM with the capacity to generate coherent English text. To the best of our knowledge, this is the first INT2 Large Language Model that has been able to reach such a performance. The overhead of our method is merely a 1.05 times increase in model size, which translates to an effective precision of INT2.1. Also, our method readily generalizes to other quantization standards, such as INT3, INT4, and INT8, restoring their lost performance, which marks a significant milestone in the field of model quantization. The strategies delineated in this paper hold promising implications for the future development and optimization of quantized models, marking a pivotal shift in the landscape of low-resource machine learning computations.", "url": "https://arxiv.org/abs/2306.08162"}, {"metadata": {"arXiv": "2306.08175 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 23:42:53 ", "Title": "DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer", "Authors": ["Goeric Huybrechts", "Srikanth Ronanki", "Xilai Li", "Hadis Nosrati", "Sravan Bodapati", "Katrin Kirchhoff"], "Categories": "eess.AS cs.AI cs.LG cs.SD"}, "abstract": "Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings.", "url": "https://arxiv.org/abs/2306.08175"}, {"metadata": {"arXiv": "2306.08193 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 01:34:16 ", "Title": "Operationalising Representation in Natural Language Processing", "Authors": ["Jacqueline Harding"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["Submitted for publication to the British Journal for the Philosophy of Science on 05/15/2023"]}, "abstract": "Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly). The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.", "url": "https://arxiv.org/abs/2306.08193"}, {"metadata": {"arXiv": "2306.08352 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 08:42:10 ", "Title": "Bayesian Non-linear Latent Variable Modeling via Random Fourier Features", "Authors": ["Michael Minyi Zhang", "Gregory W. Gundersen", "Barbara E. Engelhardt"], "Categories": "stat.ML cs.AI cs.LG"}, "abstract": "The Gaussian process latent variable model (GPLVM) is a popular probabilistic method used for nonlinear dimension reduction, matrix factorization, and state-space modeling. Inference for GPLVMs is computationally tractable only when the data likelihood is Gaussian. Moreover, inference for GPLVMs has typically been restricted to obtaining maximum a posteriori point estimates, which can lead to overfitting, or variational approximations, which mischaracterize the posterior uncertainty. Here, we present a method to perform Markov chain Monte Carlo (MCMC) inference for generalized Bayesian nonlinear latent variable modeling. The crucial insight necessary to generalize GPLVMs to arbitrary observation models is that we approximate the kernel function in the Gaussian process mappings with random Fourier features; this allows us to compute the gradient of the posterior in closed form with respect to the latent variables. We show that we can generalize GPLVMs to non-Gaussian observations, such as Poisson, negative binomial, and multinomial distributions, using our random feature latent variable model (RFLVM). Our generalized RFLVMs perform on par with state-of-the-art latent variable models on a wide range of applications, including motion capture, images, and text data for the purpose of estimating the latent structure and imputing the missing data of these complex data sets.", "url": "https://arxiv.org/abs/2306.08352"}, {"metadata": {"arXiv": "2306.08400 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 09:48:48 ", "Title": "Simple Embodied Language Learning as a Byproduct of Meta-Reinforcement Learning", "Authors": ["Evan Zheran Liu", "Sahaana Suri", "Tong Mu", "Allan Zhou", "Chelsea Finn"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["International Conference on Machine Learning (ICML)", "2023"]}, "abstract": "Whereas machine learning models typically learn language by directly training on language tasks (e.g., next-word prediction), language emerges in human children as a byproduct of solving non-language tasks (e.g., acquiring food). Motivated by this observation, we ask: can embodied reinforcement learning (RL) agents also indirectly learn language from non-language tasks? Learning to associate language with its meaning requires a dynamic environment with varied language. Therefore, we investigate this question in a multi-task environment with language that varies across the different tasks. Specifically, we design an office navigation environment, where the agent's goal is to find a particular office, and office locations differ in different buildings (i.e., tasks). Each building includes a floor plan with a simple language description of the goal office's location, which can be visually read as an RGB image when visited. We find RL agents indeed are able to indirectly learn language. Agents trained with current meta-RL algorithms successfully generalize to reading floor plans with held-out layouts and language phrases, and quickly navigate to the correct office, despite receiving no direct language supervision.", "url": "https://arxiv.org/abs/2306.08400"}, {"metadata": {"arXiv": "2306.08424 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 10:37:13 ", "Title": "Selective Concept Models: Permitting Stakeholder Customisation at Test-Time", "Authors": ["Matthew Barker", "Katherine M. Collins", "Krishnamurthy Dvijotham", "Adrian Weller", "Umang Bhatt"], "Categories": "cs.HC cs.AI cs.LG"}, "abstract": "Concept-based models perform prediction using a set of concepts that are interpretable to stakeholders. However, such models often involve a fixed, large number of concepts, which may place a substantial cognitive load on stakeholders. We propose Selective COncept Models (SCOMs) which make predictions using only a subset of concepts and can be customised by stakeholders at test-time according to their preferences. We show that SCOMs only require a fraction of the total concepts to achieve optimal accuracy on multiple real-world datasets. Further, we collect and release a new dataset, CUB-Sel, consisting of human concept set selections for 900 bird images from the popular CUB dataset. Using CUB-Sel, we show that humans have unique individual preferences for the choice of concepts they prefer to reason about, and struggle to identify the most theoretically informative concepts. The customisation and concept selection provided by SCOM improves the efficiency of interpretation and intervention for stakeholders.", "url": "https://arxiv.org/abs/2306.08424"}, {"metadata": {"arXiv": "2306.08526 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 14:21:55 ", "Title": "AlbMoRe: A Corpus of Movie Reviews for Sentiment Analysis in Albanian", "Authors": ["Erion \\c{C}ano"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["4 pages", "3 tables"]}, "abstract": "Lack of available resources such as text corpora for low-resource languages seriously hinders research on natural language processing and computational linguistics. This paper presents AlbMoRe, a corpus of 800 sentiment annotated movie reviews in Albanian. Each text is labeled as positive or negative and can be used for sentiment analysis research. Preliminary results based on traditional machine learning classifiers trained with the AlbMoRe samples are also reported. They can serve as comparison baselines for future research experiments.", "url": "https://arxiv.org/abs/2306.08526"}, {"metadata": {"arXiv": "2306.08756 (*cross-listing*)", "Date": "Wed, 14 Jun 2023 21:41:52 ", "Title": "Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq Models", "Authors": ["Saleh Soltan", "Andy Rosenbaum", "Tobias Falke", "Qin Lu", "Anna Rumshisky", "Wael Hamza"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["ACL Findings 2023 and SustaiNLP Workshop 2023"]}, "abstract": "Pre-trained encoder-only and sequence-to-sequence (seq2seq) models each have advantages, however training both model types from scratch is computationally expensive. We explore recipes to improve pre-training efficiency by initializing one model from the other. (1) Extracting the encoder from a seq2seq model, we show it under-performs a Masked Language Modeling (MLM) encoder, particularly on sequence labeling tasks. Variations of masking during seq2seq training, reducing the decoder size, and continuing with a small amount of MLM training do not close the gap. (2) Conversely, using an encoder to warm-start seq2seq training, we show that by unfreezing the encoder partway through training, we can match task performance of a from-scratch seq2seq model. Overall, this two-stage approach is an efficient recipe to obtain both a multilingual encoder and a seq2seq model, matching the performance of training each model from scratch while reducing the total compute cost by 27%.", "url": "https://arxiv.org/abs/2306.08756"}, {"metadata": {"arXiv": "2306.08970 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 09:05:36 ", "Title": "An Efficient and Multi-private Key Secure Aggregation for Federated Learning", "Authors": ["Xue Yang", "Zifeng Liu", "Xiaohu Tang", "Rongxing Lu", "and Bo Liu"], "Categories": "cs.CR cs.AI cs.CV cs.LG"}, "abstract": "With the emergence of privacy leaks in federated learning, secure aggregation protocols that mainly adopt either homomorphic encryption or threshold secret sharing have been widely developed for federated learning to protect the privacy of the local training data of each client. However, these existing protocols suffer from many shortcomings, such as the dependence on a trusted third party, the vulnerability to clients being corrupted, low efficiency, the trade-off between security and fault tolerance, etc. To solve these disadvantages, we propose an efficient and multi-private key secure aggregation scheme for federated learning. Specifically, we skillfully modify the variant ElGamal encryption technique to achieve homomorphic addition operation, which has two important advantages: 1) The server and each client can freely select public and private keys without introducing a trust third party and 2) Compared to the variant ElGamal encryption, the plaintext space is relatively large, which is more suitable for the deep model. Besides, for the high dimensional deep model parameter, we introduce a super-increasing sequence to compress multi-dimensional data into 1-D, which can greatly reduce encryption and decryption times as well as communication for ciphertext transmission. Detailed security analyses show that our proposed scheme achieves the semantic security of both individual local gradients and the aggregated result while achieving optimal robustness in tolerating both client collusion and dropped clients. Extensive simulations demonstrate that the accuracy of our scheme is almost the same as the non-private approach, while the efficiency of our scheme is much better than the state-of-the-art homomorphic encryption-based secure aggregation schemes. More importantly, the efficiency advantages of our scheme will become increasingly prominent as the number of model parameters increases.", "url": "https://arxiv.org/abs/2306.08970"}, {"metadata": {"arXiv": "2306.08997 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 09:48:14 ", "Title": "Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models", "Authors": ["Sarah J. Zhang", "Samuel Florin", "Ariel N. Lee", "Eamon Niknafs", "Andrei Marginean", "Annie Wang", "Keith Tyser", "Zad Chin", "Yann Hicke", "Nikhil Singh", "Madeleine Udell", "Yoon Kim", "Tonio Buonassisi", "Armando Solar-Lezama", "Iddo Drori"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["20 pages", "18 tables", "4 figures"]}, "abstract": "We curate a comprehensive dataset of 4,550 questions and solutions from problem sets, midterm exams, and final exams across all MIT Mathematics and Electrical Engineering and Computer Science (EECS) courses required for obtaining a degree. We evaluate the ability of large language models to fulfill the graduation requirements for any MIT major in Mathematics and EECS. Our results demonstrate that GPT-3.5 successfully solves a third of the entire MIT curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate on a test set excluding questions based on images. We fine-tune an open-source large language model on this dataset. We employ GPT-4 to automatically grade model responses, providing a detailed performance breakdown by course, question, and answer type. By embedding questions in a low-dimensional space, we explore the relationships between questions, topics, and classes and discover which questions and classes are required for solving other questions and classes through few-shot learning. Our analysis offers valuable insights into course prerequisites and curriculum design, highlighting language models' potential for learning and improving Mathematics and EECS education.", "url": "https://arxiv.org/abs/2306.08997"}, {"metadata": {"arXiv": "2306.09187 (*cross-listing*)", "Date": "Tue, 13 Jun 2023 13:48:06 ", "Title": "MolCAP: Molecular Chemical reActivity pretraining and prompted-finetuning enhanced molecular representation learning", "Authors": ["Yu Wang", "JingJie Zhang", "Junru Jin", "and Leyi Wei"], "Categories": "q-bio.BM cs.AI cs.LG"}, "abstract": "Molecular representation learning (MRL) is a fundamental task for drug discovery. However, previous deep-learning (DL) methods focus excessively on learning robust inner-molecular representations by mask-dominated pretraining framework, neglecting abundant chemical reactivity molecular relationships that have been demonstrated as the determining factor for various molecular property prediction tasks. Here, we present MolCAP to promote MRL, a graph pretraining Transformer based on chemical reactivity (IMR) knowledge with prompted finetuning. Results show that MolCAP outperforms comparative methods based on traditional molecular pretraining framework, in 13 publicly available molecular datasets across a diversity of biomedical tasks. Prompted by MolCAP, even basic graph neural networks are capable of achieving surprising performance that outperforms previous models, indicating the promising prospect of applying reactivity information for MRL. In addition, manual designed molecular templets are potential to uncover the dataset bias. All in all, we expect our MolCAP to gain more chemical meaningful insights for the entire process of drug discovery.", "url": "https://arxiv.org/abs/2306.09187"}, {"metadata": {"arXiv": "2306.09237 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 16:19:15 ", "Title": "SCALE: Scaling up the Complexity for Advanced Language Model Evaluation", "Authors": ["Vishvaksenan Rasiah", "Ronja Stern", "Veton Matoshi", "Matthias St\\\"urmer", "Ilias Chalkidis", "Daniel E. Ho", "Joel Niklaus"], "Categories": "cs.CL cs.AI cs.LG", "MSC-class": "68T50", "ACM-class": "I.2"}, "abstract": "Recent strides in Large Language Models (LLMs) have saturated many NLP benchmarks (even professional domain-specific ones), emphasizing the need for novel, more challenging novel ones to properly assess LLM capabilities. In this paper, we introduce a novel NLP benchmark that poses challenges to current LLMs across four key dimensions: processing long documents (up to 50K tokens), utilizing domain specific knowledge (embodied in legal texts), multilingual understanding (covering five languages), and multitasking (comprising legal document to document Information Retrieval, Court View Generation, Leading Decision Summarization, Citation Extraction, and eight challenging Text Classification tasks). Our benchmark comprises diverse legal NLP datasets from the Swiss legal system, allowing for a comprehensive study of the underlying Non-English, inherently multilingual, federal legal system. Despite recent advances, efficiently processing long documents for intense review/analysis tasks remains an open challenge for language models. Also, comprehensive, domain-specific benchmarks requiring high expertise to develop are rare, as are multilingual benchmarks. This scarcity underscores our contribution's value, considering most public models are trained predominantly on English corpora, while other languages remain understudied, particularly for practical domain-specific NLP tasks. Our benchmark allows for testing and advancing the state-of-the-art LLMs. As part of our study, we evaluate several pre-trained multilingual language models on our benchmark to establish strong baselines as a point of reference. Despite the large size of our datasets (tens to hundreds of thousands of examples), existing publicly available models struggle with most tasks, even after in-domain pretraining. We publish all resources (benchmark suite, pre-trained models, code) under a fully permissive open CC BY-SA license.", "url": "https://arxiv.org/abs/2306.09237"}, {"metadata": {"arXiv": "2306.09247 (*cross-listing*)", "Date": "Wed, 24 May 2023 05:27:22 ", "Title": "ATLAS: Automatically Detecting Discrepancies Between Privacy Policies and Privacy Labels", "Authors": ["Akshath Jain", "David Rodriguez", "Jose M. del Alamo", "Norman Sadeh"], "Categories": "cs.CR cs.AI cs.LG", "Comments": ["14 pages", "13 figures"]}, "abstract": "Privacy policies are long, complex documents that end-users seldom read. Privacy labels aim to ameliorate these issues by providing succinct summaries of salient data practices. In December 2020, Apple began requiring that app developers submit privacy labels describing their apps' data practices. Yet, research suggests that app developers often struggle to do so. In this paper, we automatically identify possible discrepancies between mobile app privacy policies and their privacy labels. Such discrepancies could be indicators of potential privacy compliance issues. We introduce the Automated Privacy Label Analysis System (ATLAS). ATLAS includes three components: a pipeline to systematically retrieve iOS App Store listings and privacy policies; an ensemble-based classifier capable of predicting privacy labels from the text of privacy policies with 91.3% accuracy using state-of-the-art NLP techniques; and a discrepancy analysis mechanism that enables a large-scale privacy analysis of the iOS App Store. Our system has enabled us to analyze 354,725 iOS apps. We find several interesting trends. For example, only 40.3% of apps in the App Store provide easily accessible privacy policies, and only 29.6% of apps provide both accessible privacy policies and privacy labels. Among apps that provide both, 88.0% have at least one possible discrepancy between the text of their privacy policy and their privacy label, which could be indicative of a potential compliance issue. We find that, on average, apps have 5.32 such potential compliance issues. We hope that ATLAS will help app developers, researchers, regulators, and mobile app stores alike. For example, app developers could use our classifier to check for discrepancies between their privacy policies and privacy labels, and regulators could use our system to help review apps at scale for potential compliance issues.", "url": "https://arxiv.org/abs/2306.09247"}, {"metadata": {"arXiv": "2306.09267 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 16:40:30 ", "Title": "Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?", "Authors": ["Dimitrios Ioannidis", "Jeremy Kepner", "Andrew Bowne", "Harriet S. Bryant"], "Categories": "cs.CY cs.AI cs.DL cs.LG cs.SE", "Comments": ["38 pages", "100+ references", "to appear in Fordham Law Review"]}, "abstract": "The rise of Generative Artificial Intelligence systems (``AI systems'') has created unprecedented social engagement. AI code generation systems provide responses (output) to questions or requests by accessing the vast library of open-source code created by developers over decades. However, they do so by allegedly stealing the open-source code stored in virtual libraries, known as repositories. How all this happens and whether there is a solution short of years of litigation that can protect innovation is the focus of this article. We also peripherally touch upon the array of issues raised by the relationship between AI and copyright. Looking ahead, we propose the following: (a) immediate changes to the licenses for open-source code created by developers that will allow access and/or use of any open-source code to humans only; (b) we suggest revisions to the Massachusetts Institute of Technology (``MIT'') license so that AI systems procure appropriate licenses from open-source code developers, which we believe will harmonize standards and build social consensus for the benefit of all of humanity rather than profit-driven centers of innovation; (c) We call for urgent legislative action to protect the future of AI systems while also promoting innovation; and (d) we propose that there is a shift in the burden of proof to AI systems in obfuscation cases.", "url": "https://arxiv.org/abs/2306.09267"}, {"metadata": {"arXiv": "2306.09299 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 17:27:20 ", "Title": "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind", "Authors": ["Swarnadeep Saha", "Peter Hase", "Mohit Bansal"], "Categories": "cs.CL cs.AI cs.LG", "Comments": ["21 pages", "12 figures. Our code is available at https://github.com/swarnaHub/ExplanationIntervention"]}, "abstract": "Large Language Models (LLMs) perform complex reasoning by generating explanations for their predictions. However, a complementary goal of explanations is to also communicate useful knowledge that improves weaker agents. Hence, we investigate whether LLMs also make good teachers for weaker agents. In particular, we consider a student-teacher framework between two LLM agents and study if, when, and how the teacher should intervene with natural language explanations to improve the student's performance. Since communication is expensive, we define a budget such that the teacher only communicates explanations for a fraction of the data, after which the student should perform well on its own. We decompose the teaching problem along four axes: (1) if teacher's test time intervention improve student predictions, (2) when it is worth explaining a data point, (3) how the teacher should personalize explanations to better teach the student, and (4) if teacher explanations also improve student performance on future unexplained data. We first show that teacher LLMs can indeed intervene on student reasoning to improve their performance. Next, we propose a Theory of Mind approach, in which the teacher builds two few-shot mental models of the student. The first model defines an Intervention Function that simulates the utility of an intervention, allowing the teacher to intervene when this utility is the highest and improving student performance at lower budgets. The second model enables the teacher to personalize explanations for a particular student and outperform unpersonalized teachers. We also demonstrate that in multi-turn interactions, teacher explanations generalize and learning from explained data improves student performance on future unexplained data. Finally, we also verify that misaligned teachers can lower student performance to random chance by intentionally misleading them.", "url": "https://arxiv.org/abs/2306.09299"}, {"metadata": {"arXiv": "2306.09313 (*cross-listing*)", "Date": "Thu, 15 Jun 2023 17:47:41 ", "Title": "Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction", "Authors": ["Rohit Paturi", "Sundararajan Srinivasan", "Xiang Li"], "Categories": "eess.AS cs.AI cs.CL cs.LG", "Comments": ["Accepted at INTERSPEECH 2023. arXiv admin note: text overlap with arXiv:1907.05337 by other authors"]}, "abstract": "Speaker diarization (SD) is typically used with an automatic speech recognition (ASR) system to ascribe speaker labels to recognized words. The conventional approach reconciles outputs from independently optimized ASR and SD systems, where the SD system typically uses only acoustic information to identify the speakers in the audio stream. This approach can lead to speaker errors especially around speaker turns and regions of speaker overlap. In this paper, we propose a novel second-pass speaker error correction system using lexical information, leveraging the power of modern language models (LMs). Our experiments across multiple telephony datasets show that our approach is both effective and robust. Training and tuning only on the Fisher dataset, this error correction approach leads to relative word-level diarization error rate (WDER) reductions of 15-30% on three telephony datasets: RT03-CTS, Callhome American English and held-out portions of Fisher.", "url": "https://arxiv.org/abs/2306.09313"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
