<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2309.06626", "Date": "Tue, 12 Sep 2023 22:28:53 ", "Title": "Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity", "Authors": ["Matteo Grimaldi", "Darshan C. Ganji", "Ivan Lazarevich", "Sudhakar Sah"], "Categories": "cs.CV cs.LG", "Comments": ["Code is available at http://github.com/Deeplite/activ-sparse"]}, "abstract": "The demand for efficient processing of deep neural networks (DNNs) on embedded devices is a significant challenge limiting their deployment. Exploiting sparsity in the network's feature maps is one of the ways to reduce its inference latency. It is known that unstructured sparsity results in lower accuracy degradation with respect to structured sparsity but the former needs extensive inference engine changes to get latency benefits. To tackle this challenge, we propose a solution to induce semi-structured activation sparsity exploitable through minor runtime modifications. To attain high speedup levels at inference time, we design a sparse training procedure with awareness of the final position of the activations while computing the General Matrix Multiplication (GEMM). We extensively evaluate the proposed solution across various models for image classification and object detection tasks. Remarkably, our approach yields a speed improvement of $1.25 \\times$ with a minimal accuracy drop of $1.1\\%$ for the ResNet18 model on the ImageNet dataset. Furthermore, when combined with a state-of-the-art structured pruning method, the resulting models provide a good latency-accuracy trade-off, outperforming models that solely employ structured pruning techniques.", "url": "https://arxiv.org/abs/2309.06626"}, {"metadata": {"arXiv": "2309.06703", "Date": "Wed, 13 Sep 2023 04:02:38 ", "Title": "VLSlice: Interactive Vision-and-Language Slice Discovery", "Authors": ["Eric Slyman", "Minsuk Kahng", "Stefan Lee"], "Categories": "cs.CV cs.CL cs.HC cs.LG", "Comments": ["Conference paper at ICCV 2023. 17 pages", "11 figures. https://ericslyman.com/vlslice/"], "ACM-class": "I.4.10; I.2.7; J.4"}, "abstract": "Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond \"tabular\" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly.", "url": "https://arxiv.org/abs/2309.06703"}, {"metadata": {"arXiv": "2309.06724", "Date": "Wed, 13 Sep 2023 04:57:12 ", "Title": "Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense", "Authors": ["Jianqiao Wangni"], "Categories": "cs.CV cs.LG eess.IV math.OC stat.ML"}, "abstract": "We aim to provide a general framework of for computational photography that recovers the real scene from imperfect images, via the Deep Nonparametric Convexified Filtering (DNCF). It is consists of a nonparametric deep network to resemble the physical equations behind the image formation, such as denoising, super-resolution, inpainting, and flash. DNCF has no parameterization dependent on training data, therefore has a strong generalization and robustness to adversarial image manipulation. During inference, we also encourage the network parameters to be nonnegative and create a bi-convex function on the input and parameters, and this adapts to second-order optimization algorithms with insufficient running time, having 10X acceleration over Deep Image Prior. With these tools, we empirically verify its capability to defend image classification deep networks against adversary attack algorithms in real-time.", "url": "https://arxiv.org/abs/2309.06724"}, {"metadata": {"arXiv": "2309.06742", "Date": "Wed, 13 Sep 2023 06:23:58 ", "Title": "MTD: Multi-Timestep Detector for Delayed Streaming Perception", "Authors": ["Yihui Huang", "Ningjiang Chen"], "Categories": "cs.CV cs.LG cs.RO", "Comments": ["12 pages", "accepted by PRCV 2023 (The 6th Chinese Conference on Pattern Recognition and Computer Vision)"]}, "abstract": "Autonomous driving systems require real-time environmental perception to ensure user safety and experience. Streaming perception is a task of reporting the current state of the world, which is used to evaluate the delay and accuracy of autonomous driving systems. In real-world applications, factors such as hardware limitations and high temperatures inevitably cause delays in autonomous driving systems, resulting in the offset between the model output and the world state. In order to solve this problem, this paper propose the Multi- Timestep Detector (MTD), an end-to-end detector which uses dynamic routing for multi-branch future prediction, giving model the ability to resist delay fluctuations. A Delay Analysis Module (DAM) is proposed to optimize the existing delay sensing method, continuously monitoring the model inference stack and calculating the delay trend. Moreover, a novel Timestep Branch Module (TBM) is constructed, which includes static flow and adaptive flow to adaptively predict specific timesteps according to the delay trend. The proposed method has been evaluated on the Argoverse-HD dataset, and the experimental results show that it has achieved state-of-the-art performance across various delay settings.", "url": "https://arxiv.org/abs/2309.06742"}, {"metadata": {"arXiv": "2309.06828", "Date": "Wed, 13 Sep 2023 09:22:49 ", "Title": "UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training", "Authors": ["Jiayu Lei", "Lisong Dai", "Haoyun Jiang", "Chaoyi Wu", "Xiaoman Zhang", "Yao Zhang", "Jiangchao Yao", "Weidi Xie", "Yanyong Zhang", "Yuehua Li", "Ya Zhang", "Yanfeng Wang"], "Categories": "cs.CV cs.LG"}, "abstract": "Magnetic resonance imaging~(MRI) have played a crucial role in brain disease diagnosis, with which a range of computer-aided artificial intelligence methods have been proposed. However, the early explorations usually focus on the limited types of brain diseases in one study and train the model on the data in a small scale, yielding the bottleneck of generalization. Towards a more effective and scalable paradigm, we propose a hierarchical knowledge-enhanced pre-training framework for the universal brain MRI diagnosis, termed as UniBrain. Specifically, UniBrain leverages a large-scale dataset of 24,770 imaging-report pairs from routine diagnostics. Different from previous pre-training techniques for the unitary vision or textual feature, or with the brute-force alignment between vision and language information, we leverage the unique characteristic of report information in different granularity to build a hierarchical alignment mechanism, which strengthens the efficiency in feature learning. Our UniBrain is validated on three real world datasets with severe class imbalance and the public BraTS2019 dataset. It not only consistently outperforms all state-of-the-art diagnostic methods by a large margin and provides a superior grounding performance but also shows comparable performance compared to expert radiologists on certain disease types.", "url": "https://arxiv.org/abs/2309.06828"}, {"metadata": {"arXiv": "2309.06891", "Date": "Wed, 13 Sep 2023 11:28:27 ", "Title": "Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?", "Authors": ["Bill Psomas", "Ioannis Kakogeorgiou", "Konstantinos Karantzalos", "Yannis Avrithis"], "Categories": "cs.CV cs.LG", "Comments": ["ICCV 2023. Code and models: https://github.com/billpsomas/simpool"], "Journal-ref": "International Conference on Computer Vision (2023)"}, "abstract": "Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem? In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. Code at: https://github.com/billpsomas/simpool.", "url": "https://arxiv.org/abs/2309.06891"}, {"metadata": {"arXiv": "2309.06895", "Date": "Wed, 13 Sep 2023 11:37:04 ", "Title": "MagiCapture: High-Resolution Multi-Concept Portrait Customization", "Authors": ["Junha Hyung", "Jaeyo Shin", "and Jaegul Choo"], "Categories": "cs.CV cs.GR cs.LG", "Comments": ["8 pages", "7 figures"]}, "abstract": "Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profile photos. The main challenge with this task is the absence of ground truth for the composed concepts, leading to a reduction in the quality of the final output and an identity shift of the source subject. To address these issues, we present a novel Attention Refocusing loss coupled with auxiliary priors, both of which facilitate robust learning within this weakly supervised learning setting. Our pipeline also includes additional post-processing steps to ensure the creation of highly realistic outputs. MagiCapture outperforms other baselines in both quantitative and qualitative evaluations and can also be generalized to other non-human objects.", "url": "https://arxiv.org/abs/2309.06895"}, {"metadata": {"arXiv": "2309.07113", "Date": "Wed, 13 Sep 2023 17:37:19 ", "Title": "Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology", "Authors": ["Nirhoshan Sivaroopan", "Chamuditha Jayanga", "Chalani Ekanayake", "Hasindri Watawana", "Jathurshan Pradeepkumar", "Mithunjha Anandakumar", "Ranga Rodrigo", "Chamira U. S. Edussooriya", "and Dushan N. Wadduwage"], "Categories": "cs.CV cs.LG", "Comments": ["18 pages", "8 figures"]}, "abstract": "Deep neural network models can learn clinically relevant features from millions of histopathology images. However generating high-quality annotations to train such models for each hospital, each cancer type, and each diagnostic task is prohibitively laborious. On the other hand, terabytes of training data -- while lacking reliable annotations -- are readily available in the public domain in some cases. In this work, we explore how these large datasets can be consciously utilized to pre-train deep networks to encode informative representations. We then fine-tune our pre-trained models on a fraction of annotated training data to perform specific downstream tasks. We show that our approach can reach the state-of-the-art (SOTA) for patch-level classification with only 1-10% randomly selected annotations compared to other SOTA approaches. Moreover, we propose an uncertainty-aware loss function, to quantify the model confidence during inference. Quantified uncertainty helps experts select the best instances to label for further training. Our uncertainty-aware labeling reaches the SOTA with significantly fewer annotations compared to random labeling. Last, we demonstrate how our pre-trained encoders can surpass current SOTA for whole-slide image classification with weak supervision. Our work lays the foundation for data and task-agnostic pre-trained deep networks with quantified uncertainty.", "url": "https://arxiv.org/abs/2309.07113"}, {"metadata": {"arXiv": "2309.06497", "Date": "Tue, 12 Sep 2023 18:11:10 ", "Title": "A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale", "Authors": ["Hao-Jun Michael Shi", "Tsung-Hsien Lee", "Shintaro Iwasaki", "Jose Gallego-Posada", "Zhijing Li", "Kaushik Rangadurai", "Dheevatsa Mudigere", "and Michael Rabbat"], "Categories": "cs.LG cs.DC cs.MS math.OC", "Comments": ["38 pages", "8 figures", "5 tables"]}, "abstract": "Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks. It constructs a block-diagonal preconditioner where each block consists of a coarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network. In this work, we provide a complete description of the algorithm as well as the performance optimizations that our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables fast multi-GPU distributed data-parallel training by distributing the memory and computation associated with blocks of each parameter via PyTorch's DTensor data structure and performing an AllGather primitive on the computed search directions at each iteration. This major performance enhancement enables us to achieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-scaling-based adaptive gradient methods. We validate our implementation by performing an ablation study on training ImageNet ResNet50, demonstrating Shampoo's superiority over standard training recipes with minimal hyperparameter tuning.", "url": "https://arxiv.org/abs/2309.06497"}, {"metadata": {"arXiv": "2309.06519", "Date": "Tue, 12 Sep 2023 18:50:24 ", "Title": "A Q-learning Approach for Adherence-Aware Recommendations", "Authors": ["Ioannis Faros and Aditya Dave and Andreas A. Malikopoulos"], "Categories": "cs.LG cs.SY eess.SY"}, "abstract": "In many real-world scenarios involving high-stakes and safety implications, a human decision-maker (HDM) may receive recommendations from an artificial intelligence while holding the ultimate responsibility of making decisions. In this letter, we develop an \"adherence-aware Q-learning\" algorithm to address this problem. The algorithm learns the \"adherence level\" that captures the frequency with which an HDM follows the recommended actions and derives the best recommendation policy in real time. We prove the convergence of the proposed Q-learning algorithm to the optimal value and evaluate its performance across various scenarios.", "url": "https://arxiv.org/abs/2309.06519"}, {"metadata": {"arXiv": "2309.06526", "Date": "Tue, 12 Sep 2023 19:08:26 ", "Title": "Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers", "Authors": ["Xilong Wang", "Chia-Mu Yu", "and Pin-Yu Chen"], "Categories": "cs.LG cs.CR", "Comments": ["submitted to ICASSP 2024"]}, "abstract": "For machine learning with tabular data, Table Transformer (TabTransformer) is a state-of-the-art neural network model, while Differential Privacy (DP) is an essential component to ensure data privacy. In this paper, we explore the benefits of combining these two aspects together in the scenario of transfer learning -- differentially private pre-training and fine-tuning of TabTransformers with a variety of parameter-efficient fine-tuning (PEFT) methods, including Adapter, LoRA, and Prompt Tuning. Our extensive experiments on the ACSIncome dataset show that these PEFT methods outperform traditional approaches in terms of the accuracy of the downstream task and the number of trainable parameters, thus achieving an improved trade-off among parameter efficiency, privacy, and accuracy. Our code is available at github.com/IBM/DP-TabTransformer.", "url": "https://arxiv.org/abs/2309.06526"}, {"metadata": {"arXiv": "2309.06534", "Date": "Tue, 12 Sep 2023 19:18:52 ", "Title": "Distributionally Robust Transfer Learning", "Authors": ["Xin Xiong", "Zijian Guo", "Tianxi Cai"], "Categories": "cs.LG stat.ME"}, "abstract": "Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to the baseline model. We also show that TransDRO achieves a faster convergence rate than the model fitted with the target data. Our comprehensive numerical studies and analysis of multi-institutional electronic health records data using TransDRO further substantiate the robustness and accuracy of TransDRO, highlighting its potential as a powerful tool in transfer learning applications.", "url": "https://arxiv.org/abs/2309.06534"}, {"metadata": {"arXiv": "2309.06577", "Date": "Mon, 11 Sep 2023 08:05:09 ", "Title": "Efficient Finite Initialization for Tensorized Neural Networks", "Authors": ["Alejandro Mata Ali", "I\\~nigo Perez Delgado", "Marina Ristol Roura and Aitor Moreno Fdez. de Leceta"], "Categories": "cs.LG quant-ph", "Comments": ["6 pages", "8 figures"], "MSC-class": "68Q12, 15A69, 68T07"}, "abstract": "We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb", "url": "https://arxiv.org/abs/2309.06577"}, {"metadata": {"arXiv": "2309.06584", "Date": "Tue, 12 Sep 2023 20:12:08 ", "Title": "Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction", "Authors": ["Xinyue Hu (1)", "Zenan Sun (1)", "Yi Nian (1)", "Yifang Dang (1)", "Fang Li (1)", "Jingna Feng (1)", "Evan Yu (1)", "Cui Tao (1) ((1) McWilliams School of Biomedical Informatics", "The University of Texas Health Science Center at Houston", "Houston", "TX", "USA)"], "Categories": "cs.LG"}, "abstract": "Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation. We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient Boost Machine as baselines. We further used our relation importance method to clarify the key relationships for ADRD risk prediction. VGNN surpassed other baseline models by 10% in the area under the receiver operating characteristic. The integration of the GNN model and relation importance interpretation could potentially play an essential role in providing valuable insight into factors that may contribute to or delay ADRD progression. Employing a GNN approach with claims data enhances ADRD risk prediction and provides insights into the impact of interconnected medical code relationships. This methodology not only enables ADRD risk modeling but also shows potential for other image analysis predictions using claims data.", "url": "https://arxiv.org/abs/2309.06584"}, {"metadata": {"arXiv": "2309.06599", "Date": "Tue, 12 Sep 2023 20:58:21 ", "Title": "Reasoning with Latent Diffusion in Offline Reinforcement Learning", "Authors": ["Siddarth Venkatraman", "Shivesh Khaitan", "Ravi Tej Akella", "John Dolan", "Jeff Schneider", "Glen Berseth"], "Categories": "cs.LG"}, "abstract": "Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data (as we show) or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-specific information for offline RL tasks as compared to raw state-actions. This improves credit assignment and facilitates faster reward propagation during Q-learning. Our method demonstrates state-of-the-art performance on the D4RL benchmarks, particularly excelling in long-horizon, sparse-reward tasks.", "url": "https://arxiv.org/abs/2309.06599"}, {"metadata": {"arXiv": "2309.06612", "Date": "Tue, 12 Sep 2023 21:37:26 ", "Title": "Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices", "Authors": ["Mohamed Imed Eddine Ghebriout", "Halima Bouzidi", "Smail Niar", "Hamza Ouarnoughi"], "Categories": "cs.LG cs.CV", "Comments": ["Accepted to the 15th Asian Conference on Machine Learning (ACML 2023)"]}, "abstract": "The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate information from diverse data sources. In MM-NN, features are extracted and fused from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose Harmonic-NAS, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. Harmonic-NAS involves a two-tier optimization approach for the unimodal backbone architectures and fusion strategy and operators. By incorporating the hardware dimension into the optimization, evaluation results on various devices and multimodal datasets have demonstrated the superiority of Harmonic-NAS over state-of-the-art approaches achieving up to 10.9% accuracy improvement, 1.91x latency reduction, and 2.14x energy efficiency gain.", "url": "https://arxiv.org/abs/2309.06612"}, {"metadata": {"arXiv": "2309.06613", "Date": "Tue, 12 Sep 2023 21:45:33 ", "Title": "Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials", "Authors": ["Chen Zhang", "Cl\\'emence Bos", "Stefan Sandfeld", "Ruth Schwaiger"], "Categories": "cs.LG cond-mat.mtrl-sci"}, "abstract": "In this study, Cu-Cr composites were studied by nanoindentation. Arrays of indents were placed over large areas of the samples resulting in datasets consisting of several hundred measurements of Young's modulus and hardness at varying indentation depths. The unsupervised learning technique, Gaussian mixture model, was employed to analyze the data, which helped to determine the number of \"mechanical phases\" and the respective mechanical properties. Additionally, a cross-validation approach was introduced to infer whether the data quantity was adequate and to suggest the amount of data required for reliable predictions -- one of the often encountered but difficult to resolve issues in machine learning of materials science problems.", "url": "https://arxiv.org/abs/2309.06613"}, {"metadata": {"arXiv": "2309.06619", "Date": "Tue, 12 Sep 2023 22:22:10 ", "Title": "RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models", "Authors": ["Yufei Li", "Zexin Li", "Wei Yang", "Cong Liu"], "Categories": "cs.LG cs.CL cs.SY eess.SY", "Comments": ["Accepted by RTSS 2023"]}, "abstract": "Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM, an uncertainty-aware resource management ecosystem for real-time inference of LMs. RT-LM innovatively quantifies how specific input uncertainties, adversely affect latency, often leading to an increased output length. Exploiting these insights, we devise a lightweight yet effective method to dynamically correlate input text uncertainties with output length at runtime. Utilizing this quantification as a latency heuristic, we integrate the uncertainty information into a system-level scheduler which explores several uncertainty-induced optimization opportunities, including uncertainty-aware prioritization, dynamic consolidation, and strategic CPU offloading. Quantitative experiments across five state-of-the-art LMs on two hardware platforms demonstrates that RT-LM can significantly reduce the average response time and improve throughput while incurring a rather small runtime overhead.", "url": "https://arxiv.org/abs/2309.06619"}, {"metadata": {"arXiv": "2309.06628", "Date": "Tue, 12 Sep 2023 22:34:34 ", "Title": "Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning", "Authors": ["Atticus Beachy (1)", "Harok Bae (1)", "Jose Camberos (2)", "Ramana Grandhi (2) ((1) Wright State University", "Dayton", "OH", "USA (2) Air Force Institute of Technology", "Wright-Patterson AFB", "OH", "USA)"], "Categories": "cs.LG"}, "abstract": "Emulator embedded neural networks, which are a type of physics informed neural network, leverage multi-fidelity data sources for efficient design exploration of aerospace engineering systems. Multiple realizations of the neural network models are trained with different random initializations. The ensemble of model realizations is used to assess epistemic modeling uncertainty caused due to lack of training samples. This uncertainty estimation is crucial information for successful goal-oriented adaptive learning in an aerospace system design exploration. However, the costs of training the ensemble models often become prohibitive and pose a computational challenge, especially when the models are not trained in parallel during adaptive learning. In this work, a new type of emulator embedded neural network is presented using the rapid neural network paradigm. Unlike the conventional neural network training that optimizes the weights and biases of all the network layers by using gradient-based backpropagation, rapid neural network training adjusts only the last layer connection weights by applying a linear regression technique. It is found that the proposed emulator embedded neural network trains near-instantaneously, typically without loss of prediction accuracy. The proposed method is demonstrated on multiple analytical examples, as well as an aerospace flight parameter study of a generic hypersonic vehicle.", "url": "https://arxiv.org/abs/2309.06628"}, {"metadata": {"arXiv": "2309.06634", "Date": "Tue, 12 Sep 2023 22:51:16 ", "Title": "$G$-Mapper: Learning a Cover in the Mapper Construction", "Authors": ["Enrique Alvarado", "Robin Belton", "Emily Fischer", "Kang-Ju Lee", "Sourabh Palande", "Sarah Percival", "Emilie Purvine"], "Categories": "cs.LG math.AT stat.ML"}, "abstract": "The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a \"nice\" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.", "url": "https://arxiv.org/abs/2309.06634"}, {"metadata": {"arXiv": "2309.06645", "Date": "Tue, 12 Sep 2023 23:54:24 ", "Title": "Bregman Graph Neural Network", "Authors": ["Jiayu Zhai", "Lequan Lin", "Dai Shi", "Junbin Gao"], "Categories": "cs.LG"}, "abstract": "Numerous recent research on graph neural networks (GNNs) has focused on formulating GNN architectures as an optimization problem with the smoothness assumption. However, in node classification tasks, the smoothing effect induced by GNNs tends to assimilate representations and over-homogenize labels of connected nodes, leading to adverse effects such as over-smoothing and misclassification. In this paper, we propose a novel bilevel optimization framework for GNNs inspired by the notion of Bregman distance. We demonstrate that the GNN layer proposed accordingly can effectively mitigate the over-smoothing issue by introducing a mechanism reminiscent of the \"skip connection\". We validate our theoretical results through comprehensive empirical studies in which Bregman-enhanced GNNs outperform their original counterparts in both homophilic and heterophilic graphs. Furthermore, our experiments also show that Bregman GNNs can produce more robust learning accuracy even when the number of layers is high, suggesting the effectiveness of the proposed method in alleviating the over-smoothing issue.", "url": "https://arxiv.org/abs/2309.06645"}, {"metadata": {"arXiv": "2309.06651", "Date": "Wed, 13 Sep 2023 00:30:32 ", "Title": "ConR: Contrastive Regularizer for Deep Imbalanced Regression", "Authors": ["Mahsa Keramati", "Lili Meng", "R. David Evans"], "Categories": "cs.LG"}, "abstract": "Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Conversely, local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. Serving the similarities of the predictions as an indicator of feature similarities, ConR discerns the dissagreements between the label space and feature space and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategies in a contrastive manner: incorrect proximities are penalized proportionate to the label similarities and the correct ones are encouraged to model local similarities. ConR consolidates essential considerations into a generic, easy-to-integrate, and efficient method that effectively addresses deep imbalanced regression. Moreover, ConR is orthogonal to existing approaches and smoothly extends to uni- and multi-dimensional label spaces. Our comprehensive experiments show that ConR significantly boosts the performance of all the state-of-the-art methods on three large-scale deep imbalanced regression benchmarks. Our code is publicly available in https://github.com/BorealisAI/ConR.", "url": "https://arxiv.org/abs/2309.06651"}, {"metadata": {"arXiv": "2309.06660", "Date": "Wed, 13 Sep 2023 01:22:16 ", "Title": "Generalizable Neural Fields as Partially Observed Neural Processes", "Authors": ["Jeffrey Gu", "Kuan-Chieh Wang", "Serena Yeung"], "Categories": "cs.LG cs.CV", "Comments": ["To appear ICCV 2023"]}, "abstract": "Neural fields, which represent signals as a function parameterized by a neural network, are a promising alternative to traditional discrete vector or grid-based representations. Compared to discrete representations, neural representations both scale well with increasing resolution, are continuous, and can be many-times differentiable. However, given a dataset of signals that we would like to represent, having to optimize a separate neural field for each signal is inefficient, and cannot capitalize on shared information or structures among signals. Existing generalization methods view this as a meta-learning problem and employ gradient-based meta-learning to learn an initialization which is then fine-tuned with test-time optimization, or learn hypernetworks to produce the weights of a neural field. We instead propose a new paradigm that views the large-scale training of neural representations as a part of a partially-observed neural process framework, and leverage neural process algorithms to solve this task. We demonstrate that this approach outperforms both state-of-the-art gradient-based meta-learning approaches and hypernetwork approaches.", "url": "https://arxiv.org/abs/2309.06660"}, {"metadata": {"arXiv": "2309.06683", "Date": "Wed, 13 Sep 2023 02:44:01 ", "Title": "Federated PAC-Bayesian Learning on Non-IID data", "Authors": ["Zihao Zhao", "Yang Liu", "Wenbo Ding", "Xiao-Ping Zhang"], "Categories": "cs.LG cs.DC"}, "abstract": "Existing research has either adapted the Probably Approximately Correct (PAC) Bayesian framework for federated learning (FL) or used information-theoretic PAC-Bayesian bounds while introducing their theorems, but few considering the non-IID challenges in FL. Our work presents the first non-vacuous federated PAC-Bayesian bound tailored for non-IID local data. This bound assumes unique prior knowledge for each client and variable aggregation weights. We also introduce an objective function and an innovative Gibbs-based algorithm for the optimization of the derived bound. The results are validated on real-world datasets.", "url": "https://arxiv.org/abs/2309.06683"}, {"metadata": {"arXiv": "2309.06708", "Date": "Wed, 13 Sep 2023 04:13:11 ", "Title": "Predicting Fatigue Crack Growth via Path Slicing and Re-Weighting", "Authors": ["Yingjie Zhao", "Yong Liu", "and Zhiping Xu"], "Categories": "cs.LG"}, "abstract": "Predicting potential risks associated with the fatigue of key structural components is crucial in engineering design. However, fatigue often involves entangled complexities of material microstructures and service conditions, making diagnosis and prognosis of fatigue damage challenging. We report a statistical learning framework to predict the growth of fatigue cracks and the life-to-failure of the components under loading conditions with uncertainties. Digital libraries of fatigue crack patterns and the remaining life are constructed by high-fidelity physical simulations. Dimensionality reduction and neural network architectures are then used to learn the history dependence and nonlinearity of fatigue crack growth. Path-slicing and re-weighting techniques are introduced to handle the statistical noises and rare events. The predicted fatigue crack patterns are self-updated and self-corrected by the evolving crack patterns. The end-to-end approach is validated by representative examples with fatigue cracks in plates, which showcase the digital-twin scenario in real-time structural health monitoring and fatigue life prediction for maintenance management decision-making.", "url": "https://arxiv.org/abs/2309.06708"}, {"metadata": {"arXiv": "2309.06717", "Date": "Wed, 13 Sep 2023 04:40:08 ", "Title": "Bias Amplification Enhances Minority Group Performance", "Authors": ["Gaotang Li", "Jiarui Liu", "Wei Hu"], "Categories": "cs.LG cs.CY", "Comments": ["21 pages", "14 figures"]}, "abstract": "Neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. Previous approaches based on worst-group loss minimization (e.g. Group-DRO) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. In this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. We propose BAM, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. Empirically, BAM achieves competitive performance compared with existing methods evaluated on spurious correlation benchmarks in computer vision and natural language processing. Moreover, we find a simple stopping criterion based on minimum class accuracy difference that can remove the need for group annotations, with little or no loss in worst-group accuracy. We perform extensive analyses and ablations to verify the effectiveness and robustness of our algorithm in varying class and group imbalance ratios.", "url": "https://arxiv.org/abs/2309.06717"}, {"metadata": {"arXiv": "2309.06739", "Date": "Wed, 13 Sep 2023 06:15:37 ", "Title": "MCNS: Mining Causal Natural Structures Inside Time Series via A Novel Internal Causality Scheme", "Authors": ["Yuanhao Liu and Dehui Du and Zihan Jiang and Anyan Huang and Yiyang Li"], "Categories": "cs.LG", "Comments": ["9 pages", "6 figures"]}, "abstract": "Causal inference permits us to discover covert relationships of various variables in time series. However, in most existing works, the variables mentioned above are the dimensions. The causality between dimensions could be cursory, which hinders the comprehension of the internal relationship and the benefit of the causal graph to the neural networks (NNs). In this paper, we find that causality exists not only outside but also inside the time series because it reflects a succession of events in the real world. It inspires us to seek the relationship between internal subsequences. However, the challenges are the hardship of discovering causality from subsequences and utilizing the causal natural structures to improve NNs. To address these challenges, we propose a novel framework called Mining Causal Natural Structure (MCNS), which is automatic and domain-agnostic and helps to find the causal natural structures inside time series via the internal causality scheme. We evaluate the MCNS framework and impregnation NN with MCNS on time series classification tasks. Experimental results illustrate that our impregnation, by refining attention, shape selection classification, and pruning datasets, drives NN, even the data itself preferable accuracy and interpretability. Besides, MCNS provides an in-depth, solid summary of the time series and datasets.", "url": "https://arxiv.org/abs/2309.06739"}, {"metadata": {"arXiv": "2309.06793", "Date": "Wed, 13 Sep 2023 08:28:16 ", "Title": "Electricity Demand Forecasting through Natural Language Processing with Long Short-Term Memory Networks", "Authors": ["Yun Bai", "Simon Camal", "Andrea Michiorri"], "Categories": "cs.LG", "Comments": ["5 pages", "3 figures", "2023 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)"]}, "abstract": "Electricity demand forecasting is a well established research field. Usually this task is performed considering historical loads, weather forecasts, calendar information and known major events. Recently attention has been given on the possible use of new sources of information from textual news in order to improve the performance of these predictions. This paper proposes a Long and Short-Term Memory (LSTM) network incorporating textual news features that successfully predicts the deterministic and probabilistic tasks of the UK national electricity demand. The study finds that public sentiment and word vector representations related to transport and geopolitics have time-continuity effects on electricity demand. The experimental results show that the LSTM with textual features improves by more than 3% compared to the pure LSTM benchmark and by close to 10% over the official benchmark. Furthermore, the proposed model effectively reduces forecasting uncertainty by narrowing the confidence interval and bringing the forecast distribution closer to the truth.", "url": "https://arxiv.org/abs/2309.06793"}, {"metadata": {"arXiv": "2309.06835", "Date": "Wed, 13 Sep 2023 09:34:21 ", "Title": "Safe Reinforcement Learning with Dual Robustness", "Authors": ["Zeyang Li", "Chuxiong Hu", "Yunan Wang", "Yujie Yang", "Shengbo Eben Li"], "Categories": "cs.LG"}, "abstract": "Reinforcement learning (RL) agents are vulnerable to adversarial disturbances, which can deteriorate task performance or compromise safety specifications. Existing methods either address safety requirements under the assumption of no adversary (e.g., safe RL) or only focus on robustness against performance adversaries (e.g., robust RL). Learning one policy that is both safe and robust remains a challenging open problem. The difficulty is how to tackle two intertwined aspects in the worst cases: feasibility and optimality. Optimality is only valid inside a feasible region, while identification of maximal feasible region must rely on learning the optimal policy. To address this issue, we propose a systematic framework to unify safe RL and robust RL, including problem formulation, iteration scheme, convergence analysis and practical algorithm design. This unification is built upon constrained two-player zero-sum Markov games. A dual policy iteration scheme is proposed, which simultaneously optimizes a task policy and a safety policy. The convergence of this iteration scheme is proved. Furthermore, we design a deep RL algorithm for practical implementation, called dually robust actor-critic (DRAC). The evaluations with safety-critical benchmarks demonstrate that DRAC achieves high performance and persistent safety under all scenarios (no adversary, safety adversary, performance adversary), outperforming all baselines significantly.", "url": "https://arxiv.org/abs/2309.06835"}, {"metadata": {"arXiv": "2309.06838", "Date": "Wed, 13 Sep 2023 09:39:42 ", "Title": "Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy", "Authors": ["Akshansh Mishra"], "Categories": "cs.LG math.OC stat.ML"}, "abstract": "Additive friction stir deposition (AFSD) is a novel solid-state additive manufacturing technique that circumvents issues of porosity, cracking, and properties anisotropy that plague traditional powder bed fusion and directed energy deposition approaches. However, correlations between process parameters, thermal profiles, and resulting microstructure in AFSD remain poorly understood. This hinders process optimization for properties. This work employs a cutting-edge framework combining supervised machine learning (SML) and physics-informed neural networks (PINNs) to predict peak temperature distribution in AFSD from process parameters. Eight regression algorithms were implemented for SML modeling, while four PINNs leveraged governing equations for transport, wave propagation, heat transfer, and quantum mechanics. Across multiple statistical measures, ensemble techniques like gradient boosting proved superior for SML, with lowest MSE of 165.78. The integrated ML approach was also applied to classify deposition quality from process factors, with logistic regression delivering robust accuracy. By fusing data-driven learning and fundamental physics, this dual methodology provides comprehensive insights into tailoring microstructure through thermal management in AFSD. The work demonstrates the power of bridging statistical and physics-based modeling for elucidating AM process-property relationships.", "url": "https://arxiv.org/abs/2309.06838"}, {"metadata": {"arXiv": "2309.06882", "Date": "Wed, 13 Sep 2023 11:16:52 ", "Title": "ProMap: Datasets for Product Mapping in E-commerce", "Authors": ["Kate\\v{r}ina Mackov\\'a", "Martin Pil\\'at"], "Categories": "cs.LG cs.CV cs.IR"}, "abstract": "The goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. Existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. Therefore, while predictive models trained on these datasets achieve good results on them, in practice, they are unusable as they cannot distinguish very similar but non-matching pairs of products. This paper introduces two new datasets for product mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn consisting of 1,555 English product pairs of matching and non-matching products manually scraped from two pairs of e-shops. The datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. Additionally, the non-matching products were selected in two phases, creating two types of non-matches -- close non-matches and medium non-matches. Even the medium non-matches are pairs of products that are much more similar than non-matches in other datasets -- for example, they still need to have the same brand and similar name and price. After simple data preprocessing, several machine learning algorithms were trained on these and two the other datasets to demonstrate the complexity and completeness of ProMap datasets. ProMap datasets are presented as a golden standard for further research of product mapping filling the gaps in existing ones.", "url": "https://arxiv.org/abs/2309.06882"}, {"metadata": {"arXiv": "2309.06896", "Date": "Wed, 13 Sep 2023 11:45:21 ", "Title": "Domain-Aware Augmentations for Unsupervised Online General Continual Learning", "Authors": ["Nicolas Michel", "Romain Negrel", "Giovanni Chierchia", "Jean-Fran\\c{c}ois Bercher"], "Categories": "cs.LG", "Comments": ["Accepted to BMVC'23"]}, "abstract": "Continual Learning has been challenging, especially when dealing with unsupervised scenarios such as Unsupervised Online General Continual Learning (UOGCL), where the learning agent has no prior knowledge of class boundaries or task change information. While previous research has focused on reducing forgetting in supervised setups, recent studies have shown that self-supervised learners are more resilient to forgetting. This paper proposes a novel approach that enhances memory usage for contrastive learning in UOGCL by defining and using stream-dependent data augmentations together with some implementation tricks. Our proposed method is simple yet effective, achieves state-of-the-art results compared to other unsupervised approaches in all considered setups, and reduces the gap between supervised and unsupervised continual learning. Our domain-aware augmentation procedure can be adapted to other replay-based methods, making it a promising strategy for continual learning.", "url": "https://arxiv.org/abs/2309.06896"}, {"metadata": {"arXiv": "2309.06921", "Date": "Wed, 13 Sep 2023 12:41:45 ", "Title": "Investigating the Impact of Action Representations in Policy Gradient Algorithms", "Authors": ["Jan Schneider", "Pierre Schumacher", "Daniel H\\\"aufle", "Bernhard Sch\\\"olkopf", "Dieter B\\\"uchler"], "Categories": "cs.LG", "Comments": ["Published at the Workshop on effective Representations", "Abstractions", "and Priors for Robot Learning (RAP4Robots) at ICRA 2023"]}, "abstract": "Reinforcement learning~(RL) is a versatile framework for learning to solve complex real-world tasks. However, influences on the learning performance of RL algorithms are often poorly understood in practice. We discuss different analysis techniques and assess their effectiveness for investigating the impact of action representations in RL. Our experiments demonstrate that the action representation can significantly influence the learning performance on popular RL benchmark tasks. The analysis results indicate that some of the performance differences can be attributed to changes in the complexity of the optimization landscape. Finally, we discuss open challenges of analysis techniques for RL algorithms.", "url": "https://arxiv.org/abs/2309.06921"}, {"metadata": {"arXiv": "2309.06979", "Date": "Wed, 13 Sep 2023 14:15:03 ", "Title": "Auto-Regressive Next-Token Predictors are Universal Learners", "Authors": ["Eran Malach"], "Categories": "cs.LG cs.CL"}, "abstract": "Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our results demonstrate that the power of language models can be attributed, to a great extent, to the auto-regressive next-token training scheme, and not necessarily to a particular choice of architecture.", "url": "https://arxiv.org/abs/2309.06979"}, {"metadata": {"arXiv": "2309.06991", "Date": "Wed, 13 Sep 2023 14:36:26 ", "Title": "Unsupervised Contrast-Consistent Ranking with Language Models", "Authors": ["Niklas Stoehr", "Pengxiang Cheng", "Jing Wang", "Daniel Preotiuc-Pietro", "Rajarshi Bhowmik"], "Categories": "cs.LG cs.CL stat.ML"}, "abstract": "Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pairwise or listwise comparisons. To this end, we extend the binary CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking methods such as the Max-Margin Loss, Triplet Loss, and Ordinal Regression objective. Our results confirm that, for the same language model, CCR probing outperforms prompting and even performs on a par with prompting much larger language models.", "url": "https://arxiv.org/abs/2309.06991"}, {"metadata": {"arXiv": "2309.07030", "Date": "Wed, 13 Sep 2023 15:36:39 ", "Title": "Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks", "Authors": ["James S. Nagai (1)", "Ivan G. Costa (1) and Michael T. Schaub (2) ((1) Institute for Computational Genomics", "RWTH Aachen Medical Faculty", "Germany", "(2) Department of Computer Science", "RWTH Aachen University", "Germany)"], "Categories": "cs.LG cs.SI cs.SY eess.SY q-bio.GN q-bio.MN", "Comments": ["5 pages", "1 figure"]}, "abstract": "Comparing graphs of optimal transport has recently gained significant attention, as the distances induced by optimal transport provide both a principled metric between graphs as well as an interpretable description of the associated changes between graphs in terms of a transport plan. As the lack of symmetry introduces challenges in the typically considered formulations, optimal transport distances for graphs have mostly been developed for undirected graphs. Here, we propose two distance measures to compare directed graphs based on variants of optimal transport: (i) an earth movers distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate these two distances and discuss their relative performance for both simulated graph data and real-world directed cell-cell communication graphs, inferred from single-cell RNA-seq data.", "url": "https://arxiv.org/abs/2309.07030"}, {"metadata": {"arXiv": "2309.07072", "Date": "Wed, 13 Sep 2023 16:33:27 ", "Title": "The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning", "Authors": ["Alexander Bastounis", "Alexander N. Gorban", "Anders C. Hansen", "Desmond J. Higham", "Danil Prokhorov", "Oliver Sutton", "Ivan Y. Tyukin", "Qinghua Zhou"], "Categories": "cs.LG", "MSC-class": "68T07, 68T05"}, "abstract": "In this work, we assess the theoretical limitations of determining guaranteed stability and accuracy of neural networks in classification tasks. We consider classical distribution-agnostic framework and algorithms minimising empirical risks and potentially subjected to some weights regularisation. We show that there is a large family of tasks for which computing and verifying ideal stable and accurate neural networks in the above settings is extremely challenging, if at all possible, even when such ideal solutions exist within the given class of neural architectures.", "url": "https://arxiv.org/abs/2309.07072"}, {"metadata": {"arXiv": "2309.07117", "Date": "Wed, 13 Sep 2023 17:55:11 ", "Title": "PILOT: A Pre-Trained Model-Based Continual Learning Toolbox", "Authors": ["Hai-Long Sun", "Da-Wei Zhou", "Han-Jia Ye", "De-Chuan Zhan"], "Categories": "cs.LG cs.CV", "Comments": ["Code is available at https://github.com/sun-hailong/LAMDA-PILOT"]}, "abstract": "While traditional machine learning can effectively tackle a wide range of problems, it primarily operates within a closed-world setting, which presents limitations when dealing with streaming data. As a solution, incremental learning emerges to address real-world scenarios involving new data's arrival. Recently, pre-training has made significant advancements and garnered the attention of numerous researchers. The strong performance of these pre-trained models (PTMs) presents a promising avenue for developing continual learning algorithms that can effectively adapt to real-world scenarios. Consequently, exploring the utilization of PTMs in incremental learning has become essential. This paper introduces a pre-trained model-based continual learning toolbox known as PILOT. On the one hand, PILOT implements some state-of-the-art class-incremental learning algorithms based on pre-trained models, such as L2P, DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical class-incremental learning algorithms (e.g., DER, FOSTER, and MEMO) within the context of pre-trained models to evaluate their effectiveness.", "url": "https://arxiv.org/abs/2309.07117"}, {"metadata": {"arXiv": "2309.06655", "Date": "Wed, 13 Sep 2023 01:02:42 ", "Title": "Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models", "Authors": ["Alonso Marco and Elias Morley and Claire J. Tomlin"], "Categories": "cs.RO cs.LG", "Comments": ["7 pages", "4 figures"]}, "abstract": "In order for robots to safely navigate in unseen scenarios using learning-based methods, it is important to accurately detect out-of-training-distribution (OoD) situations online. Recently, Gaussian process state-space models (GPSSMs) have proven useful to discriminate unexpected observations by comparing them against probabilistic predictions. However, the capability for the model to correctly distinguish between in- and out-of-training distribution observations hinges on the accuracy of these predictions, primarily affected by the class of functions the GPSSM kernel can represent. In this paper, we propose (i) a novel approach to embed existing domain knowledge in the kernel and (ii) an OoD online runtime monitor, based on receding-horizon predictions. Domain knowledge is assumed given as a dataset collected either in simulation or using a nominal model. Numerical results show that the informed kernel yields better regression quality with smaller datasets, as compared to standard kernel choices. We demonstrate the effectiveness of the OoD monitor on a real quadruped navigating an indoor setting, which reliably classifies previously unseen terrains.", "url": "https://arxiv.org/abs/2309.06655"}, {"metadata": {"arXiv": "2309.06569", "Date": "Tue, 12 Sep 2023 20:04:16 ", "Title": "Promises of Deep Kernel Learning for Control Synthesis", "Authors": ["Robert Reed", "Luca Laurenti", "Morteza Lahijanian"], "Categories": "eess.SY cs.LG cs.SY", "Comments": ["9 pages", "4 figures", "3 tables"]}, "abstract": "Deep Kernel Learning (DKL) combines the representational power of neural networks with the uncertainty quantification of Gaussian Processes. Hence, it is potentially a promising tool to learn and control complex dynamical systems. In this work, we develop a scalable abstraction-based framework that enables the use of DKL for control synthesis of stochastic dynamical systems against complex specifications. Specifically, we consider temporal logic specifications and create an end-to-end framework that uses DKL to learn an unknown system from data and formally abstracts the DKL model into an Interval Markov Decision Process (IMDP) to perform control synthesis with correctness guarantees. Furthermore, we identify a deep architecture that enables accurate learning and efficient abstraction computation. The effectiveness of our approach is illustrated on various benchmarks, including a 5-D nonlinear stochastic system, showing how control synthesis with DKL can substantially outperform state-of-the-art competitive methods.", "url": "https://arxiv.org/abs/2309.06569"}, {"metadata": {"arXiv": "2309.06588", "Date": "Tue, 12 Sep 2023 20:24:37 ", "Title": "Convergence of Gradient-based MAML in LQR", "Authors": ["Negin Musavi and Geir E. Dullerud"], "Categories": "eess.SY cs.LG cs.SY"}, "abstract": "The main objective of this research paper is to investigate the local convergence characteristics of Model-agnostic Meta-learning (MAML) when applied to linear system quadratic optimal control (LQR). MAML and its variations have become popular techniques for quickly adapting to new tasks by leveraging previous learning knowledge in areas like regression, classification, and reinforcement learning. However, its theoretical guarantees remain unknown due to non-convexity and its structure, making it even more challenging to ensure stability in the dynamic system setting. This study focuses on exploring MAML in the LQR setting, providing its local convergence guarantees while maintaining the stability of the dynamical system. The paper also presents simple numerical results to demonstrate the convergence properties of MAML in LQR tasks.", "url": "https://arxiv.org/abs/2309.06588"}, {"metadata": {"arXiv": "2309.06658", "Date": "Wed, 13 Sep 2023 01:13:33 ", "Title": "Dissipative Imitation Learning for Discrete Dynamic Output Feedback Control with Sparse Data Sets", "Authors": ["Amy K. Strong", "Ethan J. LoCicero", "Leila J. Bridgeman"], "Categories": "eess.SY cs.LG cs.SY"}, "abstract": "Imitation learning enables the synthesis of controllers for complex objectives and highly uncertain plant models. However, methods to provide stability guarantees to imitation learned controllers often rely on large amounts of data and/or known plant models. In this paper, we explore an input-output (IO) stability approach to dissipative imitation learning, which achieves stability with sparse data sets and with little known about the plant model. A closed-loop stable dynamic output feedback controller is learned using expert data, a coarse IO plant model, and a new constraint to enforce dissipativity on the learned controller. While the learning objective is nonconvex, iterative convex overbounding (ICO) and projected gradient descent (PGD) are explored as methods to successfully learn the controller. This new imitation learning method is applied to two unknown plants and compared to traditionally learned dynamic output feedback controller and neural network controller. With little knowledge of the plant model and a small data set, the dissipativity constrained learned controller achieves closed loop stability and successfully mimics the behavior of the expert controller, while other methods often fail to maintain stability and achieve good performance.", "url": "https://arxiv.org/abs/2309.06658"}, {"metadata": {"arXiv": "2309.06629", "Date": "Tue, 12 Sep 2023 22:44:14 ", "Title": "The Relational Bottleneck as an Inductive Bias for Efficient Abstraction", "Authors": ["Taylor W. Webb", "Steven M. Frankland", "Awni Altabaa", "Kamesh Krishnamurthy", "Declan Campbell", "Jacob Russin", "Randall O'Reilly", "John Lafferty", "Jonathan D. Cohen"], "Categories": "cs.AI cs.NE"}, "abstract": "A central challenge for cognitive science is to explain how abstract concepts are acquired from limited experience. This effort has often been framed in terms of a dichotomy between empiricist and nativist approaches, most recently embodied by debates concerning deep neural networks and symbolic cognitive models. Here, we highlight a recently emerging line of work that suggests a novel reconciliation of these approaches, by exploiting an inductive bias that we term the relational bottleneck. We review a family of models that employ this approach to induce abstractions in a data-efficient manner, emphasizing their potential as candidate models for the acquisition of abstract concepts in the human mind and brain.", "url": "https://arxiv.org/abs/2309.06629"}, {"metadata": {"arXiv": "2309.06719", "Date": "Wed, 13 Sep 2023 04:47:43 ", "Title": "TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models", "Authors": ["Siyao Zhang", "Daocheng Fu", "Zhao Zhang", "Bin Yu and Pinlong Cai"], "Categories": "cs.AI cs.HC"}, "abstract": "With the promotion of chatgpt to the public, Large language models indeed showcase remarkable common sense, reasoning, and planning skills, frequently providing insightful guidance. These capabilities hold significant promise for their application in urban traffic management and control. However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges. In parallel, specialized traffic foundation models exist but are typically designed for specific tasks with limited input-output interactions. Combining these models with LLMs presents an opportunity to enhance their capacity for tackling complex traffic-related problems and providing insightful suggestions. To bridge this gap, we present TrafficGPT, a fusion of ChatGPT and traffic foundation models. This integration yields the following key enhancements: 1) empowering ChatGPT with the capacity to view, analyze, process traffic data, and provide insightful decision support for urban transportation system management; 2) facilitating the intelligent deconstruction of broad and complex tasks and sequential utilization of traffic foundation models for their gradual completion; 3) aiding human decision-making in traffic control through natural language dialogues; and 4) enabling interactive feedback and solicitation of revised outcomes. By seamlessly intertwining large language model and traffic expertise, TrafficGPT not only advances traffic management but also offers a novel approach to leveraging AI capabilities in this domain. The TrafficGPT demo can be found in https://github.com/lijlansg/TrafficGPT.git.", "url": "https://arxiv.org/abs/2309.06719"}, {"metadata": {"arXiv": "2309.06799", "Date": "Wed, 13 Sep 2023 08:44:09 ", "Title": "When Geoscience Meets Foundation Models: Towards General Geoscience Artificial Intelligence System", "Authors": ["Hao Zhang and Jin-Jian Xu"], "Categories": "cs.AI physics.geo-ph"}, "abstract": "Geoscience foundation models represent a revolutionary approach in the field of Earth sciences by integrating massive cross-disciplinary data to simulate and understand the Earth systems dynamics. As a data-centric artificial intelligence (AI) paradigm, they uncover insights from petabytes of structured and unstructured data. Flexible task specification, diverse inputs and outputs and multi-modal knowledge representation enable comprehensive analysis infeasible with individual data sources. Critically, the scalability and generalizability of geoscience models allow for tackling diverse prediction, simulation, and decision challenges related to Earth systems interactions. Collaboration between domain experts and computer scientists leads to innovations in these invaluable tools for understanding the past, present, and future of our planet. However, challenges remain in validation and verification, scale, interpretability, knowledge representation, and social bias. Going forward, enhancing model integration, resolution, accuracy, and equity through cross-disciplinary teamwork is key. Despite current limitations, geoscience foundation models show promise for providing critical insights into pressing issues including climate change, natural hazards, and sustainability through their ability to probe scenarios and quantify uncertainties. Their continued evolution toward integrated, data-driven modeling holds paradigm-shifting potential for Earth science.", "url": "https://arxiv.org/abs/2309.06799"}, {"metadata": {"arXiv": "2309.06888", "Date": "Wed, 13 Sep 2023 11:22:42 ", "Title": "OWL Reasoners still useable in 2023", "Authors": ["Konrad Abicht"], "Categories": "cs.AI"}, "abstract": "In a systematic literature and software review over 100 OWL reasoners/systems were analyzed to see if they would still be usable in 2023. This has never been done in this capacity. OWL reasoners still play an important role in knowledge organisation and management, but the last comprehensive surveys/studies are more than 8 years old. The result of this work is a comprehensive list of 95 standalone OWL reasoners and systems using an OWL reasoner. For each item, information on project pages, source code repositories and related documentation was gathered. The raw research data is provided in a Github repository for anyone to use.", "url": "https://arxiv.org/abs/2309.06888"}, {"metadata": {"arXiv": "2309.06677", "Date": "Wed, 13 Sep 2023 02:24:37 ", "Title": "SHARM: Segmented Head Anatomical Reference Models", "Authors": ["Essam A. Rashed", "Mohammad al-Shatouri", "Ilkka Laakso", "Akimasa Hirata"], "Categories": "cs.CV cs.AI"}, "abstract": "Reliable segmentation of anatomical tissues of human head is a major step in several clinical applications such as brain mapping, surgery planning and associated computational simulation studies. Segmentation is based on identifying different anatomical structures through labeling different tissues through medical imaging modalities. The segmentation of brain structures is commonly feasible with several remarkable contributions mainly for medical perspective; however, non-brain tissues are of less interest due to anatomical complexity and difficulties to be observed using standard medical imaging protocols. The lack of whole head segmentation methods and unavailability of large human head segmented datasets limiting the variability studies, especially in the computational evaluation of electrical brain stimulation (neuromodulation), human protection from electromagnetic field, and electroencephalography where non-brain tissues are of great importance. To fill this gap, this study provides an open-access Segmented Head Anatomical Reference Models (SHARM) that consists of 196 subjects. These models are segmented into 15 different tissues; skin, fat, muscle, skull cancellous bone, skull cortical bone, brain white matter, brain gray matter, cerebellum white matter, cerebellum gray matter, cerebrospinal fluid, dura, vitreous humor, lens, mucous tissue and blood vessels. The segmented head models are generated using open-access IXI MRI dataset through convolutional neural network structure named ForkNet+. Results indicate a high consistency in statistical characteristics of different tissue distribution in age scale with real measurements. SHARM is expected to be a useful benchmark not only for electromagnetic dosimetry studies but also for different human head segmentation applications.", "url": "https://arxiv.org/abs/2309.06677"}, {"metadata": {"arXiv": "2309.06721", "Date": "Wed, 13 Sep 2023 04:51:15 ", "Title": "Dynamic Spectrum Mixer for Visual Recognition", "Authors": ["Zhiqiang Hu", "Tao Yu"], "Categories": "cs.CV cs.AI"}, "abstract": "Recently, MLP-based vision backbones have achieved promising performance in several visual recognition tasks. However, the existing MLP-based methods directly aggregate tokens with static weights, leaving the adaptability to different images untouched. Moreover, Recent research demonstrates that MLP-Transformer is great at creating long-range dependencies but ineffective at catching high frequencies that primarily transmit local information, which prevents it from applying to the downstream dense prediction tasks, such as semantic segmentation. To address these challenges, we propose a content-adaptive yet computationally efficient structure, dubbed Dynamic Spectrum Mixer (DSM). The DSM represents token interactions in the frequency domain by employing the Discrete Cosine Transform, which can learn long-term spatial dependencies with log-linear complexity. Furthermore, a dynamic spectrum weight generation layer is proposed as the spectrum bands selector, which could emphasize the informative frequency bands while diminishing others. To this end, the technique can efficiently learn detailed features from visual input that contains both high- and low-frequency information. Extensive experiments show that DSM is a powerful and adaptable backbone for a range of visual recognition tasks. Particularly, DSM outperforms previous transformer-based and MLP-based models, on image classification, object detection, and semantic segmentation tasks, such as 83.8 \\% top-1 accuracy on ImageNet, and 49.9 \\% mIoU on ADE20K.", "url": "https://arxiv.org/abs/2309.06721"}, {"metadata": {"arXiv": "2309.06807", "Date": "Wed, 13 Sep 2023 08:54:22 ", "Title": "Bayesian uncertainty-weighted loss for improved generalisability on polyp segmentation task", "Authors": ["Rebecca S. Stone", "Pedro E. Chavarrias-Solano", "Andrew J. Bulpitt", "David C. Hogg", "Sharib Ali"], "Categories": "cs.CV cs.AI", "Comments": ["To be presented at the Fairness of AI in Medical Imaging (FAIMI) MICCAI 2023 Workshop and published in volumes of the Springer Lecture Notes Computer Science (LNCS) series"]}, "abstract": "While several previous studies have devised methods for segmentation of polyps, most of these methods are not rigorously assessed on multi-center datasets. Variability due to appearance of polyps from one center to another, difference in endoscopic instrument grades, and acquisition quality result in methods with good performance on in-distribution test data, and poor performance on out-of-distribution or underrepresented samples. Unfair models have serious implications and pose a critical challenge to clinical applications. We adapt an implicit bias mitigation method which leverages Bayesian epistemic uncertainties during training to encourage the model to focus on underrepresented sample regions. We demonstrate the potential of this approach to improve generalisability without sacrificing state-of-the-art performance on a challenging multi-center polyp segmentation dataset (PolypGen) with different centers and image modalities.", "url": "https://arxiv.org/abs/2309.06807"}, {"metadata": {"arXiv": "2309.06810", "Date": "Wed, 13 Sep 2023 09:00:45 ", "Title": "Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly", "Authors": ["Ruihai Wu", "Chenrui Tie", "Yushi Du", "Yan Zhao", "Hao Dong"], "Categories": "cs.CV cs.AI", "Comments": ["ICCV 2023", "Project page: https://crtie.github.io/SE-3-part-assembly/ ", "Code: https://github.com/crtie/Leveraging-SE-3-Equivariance-for-Learning-3D-Geometric-Shape-Assembly"]}, "abstract": "Shape assembly aims to reassemble parts (or fragments) into a complete object, which is a common task in our daily life. Different from the semantic part assembly (e.g., assembling a chair's semantic parts like legs into a whole chair), geometric part assembly (e.g., assembling bowl fragments into a complete bowl) is an emerging task in computer vision and robotics. Instead of semantic information, this task focuses on geometric information of parts. As the both geometric and pose space of fractured parts are exceptionally large, shape pose disentanglement of part representations is beneficial to geometric shape assembly. In our paper, we propose to leverage SE(3) equivariance for such shape pose disentanglement. Moreover, while previous works in vision and robotics only consider SE(3) equivariance for the representations of single objects, we move a step forward and propose leveraging SE(3) equivariance for representations considering multi-part correlations, which further boosts the performance of the multi-part assembly. Experiments demonstrate the significance of SE(3) equivariance and our proposed method for geometric shape assembly. Project page: https://crtie.github.io/SE-3-part-assembly/", "url": "https://arxiv.org/abs/2309.06810"}, {"metadata": {"arXiv": "2309.06824", "Date": "Wed, 13 Sep 2023 09:15:20 ", "Title": "SAMUS: Adapting Segment Anything Model for Clinically-Friendly and Generalizable Ultrasound Image Segmentation", "Authors": ["Xian Lin", "Yangyang Xiang", "Li Zhang", "Xin Yang", "Zengqiang Yan", "and Li Yu"], "Categories": "cs.CV cs.AI"}, "abstract": "Segment anything model (SAM), an eminent universal image segmentation model, has recently gathered considerable attention within the domain of medical image segmentation. Despite the remarkable performance of SAM on natural images, it grapples with significant performance degradation and limited generalization when confronted with medical images, particularly with those involving objects of low contrast, faint boundaries, intricate shapes, and diminutive sizes. In this paper, we propose SAMUS, a universal model tailored for ultrasound image segmentation. In contrast to previous SAM-based universal models, SAMUS pursues not only better generalization but also lower deployment cost, rendering it more suitable for clinical applications. Specifically, based on SAM, a parallel CNN branch is introduced to inject local features into the ViT encoder through cross-branch attention for better medical image segmentation. Then, a position adapter and a feature adapter are developed to adapt SAM from natural to medical domains and from requiring large-size inputs (1024x1024) to small-size inputs (256x256) for more clinical-friendly deployment. A comprehensive ultrasound dataset, comprising about 30k images and 69k masks and covering six object categories, is collected for verification. Extensive comparison experiments demonstrate SAMUS's superiority against the state-of-the-art task-specific models and universal foundation models under both task-specific evaluation and generalization evaluation. Moreover, SAMUS is deployable on entry-level GPUs, as it has been liberated from the constraints of long sequence encoding. The code, data, and models will be released at https://github.com/xianlin7/SAMUS.", "url": "https://arxiv.org/abs/2309.06824"}, {"metadata": {"arXiv": "2309.06941", "Date": "Wed, 13 Sep 2023 13:24:27 ", "Title": "DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision", "Authors": ["Xiangchen Yin", "Zhenda Yu", "Xin Gao", "Ran Ju", "Xiao Sun", "Xinyu Zhang"], "Categories": "cs.CV cs.AI", "Comments": ["submit to ICRA2024"]}, "abstract": "The goal of low-light image enhancement is to restore the color and details of the image and is of great significance for high-level visual tasks in autonomous driving. However, it is difficult to restore the lost details in the dark area by relying only on the RGB domain. In this paper we introduce frequency as a new clue into the network and propose a novel DCT-driven enhancement transformer (DEFormer). First, we propose a learnable frequency branch (LFB) for frequency enhancement contains DCT processing and curvature-based frequency enhancement (CFE). CFE calculates the curvature of each channel to represent the detail richness of different frequency bands, then we divides the frequency features, which focuses on frequency bands with richer textures. In addition, we propose a cross domain fusion (CDF) for reducing the differences between the RGB domain and the frequency domain. We also adopt DEFormer as a preprocessing in dark detection, DEFormer effectively improves the performance of the detector, bringing 2.1% and 3.4% improvement in ExDark and DARK FACE datasets on mAP respectively.", "url": "https://arxiv.org/abs/2309.06941"}, {"metadata": {"arXiv": "2309.06961", "Date": "Wed, 13 Sep 2023 13:54:32 ", "Title": "Towards Reliable Dermatology Evaluation Benchmarks", "Authors": ["Fabian Gr\\\"oger", "Simone Lionetti", "Philippe Gottfrois", "Alvaro Gonzalez-Jimenez", "Matthew Groh", "Roxana Daneshjou", "Labelling Consortium", "Alexander A. Navarini", "Marc Pouly"], "Categories": "cs.CV cs.AI", "Comments": ["Link to the revised file lists: https://github.com/Digital-Dermatology/SelfClean-Revised-Benchmarks"]}, "abstract": "Benchmark datasets for digital dermatology unwittingly contain inaccuracies that reduce trust in model performance estimates. We propose a resource-efficient data cleaning protocol to identify issues that escaped previous curation. The protocol leverages an existing algorithmic cleaning strategy and is followed by a confirmation process terminated by an intuitive stopping criterion. Based on confirmation by multiple dermatologists, we remove irrelevant samples and near duplicates and estimate the percentage of label errors in six dermatology image datasets for model evaluation promoted by the International Skin Imaging Collaboration. Along with this paper, we publish revised file lists for each dataset which should be used for model evaluation. Our work paves the way for more trustworthy performance assessment in digital dermatology.", "url": "https://arxiv.org/abs/2309.06961"}, {"metadata": {"arXiv": "2309.06621", "Date": "Tue, 12 Sep 2023 22:22:28 ", "Title": "A Reinforcement Learning Approach for Robotic Unloading from Visual Observations", "Authors": ["Vittorio Giammarino", "Alberto Giammarino", "Matthew Pearce"], "Categories": "cs.RO cs.AI cs.SY eess.SY"}, "abstract": "In this work, we focus on a robotic unloading problem from visual observations, where robots are required to autonomously unload stacks of parcels using RGB-D images as their primary input source. While supervised and imitation learning have accomplished good results in these types of tasks, they heavily rely on labeled data, which are challenging to obtain in realistic scenarios. Our study aims to develop a sample efficient controller framework that can learn unloading tasks without the need for labeled data during the learning process. To tackle this challenge, we propose a hierarchical controller structure that combines a high-level decision-making module with classical motion control. The high-level module is trained using Deep Reinforcement Learning (DRL), wherein we incorporate a safety bias mechanism and design a reward function tailored to this task. Our experiments demonstrate that both these elements play a crucial role in achieving improved learning performance. Furthermore, to ensure reproducibility and establish a benchmark for future research, we provide free access to our code and simulation.", "url": "https://arxiv.org/abs/2309.06621"}, {"metadata": {"arXiv": "2309.06687", "Date": "Wed, 13 Sep 2023 02:56:56 ", "Title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics", "Authors": ["Jiayang Song", "Zhehua Zhou", "Jiawei Liu", "Chunrong Fang", "Zhan Shu", "Lei Ma"], "Categories": "cs.RO cs.AI"}, "abstract": "Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robotic control tasks across three diverse robotic systems. The results indicate that our LLM-designed reward functions are able to rival or even surpass manually designed reward functions, highlighting the efficacy and applicability of our approach.", "url": "https://arxiv.org/abs/2309.06687"}, {"metadata": {"arXiv": "2309.07038", "Date": "Wed, 13 Sep 2023 15:46:40 ", "Title": "Efficient Reinforcement Learning for Jumping Monopods", "Authors": ["Riccardo Bussola", "Michele Focchi", "Andrea Del Prete", "Daniele Fontanelli", "Luigi Palopoli"], "Categories": "cs.RO cs.AI"}, "abstract": "In this work, we consider the complex control problem of making a monopod reach a target with a jump. The monopod can jump in any direction and the terrain underneath its foot can be uneven. This is a template of a much larger class of problems, which are extremely challenging and computationally expensive to solve using standard optimisation-based techniques. Reinforcement Learning (RL) could be an interesting alternative, but the application of an end-to-end approach in which the controller must learn everything from scratch, is impractical. The solution advocated in this paper is to guide the learning process within an RL framework by injecting physical knowledge. This expedient brings to widespread benefits, such as a drastic reduction of the learning time, and the ability to learn and compensate for possible errors in the low-level controller executing the motion. We demonstrate the advantage of our approach with respect to both optimization-based and end-to-end RL approaches.", "url": "https://arxiv.org/abs/2309.07038"}, {"metadata": {"arXiv": "2309.06558", "Date": "Sun, 10 Sep 2023 19:58:15 ", "Title": "High Fidelity Fast Simulation of Human in the Loop Human in the Plant (HIL-HIP) Systems", "Authors": ["Ayan Banerjee", "Payal Kamboj", "Aranyak Maity", "Riya Sudhakar Salian", "Sandeep K.S. Gupta"], "Categories": "eess.SY cs.AI cs.NA cs.SY math.DS math.NA", "Comments": ["To appear in ACM MSWIM 2023"]}, "abstract": "Non-linearities in simulation arise from the time variance in wireless mobile networks when integrated with human in the loop, human in the plant (HIL-HIP) physical systems under dynamic contexts, leading to simulation slowdown. Time variance is handled by deriving a series of piece wise linear time invariant simulations (PLIS) in intervals, which are then concatenated in time domain. In this paper, we conduct a formal analysis of the impact of discretizing time-varying components in wireless network-controlled HIL-HIP systems on simulation accuracy and speedup, and evaluate trade-offs with reliable guarantees. We develop an accurate simulation framework for an artificial pancreas wireless network system that controls blood glucose in Type 1 Diabetes patients with time varying properties such as physiological changes associated with psychological stress and meal patterns. PLIS approach achieves accurate simulation with greater than 2.1 times speedup than a non-linear system simulation for the given dataset.", "url": "https://arxiv.org/abs/2309.06558"}, {"metadata": {"arXiv": "2309.06841", "Date": "Wed, 13 Sep 2023 09:43:55 ", "Title": "On the Local Quadratic Stability of T-S Fuzzy Systems in the Vicinity of the Origin", "Authors": ["Donghwan Lee and Do Wan Kim"], "Categories": "eess.SY cs.AI cs.SY"}, "abstract": "The main goal of this paper is to introduce new local stability conditions for continuous-time Takagi-Sugeno (T-S) fuzzy systems. These stability conditions are based on linear matrix inequalities (LMIs) in combination with quadratic Lyapunov functions. Moreover, they integrate information on the membership functions at the origin and effectively leverage the linear structure of the underlying nonlinear system in the vicinity of the origin. As a result, the proposed conditions are proved to be less conservative compared to existing methods using fuzzy Lyapunov functions in the literature. Moreover, we establish that the proposed methods offer necessary and sufficient conditions for the local exponential stability of T-S fuzzy systems. The paper also includes discussions on the inherent limitations associated with fuzzy Lyapunov approaches. To demonstrate the theoretical results, we provide comprehensive examples that elucidate the core concepts and validate the efficacy of the proposed conditions.", "url": "https://arxiv.org/abs/2309.06841"}, {"metadata": {"arXiv": "2309.06938", "Date": "Wed, 13 Sep 2023 13:20:17 ", "Title": "Collectionless Artificial Intelligence", "Authors": ["Marco Gori and Stefano Melacci"], "Categories": "cs.AI cs.CY cs.LG"}, "abstract": "By and large, the professional handling of huge data collections is regarded as a fundamental ingredient of the progress of machine learning and of its spectacular results in related disciplines, with a growing agreement on risks connected to the centralization of such data collections. This paper sustains the position that the time has come for thinking of new learning protocols where machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Basically, there is neither permission to store the temporal information coming from the sensors, thus promoting the development of self-organized memorization skills at a more abstract level, instead of relying on bare storage to simulate learning dynamics that are typical of offline learning algorithms. This purposely extreme position is intended to stimulate the development of machines that learn to dynamically organize the information by following human-based schemes. The proposition of this challenge suggests developing new foundations on computational processes of learning and reasoning that might open the doors to a truly orthogonal competitive track on AI technologies that avoid data accumulation by design, thus offering a framework which is better suited concerning privacy issues, control and customizability. Finally, pushing towards massively distributed computation, the collectionless approach to AI will likely reduce the concentration of power in companies and governments, thus better facing geopolitical issues.", "url": "https://arxiv.org/abs/2309.06938"}, {"metadata": {"arXiv": "2309.06597", "Date": "Tue, 12 Sep 2023 20:51:07 ", "Title": "Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning", "Authors": ["Enna Sachdeva", "Nakul Agarwal", "Suhas Chundi", "Sean Roelofs", "Jiachen Li", "Behzad Dariush", "Chiho Choi", "Mykel Kochenderfer"], "Categories": "cs.CV cs.AI cs.LG cs.RO"}, "abstract": "The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Further, we introduce a joint model for joint importance level ranking and natural language captions generation to benchmark our dataset and demonstrate performance with quantitative evaluations.", "url": "https://arxiv.org/abs/2309.06597"}, {"metadata": {"arXiv": "2309.06604", "Date": "Tue, 12 Sep 2023 21:07:23 ", "Title": "Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach", "Authors": ["Ahmad Esmaeili", "Eric T. Matson", "Julia T. Rayz"], "Categories": "cs.LG cs.AI cs.MA"}, "abstract": "Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical study to demonstrate the correctness, resource utilization, and computational efficiency of our technique. According to the results, our solution is totally correct and exhibits linear time and space complexity in relation to the size of available resources. To provide concrete examples of how the proposed methodologies can effectively adapt and perform across a range of algorithmic options and datasets, we have also conducted a series of experiments using a system comprised of 24 algorithms and 9 datasets.", "url": "https://arxiv.org/abs/2309.06604"}, {"metadata": {"arXiv": "2309.06684", "Date": "Wed, 13 Sep 2023 02:49:32 ", "Title": "Attention Loss Adjusted Prioritized Experience Replay", "Authors": ["Zhuoying Chen", "Huiping Li", "Rizhong Wang"], "Categories": "cs.LG cs.AI"}, "abstract": "Prioritized Experience Replay (PER) is a technical means of deep reinforcement learning by selecting experience samples with more knowledge quantity to improve the training rate of neural network. However, the non-uniform sampling used in PER inevitably shifts the state-action space distribution and brings the estimation error of Q-value function. In this paper, an Attention Loss Adjusted Prioritized (ALAP) Experience Replay algorithm is proposed, which integrates the improved Self-Attention network with Double-Sampling mechanism to fit the hyperparameter that can regulate the importance sampling weights to eliminate the estimation error caused by PER. In order to verify the effectiveness and generality of the algorithm, the ALAP is tested with value-function based, policy-gradient based and multi-agent reinforcement learning algorithms in OPENAI gym, and comparison studies verify the advantage and efficiency of the proposed training framework.", "url": "https://arxiv.org/abs/2309.06684"}, {"metadata": {"arXiv": "2309.06692", "Date": "Wed, 13 Sep 2023 03:27:21 ", "Title": "Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization", "Authors": ["Xinyu Zhang", "Weiyu Sun", "Ying Chen"], "Categories": "cs.LG cs.AI"}, "abstract": "Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios with stronger heterogeneity. As a plug-and-play module, FedGH can be seamlessly integrated into any FL framework without requiring hyperparameter tuning.", "url": "https://arxiv.org/abs/2309.06692"}, {"metadata": {"arXiv": "2309.06774", "Date": "Wed, 13 Sep 2023 07:49:28 ", "Title": "Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss", "Authors": ["Tilahun M. Getu", "Georges Kaddoum"], "Categories": "cs.LG cs.AI"}, "abstract": "Although deep learning (DL) has led to several breakthroughs in many disciplines as diverse as chemistry, computer science, electrical engineering, mathematics, medicine, neuroscience, and physics, a comprehensive understanding of why and how DL is empirically successful remains fundamentally elusive. To attack this fundamental problem and unravel the mysteries behind DL's empirical successes, significant innovations toward a unified theory of DL have been made. These innovations encompass nearly fundamental advances in optimization, generalization, and approximation. Despite these advances, however, no work to date has offered a way to quantify the testing performance of a DL-based algorithm employed to solve a pattern classification problem. To overcome this fundamental challenge in part, this paper exposes the fundamental testing performance limits of DL-based binary classifiers trained with hinge loss. For binary classifiers that are based on deep rectified linear unit (ReLU) feedforward neural networks (FNNs) and ones that are based on deep FNNs with ReLU and Tanh activation, we derive their respective novel asymptotic testing performance limits. The derived testing performance limits are validated by extensive computer experiments.", "url": "https://arxiv.org/abs/2309.06774"}, {"metadata": {"arXiv": "2309.06800", "Date": "Wed, 13 Sep 2023 08:48:00 ", "Title": "Uncertainty-aware Traffic Prediction under Missing Data", "Authors": ["Hao Mei", "Junxian Li", "Zhiming Liang", "Guanjie Zheng", "Bin Shi", "Hua Wei"], "Categories": "cs.LG cs.AI", "Comments": ["11 pages", "3 figures", "Accepted as a short paper of IEEE International Conference on Data Mining (ICDM) 2023"]}, "abstract": "Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-aware framework with the ability to 1) extend prediction to missing locations with no historical records and significantly extend spatial coverage of prediction locations while reducing deployment of sensors and 2) generate probabilistic prediction with uncertainty quantification to help the management of risk and decision making in the down-stream tasks. Through extensive experiments on real-life datasets, the result shows our method achieved promising results on prediction tasks, and the uncertainty quantification gives consistent results which highly correlated with the locations with and without historical data. We also show that our model could help support sensor deployment tasks in the transportation field to achieve higher accuracy with a limited sensor deployment budget.", "url": "https://arxiv.org/abs/2309.06800"}, {"metadata": {"arXiv": "2309.06805", "Date": "Wed, 13 Sep 2023 08:51:19 ", "Title": "FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization", "Authors": ["Qianyu Long", "Christos Anagnostopoulos", "Shameem Puthiya Parambath", "Daning Bi"], "Categories": "cs.LG cs.AI cs.DC", "Comments": ["Accepted for publication at ICDM 2023 (Full version in arxiv). The associated code is available at https://github.com/EricLoong/feddip"], "ACM-class": "H.4; I.2"}, "abstract": "Federated Learning (FL) has been successfully adopted for distributed training and inference of large-scale Deep Neural Networks (DNNs). However, DNNs are characterized by an extremely large number of parameters, thus, yielding significant challenges in exchanging these parameters among distributed nodes and managing the memory. Although recent DNN compression methods (e.g., sparsification, pruning) tackle such challenges, they do not holistically consider an adaptively controlled reduction of parameter exchange while maintaining high accuracy levels. We, therefore, contribute with a novel FL framework (coined FedDIP), which combines (i) dynamic model pruning with error feedback to eliminate redundant information exchange, which contributes to significant performance improvement, with (ii) incremental regularization that can achieve \\textit{extreme} sparsity of models. We provide convergence analysis of FedDIP and report on a comprehensive performance and comparative assessment against state-of-the-art methods using benchmark data sets and DNN models. Our results showcase that FedDIP not only controls the model sparsity but efficiently achieves similar or better performance compared to other model pruning methods adopting incremental regularization during distributed model training. The code is available at: https://github.com/EricLoong/feddip.", "url": "https://arxiv.org/abs/2309.06805"}, {"metadata": {"arXiv": "2309.06969", "Date": "Wed, 13 Sep 2023 14:04:15 ", "Title": "Setting the Right Expectations: Algorithmic Recourse Over Time", "Authors": ["Joao Fonseca", "Andrew Bell", "Carlo Abrate", "Francesco Bonchi", "Julia Stoyanovich"], "Categories": "cs.LG cs.AI cs.CY", "DOI": "10.1145/3617694.3623251"}, "abstract": "Algorithmic systems are often called upon to assist in high-stakes decision making. In light of this, algorithmic recourse, the principle wherein individuals should be able to take action against an undesirable outcome made by an algorithmic system, is receiving growing attention. The bulk of the literature on algorithmic recourse to-date focuses primarily on how to provide recourse to a single individual, overlooking a critical element: the effects of a continuously changing context. Disregarding these effects on recourse is a significant oversight, since, in almost all cases, recourse consists of an individual making a first, unfavorable attempt, and then being given an opportunity to make one or several attempts at a later date - when the context might have changed. This can create false expectations, as initial recourse recommendations may become less reliable over time due to model drift and competition for access to the favorable outcome between individuals. In this work we propose an agent-based simulation framework for studying the effects of a continuously changing environment on algorithmic recourse. In particular, we identify two main effects that can alter the reliability of recourse for individuals represented by the agents: (1) competition with other agents acting upon recourse, and (2) competition with new agents entering the environment. Our findings highlight that only a small set of specific parameterizations result in algorithmic recourse that is reliable for agents over time. Consequently, we argue that substantial additional work is needed to understand recourse reliability over time, and to develop recourse methods that reward agents' effort.", "url": "https://arxiv.org/abs/2309.06969"}, {"metadata": {"arXiv": "2309.06973", "Date": "Wed, 13 Sep 2023 14:05:50 ", "Title": "DNNShifter: An Efficient DNN Pruning System for Edge Computing", "Authors": ["Bailey J. Eccles", "Philip Rodgers", "Peter Kilpatrick", "Ivor Spence", "Blesson Varghese"], "Categories": "cs.LG cs.AI", "Comments": ["14 pages", "7 figures", "5 tables"], "MSC-class": "68T07", "ACM-class": "I.2.1"}, "abstract": "Deep neural networks (DNNs) underpin many machine learning applications. Production quality DNN models achieve high inference accuracy by training millions of DNN parameters which has a significant resource footprint. This presents a challenge for resources operating at the extreme edge of the network, such as mobile and embedded devices that have limited computational and memory resources. To address this, models are pruned to create lightweight, more suitable variants for these devices. Existing pruning methods are unable to provide similar quality models compared to their unpruned counterparts without significant time costs and overheads or are limited to offline use cases. Our work rapidly derives suitable model variants while maintaining the accuracy of the original model. The model variants can be swapped quickly when system and network conditions change to match workload demand. This paper presents DNNShifter, an end-to-end DNN training, spatial pruning, and model switching system that addresses the challenges mentioned above. At the heart of DNNShifter is a novel methodology that prunes sparse models using structured pruning. The pruned model variants generated by DNNShifter are smaller in size and thus faster than dense and sparse model predecessors, making them suitable for inference at the edge while retaining near similar accuracy as of the original dense model. DNNShifter generates a portfolio of model variants that can be swiftly interchanged depending on operational conditions. DNNShifter produces pruned model variants up to 93x faster than conventional training methods. Compared to sparse models, the pruned model variants are up to 5.14x smaller and have a 1.67x inference latency speedup, with no compromise to sparse model accuracy. In addition, DNNShifter has up to 11.9x lower overhead for switching models and up to 3.8x lower memory utilisation than existing approaches.", "url": "https://arxiv.org/abs/2309.06973"}, {"metadata": {"arXiv": "2309.07085", "Date": "Wed, 13 Sep 2023 16:53:48 ", "Title": "Mitigating Group Bias in Federated Learning for Heterogeneous Devices", "Authors": ["Khotso Selialia", "Yasra Chandio", "Fatima M. Anwar"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "Federated Learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients and produces biased global models i.e. models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity and do not address global group-fairness property. Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy and without resource utilization overhead. Our main idea is to leverage average conditional probabilities to compute a cross-domain group \\textit{importance weights} derived from heterogeneous training data to optimize the performance of the worst-performing group using a modified multiplicative weights update method. Additionally, we propose regularization techniques to minimize the difference between the worst and best-performing groups while making sure through our thresholding mechanism to strike a balance between bias reduction and group performance degradation. Our evaluation of human emotion recognition and image classification benchmarks assesses the fair decision-making of our framework in real-world heterogeneous settings.", "url": "https://arxiv.org/abs/2309.07085"}, {"metadata": {"arXiv": "2309.07108", "Date": "Wed, 13 Sep 2023 17:26:36 ", "Title": "Characterizing Speed Performance of Multi-Agent Reinforcement Learning", "Authors": ["Samuel Wiggins", "Yuan Meng", "Rajgopal Kannan", "Viktor Prasanna"], "Categories": "cs.LG cs.AI cs.MA", "Journal-ref": "In Proceedings of the 12th International Conference on Data Science, Technology and Applications - DATA (2023) 327-334", "DOI": "10.5220/0012082200003541"}, "abstract": "Multi-Agent Reinforcement Learning (MARL) has achieved significant success in large-scale AI systems and big-data applications such as smart grids, surveillance, etc. Existing advancements in MARL algorithms focus on improving the rewards obtained by introducing various mechanisms for inter-agent cooperation. However, these optimizations are usually compute- and memory-intensive, thus leading to suboptimal speed performance in end-to-end training time. In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmark algorithms, and provide a systematic analysis of their performance bottlenecks on a homogeneous multi-core CPU platform. We justify the need for MARL latency-bounded throughput to be a key performance metric in future literature while also addressing opportunities for parallelization and acceleration.", "url": "https://arxiv.org/abs/2309.07108"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
