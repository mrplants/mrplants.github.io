<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2308.06394", "Date": "Fri, 11 Aug 2023 21:35:20 ", "Title": "Detecting and Preventing Hallucinations in Large Vision Language Models", "Authors": ["Anisha Gunjal", "Jihan Yin", "Erhan Bas"], "Categories": "cs.CV cs.LG", "Comments": ["preprint"]}, "abstract": "Instruction tuned Large Vision Language Models (LVLMs) have made significant advancements in generalizing across a diverse set of multimodal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a {M}ultimodal {Hal}lucination {Detect}ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained labels on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for preference alignment, we propose fine-grained Direct Preference Optimization, as well as train fine-grained multi-modal reward models and evaluate their effectiveness with best-of-n rejection sampling. We perform human evaluation on both DPO and rejection sampling, and find that they reduce hallucination rates by 41% and 55% respectively, a significant improvement over the baseline.", "url": "https://arxiv.org/abs/2308.06394"}, {"metadata": {"arXiv": "2308.06468", "Date": "Sat, 12 Aug 2023 05:23:36 ", "Title": "Tiny and Efficient Model for the Edge Detection Generalization", "Authors": ["Xavier Soria", "Yachuan Li", "Mohammad Rouhani and Angel D. Sappa"], "Categories": "cs.CV cs.LG", "Comments": ["To Appear in ICCV 2023"]}, "abstract": "Most high-level computer vision tasks rely on low-level image operations as their initial processes. Operations such as edge detection, image enhancement, and super-resolution, provide the foundations for higher level image analysis. In this work we address the edge detection considering three main objectives: simplicity, efficiency, and generalization since current state-of-the-art (SOTA) edge detection models are increased in complexity for better accuracy. To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light convolutional neural network with only $58K$ parameters, less than $0.2$% of the state-of-the-art models. Training on the BIPED dataset takes $less than 30 minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model is easy to train and it quickly converges within very first few epochs, while the predicted edge-maps are crisp and of high quality. Additionally, we propose a new dataset to test the generalization of edge detection, which comprises samples from popular images used in edge detection and image segmentation. The source code is available in https://github.com/xavysp/TEED.", "url": "https://arxiv.org/abs/2308.06468"}, {"metadata": {"arXiv": "2308.06603", "Date": "Sat, 12 Aug 2023 16:14:44 ", "Title": "LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net", "Authors": ["Tonghui Zou"], "Categories": "cs.CV cs.LG eess.IV"}, "abstract": "The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibility by allowing the substitution of its network architecture with semantic segmentation networks, thereby establishing more abstract semantic spaces to bolster model performance. Consequently, we propose LadleNet+, which replaces LadleNet's Handle module with the pre-trained DeepLabv3+ network, thereby endowing the model with enhanced semantic space construction capabilities. The proposed method is evaluated and tested on the KAIST dataset, accompanied by quantitative and qualitative analyses. Compared to existing methodologies, our approach achieves state-of-the-art performance in terms of image clarity and perceptual quality. The source code will be made available at https://github.com/Ach-1914/LadleNet/tree/main/.", "url": "https://arxiv.org/abs/2308.06603"}, {"metadata": {"arXiv": "2308.06692", "Date": "Sun, 13 Aug 2023 05:56:36 ", "Title": "SimMatchV2: Semi-Supervised Learning with Graph Consistency", "Authors": ["Mingkai Zheng", "Shan You", "Lang Huang", "Chen Luo", "Fei Wang", "Chen Qian", "Chang Xu"], "Categories": "cs.CV cs.LG"}, "abstract": "Semi-Supervised image classification is one of the most fundamental problem in computer vision, which significantly reduces the need for human labor. In this paper, we introduce a new semi-supervised learning algorithm - SimMatchV2, which formulates various consistency regularizations between labeled and unlabeled data from the graph perspective. In SimMatchV2, we regard the augmented view of a sample as a node, which consists of a label and its corresponding representation. Different nodes are connected with the edges, which are measured by the similarity of the node representations. Inspired by the message passing and node classification in graph theory, we propose four types of consistencies, namely 1) node-node consistency, 2) node-edge consistency, 3) edge-edge consistency, and 4) edge-node consistency. We also uncover that a simple feature normalization can reduce the gaps of the feature norm between different augmented views, significantly improving the performance of SimMatchV2. Our SimMatchV2 has been validated on multiple semi-supervised learning benchmarks. Notably, with ResNet-50 as our backbone and 300 epochs of training, SimMatchV2 achieves 71.9\\% and 76.2\\% Top-1 Accuracy with 1\\% and 10\\% labeled examples on ImageNet, which significantly outperforms the previous methods and achieves state-of-the-art performance. Code and pre-trained models are available at \\href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2}.", "url": "https://arxiv.org/abs/2308.06692"}, {"metadata": {"arXiv": "2308.06945", "Date": "Mon, 14 Aug 2023 05:37:07 ", "Title": "Semantic-aware Network for Aerial-to-Ground Image Synthesis", "Authors": ["Jinhyun Jang", "Taeyong Song", "Kwanghoon Sohn"], "Categories": "cs.CV cs.LG", "Comments": ["ICIP 2021. Code is available at https://github.com/jinhyunj/SANet"]}, "abstract": "Aerial-to-ground image synthesis is an emerging and challenging problem that aims to synthesize a ground image from an aerial image. Due to the highly different layout and object representation between the aerial and ground images, existing approaches usually fail to transfer the components of the aerial scene into the ground scene. In this paper, we propose a novel framework to explore the challenges by imposing enhanced structural alignment and semantic awareness. We introduce a novel semantic-attentive feature transformation module that allows to reconstruct the complex geographic structures by aligning the aerial feature to the ground layout. Furthermore, we propose semantic-aware loss functions by leveraging a pre-trained segmentation network. The network is enforced to synthesize realistic objects across various classes by separately calculating losses for different classes and balancing them. Extensive experiments including comparisons with previous methods and ablation studies show the effectiveness of the proposed framework both qualitatively and quantitatively.", "url": "https://arxiv.org/abs/2308.06945"}, {"metadata": {"arXiv": "2308.06947", "Date": "Mon, 14 Aug 2023 05:54:32 ", "Title": "Knowing Where to Focus: Event-aware Transformer for Video Grounding", "Authors": ["Jinhyun Jang", "Jungin Park", "Jin Kim", "Hyeongjun Kwon", "Kwanghoon Sohn"], "Categories": "cs.CV cs.LG", "Comments": ["ICCV 2023. Code is available at https://github.com/jinhyunj/EaTR"]}, "abstract": "Recent DETR-based video grounding models have made the model directly predict moment timestamps without any hand-crafted components, such as a pre-defined proposal or non-maximum suppression, by learning moment queries. However, their input-agnostic moment queries inevitably overlook an intrinsic temporal structure of a video, providing limited positional information. In this paper, we formulate an event-aware dynamic moment query to enable the model to take the input-specific content and positional information of the video into account. To this end, we present two levels of reasoning: 1) Event reasoning that captures distinctive event units constituting a given video using a slot attention mechanism; and 2) moment reasoning that fuses the moment queries with a given sentence through a gated fusion transformer layer and learns interactions between the moment queries and video-sentence representations to predict moment timestamps. Extensive experiments demonstrate the effectiveness and efficiency of the event-aware dynamic moment queries, outperforming state-of-the-art approaches on several video grounding benchmarks.", "url": "https://arxiv.org/abs/2308.06947"}, {"metadata": {"arXiv": "2308.07032", "Date": "Mon, 14 Aug 2023 09:45:28 ", "Title": "S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields", "Authors": ["Zeke Xie", "Xindi Yang", "Yujie Yang", "Qi Sun", "Yixiang Jiang", "Haoran Wang", "Yunfeng Cai", "Mingming Sun"], "Categories": "cs.CV cs.LG", "Comments": ["ICCV 2023 main conference. Code: https://github.com/Madaoer/S3IM. 14 pages", "5 figures", "17 tables"]}, "abstract": "Recently, Neural Radiance Field (NeRF) has shown great success in rendering novel-view images of a given scene by learning an implicit representation with only posed RGB images. NeRF and relevant neural field methods (e.g., neural surface representation) typically optimize a point-wise loss and make point-wise predictions, where one data point corresponds to one pixel. Unfortunately, this line of research failed to use the collective supervision of distant pixels, although it is known that pixels in an image or scene can provide rich structural information. To the best of our knowledge, we are the first to design a nonlocal multiplex training paradigm for NeRF and relevant neural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss that processes multiple data points as a whole set instead of process multiple inputs independently. Our extensive experiments demonstrate the unreasonable effectiveness of S3IM in improving NeRF and neural surface representation for nearly free. The improvements of quality metrics can be particularly significant for those relatively difficult tasks: e.g., the test MSE loss unexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view synthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance reduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is consistently robust even with sparse inputs, corrupted images, and dynamic scenes.", "url": "https://arxiv.org/abs/2308.07032"}, {"metadata": {"arXiv": "2308.07223", "Date": "Mon, 14 Aug 2023 15:49:19 ", "Title": "Distance Matters For Improving Performance Estimation Under Covariate Shift", "Authors": ["M\\'elanie Roschewitz and Ben Glocker"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted to ICCV Workshop on Uncertainty Quantification for Computer Vision 2023"]}, "abstract": "Performance estimation under covariate shift is a crucial component of safe AI model deployment, especially for sensitive use-cases. Recently, several solutions were proposed to tackle this problem, most leveraging model predictions or softmax confidence to derive accuracy estimates. However, under dataset shifts, confidence scores may become ill-calibrated if samples are too far from the training distribution. In this work, we show that taking into account distances of test samples to their expected training distribution can significantly improve performance estimation under covariate shift. Precisely, we introduce a \"distance-check\" to flag samples that lie too far from the expected distribution, to avoid relying on their untrustworthy model outputs in the accuracy estimation step. We demonstrate the effectiveness of this method on 13 image classification tasks, across a wide-range of natural and synthetic distribution shifts and hundreds of models, with a median relative MAE improvement of 27% over the best baseline across all tasks, and SOTA performance on 10 out of 13 tasks. Our code is publicly available at https://github.com/melanibe/distance_matters_performance_estimation.", "url": "https://arxiv.org/abs/2308.07223"}, {"metadata": {"arXiv": "2308.06267", "Date": "Sun, 16 Jul 2023 19:09:31 ", "Title": "DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning", "Authors": ["Bocheng Chen", "Nikolay Ivanov", "Guangjing Wang", "Qiben Yan"], "Categories": "cs.DC cs.LG", "Journal-ref": "2023 SECON"}, "abstract": "Federated Learning (FL) is a distributed machine learning (ML) paradigm, aiming to train a global model by exploiting the decentralized data across millions of edge devices. Compared with centralized learning, FL preserves the clients' privacy by refraining from explicitly downloading their data. However, given the geo-distributed edge devices (e.g., mobile, car, train, or subway) with highly dynamic networks in the wild, aggregating all the model updates from those participating devices will result in inevitable long-tail delays in FL. This will significantly degrade the efficiency of the training process. To resolve the high system heterogeneity in time-sensitive FL scenarios, we propose a novel FL framework, DynamicFL, by considering the communication dynamics and data quality across massive edge devices with a specially designed client manipulation strategy. \\ours actively selects clients for model updating based on the network prediction from its dynamic network conditions and the quality of its training data. Additionally, our long-term greedy strategy in client selection tackles the problem of system performance degradation caused by short-term scheduling in a dynamic network. Lastly, to balance the trade-off between client performance evaluation and client manipulation granularity, we dynamically adjust the length of the observation window in the training process to optimize the long-term system efficiency. Compared with the state-of-the-art client selection scheme in FL, \\ours can achieve a better model accuracy while consuming only 18.9\\% -- 84.0\\% of the wall-clock time. Our component-wise and sensitivity studies further demonstrate the robustness of \\ours under various real-life scenarios.", "url": "https://arxiv.org/abs/2308.06267"}, {"metadata": {"arXiv": "2308.06338", "Date": "Fri, 11 Aug 2023 18:26:09 ", "Title": "Size Lowerbounds for Deep Operator Networks", "Authors": ["Anirbit Mukherjee and Amartya Roy"], "Categories": "cs.LG cs.CC cs.NA math.AP math.NA", "Comments": ["21 pages", "3 figures"]}, "abstract": "Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\\Omega \\left ( {\\sqrt{n}} \\right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.", "url": "https://arxiv.org/abs/2308.06338"}, {"metadata": {"arXiv": "2308.06342", "Date": "Fri, 11 Aug 2023 18:31:54 ", "Title": "Mirror Diffusion Models", "Authors": ["Jaesung Tae"], "Categories": "cs.LG"}, "abstract": "Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.", "url": "https://arxiv.org/abs/2308.06342"}, {"metadata": {"arXiv": "2308.06352", "Date": "Fri, 11 Aug 2023 19:08:06 ", "Title": "Learning Distributions via Monte-Carlo Marginalization", "Authors": ["Chenqiu Zhao", "Guanfang Dong", "Anup Basu"], "Categories": "cs.LG"}, "abstract": "We propose a novel method to learn intractable distributions from their samples. The main idea is to use a parametric distribution model, such as a Gaussian Mixture Model (GMM), to approximate intractable distributions by minimizing the KL-divergence. Based on this idea, there are two challenges that need to be addressed. First, the computational complexity of KL-divergence is unacceptable when the dimensions of distributions increases. The Monte-Carlo Marginalization (MCMarg) is proposed to address this issue. The second challenge is the differentiability of the optimization process, since the target distribution is intractable. We handle this problem by using Kernel Density Estimation (KDE). The proposed approach is a powerful tool to learn complex distributions and the entire process is differentiable. Thus, it can be a better substitute of the variational inference in variational auto-encoders (VAE). One strong evidence of the benefit of our method is that the distributions learned by the proposed approach can generate better images even based on a pre-trained VAE's decoder. Based on this point, we devise a distribution learning auto-encoder which is better than VAE under the same network architecture. Experiments on standard dataset and synthetic data demonstrate the efficiency of the proposed approach.", "url": "https://arxiv.org/abs/2308.06352"}, {"metadata": {"arXiv": "2308.06375", "Date": "Fri, 11 Aug 2023 20:17:22 ", "Title": "UAMM: UBET Automated Market Maker", "Authors": ["Daniel Jiwoong Im", "Alexander Kondratskiy", "Vincent Harvey", "Hsuan-Wei Fu"], "Categories": "cs.LG cs.CE q-fin.CP"}, "abstract": "Automated market makers (AMMs) are pricing mechanisms utilized by decentralized exchanges (DEX). Traditional AMM approaches are constrained by pricing solely based on their own liquidity pool, without consideration of external markets or risk management for liquidity providers. In this paper, we propose a new approach known as UBET AMM (UAMM), which calculates prices by considering external market prices and the impermanent loss of the liquidity pool. Despite relying on external market prices, our method maintains the desired properties of a constant product curve when computing slippages. The key element of UAMM is determining the appropriate slippage amount based on the desired target balance, which encourages the liquidity pool to minimize impermanent loss. We demonstrate that our approach eliminates arbitrage opportunities when external market prices are efficient.", "url": "https://arxiv.org/abs/2308.06375"}, {"metadata": {"arXiv": "2308.06424", "Date": "Sat, 12 Aug 2023 00:26:08 ", "Title": "Multiclass Learnability Does Not Imply Sample Compression", "Authors": ["Chirag Pabbaraju"], "Categories": "cs.LG"}, "abstract": "A hypothesis class admits a sample compression scheme, if for every sample labeled by a hypothesis from the class, it is possible to retain only a small subsample, using which the labels on the entire sample can be inferred. The size of the compression scheme is an upper bound on the size of the subsample produced. Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size. For multiclass hypothesis classes, the analog of VC dimension is the DS dimension. We show that the analogous statement pertaining to sample compression is not true for multiclass hypothesis classes: every learnable multiclass hypothesis class, which must necessarily have finite DS dimension, does not admit a sample compression scheme of size only a finite function of its DS dimension.", "url": "https://arxiv.org/abs/2308.06424"}, {"metadata": {"arXiv": "2308.06429", "Date": "Sat, 12 Aug 2023 01:28:26 ", "Title": "Genetic heterogeneity analysis using genetic algorithm and network science", "Authors": ["Zhendong Sha", "Yuanzhu Chen", "Ting Hu"], "Categories": "cs.LG cs.CE"}, "abstract": "Through genome-wide association studies (GWAS), disease susceptible genetic variables can be identified by comparing the genetic data of individuals with and without a specific disease. However, the discovery of these associations poses a significant challenge due to genetic heterogeneity and feature interactions. Genetic variables intertwined with these effects often exhibit lower effect-size, and thus can be difficult to be detected using machine learning feature selection methods. To address these challenges, this paper introduces a novel feature selection mechanism for GWAS, named Feature Co-selection Network (FCSNet). FCS-Net is designed to extract heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA), an evolutionary learning algorithm. We employ a non-linear machine learning algorithm to detect feature interaction. We introduce the Community Risk Score (CRS), a synthetic feature designed to quantify the collective disease association of each variable subset. Our experiment showcases the effectiveness of the utilized GA-based feature selection method in identifying feature interactions through synthetic data analysis. Furthermore, we apply our novel approach to a case-control colorectal cancer GWAS dataset. The resulting synthetic features are then used to explain the genetic heterogeneity in an additional case-only GWAS dataset.", "url": "https://arxiv.org/abs/2308.06429"}, {"metadata": {"arXiv": "2308.06436", "Date": "Sat, 12 Aug 2023 02:14:52 ", "Title": "A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell's Equations in Heterogeneous Media", "Authors": ["Shiyuan Piao", "Hong Gu", "Aina Wang", "Pan Qin"], "Categories": "cs.LG cs.NA math.NA", "Comments": ["5 pages,4 figures"]}, "abstract": "Maxwell's equations are a collection of coupled partial differential equations (PDEs) that, together with the Lorentz force law, constitute the basis of classical electromagnetism and electric circuits. Effectively solving Maxwell's equations is crucial in various fields, like electromagnetic scattering and antenna design optimization. Physics-informed neural networks (PINNs) have shown powerful ability in solving PDEs. However, PINNs still struggle to solve Maxwell's equations in heterogeneous media. To this end, we propose a domain-adaptive PINN (da-PINN) to solve inverse problems of Maxwell's equations in heterogeneous media. First, we propose a location parameter of media interface to decompose the whole domain into several sub-domains. Furthermore, the electromagnetic interface conditions are incorporated into a loss function to improve the prediction performance near the interface. Then, we propose a domain-adaptive training strategy for da-PINN. Finally, the effectiveness of da-PINN is verified with two case studies.", "url": "https://arxiv.org/abs/2308.06436"}, {"metadata": {"arXiv": "2308.06443", "Date": "Sat, 12 Aug 2023 02:35:24 ", "Title": "Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data", "Authors": ["Cheol Jun Cho", "Edward F. Chang", "and Gopala K. Anumanchipalli"], "Categories": "cs.LG eess.AS", "Comments": ["Accepted at ICML 2023"], "Journal-ref": "Proceedings of the 40th International Conference on Machine Learning (2023), PMLR 202:5661-5676"}, "abstract": "Understanding the neural implementation of complex human behaviors is one of the major goals in neuroscience. To this end, it is crucial to find a true representation of the neural data, which is challenging due to the high complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here, we propose a novel unsupervised learning framework, Neural Latent Aligner (NLA), to find well-constrained, behaviorally relevant neural representations of complex behaviors. The key idea is to align representations across repeated trials to learn cross-trial consistent information. Furthermore, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space. The TWM is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.", "url": "https://arxiv.org/abs/2308.06443"}, {"metadata": {"arXiv": "2308.06448", "Date": "Sat, 12 Aug 2023 02:47:57 ", "Title": "Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More", "Authors": ["Sudhanshu Chanpuriya and Cameron Musco"], "Categories": "cs.LG cs.SI"}, "abstract": "Algorithms for node clustering typically focus on finding homophilous structure in graphs. That is, they find sets of similar nodes with many edges within, rather than across, the clusters. However, graphs often also exhibit heterophilous structure, as exemplified by (nearly) bipartite and tripartite graphs, where most edges occur across the clusters. Grappling with such structure is typically left to the task of graph simplification. We present a probabilistic model based on non-negative matrix factorization which unifies clustering and simplification, and provides a framework for modeling arbitrary graph structure. Our model is based on factorizing the process of taking a random walk on the graph. It permits an unconstrained parametrization, allowing for optimization via simple gradient descent. By relaxing the hard clustering to a soft clustering, our algorithm relaxes potentially hard clustering problems to a tractable ones. We illustrate our algorithm's capabilities on a synthetic graph, as well as simple unsupervised learning tasks involving bipartite and tripartite clustering of orthographic and phonological data.", "url": "https://arxiv.org/abs/2308.06448"}, {"metadata": {"arXiv": "2308.06471", "Date": "Sat, 12 Aug 2023 05:28:49 ", "Title": "Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest", "Authors": ["Karthik R.", "and Ramamoorthy A."], "Categories": "cs.LG"}, "abstract": "Intelligent automation supports us against cyclones, droughts, and seismic events with recent technology advancements. Algorithmic learning has advanced fields like neuroscience, genetics, and human-computer interaction. Time-series data boosts progress. Challenges persist in adopting these approaches in traditional fields. Neural networks face comprehension and bias issues. AI's expansion across scientific areas is due to adaptable descriptors and combinatorial argumentation. This article focuses on modeling Forest loss using the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest cover, demonstrated on Amazon Rainforest data against other forecasters like Long Short-Term Memory, N-BEATS, RCN.", "url": "https://arxiv.org/abs/2308.06471"}, {"metadata": {"arXiv": "2308.06496", "Date": "Sat, 12 Aug 2023 07:56:48 ", "Title": "Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks", "Authors": ["Zhigang Yan and Dong Li"], "Categories": "cs.LG cs.PF eess.SP"}, "abstract": "Federated learning (FL) can lead to significant communication overhead and reliance on a central server. To address these challenges, decentralized federated learning (DFL) has been proposed as a more resilient framework. DFL involves parameter exchange between devices through a wireless network. This study analyzes the performance of resource-constrained DFL using different communication schemes (digital and analog) over wireless networks to optimize communication efficiency. Specifically, we provide convergence bounds for both digital and analog transmission approaches, enabling analysis of the model performance trained on DFL. Furthermore, for digital transmission, we investigate and analyze resource allocation between computation and communication and convergence rates, obtaining its communication complexity and the minimum probability of correction communication required for convergence guarantee. For analog transmission, we discuss the impact of channel fading and noise on the model performance and the maximum errors accumulation with convergence guarantee over fading channels. Finally, we conduct numerical simulations to evaluate the performance and convergence rate of convolutional neural networks (CNNs) and Vision Transformer (ViT) trained in the DFL framework on fashion-MNIST and CIFAR-10 datasets. Our simulation results validate our analysis and discussion, revealing how to improve performance by optimizing system parameters under different communication conditions.", "url": "https://arxiv.org/abs/2308.06496"}, {"metadata": {"arXiv": "2308.06564", "Date": "Sat, 12 Aug 2023 13:17:09 ", "Title": "EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction", "Authors": ["Kehua Chen", "Xianda Chen", "Zihan Yu", "Meixin Zhu", "Hai Yang"], "Categories": "cs.LG cs.RO"}, "abstract": "Accurate trajectory prediction is crucial for the safe and efficient operation of autonomous vehicles. The growing popularity of deep learning has led to the development of numerous methods for trajectory prediction. While deterministic deep learning models have been widely used, deep generative models have gained popularity as they learn data distributions from training data and account for trajectory uncertainties. In this study, we propose EquiDiff, a deep generative model for predicting future vehicle trajectories. EquiDiff is based on the conditional diffusion model, which generates future trajectories by incorporating historical information and random Gaussian noise. The backbone model of EquiDiff is an SO(2)-equivariant transformer that fully utilizes the geometric properties of location coordinates. In addition, we employ Recurrent Neural Networks and Graph Attention Networks to extract social interactions from historical trajectories. To evaluate the performance of EquiDiff, we conduct extensive experiments on the NGSIM dataset. Our results demonstrate that EquiDiff outperforms other baseline models in short-term prediction, but has slightly higher errors for long-term prediction. Furthermore, we conduct an ablation study to investigate the contribution of each component of EquiDiff to the prediction accuracy. Additionally, we present a visualization of the generation process of our diffusion model, providing insights into the uncertainty of the prediction.", "url": "https://arxiv.org/abs/2308.06564"}, {"metadata": {"arXiv": "2308.06624", "Date": "Sat, 12 Aug 2023 17:52:21 ", "Title": "ADRMX: Additive Disentanglement of Domain Features with Remix Loss", "Authors": ["Berker Demirel", "Erchan Aptoula and Huseyin Ozkan"], "Categories": "cs.LG cs.CV"}, "abstract": "The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original additive disentanglement strategy. Moreover, a new data augmentation technique is introduced to further support the generalization capacity of ADRMX, where samples from different domains are mixed within the latent space. Through extensive experiments conducted on DomainBed under fair conditions, ADRMX is shown to achieve state-of-the-art performance. Code will be made available at GitHub after the revision process.", "url": "https://arxiv.org/abs/2308.06624"}, {"metadata": {"arXiv": "2308.06668", "Date": "Sun, 13 Aug 2023 02:59:36 ", "Title": "Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges", "Authors": ["Jiajia Li", "Mingle Xu", "Lirong Xiang", "Dong Chen", "Weichao Zhuang", "Xunyuan Yin and Zhaojian Li"], "Categories": "cs.LG cs.CV", "Comments": ["16 pages", "2 figures"]}, "abstract": "The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field. To this end, we first review recent FMs in the general computer science domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Subsequently, we outline the process of developing agriculture FMs and discuss their potential applications in smart agriculture. We also discuss the unique challenges associated with developing AFMs, including model training, validation, and deployment. Through this study, we contribute to the advancement of AI in agriculture by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.", "url": "https://arxiv.org/abs/2308.06668"}, {"metadata": {"arXiv": "2308.06672", "Date": "Sun, 13 Aug 2023 03:26:01 ", "Title": "A deep learning framework for multi-scale models based on physics-informed neural networks", "Authors": ["Yong Wang and Yanzhong Yao and Jiawei Guo and Zhiming Gao"], "Categories": "cs.LG"}, "abstract": "Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdomains. The proposed method enables loss terms with different magnitudes to be optimized simultaneously, and it advances the application of PINN for multi-scale problems.", "url": "https://arxiv.org/abs/2308.06672"}, {"metadata": {"arXiv": "2308.06679", "Date": "Sun, 13 Aug 2023 03:54:30 ", "Title": "Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations", "Authors": ["Siyuan Xing and Jianqiao Sun"], "Categories": "cs.LG"}, "abstract": "The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GRBFNN on tri-variate function approximations. The SGNN also has better trainability and is more tuning-friendly than DNNs with RuLU and Sigmoid functions. For approximating functions with complex geometry, SGNN can lead to three orders of magnitude more accurate results than a RuLU-DNN with twice the number of layers and the number of neurons per layer.", "url": "https://arxiv.org/abs/2308.06679"}, {"metadata": {"arXiv": "2308.06703", "Date": "Sun, 13 Aug 2023 07:03:22 ", "Title": "Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods", "Authors": ["Avery Ma", "Yangchen Pan and Amir-massoud Farahmand"], "Categories": "cs.LG"}, "abstract": "Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risks close to zero but vary in their adversarial risks. Our result shows that linear models' robustness to $\\ell_2$-norm bounded changes is inversely proportional to the model parameters' weight norm: a smaller weight norm implies better robustness. In the context of deep learning, our experiments show that SGD-trained neural networks show smaller Lipschitz constants, explaining the better robustness to input perturbations than those trained with adaptive gradient methods.", "url": "https://arxiv.org/abs/2308.06703"}, {"metadata": {"arXiv": "2308.06708", "Date": "Sun, 13 Aug 2023 07:55:46 ", "Title": "Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model", "Authors": ["Yuuichi Asahi", "Yuta Hasegawa", "Naoyuki Onodera", "Takashi Shimokawabe", "Hayato Shiba", "Yasuhiro Idomura"], "Categories": "cs.LG physics.comp-ph"}, "abstract": "This paper presents an ensemble data assimilation method using the pseudo ensembles generated by denoising diffusion probabilistic model. Since the model is trained against noisy and sparse observation data, this model can produce divergent ensembles close to observations. Thanks to the variance in generated ensembles, our proposed method displays better performance than the well-established ensemble data assimilation method when the simulation model is imperfect.", "url": "https://arxiv.org/abs/2308.06708"}, {"metadata": {"arXiv": "2308.06740", "Date": "Sun, 13 Aug 2023 10:09:25 ", "Title": "Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection", "Authors": ["Wenwen Min", "Taosheng Xu and Chris Ding"], "Categories": "cs.LG stat.ML"}, "abstract": "Sparse Partial Least Squares (sPLS) is a common dimensionality reduction technique for data fusion, which projects data samples from two views by seeking linear combinations with a small number of variables with the maximum variance. However, sPLS extracts the combinations between two data sets with all data samples so that it cannot detect latent subsets of samples. To extend the application of sPLS by identifying a specific subset of samples and remove outliers, we propose an $\\ell_\\infty/\\ell_0$-norm constrained weighted sparse PLS ($\\ell_\\infty/\\ell_0$-wsPLS) method for joint sample and feature selection, where the $\\ell_\\infty/\\ell_0$-norm constrains are used to select a subset of samples. We prove that the $\\ell_\\infty/\\ell_0$-norm constrains have the Kurdyka-\\L{ojasiewicz}~property so that a globally convergent algorithm is developed to solve it. Moreover, multi-view data with a same set of samples can be available in various real problems. To this end, we extend the $\\ell_\\infty/\\ell_0$-wsPLS model and propose two multi-view wsPLS models for multi-view data fusion. We develop an efficient iterative algorithm for each multi-view wsPLS model and show its convergence property. As well as numerical and biomedical data experiments demonstrate the efficiency of the proposed methods.", "url": "https://arxiv.org/abs/2308.06740"}, {"metadata": {"arXiv": "2308.06763", "Date": "Sun, 13 Aug 2023 13:00:50 ", "Title": "Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining", "Authors": ["Mohammad Dehghani", "Zahra Yazdanparast", "Mobin Mohammadi"], "Categories": "cs.LG cs.DB"}, "abstract": "The COVID-19 pandemic has a devastating impact globally, claiming millions of lives and causing significant social and economic disruptions. In order to optimize decision-making and allocate limited resources, it is essential to identify COVID-19 symptoms and determine the severity of each case. Machine learning algorithms offer a potent tool in the medical field, particularly in mining clinical datasets for useful information and guiding scientific decisions. Association rule mining is a machine learning technique for extracting hidden patterns from data. This paper presents an application of association rule mining based Apriori algorithm to discover symptom patterns from COVID-19 patients. The study, using 2875 records of patient, identified the most common symptoms as apnea (72%), cough (64%), fever (59%), weakness (18%), myalgia (14.5%), and sore throat (12%). The proposed method provides clinicians with valuable insight into disease that can assist them in managing and treating it effectively.", "url": "https://arxiv.org/abs/2308.06763"}, {"metadata": {"arXiv": "2308.06767", "Date": "Sun, 13 Aug 2023 13:34:04 ", "Title": "A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations", "Authors": ["Hongrong Cheng", "Miao Zhang", "Javen Qinfeng Shi"], "Categories": "cs.LG cs.CV"}, "abstract": "Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.", "url": "https://arxiv.org/abs/2308.06767"}, {"metadata": {"arXiv": "2308.06780", "Date": "Sun, 13 Aug 2023 14:25:54 ", "Title": "Neural Networks at a Fraction with Pruned Quaternions", "Authors": ["Sahel Mohammad Iqbal and Subhankar Mishra"], "Categories": "cs.LG cs.CV", "Journal-ref": "In Proceedings of the 6th Joint International Conference on Data Science & Management of Data (CODS-COMAD '23). ACM. 19-27 (2023)", "DOI": "10.1145/3570991.3570997"}, "abstract": "Contemporary state-of-the-art neural networks have increasingly large numbers of parameters, which prevents their deployment on devices with limited computational power. Pruning is one technique to remove unnecessary weights and reduce resource requirements for training and inference. In addition, for ML tasks where the input data is multi-dimensional, using higher-dimensional data embeddings such as complex numbers or quaternions has been shown to reduce the parameter count while maintaining accuracy. In this work, we conduct pruning on real and quaternion-valued implementations of different architectures on classification tasks. We find that for some architectures, at very high sparsity levels, quaternion models provide higher accuracies than their real counterparts. For example, at the task of image classification on CIFAR-10 using Conv-4, at $3\\%$ of the number of parameters as the original model, the pruned quaternion version outperforms the pruned real by more than $10\\%$. Experiments on various network architectures and datasets show that for deployment in extremely resource-constrained environments, a sparse quaternion network might be a better candidate than a real sparse model of similar architecture.", "url": "https://arxiv.org/abs/2308.06780"}, {"metadata": {"arXiv": "2308.06838", "Date": "Sun, 13 Aug 2023 19:45:20 ", "Title": "Generalizing Topological Graph Neural Networks with Paths", "Authors": ["Quang Truong and Peter Chin"], "Categories": "cs.LG"}, "abstract": "While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.", "url": "https://arxiv.org/abs/2308.06838"}, {"metadata": {"arXiv": "2308.06849", "Date": "Sun, 13 Aug 2023 21:42:31 ", "Title": "When Monte-Carlo Dropout Meets Multi-Exit: Optimizing Bayesian Neural Networks on FPGA", "Authors": ["Hongxiang Fan and Hao Chen and Liam Castelli and Zhiqiang Que and He Li and Kenneth Long and Wayne Luk"], "Categories": "cs.LG cs.AR"}, "abstract": "Bayesian Neural Networks (BayesNNs) have demonstrated their capability of providing calibrated prediction for safety-critical applications such as medical imaging and autonomous driving. However, the high algorithmic complexity and the poor hardware performance of BayesNNs hinder their deployment in real-life applications. To bridge this gap, this paper proposes a novel multi-exit Monte-Carlo Dropout (MCD)-based BayesNN that achieves well-calibrated predictions with low algorithmic complexity. To further reduce the barrier to adopting BayesNNs, we propose a transformation framework that can generate FPGA-based accelerators for multi-exit MCD-based BayesNNs. Several novel optimization techniques are introduced to improve hardware performance. Our experiments demonstrate that our auto-generated accelerator achieves higher energy efficiency than CPU, GPU, and other state-of-the-art hardware implementations.", "url": "https://arxiv.org/abs/2308.06849"}, {"metadata": {"arXiv": "2308.06862", "Date": "Sun, 13 Aug 2023 23:34:36 ", "Title": "Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks", "Authors": ["Erfan Loghmani", "MohammadAmin Fazli"], "Categories": "cs.LG cs.SI", "Comments": ["29 pages", "10 figures", "4 tables", "Submitted to Information Sciences"]}, "abstract": "Representation learning methods have revolutionized machine learning on networks by converting discrete network structures into continuous domains. However, dynamic networks that evolve over time pose new challenges. To address this, dynamic representation learning methods have gained attention, offering benefits like reduced learning time and improved accuracy by utilizing temporal information. T-batching is a valuable technique for training dynamic network models that reduces training time while preserving vital conditions for accurate modeling. However, we have identified a limitation in the training loss function used with t-batching. Through mathematical analysis, we propose two alternative loss functions that overcome these issues, resulting in enhanced training performance. We extensively evaluate the proposed loss functions on synthetic and real-world dynamic networks. The results consistently demonstrate superior performance compared to the original loss function. Notably, in a real-world network characterized by diverse user interaction histories, the proposed loss functions achieved more than 26.9% enhancement in Mean Reciprocal Rank (MRR) and more than 11.8% improvement in Recall@10. These findings underscore the efficacy of the proposed loss functions in dynamic network modeling.", "url": "https://arxiv.org/abs/2308.06862"}, {"metadata": {"arXiv": "2308.06895", "Date": "Mon, 14 Aug 2023 02:25:48 ", "Title": "Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls", "Authors": ["Saurav Prakash", "Jin Sima", "Chao Pan", "Eli Chien", "Olgica Milenkovic"], "Categories": "cs.LG cs.CR cs.DC"}, "abstract": "Hierarchical and tree-like data sets arise in many applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion. This problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we introduce a number-theoretic approach for label recovery based on the so-called integer $B_h$ sequences. Third, we compute the complexity of the convex hulls in hyperbolic spaces to assess the extent of data leakage; at the same time, in order to limit the communication cost for the hulls, we propose a new quantization method for the Poincar\\'e disc coupled with Reed-Solomon-like encoding. Fourth, at server level, we introduce a new approach for aggregating convex hulls of the clients based on balanced graph partitioning. We test our method on a collection of diverse data sets, including hierarchical single-cell RNA-seq data from different patients distributed across different repositories that have stringent privacy constraints. The classification accuracy of our method is up to $\\sim 11\\%$ better than its Euclidean counterpart, demonstrating the importance of privacy-preserving learning in hyperbolic spaces.", "url": "https://arxiv.org/abs/2308.06895"}, {"metadata": {"arXiv": "2308.06911", "Date": "Mon, 14 Aug 2023 03:12:29 ", "Title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text", "Authors": ["Pengfei Liu", "Yiming Ren and Zhixiang Ren"], "Categories": "cs.LG cs.CL q-bio.BM", "Comments": ["16 pages", "5 figures"]}, "abstract": "Large language models have made significant strides in natural language processing, paving the way for innovative applications including molecular representation and generation. However, most existing single-modality approaches cannot capture the abundant and complex information in molecular data. Here, we introduce GIT-Mol, a multi-modal large language model that integrates the structure Graph, Image, and Text information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture capable of mapping all modalities into a unified latent space. Our study develops an innovative any-to-language molecular translation strategy and achieves a 10%-15% improvement in molecular captioning, a 5%-10% accuracy increase in property prediction, and a 20% boost in molecule generation validity compared to baseline or single-modality models.", "url": "https://arxiv.org/abs/2308.06911"}, {"metadata": {"arXiv": "2308.06912", "Date": "Mon, 14 Aug 2023 03:14:38 ", "Title": "CausalLM is not optimal for in-context learning", "Authors": ["Nan Ding", "Tomer Levinboim", "Jialin Wu", "Sebastian Goodman", "Radu Soricut"], "Categories": "cs.LG cs.CL"}, "abstract": "Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.", "url": "https://arxiv.org/abs/2308.06912"}, {"metadata": {"arXiv": "2308.06925", "Date": "Mon, 14 Aug 2023 04:03:51 ", "Title": "CBA: Improving Online Continual Learning via Continual Bias Adaptor", "Authors": ["Quanziang Wang", "Renzhen Wang", "Yichen Wu", "Xixi Jia", "Deyu Meng"], "Categories": "cs.LG cs.CV", "Comments": ["Accepted by ICCV 2023"]}, "abstract": "Online continual learning (CL) aims to learn new knowledge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying training setting, the model learned from a changing distribution easily forgets the previously learned knowledge and biases toward the newly received task. To address this problem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that the classifier network is able to learn a stable consolidation of previously learned tasks. In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead. We theoretically reveal the reason why the proposed method can effectively alleviate catastrophic distribution shifts, and empirically demonstrate its effectiveness through extensive experiments based on four rehearsal-based baselines and three public continual learning benchmarks.", "url": "https://arxiv.org/abs/2308.06925"}, {"metadata": {"arXiv": "2308.06929", "Date": "Mon, 14 Aug 2023 04:15:09 ", "Title": "Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models", "Authors": ["Sam Chapman", "Seifey Mohammad", "Kimberly Villegas"], "Categories": "cs.LG", "Comments": ["40 pages", "10 tables", "12 figures"]}, "abstract": "Our research group wanted to take on the difficult task of predicting prices in a dynamic market. And short term rentals such as Airbnb listings seemed to be the perfect proving ground to do such a thing. Airbnb has revolutionized the travel industry by providing a platform for homeowners to rent out their properties to travelers. The pricing of Airbnb rentals is prone to high fluctuations, with prices changing frequently based on demand, seasonality, and other factors. Accurate prediction of Airbnb rental prices is crucial for hosts to optimize their revenue and for travelers to make informed booking decisions. In this project, we aim to predict the prices of Airbnb rentals using a machine learning modeling approach. Our project expands on earlier research in the area of analyzing Airbnb rental prices by taking a methodical machine learning approach as well as incorporating sentiment analysis into our feature engineering. We intend to gain a deeper understanding on periodic changes of Airbnb rental prices. The primary objective of this study is to construct an accurate machine learning model for predicting Airbnb rental prices specifically in Austin, Texas. Our project's secondary objective is to identify the key factors that drive Airbnb rental prices and to investigate how these factors vary across different locations and property types.", "url": "https://arxiv.org/abs/2308.06929"}, {"metadata": {"arXiv": "2308.06952", "Date": "Mon, 14 Aug 2023 06:04:50 ", "Title": "Channel-Wise Contrastive Learning for Learning with Noisy Labels", "Authors": ["Hui Kang", "Sheng Liu", "Huaxi Huang", "Tongliang Liu"], "Categories": "cs.LG cs.CV"}, "abstract": "In real-world datasets, noisy labels are pervasive. The challenge of learning with noisy labels (LNL) is to train a classifier that discerns the actual classes from given instances. For this, the model must identify features indicative of the authentic labels. While research indicates that genuine label information is embedded in the learned features of even inaccurately labeled data, it's often intertwined with noise, complicating its direct application. Addressing this, we introduce channel-wise contrastive learning (CWCL). This method distinguishes authentic label information from noise by undertaking contrastive learning across diverse channels. Unlike conventional instance-wise contrastive learning (IWCL), CWCL tends to yield more nuanced and resilient features aligned with the authentic labels. Our strategy is twofold: firstly, using CWCL to extract pertinent features to identify cleanly labeled samples, and secondly, progressively fine-tuning using these samples. Evaluations on several benchmark datasets validate our method's superiority over existing approaches.", "url": "https://arxiv.org/abs/2308.06952"}, {"metadata": {"arXiv": "2308.06959", "Date": "Mon, 14 Aug 2023 06:29:09 ", "Title": "Data-Driven Allocation of Preventive Care With Application to Diabetes Mellitus Type II", "Authors": ["Mathias Kraus", "Stefan Feuerriegel", "Maytal Saar-Tsechansky"], "Categories": "cs.LG", "Comments": ["Accepted by Manufacturing & Service Operations Management"], "DOI": "10.1287/msom.2021.0251"}, "abstract": "Problem Definition. Increasing costs of healthcare highlight the importance of effective disease prevention. However, decision models for allocating preventive care are lacking. Methodology/Results. In this paper, we develop a data-driven decision model for determining a cost-effective allocation of preventive treatments to patients at risk. Specifically, we combine counterfactual inference, machine learning, and optimization techniques to build a scalable decision model that can exploit high-dimensional medical data, such as the data found in modern electronic health records. Our decision model is evaluated based on electronic health records from 89,191 prediabetic patients. We compare the allocation of preventive treatments (metformin) prescribed by our data-driven decision model with that of current practice. We find that if our approach is applied to the U.S. population, it can yield annual savings of $1.1 billion. Finally, we analyze the cost-effectiveness under varying budget levels. Managerial Implications. Our work supports decision-making in health management, with the goal of achieving effective disease prevention at lower costs. Importantly, our decision model is generic and can thus be used for effective allocation of preventive care for other preventable diseases.", "url": "https://arxiv.org/abs/2308.06959"}, {"metadata": {"arXiv": "2308.07047", "Date": "Mon, 14 Aug 2023 10:16:12 ", "Title": "No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning", "Authors": ["Xiang Li and Songcan Chen"], "Categories": "cs.LG stat.ML", "Comments": ["9 pages", "4 figures"]}, "abstract": "Label Distribution Learning (LDL) assigns soft labels, a.k.a. degrees, to a sample. In reality, it is always laborious to obtain complete degrees, giving birth to the Incomplete LDL (InLDL). However, InLDL often suffers from performance degeneration. To remedy it, existing methods need one or more explicit regularizations, leading to burdensome parameter tuning and extra computation. We argue that label distribution itself may provide useful prior, when used appropriately, the InLDL problem can be solved without any explicit regularization. In this paper, we offer a rational alternative to use such a prior. Our intuition is that large degrees are likely to get more concern, the small ones are easily overlooked, whereas the missing degrees are completely neglected in InLDL. To learn an accurate label distribution, it is crucial not to ignore the small observed degrees but to give them properly large weights, while gradually increasing the weights of the missing degrees. To this end, we first define a weighted empirical risk and derive upper bounds between the expected risk and the weighted empirical risk, which reveals in principle that weighting plays an implicit regularization role. Then, by using the prior of degrees, we design a weighted scheme and verify its effectiveness. To sum up, our model has four advantages, it is 1) model selection free, as no explicit regularization is imposed; 2) with closed form solution (sub-problem) and easy-to-implement (a few lines of codes); 3) with linear computational complexity in the number of samples, thus scalable to large datasets; 4) competitive with state-of-the-arts even without any explicit regularization.", "url": "https://arxiv.org/abs/2308.07047"}, {"metadata": {"arXiv": "2308.07051", "Date": "Mon, 14 Aug 2023 10:22:51 ", "Title": "Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems", "Authors": ["Bilal Thonnam Thodi and Sai Venkata Ramana Ambadipudi and Saif Eddin Jabari"], "Categories": "cs.LG cs.SY eess.SY"}, "abstract": "Deep learning methods are emerging as popular computational tools for solving forward and inverse problems in traffic flow. In this paper, we study a neural operator framework for learning solutions to nonlinear hyperbolic partial differential equations with applications in macroscopic traffic flow models. In this framework, an operator is trained to map heterogeneous and sparse traffic input data to the complete macroscopic traffic state in a supervised learning setting. We chose a physics-informed Fourier neural operator ($\\pi$-FNO) as the operator, where an additional physics loss based on a discrete conservation law regularizes the problem during training to improve the shock predictions. We also propose to use training data generated from random piecewise constant input data to systematically capture the shock and rarefied solutions. From experiments using the LWR traffic flow model, we found superior accuracy in predicting the density dynamics of a ring-road network and urban signalized road. We also found that the operator can be trained using simple traffic density dynamics, e.g., consisting of $2-3$ vehicle queues and $1-2$ traffic signal cycles, and it can predict density dynamics for heterogeneous vehicle queue distributions and multiple traffic signal cycles $(\\geq 2)$ with an acceptable error. The extrapolation error grew sub-linearly with input complexity for a proper choice of the model architecture and training data. Adding a physics regularizer aided in learning long-term traffic density dynamics, especially for problems with periodic boundary data.", "url": "https://arxiv.org/abs/2308.07051"}, {"metadata": {"arXiv": "2308.07126", "Date": "Mon, 14 Aug 2023 13:13:50 ", "Title": "A Time-aware tensor decomposition for tracking evolving patterns", "Authors": ["Christos Chatzis", "Max Pfeffer", "Pedro Lind", "Evrim Acar"], "Categories": "cs.LG cs.CV stat.ML", "Comments": ["6 pages", "5 figures"]}, "abstract": "Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regularization.", "url": "https://arxiv.org/abs/2308.07126"}, {"metadata": {"arXiv": "2308.07209", "Date": "Mon, 14 Aug 2023 15:25:07 ", "Title": "Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning", "Authors": ["Shipeng Bai", "Jun Chen", "Xintian Shen", "Yixuan Qian", "Yong Liu"], "Categories": "cs.LG cs.CV eess.IV", "Comments": ["ICCV2023"]}, "abstract": "Structured pruning and quantization are promising approaches for reducing the inference time and memory footprint of neural networks. However, most existing methods require the original training dataset to fine-tune the model. This not only brings heavy resource consumption but also is not possible for applications with sensitive or proprietary data due to privacy and security concerns. Therefore, a few data-free methods are proposed to address this problem, but they perform data-free pruning and quantization separately, which does not explore the complementarity of pruning and quantization. In this paper, we propose a novel framework named Unified Data-Free Compression(UDFC), which performs pruning and quantization simultaneously without any data and fine-tuning process. Specifically, UDFC starts with the assumption that the partial information of a damaged(e.g., pruned or quantized) channel can be preserved by a linear combination of other channels, and then derives the reconstruction form from the assumption to restore the information loss due to compression. Finally, we formulate the reconstruction error between the original network and its compressed network, and theoretically deduce the closed-form solution. We evaluate the UDFC on the large-scale image classification task and obtain significant improvements over various network architectures and compression methods. For example, we achieve a 20.54% accuracy improvement on ImageNet dataset compared to SOTA method with 30% pruning ratio and 6-bit quantization on ResNet-34.", "url": "https://arxiv.org/abs/2308.07209"}, {"metadata": {"arXiv": "2308.07233", "Date": "Mon, 14 Aug 2023 16:16:31 ", "Title": "A Unifying Generator Loss Function for Generative Adversarial Networks", "Authors": ["Justin Veiner", "Fady Alajaji", "Bahman Gharesifard"], "Categories": "cs.LG", "Comments": ["31 pages", "4 figures", "12 tables"]}, "abstract": "A unifying $\\alpha$-parametrized generator loss function is introduced for a dual-objective generative adversarial network (GAN), which uses a canonical (or classical) discriminator loss function such as the one in the original GAN (VanillaGAN) system. The generator loss function is based on a symmetric class probability estimation type function, $\\mathcal{L}_\\alpha$, and the resulting GAN system is termed $\\mathcal{L}_\\alpha$-GAN. Under an optimal discriminator, it is shown that the generator's optimization problem consists of minimizing a Jensen-$f_\\alpha$-divergence, a natural generalization of the Jensen-Shannon divergence, where $f_\\alpha$ is a convex function expressed in terms of the loss function $\\mathcal{L}_\\alpha$. It is also demonstrated that this $\\mathcal{L}_\\alpha$-GAN problem recovers as special cases a number of GAN problems in the literature, including VanillaGAN, Least Squares GAN (LSGAN), Least $k$th order GAN (L$k$GAN) and the recently introduced $(\\alpha_D,\\alpha_G)$-GAN with $\\alpha_D=1$. Finally, experimental results are conducted on three datasets, MNIST, CIFAR-10, and Stacked MNIST to illustrate the performance of various examples of the $\\mathcal{L}_\\alpha$-GAN system.", "url": "https://arxiv.org/abs/2308.07233"}, {"metadata": {"arXiv": "2308.07250", "Date": "Mon, 14 Aug 2023 16:34:47 ", "Title": "LCE -- An Augmented Combination of Bagging and Boosting in Python", "Authors": ["Kevin Fauvel", "\\'Elisa Fromont", "V\\'eronique Masson", "Philippe Faverdin and Alexandre Termier"], "Categories": "cs.LG"}, "abstract": "lcensemble is a high-performing, scalable and user-friendly Python package for the general tasks of classification and regression. The package implements Local Cascade Ensemble (LCE), a machine learning method that further enhances the prediction performance of the current state-of-the-art methods Random Forest and XGBoost. LCE combines their strengths and adopts a complementary diversification approach to obtain a better generalizing predictor. The package is compatible with scikit-learn, therefore it can interact with scikit-learn pipelines and model selection tools. It is distributed under the Apache 2.0 license, and its source code is available at https://github.com/LocalCascadeEnsemble/LCE.", "url": "https://arxiv.org/abs/2308.07250"}, {"metadata": {"arXiv": "2308.07272", "Date": "Mon, 14 Aug 2023 16:58:50 ", "Title": "Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning", "Authors": ["Chengzhengxu Li", "Xiaoming Liu", "Yichen Wang", "Duyi Li", "Yu Lan", "Chao Shen"], "Categories": "cs.LG cs.CL"}, "abstract": "Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.67% of the PLM parameter size on the tasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that $DP_2O$ has good universality, robustness, and generalization ability.", "url": "https://arxiv.org/abs/2308.07272"}, {"metadata": {"arXiv": "2308.06594", "Date": "Sat, 12 Aug 2023 15:19:49 ", "Title": "CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning", "Authors": ["Jumman Hossain", "Abu-Zaher Faridee", "Nirmalya Roy", "Anjan Basak", "Derrik E. Asher"], "Categories": "cs.RO cs.LG"}, "abstract": "Autonomous navigation in offroad environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an underexplored area. In this paper, we propose a novel Deep Reinforcement Learning (DRL) based algorithm, called CoverNav, for identifying covert and navigable trajectories with minimal cost in offroad terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. CoverNav helps robot agents to learn the low elevation terrain using a reward function while penalizing it proportionately when it experiences high elevation. If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, disabled vehicles, trees, etc.) and use them as shelters to hide behind. We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL based navigation algorithm. Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects. We observe competitive performance comparable to state of the art (SOTA) methods without compromising accuracy.", "url": "https://arxiv.org/abs/2308.06594"}, {"metadata": {"arXiv": "2308.06654", "Date": "Sun, 13 Aug 2023 00:20:22 ", "Title": "Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks", "Authors": ["Mahsa Golchoubian", "Moojan Ghafurian", "Kerstin Dautenhahn", "Nasser Lashgarian Azad"], "Categories": "cs.RO cs.CV cs.HC cs.LG", "Comments": ["Accepted for publication as a conference paper in IEEE Intelligent Transportation Systems Conference (ITSC)", "2023"]}, "abstract": "Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on interactions of potentially colliding agents with a target pedestrian, we propose the use of time-to-collision and the approach direction angle of two agents for encoding the interaction effect. This is done by introducing a novel polar collision grid map. Our results have shown predicted trajectories closer to the ground truth compared to existing methods (used as a baseline) on the HBS dataset.", "url": "https://arxiv.org/abs/2308.06654"}, {"metadata": {"arXiv": "2308.07118", "Date": "Mon, 14 Aug 2023 12:57:12 ", "Title": "Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases", "Authors": ["Eugen \\v{S}lapak", "Enric Pardo", "Mat\\'u\\v{s} Dopiriak", "Taras Maksymyuk and Juraj Gazda"], "Categories": "cs.RO cs.LG"}, "abstract": "The proliferation of technologies, such as extended reality (XR), has increased the demand for high-quality three-dimensional (3D) graphical representations. Industrial 3D applications encompass computer-aided design (CAD), finite element analysis (FEA), scanning, and robotics. However, current methods employed for industrial 3D representations suffer from high implementation costs and reliance on manual human input for accurate 3D modeling. To address these challenges, neural radiance fields (NeRFs) have emerged as a promising approach for learning 3D scene representations based on provided training 2D images. Despite a growing interest in NeRFs, their potential applications in various industrial subdomains are still unexplored. In this paper, we deliver a comprehensive examination of NeRF industrial applications while also providing direction for future research endeavors. We also present a series of proof-of-concept experiments that demonstrate the potential of NeRFs in the industrial domain. These experiments include NeRF-based video compression techniques and using NeRFs for 3D motion estimation in the context of collision avoidance. In the video compression experiment, our results show compression savings up to 48\\% and 74\\% for resolutions of 1920x1080 and 300x168, respectively. The motion estimation experiment used a 3D animation of a robotic arm to train Dynamic-NeRF (D-NeRF) and achieved an average disparity map PSNR of 23 dB and an SSIM of 0.97. The code for our experiments is publicly available at https://github.com/Maftej/iisnerf .", "url": "https://arxiv.org/abs/2308.07118"}, {"metadata": {"arXiv": "2308.06309", "Date": "Fri, 11 Aug 2023 17:29:49 ", "Title": "Predicting Resilience with Neural Networks", "Authors": ["Karen da Mata", "Priscila Silva and Lance Fiondella"], "Categories": "eess.SY cs.LG cs.PF cs.SY"}, "abstract": "Resilience engineering studies the ability of a system to survive and recover from disruptive events, which finds applications in several domains. Most studies emphasize resilience metrics to quantify system performance, whereas recent studies propose statistical modeling approaches to project system recovery time after degradation. Moreover, past studies are either performed on data after recovering or limited to idealized trends. Therefore, this paper proposes three alternative neural network (NN) approaches including (i) Artificial Neural Networks, (ii) Recurrent Neural Networks, and (iii) Long-Short Term Memory (LSTM) to model and predict system performance, including negative and positive factors driving resilience to quantify the impact of disruptive events and restorative activities. Goodness-of-fit measures are computed to evaluate the models and compared with a classical statistical model, including mean squared error and adjusted R squared. Our results indicate that NN models outperformed the traditional model on all goodness-of-fit measures. More specifically, LSTMs achieved an over 60\\% higher adjusted R squared, and decreased predictive error by 34-fold compared to the traditional method. These results suggest that NN models to predict resilience are both feasible and accurate and may find practical use in many important domains.", "url": "https://arxiv.org/abs/2308.06309"}, {"metadata": {"arXiv": "2308.06374", "Date": "Fri, 11 Aug 2023 20:16:57 ", "Title": "Large Language Models and Knowledge Graphs: Opportunities and Challenges", "Authors": ["Jeff Z. Pan", "Simon Razniewski", "Jan-Christoph Kalo", "Sneha Singhania", "Jiaoyan Chen", "Stefan Dietze", "Hajira Jabeen", "Janna Omeliyanenko", "Wen Zhang", "Matteo Lissandrini", "Russa Biswas", "Gerard de Melo", "Angela Bonifati", "Edlira Vakaj", "Mauro Dragoni", "Damien Graux"], "Categories": "cs.AI cs.CL", "Comments": ["30 pages"]}, "abstract": "Large Language Models (LLMs) have taken Knowledge Representation -- and the world -- by storm. This inflection point marks a shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge. In this position paper, we will discuss some of the common debate points within the community on LLMs (parametric knowledge) and Knowledge Graphs (explicit knowledge) and speculate on opportunities and visions that the renewed focus brings, as well as related research topics and challenges.", "url": "https://arxiv.org/abs/2308.06374"}, {"metadata": {"arXiv": "2308.06411", "Date": "Fri, 11 Aug 2023 23:01:59 ", "Title": "Dialogue Possibilities between a Human Supervisor and UAM Air Traffic Management: Route Alteration", "Authors": ["Jeongseok Kim and Kangjin Kim"], "Categories": "cs.AI cs.RO", "Comments": ["18 pages", "2 figures", "accepted to the Advances in Artificial Intelligence and Machine Learning (AAIML) journal"]}, "abstract": "This paper introduces a novel approach to detour management in Urban Air Traffic Management (UATM) using knowledge representation and reasoning. It aims to understand the complexities and requirements of UAM detours, enabling a method that quickly identifies safe and efficient routes in a carefully sampled environment. This method implemented in Answer Set Programming uses non-monotonic reasoning and a two-phase conversation between a human manager and the UATM system, considering factors like safety and potential impacts. The robustness and efficacy of the proposed method were validated through several queries from two simulation scenarios, contributing to the symbiosis of human knowledge and advanced AI techniques. The paper provides an introduction, citing relevant studies, problem formulation, solution, discussions, and concluding comments.", "url": "https://arxiv.org/abs/2308.06411"}, {"metadata": {"arXiv": "2308.06498", "Date": "Sat, 12 Aug 2023 08:22:11 ", "Title": "Latent Emission-Augmented Perspective-Taking (LEAPT) for Human-Robot Interaction", "Authors": ["Kaiqi Chen", "Jing Yu Lim", "Kingsley Kuan", "Harold Soh"], "Categories": "cs.AI cs.HC cs.RO"}, "abstract": "Perspective-taking is the ability to perceive or understand a situation or concept from another individual's point of view, and is crucial in daily human interactions. Enabling robots to perform perspective-taking remains an unsolved problem; existing approaches that use deterministic or handcrafted methods are unable to accurately account for uncertainty in partially-observable settings. This work proposes to address this limitation via a deep world model that enables a robot to perform both perception and conceptual perspective taking, i.e., the robot is able to infer what a human sees and believes. The key innovation is a decomposed multi-modal latent state space model able to generate and augment fictitious observations/emissions. Optimizing the ELBO that arises from this probabilistic graphical model enables the learning of uncertainty in latent space, which facilitates uncertainty estimation from high-dimensional observations. We tasked our model to predict human observations and beliefs on three partially-observable HRI tasks. Experiments show that our method significantly outperforms existing baselines and is able to infer visual observations available to other agent and their internal beliefs.", "url": "https://arxiv.org/abs/2308.06498"}, {"metadata": {"arXiv": "2308.06512", "Date": "Sat, 12 Aug 2023 09:31:43 ", "Title": "HyperFormer: Enhancing Entity and Relation Interaction for Hyper-Relational Knowledge Graph Completion", "Authors": ["Zhiwei Hu", "V\\'ictor Guti\\'errez-Basulto", "Zhiliang Xiang", "Ru Li", "Jeff Z. Pan"], "Categories": "cs.AI cs.CL", "Comments": ["Accepted at CIKM'23"]}, "abstract": "Hyper-relational knowledge graphs (HKGs) extend standard knowledge graphs by associating attribute-value qualifiers to triples, which effectively represent additional fine-grained information about its associated triple. Hyper-relational knowledge graph completion (HKGC) aims at inferring unknown triples while considering its qualifiers. Most existing approaches to HKGC exploit a global-level graph structure to encode hyper-relational knowledge into the graph convolution message passing process. However, the addition of multi-hop information might bring noise into the triple prediction process. To address this problem, we propose HyperFormer, a model that considers local-level sequential information, which encodes the content of the entities, relations and qualifiers of a triple. More precisely, HyperFormer is composed of three different modules: an entity neighbor aggregator module allowing to integrate the information of the neighbors of an entity to capture different perspectives of it; a relation qualifier aggregator module to integrate hyper-relational knowledge into the corresponding relation to refine the representation of relational content; a convolution-based bidirectional interaction module based on a convolutional operation, capturing pairwise bidirectional interactions of entity-relation, entity-qualifier, and relation-qualifier. realize the depth perception of the content related to the current statement. Furthermore, we introduce a Mixture-of-Experts strategy into the feed-forward layers of HyperFormer to strengthen its representation capabilities while reducing the amount of model parameters and computation. Extensive experiments on three well-known datasets with four different conditions demonstrate HyperFormer's effectiveness. Datasets and code are available at https://github.com/zhiweihu1103/HKGC-HyperFormer.", "url": "https://arxiv.org/abs/2308.06512"}, {"metadata": {"arXiv": "2308.06920", "Date": "Mon, 14 Aug 2023 03:43:57 ", "Title": "Chatbots in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with ChatGPT", "Authors": ["Rui Wang", "Hongsong Feng", "Guo-Wei Wei"], "Categories": "cs.AI cs.HC q-bio.BM"}, "abstract": "The birth of ChatGPT, a cutting-edge language model chatbot developed by OpenAI, ushered in a new era in AI, and this paper vividly showcases its innovative application within the field of drug discovery. Focused specifically on developing anti-cocaine addiction drugs, the study employs GPT-4 as a virtual guide, offering strategic and methodological insights to researchers working on generative models for drug candidates. The primary objective is to generate optimal drug-like molecules with desired properties. By leveraging the capabilities of ChatGPT, the study introduces a novel approach to the drug discovery process. This symbiotic partnership between AI and researchers transforms how drug development is approached. Chatbots become facilitators, steering researchers towards innovative methodologies and productive paths for creating effective drug candidates. This research sheds light on the collaborative synergy between human expertise and AI assistance, wherein ChatGPT's cognitive abilities enhance the design and development of potential pharmaceutical solutions. This paper not only explores the integration of advanced AI in drug discovery but also reimagines the landscape by advocating for AI-powered chatbots as trailblazers in revolutionizing therapeutic innovation.", "url": "https://arxiv.org/abs/2308.06920"}, {"metadata": {"arXiv": "2308.06922", "Date": "Mon, 14 Aug 2023 03:55:14 ", "Title": "Probabilistic contingent planning based on HTN for high-quality plans", "Authors": ["Peng Zhao"], "Categories": "cs.AI", "Comments": ["10 pages", "1 figure"]}, "abstract": "Deterministic planning assumes that the planning evolves along a fully predictable path, and therefore it loses the practical value in most real projections. A more realistic view is that planning ought to take into consideration partial observability beforehand and aim for a more flexible and robust solution. What is more significant, it is inevitable that the quality of plan varies dramatically in the partially observable environment. In this paper we propose a probabilistic contingent Hierarchical Task Network (HTN) planner, named High-Quality Contingent Planner (HQCP), to generate high-quality plans in the partially observable environment. The formalisms in HTN planning are extended into partial observability and are evaluated regarding the cost. Next, we explore a novel heuristic for high-quality plans and develop the integrated planning algorithm. Finally, an empirical study verifies the effectiveness and efficiency of the planner both in probabilistic contingent planning and for obtaining high-quality plans.", "url": "https://arxiv.org/abs/2308.06922"}, {"metadata": {"arXiv": "2308.06942", "Date": "Mon, 14 Aug 2023 05:22:33 ", "Title": "Approximating Human-Like Few-shot Learning with GPT-based Compression", "Authors": ["Cynthia Huang", "Yuqing Xie", "Zhiying Jiang", "Jimmy Lin", "Ming Li"], "Categories": "cs.AI cs.CL cs.IT math.IT"}, "abstract": "In this work, we conceptualize the learning process as information compression. We seek to equip generative pre-trained models with human-like learning capabilities that enable data compression during inference. We present a novel approach that utilizes the Generative Pre-trained Transformer (GPT) to approximate Kolmogorov complexity, with the aim of estimating the optimal Information Distance for few-shot learning. We first propose using GPT as a prior for lossless text compression, achieving a noteworthy compression ratio. Experiment with LLAMA2-7B backbone achieves a compression ratio of 15.5 on enwik9. We justify the pre-training objective of GPT models by demonstrating its equivalence to the compression length, and, consequently, its ability to approximate the information distance for texts. Leveraging the approximated information distance, our method allows the direct application of GPT models in quantitative text similarity measurements. Experiment results show that our method overall achieves superior performance compared to embedding and prompt baselines on challenging NLP tasks, including semantic similarity, zero and one-shot text classification, and zero-shot text ranking.", "url": "https://arxiv.org/abs/2308.06942"}, {"metadata": {"arXiv": "2308.07054", "Date": "Mon, 14 Aug 2023 10:27:58 ", "Title": "Distinguishing Risk Preferences using Repeated Gambles", "Authors": ["James Price", "Colm Connaughton"], "Categories": "cs.AI math.PR"}, "abstract": "Sequences of repeated gambles provide an experimental tool to characterize the risk preferences of humans or artificial decision-making agents. The difficulty of this inference depends on factors including the details of the gambles offered and the number of iterations of the game played. In this paper we explore in detail the practical challenges of inferring risk preferences from the observed choices of artificial agents who are presented with finite sequences of repeated gambles. We are motivated by the fact that the strategy to maximize long-run wealth for sequences of repeated additive gambles (where gains and losses are independent of current wealth) is different to the strategy for repeated multiplicative gambles (where gains and losses are proportional to current wealth.) Accurate measurement of risk preferences would be needed to tell whether an agent is employing the optimal strategy or not. To generalize the types of gambles our agents face we use the Yeo-Johnson transformation, a tool borrowed from feature engineering for time series analysis, to construct a family of gambles that interpolates smoothly between the additive and multiplicative cases. We then analyze the optimal strategy for this family, both analytically and numerically. We find that it becomes increasingly difficult to distinguish the risk preferences of agents as their wealth increases. This is because agents with different risk preferences eventually make the same decisions for sufficiently high wealth. We believe that these findings are informative for the effective design of experiments to measure risk preferences in humans.", "url": "https://arxiv.org/abs/2308.07054"}, {"metadata": {"arXiv": "2308.07294", "Date": "Mon, 14 Aug 2023 17:30:03 ", "Title": "Why Not? Explaining Missing Entailments with $\\rm E{\\scriptsize VEE}$ (Technical Report)", "Authors": ["Christian Alrabbaa", "Stefan Borgwardt", "Tom Friese", "Patrick Koopmann", "Mikhail Kotlov"], "Categories": "cs.AI cs.LO"}, "abstract": "Understanding logical entailments derived by a description logic reasoner is not always straight-forward for ontology users. For this reason, various methods for explaining entailments using justifications and proofs have been developed and implemented as plug-ins for the ontology editor Prot\\'eg\\'e. However, when the user expects a missing consequence to hold, it is equally important to explain why it does not follow from the ontology. In this paper, we describe a new version of $\\rm E{\\scriptsize VEE}$, a Prot\\'eg\\'e plugin that now also provides explanations for missing consequences, via existing and new techniques based on abduction and counterexamples.", "url": "https://arxiv.org/abs/2308.07294"}, {"metadata": {"arXiv": "2308.07307", "Date": "Mon, 14 Aug 2023 17:50:38 ", "Title": "Extend Wave Function Collapse to Large-Scale Content Generation", "Authors": ["Yuhe Nie", "Shaoming Zheng", "Zhan Zhuang", "Xuan Song"], "Categories": "cs.AI", "Comments": ["This paper is accepted by IEEE Conference on Games 2023 (nomination of the Best Paper Award)"]}, "abstract": "Wave Function Collapse (WFC) is a widely used tile-based algorithm in procedural content generation, including textures, objects, and scenes. However, the current WFC algorithm and related research lack the ability to generate commercialized large-scale or infinite content due to constraint conflict and time complexity costs. This paper proposes a Nested WFC (N-WFC) algorithm framework to reduce time complexity. To avoid conflict and backtracking problems, we offer a complete and sub-complete tileset preparation strategy, which requires only a small number of tiles to generate aperiodic and deterministic infinite content. We also introduce the weight-brush system that combines N-WFC and sub-complete tileset, proving its suitability for game design. Our contribution addresses WFC's challenge in massive content generation and provides a theoretical basis for implementing concrete games.", "url": "https://arxiv.org/abs/2308.07307"}, {"metadata": {"arXiv": "2308.06335", "Date": "Fri, 11 Aug 2023 18:19:16 ", "Title": "Combining feature aggregation and geometric similarity for re-identification of patterned animals", "Authors": ["Veikka Immonen", "Ekaterina Nepovinnykh", "Tuomas Eerola", "Charles V. Stewart", "Heikki K\\\"alvi\\\"ainen"], "Categories": "cs.CV cs.AI", "Comments": ["Camera traps", "AI", "and Ecology", "3rd International Workshop"]}, "abstract": "Image-based re-identification of animal individuals allows gathering of information such as migration patterns of the animals over time. This, together with large image volumes collected using camera traps and crowdsourcing, opens novel possibilities to study animal populations. For many species, the re-identification can be done by analyzing the permanent fur, feather, or skin patterns that are unique to each individual. In this paper, we address the re-identification by combining two types of pattern similarity metrics: 1) pattern appearance similarity obtained by pattern feature aggregation and 2) geometric pattern similarity obtained by analyzing the geometric consistency of pattern similarities. The proposed combination allows to efficiently utilize both the local and global pattern features, providing a general re-identification approach that can be applied to a wide variety of different pattern types. In the experimental part of the work, we demonstrate that the method achieves promising re-identification accuracies for Saimaa ringed seals and whale sharks.", "url": "https://arxiv.org/abs/2308.06335"}, {"metadata": {"arXiv": "2308.06451", "Date": "Sat, 12 Aug 2023 03:05:53 ", "Title": "Semantic Equivariant Mixup", "Authors": ["Zongbo Han", "Tianchi Xie", "Bingzhe Wu", "Qinghua Hu", "Changqing Zhang"], "Categories": "cs.CV cs.AI", "Comments": ["Under review"]}, "abstract": "Mixup is a well-established data augmentation technique, which can extend the training distribution and regularize the neural networks by creating ''mixed'' samples based on the label-equivariance assumption, i.e., a proportional mixup of the input data results in the corresponding labels being mixed in the same proportion. However, previous mixup variants may fail to exploit the label-independent information in mixed samples during training, which usually contains richer semantic information. To further release the power of mixup, we first improve the previous label-equivariance assumption by the semantic-equivariance assumption, which states that the proportional mixup of the input data should lead to the corresponding representation being mixed in the same proportion. Then a generic mixup regularization at the representation level is proposed, which can further regularize the model with the semantic information in mixed samples. At a high level, the proposed semantic equivariant mixup (sem) encourages the structure of the input data to be preserved in the representation space, i.e., the change of input will result in the obtained representation information changing in the same way. Different from previous mixup variants, which tend to over-focus on the label-related information, the proposed method aims to preserve richer semantic information in the input with semantic-equivariance assumption, thereby improving the robustness of the model against distribution shifts. We conduct extensive empirical studies and qualitative analyzes to demonstrate the effectiveness of our proposed method. The code of the manuscript is in the supplement.", "url": "https://arxiv.org/abs/2308.06451"}, {"metadata": {"arXiv": "2308.06493", "Date": "Sat, 12 Aug 2023 07:46:50 ", "Title": "EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes", "Authors": ["Jiaxi Jiang", "Paul Streli", "Manuel Meier", "Andreas Fender", "Christian Holz"], "Categories": "cs.CV cs.AI cs.GR cs.HC", "MSC-class": "68T07, 68T45, 68U01", "ACM-class": "I.2; I.3; I.4; I.5"}, "abstract": "Full-body ego-pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representation on headset-based platforms. However, existing methods over-rely on the confines of the motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous capture of joint motions and uniform body dimensions. In this paper, we propose EgoPoser, which overcomes these limitations by 1) rethinking the input representation for headset-based ego-pose estimation and introducing a novel motion decomposition method that predicts full-body pose independent of global positions, 2) robustly modeling body pose from intermittent hand position and orientation tracking only when inside a headset's field of view, and 3) generalizing across various body sizes for different users. Our experiments show that EgoPoser outperforms state-of-the-art methods both qualitatively and quantitatively, while maintaining a high inference speed of over 600 fps. EgoPoser establishes a robust baseline for future work, where full-body pose estimation needs no longer rely on outside-in capture and can scale to large-scene environments.", "url": "https://arxiv.org/abs/2308.06493"}, {"metadata": {"arXiv": "2308.06571", "Date": "Sat, 12 Aug 2023 13:53:10 ", "Title": "ModelScope Text-to-Video Technical Report", "Authors": ["Jiuniu Wang", "Hangjie Yuan", "Dayou Chen", "Yingya Zhang", "Xiang Wang", "Shiwei Zhang"], "Categories": "cs.CV cs.AI", "Comments": ["Technical report. Project page: \\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}"]}, "abstract": "This paper introduces ModelScopeT2V, a text-to-video synthesis model that evolves from a text-to-image synthesis model (i.e., Stable Diffusion). ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame generation and smooth movement transitions. The model could adapt to varying frame numbers during training and inference, rendering it suitable for both image-text and video-text datasets. ModelScopeT2V brings together three components (i.e., VQGAN, a text encoder, and a denoising UNet), totally comprising 1.7 billion parameters, in which 0.5 billion parameters are dedicated to temporal capabilities. The model demonstrates superior performance over state-of-the-art methods across three evaluation metrics. The code and an online demo are available at \\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.", "url": "https://arxiv.org/abs/2308.06571"}, {"metadata": {"arXiv": "2308.06573", "Date": "Sat, 12 Aug 2023 14:00:09 ", "Title": "4DRVO-Net: Deep 4D Radar-Visual Odometry Using Multi-Modal and Multi-Scale Adaptive Fusion", "Authors": ["Guirong Zhuo", "Shouyi Lu", "Huanyu Zhou", "Lianqing Zheng", "Lu Xiong"], "Categories": "cs.CV cs.AI", "Comments": ["14 pages,12 figures"]}, "abstract": "Four-dimensional (4D) radar--visual odometry (4DRVO) integrates complementary information from 4D radar and cameras, making it an attractive solution for achieving accurate and robust pose estimation. However, 4DRVO may exhibit significant tracking errors owing to three main factors: 1) sparsity of 4D radar point clouds; 2) inaccurate data association and insufficient feature interaction between the 4D radar and camera; and 3) disturbances caused by dynamic objects in the environment, affecting odometry estimation. In this paper, we present 4DRVO-Net, which is a method for 4D radar--visual odometry. This method leverages the feature pyramid, pose warping, and cost volume (PWC) network architecture to progressively estimate and refine poses. Specifically, we propose a multi-scale feature extraction network called Radar-PointNet++ that fully considers rich 4D radar point information, enabling fine-grained learning for sparse 4D radar point clouds. To effectively integrate the two modalities, we design an adaptive 4D radar--camera fusion module (A-RCFM) that automatically selects image features based on 4D radar point features, facilitating multi-scale cross-modal feature interaction and adaptive multi-modal feature fusion. In addition, we introduce a velocity-guided point-confidence estimation module to measure local motion patterns, reduce the influence of dynamic objects and outliers, and provide continuous updates during pose refinement. We demonstrate the excellent performance of our method and the effectiveness of each module design on both the VoD and in-house datasets. Our method outperforms all learning-based and geometry-based methods for most sequences in the VoD dataset. Furthermore, it has exhibited promising performance that closely approaches that of the 64-line LiDAR odometry results of A-LOAM without mapping optimization.", "url": "https://arxiv.org/abs/2308.06573"}, {"metadata": {"arXiv": "2308.06665", "Date": "Sun, 13 Aug 2023 02:37:08 ", "Title": "Unsupervised Adaptation of Polyp Segmentation Models via Coarse-to-Fine Self-Supervision", "Authors": ["Jiexiang Wang", "Chaoqi Chen"], "Categories": "cs.CV cs.AI", "Comments": ["Accepted by IPMI 2023"]}, "abstract": "Unsupervised Domain Adaptation~(UDA) has attracted a surge of interest over the past decade but is difficult to be used in real-world applications. Considering the privacy-preservation issues and security concerns, in this work, we study a practical problem of Source-Free Domain Adaptation (SFDA), which eliminates the reliance on annotated source data. Current SFDA methods focus on extracting domain knowledge from the source-trained model but neglects the intrinsic structure of the target domain. Moreover, they typically utilize pseudo labels for self-training in the target domain, but suffer from the notorious error accumulation problem. To address these issues, we propose a new SFDA framework, called Region-to-Pixel Adaptation Network~(RPANet), which learns the region-level and pixel-level discriminative representations through coarse-to-fine self-supervision. The proposed RPANet consists of two modules, Foreground-aware Contrastive Learning (FCL) and Confidence-Calibrated Pseudo-Labeling (CCPL), which explicitly address the key challenges of ``how to distinguish'' and ``how to refine''. To be specific, FCL introduces a supervised contrastive learning paradigm in the region level to contrast different region centroids across different target images, which efficiently involves all pseudo labels while robust to noisy samples. CCPL designs a novel fusion strategy to reduce the overconfidence problem of pseudo labels by fusing two different target predictions without introducing any additional network modules. Extensive experiments on three cross-domain polyp segmentation tasks reveal that RPANet significantly outperforms state-of-the-art SFDA and UDA methods without access to source data, revealing the potential of SFDA in medical applications.", "url": "https://arxiv.org/abs/2308.06665"}, {"metadata": {"arXiv": "2308.06685", "Date": "Sun, 13 Aug 2023 05:18:08 ", "Title": "Video Captioning with Aggregated Features Based on Dual Graphs and Gated Fusion", "Authors": ["Yutao Jin", "Bin Liu", "Jing Wang"], "Categories": "cs.CV cs.AI"}, "abstract": "The application of video captioning models aims at translating the content of videos by using accurate natural language. Due to the complex nature inbetween object interaction in the video, the comprehensive understanding of spatio-temporal relations of objects remains a challenging task. Existing methods often fail in generating sufficient feature representations of video content. In this paper, we propose a video captioning model based on dual graphs and gated fusion: we adapt two types of graphs to generate feature representations of video content and utilize gated fusion to further understand these different levels of information. Using a dual-graphs model to generate appearance features and motion features respectively can utilize the content correlation in frames to generate various features from multiple perspectives. Among them, dual-graphs reasoning can enhance the content correlation in frame sequences to generate advanced semantic features; The gated fusion, on the other hand, aggregates the information in multiple feature representations for comprehensive video content understanding. The experiments conducted on worldly used datasets MSVD and MSR-VTT demonstrate state-of-the-art performance of our proposed approach.", "url": "https://arxiv.org/abs/2308.06685"}, {"metadata": {"arXiv": "2308.06721", "Date": "Sun, 13 Aug 2023 08:34:51 ", "Title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models", "Authors": ["Hu Ye", "Jun Zhang", "Sibo Liu", "Xiao Han", "Wei Yang"], "Categories": "cs.CV cs.AI"}, "abstract": "Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. However, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes: \"an image is worth a thousand words\". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fully fine-tuned image prompt model. As we freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation. The project page is available at \\url{https://ip-adapter.github.io}.", "url": "https://arxiv.org/abs/2308.06721"}, {"metadata": {"arXiv": "2308.06725", "Date": "Sun, 13 Aug 2023 09:05:56 ", "Title": "CLE Diffusion: Controllable Light Enhancement Diffusion Model", "Authors": ["Yuyang Yin", "Dejia Xu", "Chuangchuang Tan", "Ping Liu", "Yao Zhao", "Yunchao Wei"], "Categories": "cs.CV cs.AI cs.MM", "DOI": "10.1145/3581783.3612145"}, "abstract": "Low light enhancement has gained increasing importance with the rapid development of visual creation and editing. However, most existing enhancement algorithms are designed to homogeneously increase the brightness of images to a pre-defined extent, limiting the user experience. To address this issue, we propose Controllable Light Enhancement Diffusion Model, dubbed CLE Diffusion, a novel diffusion framework to provide users with rich controllability. Built with a conditional diffusion model, we introduce an illumination embedding to let users control their desired brightness level. Additionally, we incorporate the Segment-Anything Model (SAM) to enable user-friendly region controllability, where users can click on objects to specify the regions they wish to enhance. Extensive experiments demonstrate that CLE Diffusion achieves competitive performance regarding quantitative metrics, qualitative results, and versatile controllability. Project page: \\url{https://yuyangyin.github.io/CLEDiffusion/}", "url": "https://arxiv.org/abs/2308.06725"}, {"metadata": {"arXiv": "2308.06735", "Date": "Sun, 13 Aug 2023 09:55:04 ", "Title": "AerialVLN: Vision-and-Language Navigation for UAVs", "Authors": ["Shubo Liu and Hongsheng Zhang and Yuankai Qi and Peng Wang and Yaning Zhang and Qi Wu"], "Categories": "cs.CV cs.AI cs.RO", "Comments": ["Accepted by ICCV 2023"]}, "abstract": "Recently emerged Vision-and-Language Navigation (VLN) tasks have drawn significant attention in both computer vision and natural language processing communities. Existing VLN tasks are built for agents that navigate on the ground, either indoors or outdoors. However, many tasks require intelligent agents to carry out in the sky, such as UAV-based goods delivery, traffic/security patrol, and scenery tour, to name a few. Navigating in the sky is more complicated than on the ground because agents need to consider the flying height and more complex spatial relationship reasoning. To fill this gap and facilitate research in this field, we propose a new task named AerialVLN, which is UAV-based and towards outdoor environments. We develop a 3D simulator rendered by near-realistic pictures of 25 city-level scenarios. Our simulator supports continuous navigation, environment extension and configuration. We also proposed an extended baseline model based on the widely-used cross-modal-alignment (CMA) navigation methods. We find that there is still a significant gap between the baseline model and human performance, which suggests AerialVLN is a new challenging task. Dataset and code is available at https://github.com/AirVLN/AirVLN.", "url": "https://arxiv.org/abs/2308.06735"}, {"metadata": {"arXiv": "2308.06774", "Date": "Sun, 13 Aug 2023 14:02:27 ", "Title": "Dual Meta-Learning with Longitudinally Generalized Regularization for One-Shot Brain Tissue Segmentation Across the Human Lifespan", "Authors": ["Yongheng Sun", "Fan Wang", "Jun Shu", "Haifeng Wang", "Li Wang. Deyu Meng", "Chunfeng Lian"], "Categories": "cs.CV cs.AI", "Comments": ["ICCV 2023"]}, "abstract": "Brain tissue segmentation is essential for neuroscience and clinical studies. However, segmentation on longitudinal data is challenging due to dynamic brain changes across the lifespan. Previous researches mainly focus on self-supervision with regularizations and will lose longitudinal generalization when fine-tuning on a specific age group. In this paper, we propose a dual meta-learning paradigm to learn longitudinally consistent representations and persist when fine-tuning. Specifically, we learn a plug-and-play feature extractor to extract longitudinal-consistent anatomical representations by meta-feature learning and a well-initialized task head for fine-tuning by meta-initialization learning. Besides, two class-aware regularizations are proposed to encourage longitudinal consistency. Experimental results on the iSeg2019 and ADNI datasets demonstrate the effectiveness of our method. Our code is available at https://github.com/ladderlab-xjtu/DuMeta.", "url": "https://arxiv.org/abs/2308.06774"}, {"metadata": {"arXiv": "2308.06887", "Date": "Mon, 14 Aug 2023 01:47:26 ", "Title": "Robustified ANNs Reveal Wormholes Between Human Category Percepts", "Authors": ["Guy Gaziv", "Michael J. Lee", "James J. DiCarlo"], "Categories": "cs.CV cs.AI q-bio.NC", "Comments": ["*Equal contribution"]}, "abstract": "The visual object category reports of artificial neural networks (ANNs) are notoriously sensitive to tiny, adversarial image perturbations. Because human category reports (aka human percepts) are thought to be insensitive to those same small-norm perturbations -- and locally stable in general -- this argues that ANNs are incomplete scientific models of human visual perception. Consistent with this, we show that when small-norm image perturbations are generated by standard ANN models, human object category percepts are indeed highly stable. However, in this very same \"human-presumed-stable\" regime, we find that robustified ANNs reliably discover low-norm image perturbations that strongly disrupt human percepts. These previously undetectable human perceptual disruptions are massive in amplitude, approaching the same level of sensitivity seen in robustified ANNs. Further, we show that robustified ANNs support precise perceptual state interventions: they guide the construction of low-norm image perturbations that strongly alter human category percepts toward specific prescribed percepts. These observations suggest that for arbitrary starting points in image space, there exists a set of nearby \"wormholes\", each leading the subject from their current category perceptual state into a semantically very different state. Moreover, contemporary ANN models of biological visual processing are now accurate enough to consistently guide us to those portals.", "url": "https://arxiv.org/abs/2308.06887"}, {"metadata": {"arXiv": "2308.06909", "Date": "Mon, 14 Aug 2023 03:11:17 ", "Title": "Hierarchy Flow For High-Fidelity Image-to-Image Translation", "Authors": ["Weichen Fan", "Jinghuan Chen", "Ziwei Liu"], "Categories": "cs.CV cs.AI", "Comments": ["arXiv admin note: text overlap with arXiv:2207.01909"]}, "abstract": "Image-to-image (I2I) translation comprises a wide spectrum of tasks. Here we divide this problem into three levels: strong-fidelity translation, normal-fidelity translation, and weak-fidelity translation, indicating the extent to which the content of the original image is preserved. Although existing methods achieve good performance in weak-fidelity translation, they fail to fully preserve the content in both strong- and normal-fidelity tasks, e.g. sim2real, style transfer and low-level vision. In this work, we propose Hierarchy Flow, a novel flow-based model to achieve better content preservation during translation. Specifically, 1) we first unveil the drawbacks of standard flow-based models when applied to I2I translation. 2) Next, we propose a new design, namely hierarchical coupling for reversible feature transformation and multi-scale modeling, to constitute Hierarchy Flow. 3) Finally, we present a dedicated aligned-style loss for a better trade-off between content preservation and stylization during translation. Extensive experiments on a wide range of I2I translation benchmarks demonstrate that our approach achieves state-of-the-art performance, with convincing advantages in both strong- and normal-fidelity tasks. Code and models will be at https://github.com/WeichenFan/HierarchyFlow.", "url": "https://arxiv.org/abs/2308.06909"}, {"metadata": {"arXiv": "2308.07039", "Date": "Mon, 14 Aug 2023 10:02:30 ", "Title": "The minimal computational substrate of fluid intelligence", "Authors": ["Amy PK Nelson", "Joe Mole", "Guilherme Pombo", "Robert J Gray", "James K Ruffle", "Edgar Chan", "Geraint E Rees", "Lisa Cipolotti", "Parashkev Nachev"], "Categories": "cs.CV cs.AI q-bio.NC", "Comments": ["26 pages", "5 figures"]}, "abstract": "The quantification of cognitive powers rests on identifying a behavioural task that depends on them. Such dependence cannot be assured, for the powers a task invokes cannot be experimentally controlled or constrained a priori, resulting in unknown vulnerability to failure of specificity and generalisability. Evaluating a compact version of Raven's Advanced Progressive Matrices (RAPM), a widely used clinical test of fluid intelligence, we show that LaMa, a self-supervised artificial neural network trained solely on the completion of partially masked images of natural environmental scenes, achieves human-level test scores a prima vista, without any task-specific inductive bias or training. Compared with cohorts of healthy and focally lesioned participants, LaMa exhibits human-like variation with item difficulty, and produces errors characteristic of right frontal lobe damage under degradation of its ability to integrate global spatial patterns. LaMa's narrow training and limited capacity -- comparable to the nervous system of the fruit fly -- suggest RAPM may be open to computationally simple solutions that need not necessarily invoke abstract reasoning.", "url": "https://arxiv.org/abs/2308.07039"}, {"metadata": {"arXiv": "2308.07146", "Date": "Mon, 14 Aug 2023 13:53:18 ", "Title": "CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation", "Authors": ["Hongguang Zhu", "Yunchao Wei", "Xiaodan Liang", "Chunjie Zhang", "Yao Zhao"], "Categories": "cs.CV cs.AI cs.MM", "Comments": ["Accepted by ICCV 2023. Code: https://github.com/KevinLight831/CTP"]}, "abstract": "Vision-Language Pretraining (VLP) has shown impressive results on diverse downstream tasks by offline training on large-scale datasets. Regarding the growing nature of real-world data, such an offline training paradigm on ever-expanding data is unsustainable, because models lack the continual learning ability to accumulate knowledge constantly. However, most continual learning studies are limited to uni-modal classification and existing multi-modal datasets cannot simulate continual non-stationary data stream scenarios. To support the study of Vision-Language Continual Pretraining (VLCP), we first contribute a comprehensive and unified benchmark dataset P9D which contains over one million product image-text pairs from 9 industries. The data from each industry as an independent task supports continual learning and conforms to the real-world long-tail nature to simulate pretraining on web data. We comprehensively study the characteristics and challenges of VLCP, and propose a new algorithm: Compatible momentum contrast with Topology Preservation, dubbed CTP. The compatible momentum model absorbs the knowledge of the current and previous-task models to flexibly update the modal feature. Moreover, Topology Preservation transfers the knowledge of embedding across tasks while preserving the flexibility of feature adjustment. The experimental results demonstrate our method not only achieves superior performance compared with other baselines but also does not bring an expensive training burden. Dataset and codes are available at https://github.com/KevinLight831/CTP.", "url": "https://arxiv.org/abs/2308.07146"}, {"metadata": {"arXiv": "2308.06810", "Date": "Sun, 13 Aug 2023 16:52:36 ", "Title": "Ground Manipulator Primitive Tasks to Executable Actions using Large Language Models", "Authors": ["Yue Cao and C.S. George Lee"], "Categories": "cs.RO cs.AI", "Comments": ["The abstract of this paper is accepted as a late-breaking result poster in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)"]}, "abstract": "Layered architectures have been widely used in robot systems. The majority of them implement planning and execution functions in separate layers. However, there still lacks a straightforward way to transit high-level tasks in the planning layer to the low-level motor commands in the execution layer. In order to tackle this challenge, we propose a novel approach to ground the manipulator primitive tasks to robot low-level actions using large language models (LLMs). We designed a program-like prompt based on the task frame formalism. In this way, we enable LLMs to generate position/force set-points for hybrid control. Evaluations over several state-of-the-art LLMs are provided.", "url": "https://arxiv.org/abs/2308.06810"}, {"metadata": {"arXiv": "2308.06905", "Date": "Mon, 14 Aug 2023 02:53:20 ", "Title": "The Michigan Robotics Undergraduate Curriculum: Defining the Discipline of Robotics for Equity and Excellence", "Authors": ["Odest Chadwicke Jenkins", "Jessy Grizzle", "Ella Atkins", "Leia Stirling", "Elliott Rouse", "Mark Guzdial", "Damen Provost", "Kimberly Mann", "and Joanna Millunchick"], "Categories": "cs.RO cs.AI cs.CV cs.HC cs.SY eess.SY", "Comments": ["49 pages", "approximately 25 figures"], "MSC-class": "97", "ACM-class": "I.2.9"}, "abstract": "The Robotics Major at the University of Michigan was successfully launched in the 2022-23 academic year as an innovative step forward to better serve students, our communities, and our society. Building on our guiding principle of \"Robotics with Respect\" and our larger Robotics Pathways model, the Michigan Robotics Major was designed to define robotics as a true academic discipline with both equity and excellence as our highest priorities. Understanding that talent is equally distributed but opportunity is not, the Michigan Robotics Major has embraced an adaptable curriculum that is accessible through a diversity of student pathways and enables successful and sustained career-long participation in robotics, AI, and automation professions. The results after our planning efforts (2019-22) and first academic year (2022-23) have been highly encouraging: more than 100 students declared Robotics as their major, completion of the Robotics major by our first two graduates, soaring enrollments in our Robotics classes, thriving partnerships with Historically Black Colleges and Universities. This document provides our original curricular proposal for the Robotics Undergraduate Program at the University of Michigan, submitted to the Michigan Association of State Universities in April 2022 and approved in June 2022. The dissemination of our program design is in the spirit of continued growth for higher education towards realizing equity and excellence. The most recent version of this document is also available on Google Docs through this link: https://ocj.me/robotics_major", "url": "https://arxiv.org/abs/2308.06905"}, {"metadata": {"arXiv": "2308.06931", "Date": "Mon, 14 Aug 2023 04:18:07 ", "Title": "FusionPlanner: A Multi-task Motion Planner for Mining Trucks using Multi-sensor Fusion Method", "Authors": ["Siyu Teng", "Luxi Li", "Yuchen Li", "Xuemin Hu", "Lingxi Li", "Yunfeng Ai", "Long Chen"], "Categories": "cs.RO cs.AI", "Comments": ["2Pages", "10 figures"]}, "abstract": "In recent years, significant achievements have been made in motion planning for intelligent vehicles. However, as a typical unstructured environment, open-pit mining attracts limited attention due to its complex operational conditions and adverse environmental factors. A comprehensive paradigm for unmanned transportation in open-pit mines is proposed in this research, including a simulation platform, a testing benchmark, and a trustworthy and robust motion planner. \\textcolor{red}{Firstly, we propose a multi-task motion planning algorithm, called FusionPlanner, for autonomous mining trucks by the Multi-sensor fusion method to adapt both lateral and longitudinal control tasks for unmanned transportation. Then, we develop a novel benchmark called MiningNav, which offers three validation approaches to evaluate the trustworthiness and robustness of well-trained algorithms in transportation roads of open-pit mines. Finally, we introduce the Parallel Mining Simulator (PMS), a new high-fidelity simulator specifically designed for open-pit mining scenarios. PMS enables the users to manage and control open-pit mine transportation from both the single-truck control and multi-truck scheduling perspectives.} \\textcolor{red}{The performance of FusionPlanner is tested by MiningNav in PMS, and the empirical results demonstrate a significant reduction in the number of collisions and takeovers of our planner. We anticipate our unmanned transportation paradigm will bring mining trucks one step closer to trustworthiness and robustness in continuous round-the-clock unmanned transportation.", "url": "https://arxiv.org/abs/2308.06931"}, {"metadata": {"arXiv": "2308.07241", "Date": "Mon, 14 Aug 2023 16:23:21 ", "Title": "Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents", "Authors": ["Byeonghwi Kim", "Jinyeon Kim", "Yuyeong Kim", "Cheolhong Min", "Jonghyun Choi"], "Categories": "cs.RO cs.AI", "Comments": ["ICCV 2023"]}, "abstract": "Accomplishing household tasks such as 'bringing a cup of water' requires planning step-by-step actions by maintaining knowledge about the spatial arrangement of objects and the consequences of previous actions. Perception models of the current embodied AI agents, however, often make mistakes due to a lack of such knowledge but rely on imperfect learning of imitating agents or an algorithmic planner without knowledge about the changed environment by the previous actions. To address the issue, we propose CPEM (Context-aware Planner and Environment-aware Memory) to incorporate the contextual information of previous actions for planning and maintaining spatial arrangement of objects with their states (e.g., if an object has been moved or not) in an environment to the perception model for improving both visual navigation and object interaction. We observe that CPEM achieves state-of-the-art task success performance in various metrics using a challenging interactive instruction following benchmark both in seen and unseen environments by large margins (up to +10.70% in unseen env.). CPEM with the templated actions, named ECLAIR, also won the 1st generalist language grounding agents challenge at Embodied AI Workshop in CVPR'23.", "url": "https://arxiv.org/abs/2308.07241"}, {"metadata": {"arXiv": "2308.07283", "Date": "Mon, 14 Aug 2023 17:14:58 ", "Title": "Autonomous Point Cloud Segmentation for Power Lines Inspection in Smart Grid", "Authors": ["Alexander Kyuroson", "Anton Koval and George Nikolakopoulos"], "Categories": "cs.RO cs.AI", "Comments": ["Accepted in the 22nd World Congress of the International Federation of Automatic Control [IFAC WC 2023]"]}, "abstract": "LiDAR is currently one of the most utilized sensors to effectively monitor the status of power lines and facilitate the inspection of remote power distribution networks and related infrastructures. To ensure the safe operation of the smart grid, various remote data acquisition strategies, such as Airborne Laser Scanning (ALS), Mobile Laser Scanning (MLS), and Terrestrial Laser Scanning (TSL) have been leveraged to allow continuous monitoring of regional power networks, which are typically surrounded by dense vegetation. In this article, an unsupervised Machine Learning (ML) framework is proposed, to detect, extract and analyze the characteristics of power lines of both high and low voltage, as well as the surrounding vegetation in a Power Line Corridor (PLC) solely from LiDAR data. Initially, the proposed approach eliminates the ground points from higher elevation points based on statistical analysis that applies density criteria and histogram thresholding. After denoising and transforming of the remaining candidate points by applying Principle Component Analysis (PCA) and Kd-tree, power line segmentation is achieved by utilizing a two-stage DBSCAN clustering to identify each power line individually. Finally, all high elevation points in the PLC are identified based on their distance to the newly segmented power lines. Conducted experiments illustrate that the proposed framework is an agnostic method that can efficiently detect the power lines and perform PLC-based hazard analysis.", "url": "https://arxiv.org/abs/2308.07283"}, {"metadata": {"arXiv": "2308.06378", "Date": "Fri, 11 Aug 2023 20:32:39 ", "Title": "DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System", "Authors": ["Mojtaba Yeganejou", "Kimia Honari", "Ryan Kluzinski", "Scott Dick", "Michael Lipsett", "James Miller"], "Categories": "cs.AI cs.CV cs.LG"}, "abstract": "A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.", "url": "https://arxiv.org/abs/2308.06378"}, {"metadata": {"arXiv": "2308.06528", "Date": "Sat, 12 Aug 2023 11:02:21 ", "Title": "Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices", "Authors": ["Jakub Kwiatkowski and Krzysztof Krawiec"], "Categories": "cs.AI cs.CV cs.LG", "Comments": ["12 pages", "3 figures"], "MSC-class": "68T20 (Primary) 68T05 (Secondary)", "ACM-class": "I.2.8; I.2.6; I.2.10"}, "abstract": "One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.", "url": "https://arxiv.org/abs/2308.06528"}, {"metadata": {"arXiv": "2308.06961", "Date": "Mon, 14 Aug 2023 06:32:52 ", "Title": "Graph Structural Residuals: A Learning Approach to Diagnosis", "Authors": ["Jan Lukas Augustin and Oliver Niggemann"], "Categories": "cs.AI cs.LG"}, "abstract": "Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.", "url": "https://arxiv.org/abs/2308.06961"}, {"metadata": {"arXiv": "2308.06271", "Date": "Thu, 27 Jul 2023 20:18:11 ", "Title": "Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds", "Authors": ["Owen Melia", "Eric Jonas", "and Rebecca Willett"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi & Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. We show through experiments that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods.", "url": "https://arxiv.org/abs/2308.06271"}, {"metadata": {"arXiv": "2308.06299", "Date": "Fri, 11 Aug 2023 07:45:36 ", "Title": "Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment", "Authors": ["Hendrik Vogt", "Stefan Buehler", "Mark Schutera"], "Categories": "cs.CV cs.AI cs.LG cs.RO cs.SY eess.SY"}, "abstract": "In this paper, we propose a method for addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving. Our approach is based on the idea that deep learning-based perception for autonomous driving is uncertain and best represented as a probability distribution. As autonomous vehicles' safety is paramount, it is crucial for perception systems to recognize when the vehicle is leaving its operational design domain, anticipate hazardous uncertainty, and reduce the performance of the perception system. To address this, we propose to encapsulate the neural network under deployment within an uncertainty estimation envelope that is based on the epistemic uncertainty estimation through the Monte Carlo Dropout approach. This approach does not require modification of the deployed neural network and guarantees expected model performance. Our defensive perception envelope has the capability to estimate a neural network's performance, enabling monitoring and notification of entering domains of reduced neural network performance under deployment. Furthermore, our envelope is extended by novel methods to improve the application in deployment settings, including reducing compute expenses and confining estimation noise. Finally, we demonstrate the applicability of our method for multiple different potential deployment shifts relevant to autonomous driving, such as transitions into the night, rainy, or snowy domain. Overall, our approach shows great potential for application in deployment settings and enables operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.", "url": "https://arxiv.org/abs/2308.06299"}, {"metadata": {"arXiv": "2308.06534", "Date": "Sat, 12 Aug 2023 11:31:01 ", "Title": "Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models", "Authors": ["Daniel Wolf", "Tristan Payer", "Catharina Silvia Lisson", "Christoph Gerhard Lisson", "Meinrad Beer", "Timo Ropinski", "Michael G\\\"otz"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["This paper is under review. The code will be released if accepted"]}, "abstract": "Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task\". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares state-of-the-art contrastive learning methods with the recently introduced masked autoencoder approach \"SparK\" for convolutional neural networks (CNNs) on medical images. Therefore we pre-train on a large unannotated CT image dataset and fine-tune on several downstream CT classification tasks. Due to the challenge of obtaining sufficient annotated training data in the medical imaging domain, it is of particular interest to evaluate how the self-supervised pre-training methods perform on small downstream datasets. By experimenting with gradually reducing the training dataset size of our downstream tasks, we find that the reduction has different effects depending on the type of pre-training chosen. The SparK pre-training method is more robust to the training dataset size than the contrastive methods. Based on our results, we propose the SparK pre-training for medical downstream tasks with small datasets.", "url": "https://arxiv.org/abs/2308.06534"}, {"metadata": {"arXiv": "2308.06612", "Date": "Sat, 12 Aug 2023 17:06:48 ", "Title": "On the Interplay of Convolutional Padding and Adversarial Robustness", "Authors": ["Paul Gavrikov and Janis Keuper"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Accepted as full paper at ICCV-W 2023 BRAVO"]}, "abstract": "It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.", "url": "https://arxiv.org/abs/2308.06612"}, {"metadata": {"arXiv": "2308.06701", "Date": "Sun, 13 Aug 2023 06:55:05 ", "Title": "Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection", "Authors": ["Haichao Zhang", "Can Qin", "Yu Yin", "Yun Fu"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-and-play data generation and augmentation module for existing camouflaged object detection tasks and provides a novel way to introduce more diversity and distributions into current camouflage datasets.", "url": "https://arxiv.org/abs/2308.06701"}, {"metadata": {"arXiv": "2308.06983", "Date": "Mon, 14 Aug 2023 07:35:43 ", "Title": "pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems", "Authors": ["Momojit Biswas", "Himanshu Buckchash", "Dilip K. Prasad"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["15 pages", "5 figures"]}, "abstract": "Nearest neighbor (NN) sampling provides more semantic variations than pre-defined transformations for self-supervised learning (SSL) based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline (pNNCLR) to the nearest neighbor based SSL approach (NNCLR). To this end, we introduce pseudo nearest neighbors (pNN) to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.", "url": "https://arxiv.org/abs/2308.06983"}, {"metadata": {"arXiv": "2308.07052", "Date": "Mon, 14 Aug 2023 10:23:25 ", "Title": "Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach -- A Review", "Authors": ["Hrishabh Tiwari", "Jatin Moolchandani", "Shamla Mantri"], "Categories": "cs.CV cs.AI cs.CY cs.LG"}, "abstract": "The morbidity of scalp diseases is minuscule compared to other diseases, but the impact on the patient's life is enormous. It is common for people to experience scalp problems that include Dandruff, Psoriasis, Tinea-Capitis, Alopecia and Atopic-Dermatitis. In accordance with WHO research, approximately 70% of adults have problems with their scalp. It has been demonstrated in descriptive research that hair quality is impaired by impaired scalp, but these impacts are reversible with early diagnosis and treatment. Deep Learning advances have demonstrated the effectiveness of CNN paired with FCN in diagnosing scalp and skin disorders. In one proposed Deep-Learning-based scalp inspection and diagnosis system, an imaging microscope and a trained model are combined with an app that classifies scalp disorders accurately with an average precision of 97.41%- 99.09%. Another research dealt with classifying the Psoriasis using the CNN with an accuracy of 82.9%. As part of another study, an ML based algorithm was also employed. It accurately classified the healthy scalp and alopecia areata with 91.4% and 88.9% accuracy with SVM and KNN algorithms. Using deep learning models to diagnose scalp related diseases has improved due to advancements i computation capabilities and computer vision, but there remains a wide horizon for further improvements.", "url": "https://arxiv.org/abs/2308.07052"}, {"metadata": {"arXiv": "2308.06422", "Date": "Sat, 12 Aug 2023 00:16:51 ", "Title": "Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation", "Authors": ["Seyedarmin Azizi", "Mahdi Nazemi", "Arash Fayyazi", "Massoud Pedram"], "Categories": "cs.LG cs.AI cs.NE stat.ML"}, "abstract": "As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 20% decrease in model size without compromising accuracy. Additionally, our method boasts a 12x reduction in search time relative to the best search-focused strategies currently available. As a result, our proposed method represents a leap forward in neural network design optimization, paving the way for quick model design and implementation in settings with limited resources, thereby propelling the potential of scalable deep learning solutions.", "url": "https://arxiv.org/abs/2308.06422"}, {"metadata": {"arXiv": "2308.06447", "Date": "Sat, 12 Aug 2023 02:46:54 ", "Title": "A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing", "Authors": ["Milad Ramezankhani and Abbas S. Milani"], "Categories": "cs.LG cs.AI"}, "abstract": "Physics-Informed Neural Networks (PINNs) have gained popularity in solving nonlinear partial differential equations (PDEs) via integrating physical laws into the training of neural networks, making them superior in many scientific and engineering applications. However, conventional PINNs still fall short in accurately approximating the solution of complex systems with strong nonlinearity, especially in long temporal domains. Besides, since PINNs are designed to approximate a specific realization of a given PDE system, they lack the necessary generalizability to efficiently adapt to new system configurations. This entails computationally expensive re-training from scratch for any new change in the system. To address these shortfalls, in this work a novel sequential meta-transfer (SMT) learning framework is proposed, offering a unified solution for both fast training and efficient adaptation of PINNs in highly nonlinear systems with long temporal domains. Specifically, the framework decomposes PDE's time domain into smaller time segments to create \"easier\" PDE problems for PINNs training. Then for each time interval, a meta-learner is assigned and trained to achieve an optimal initial state for rapid adaptation to a range of related tasks. Transfer learning principles are then leveraged across time intervals to further reduce the computational cost.Through a composites autoclave processing case study, it is shown that SMT is clearly able to enhance the adaptability of PINNs while significantly reducing computational cost, by a factor of 100.", "url": "https://arxiv.org/abs/2308.06447"}, {"metadata": {"arXiv": "2308.06453", "Date": "Sat, 12 Aug 2023 03:19:08 ", "Title": "Multi-Label Knowledge Distillation", "Authors": ["Penghui Yang", "Ming-Kun Xie", "Chen-Chen Zong", "Lei Feng", "Gang Niu", "Masashi Sugiyama", "Sheng-Jun Huang"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Accepted by ICCV 2023. The first two authors contributed equally to this work"]}, "abstract": "Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, thus achieving superior performance against diverse comparing methods. Our code is available at: https://github.com/penghui-yang/L2D", "url": "https://arxiv.org/abs/2308.06453"}, {"metadata": {"arXiv": "2308.06467", "Date": "Sat, 12 Aug 2023 05:21:34 ", "Title": "Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks", "Authors": ["Roman Garaev", "Bader Rasheed and Adil Khan"], "Categories": "cs.LG cs.AI", "Comments": ["16 pages", "5 figures"], "ACM-class": "I.2.10"}, "abstract": "Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the impact of adversarial attack norms on DNN representations, focusing on samples subjected to $L_2$ and $L_{\\infty}$ norm attacks. Further, we employ canonical correlation analysis, visualize the representations, and calculate the mean distance between these representations and various DNN decision boundaries. Our results reveal a significant difference between $L_2$ and $L_{\\infty}$ norms, which could provide insights into the potential dangers posed by $L_{\\infty}$ norm attacks, previously underestimated by the research community.", "url": "https://arxiv.org/abs/2308.06467"}, {"metadata": {"arXiv": "2308.06522", "Date": "Sat, 12 Aug 2023 10:33:57 ", "Title": "SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models", "Authors": ["Sara Babakniya", "Ahmed Roushdy Elkordy", "Yahya H. Ezzeldin", "Qingfeng Liu", "Kee-Bong Song", "Mostafa El-Khamy", "Salman Avestimehr"], "Categories": "cs.LG cs.AI"}, "abstract": "Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data scenarios through a novel data-driven initialization technique. Our experimental results demonstrate that SLoRA achieves performance comparable to full fine-tuning, with significant sparse updates with approximately $\\sim 1\\%$ density while reducing training time by up to $90\\%$.", "url": "https://arxiv.org/abs/2308.06522"}, {"metadata": {"arXiv": "2308.06545", "Date": "Sat, 12 Aug 2023 12:03:31 ", "Title": "Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters", "Authors": ["Chukwuma Okolie", "Jon Mills", "Adedayo Adeleke", "Julian Smit"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["8 pages"]}, "abstract": "The accuracy of digital elevation models (DEMs) in urban areas is influenced by numerous factors including land cover and terrain irregularities. Moreover, building artifacts in global DEMs cause artificial blocking of surface flow pathways. This compromises their quality and adequacy for hydrological and environmental modelling in urban landscapes where precise and accurate terrain information is needed. In this study, the extreme gradient boosting (XGBoost) ensemble algorithm is adopted for enhancing the accuracy of two medium-resolution 30m DEMs over Cape Town, South Africa: Copernicus GLO-30 and ALOS World 3D (AW3D). XGBoost is a scalable, portable and versatile gradient boosting library that can solve many environmental modelling problems. The training datasets are comprised of eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, topographic position index, terrain ruggedness index, terrain surface texture, vector roughness measure, forest cover and bare ground cover. The target variable (elevation error) was calculated with respect to highly accurate airborne LiDAR. After training and testing, the model was applied for correcting the DEMs at two implementation sites. The correction achieved significant accuracy gains which are competitive with other proposed methods. The root mean square error (RMSE) of Copernicus DEM improved by 46 to 53% while the RMSE of AW3D DEM improved by 72 to 73%. These results showcase the potential of gradient boosted trees for enhancing the quality of DEMs, and for improved hydrological modelling in urban catchments.", "url": "https://arxiv.org/abs/2308.06545"}, {"metadata": {"arXiv": "2308.06585", "Date": "Sat, 12 Aug 2023 14:47:21 ", "Title": "Approximate Answering of Graph Queries", "Authors": ["Michael Cochez", "Dimitrios Alivanistos", "Erik Arakelyan", "Max Berrendorf", "Daniel Daza", "Mikhail Galkin", "Pasquale Minervini", "Mathias Niepert", "Hongyu Ren"], "Categories": "cs.LG cs.AI cs.DB cs.LO cs.NE", "Comments": ["Preprint of Ch. 17 \"Approximate Answering of Graph Queries\" in \"Compendium of Neurosymbolic Artificial Intelligence\"", "https://ebooks.iospress.nl/ISBN/978-1-64368-406-2"]}, "abstract": "Knowledge graphs (KGs) are inherently incomplete because of incomplete world knowledge and bias in what is the input to the KG. Additionally, world knowledge constantly expands and evolves, making existing facts deprecated or introducing new ones. However, we would still want to be able to answer queries as if the graph were complete. In this chapter, we will give an overview of several methods which have been proposed to answer queries in such a setting. We will first provide an overview of the different query types which can be supported by these methods and datasets typically used for evaluation, as well as an insight into their limitations. Then, we give an overview of the different approaches and describe them in terms of expressiveness, supported graph types, and inference capabilities.", "url": "https://arxiv.org/abs/2308.06585"}, {"metadata": {"arXiv": "2308.06590", "Date": "Sat, 12 Aug 2023 14:59:19 ", "Title": "Value-Distributional Model-Based Reinforcement Learning", "Authors": ["Carlos E. Luis", "Alessandro G. Bottero", "Julia Vinogradska", "Felix Berkenkamp", "Jan Peters"], "Categories": "cs.LG cs.AI"}, "abstract": "Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.", "url": "https://arxiv.org/abs/2308.06590"}, {"metadata": {"arXiv": "2308.06619", "Date": "Sat, 12 Aug 2023 17:27:49 ", "Title": "Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?", "Authors": ["Liao Zhu", "Victor Qu\\'etu", "Van-Tam Nguyen", "Enzo Tartaglione"], "Categories": "cs.LG cs.AI"}, "abstract": "Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relationship between entropy, pruning techniques, and deep learning performance. The EGP algorithm and its insights hold great promise for advancing the field of network compression and optimization. The source code for EGP is released open-source.", "url": "https://arxiv.org/abs/2308.06619"}, {"metadata": {"arXiv": "2308.06644", "Date": "Sat, 12 Aug 2023 21:25:24 ", "Title": "Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation", "Authors": ["Junwei Huang", "Zhiqing Sun", "Yiming Yang"], "Categories": "cs.LG cs.AI"}, "abstract": "Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.", "url": "https://arxiv.org/abs/2308.06644"}, {"metadata": {"arXiv": "2308.06663", "Date": "Sun, 13 Aug 2023 02:17:19 ", "Title": "ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN", "Authors": ["Md Abul Bashar", "Richi Nayak"], "Categories": "cs.LG cs.AI"}, "abstract": "Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.", "url": "https://arxiv.org/abs/2308.06663"}, {"metadata": {"arXiv": "2308.06671", "Date": "Sun, 13 Aug 2023 03:13:03 ", "Title": "Law of Balance and Stationary Distribution of Stochastic Gradient Descent", "Authors": ["Liu Ziyin", "Hongchao Li", "Masahito Ueda"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["Preprint"]}, "abstract": "The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.", "url": "https://arxiv.org/abs/2308.06671"}, {"metadata": {"arXiv": "2308.06714", "Date": "Sun, 13 Aug 2023 08:10:23 ", "Title": "Learning on Graphs with Out-of-Distribution Nodes", "Authors": ["Yu Song and Donglin Wang"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted by KDD'22"], "DOI": "10.1145/3534678.3539457"}, "abstract": "Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT), a novel GNN model which explicitly models the interaction between different kinds of nodes and separate inliers from outliers during feature propagation. Extensive experiments show that OODGAT outperforms existing outlier detection methods by a large margin, while being better or comparable in terms of in-distribution classification.", "url": "https://arxiv.org/abs/2308.06714"}, {"metadata": {"arXiv": "2308.06717", "Date": "Sun, 13 Aug 2023 08:12:01 ", "Title": "Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards", "Authors": ["Ilgin Dogan", "Zuo-Jun Max Shen", "Anil Aswani"], "Categories": "cs.LG cs.AI cs.GT stat.ML", "Comments": ["72 pages", "6 figures. arXiv admin note: text overlap with arXiv:2304.07407"]}, "abstract": "In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-off between consistently estimating the agent's unknown rewards and maximizing their own utility by offering adaptive incentives to lead the agent. For a non-parametric model, we introduce an estimator whose only input is the history of principal's incentives and agent's choices. We unite this estimator with a proposed data-driven incentive policy within a MAB framework. Without restricting the type of the agent's algorithm, we prove finite-sample consistency of the estimator and a rigorous regret bound for the principal by considering the sequential externality imposed by the agent. Lastly, our theoretical results are reinforced by simulations justifying applicability of our framework to green energy aggregator contracts.", "url": "https://arxiv.org/abs/2308.06717"}, {"metadata": {"arXiv": "2308.06718", "Date": "Sun, 13 Aug 2023 08:13:34 ", "Title": "Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables", "Authors": ["Feng Xie", "Biwei Huang", "Zhengming Chen", "Ruichu Cai", "Clark Glymour", "Zhi Geng", "and Kun Zhang"], "Categories": "cs.LG cs.AI stat.ME"}, "abstract": "We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\\bf{Y}$ and $\\bf{Z}$, GIN holds if and only if $\\omega^{\\intercal}\\mathbf{Y}$ and $\\mathbf{Z}$ are independent, where $\\omega$ is a non-zero parameter vector determined by the cross-covariance between $\\mathbf{Y}$ and $\\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous set $\\mathcal{S}$ relative to the parent set of $\\mathbf{Y}$ (w.r.t. the causal ordering), such that $\\mathcal{S}$ d-separates $\\mathbf{Y}$ from $\\mathbf{Z}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the underlying causal structure of a LiNGLaH is identifiable in light of GIN conditions under mild assumptions. Experimental results show the effectiveness of the proposed approach.", "url": "https://arxiv.org/abs/2308.06718"}, {"metadata": {"arXiv": "2308.06733", "Date": "Sun, 13 Aug 2023 09:51:16 ", "Title": "Precipitation nowcasting with generative diffusion models", "Authors": ["Andrea Asperti", "Fabio Merizzi", "Alberto Paparella", "Giorgio Pedrazzi", "Matteo Angelinelli and Stefano Colamonaco"], "Categories": "cs.LG cs.AI physics.ao-ph", "Comments": ["21 pages", "6 figures"]}, "abstract": "In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely prediction. In our study, we focus on a specific subset of the ERA-5 dataset, which includes hourly data pertaining to Central Europe from the years 2016 to 2021. Within this context, we examine the efficacy of diffusion models in handling the task of precipitation nowcasting. Our work is conducted in comparison to the performance of well-established U-Net models, as documented in the existing literature. Our proposed approach of Generative Ensemble Diffusion (GED) utilizes a diffusion model to generate a set of possible weather scenarios which are then amalgamated into a probable prediction via the use of a post-processing network. This approach, in comparison to recent deep learning models, substantially outperformed them in terms of overall performance.", "url": "https://arxiv.org/abs/2308.06733"}, {"metadata": {"arXiv": "2308.06738", "Date": "Sun, 13 Aug 2023 10:04:13 ", "Title": "Probabilistic Imputation for Time-series Classification with Missing Data", "Authors": ["SeungHyun Kim", "Hyunsu Kim", "EungGu Yun", "Hwangrae Lee", "Jaehun Lee", "Juho Lee"], "Categories": "cs.LG cs.AI"}, "abstract": "Multivariate time series data for real-world applications typically contain a significant amount of missing values. The dominant approach for classification with such missing values is to impute them heuristically with specific values (zero, mean, values of adjacent time-steps) or learnable parameters. However, these simple strategies do not take the data generative process into account, and more importantly, do not effectively capture the uncertainty in prediction due to the multiple possibilities for the missing values. In this paper, we propose a novel probabilistic framework for classification with multivariate time series data with missing values. Our model consists of two parts; a deep generative model for missing value imputation and a classifier. Extending the existing deep generative models to better capture structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier part takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations. Importantly, we show that na\\\"ively combining the generative model and the classifier could result in trivial solutions where the generative model does not produce meaningful imputations. To resolve this, we present a novel regularization technique that can promote the model to produce useful imputation values that help classification. Through extensive experiments on real-world time series data with missing values, we demonstrate the effectiveness of our method.", "url": "https://arxiv.org/abs/2308.06738"}, {"metadata": {"arXiv": "2308.06741", "Date": "Sun, 13 Aug 2023 10:18:10 ", "Title": "Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization", "Authors": ["Mohammad Mehdi Nasiri", "Mansoor Rezghi"], "Categories": "cs.LG cs.AI cs.MA", "MSC-class": "68T", "ACM-class": "I.2"}, "abstract": "This paper presents an extension of the Mirror Descent method to overcome challenges in cooperative Multi-Agent Reinforcement Learning (MARL) settings, where agents have varying abilities and individual policies. The proposed Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO) algorithm utilizes the multi-agent advantage decomposition lemma to enable efficient policy updates for each agent while ensuring overall performance improvements. By iteratively updating agent policies through an approximate solution of the trust-region problem, HAMDPO guarantees stability and improves performance. Moreover, the HAMDPO algorithm is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. We evaluate HAMDPO on Multi-Agent MuJoCo and StarCraftII tasks, demonstrating its superiority over state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.", "url": "https://arxiv.org/abs/2308.06741"}, {"metadata": {"arXiv": "2308.06764", "Date": "Sun, 13 Aug 2023 13:01:21 ", "Title": "Few-shot Class-incremental Learning: A Survey", "Authors": ["Jinghua Zhang and Li Liu and Olli Silven and Matti Pietik\\\"ainen and Dewen Hu"], "Categories": "cs.LG cs.AI"}, "abstract": "Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in machine learning, as it necessitates the continuous learning of new classes from sparse labeled training samples without forgetting previous knowledge. While this field has seen recent progress, it remains an active area of exploration. This paper aims to provide a comprehensive and systematic review of FSCIL. In our in-depth examination, we delve into various facets of FSCIL, encompassing the problem definition, the discussion of primary challenges of unreliable empirical risk minimization and the stability-plasticity dilemma, general schemes, and relevant problems of incremental learning and few-shot learning. Besides, we offer an overview of benchmark datasets and evaluation metrics. Furthermore, we introduce the classification methods in FSCIL from data-based, structure-based, and optimization-based approaches and the object detection methods in FSCIL from anchor-free and anchor-based approaches. Beyond these, we illuminate several promising research directions within FSCIL that merit further investigation.", "url": "https://arxiv.org/abs/2308.06764"}, {"metadata": {"arXiv": "2308.06801", "Date": "Sun, 13 Aug 2023 16:04:03 ", "Title": "SAILOR: Structural Augmentation Based Tail Node Representation Learning", "Authors": ["Jie Liao", "Jintang Li", "Liang Chen", "Bingzhe Wu", "Yatao Bian", "Zibin Zheng"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted by CIKM 2023; Code is available at https://github.com/Jie-Re/SAILO"]}, "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in representation learning for graphs recently. However, the effectiveness of GNNs, which capitalize on the key operation of message propagation, highly depends on the quality of the topology structure. Most of the graphs in real-world scenarios follow a long-tailed distribution on their node degrees, that is, a vast majority of the nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes since they lack structural information. In the pursuit of promoting the expressiveness of GNNs for tail nodes, we explore how the deficiency of structural information deteriorates the performance of tail nodes and propose a general Structural Augmentation based taIL nOde Representation learning framework, dubbed as SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on public benchmark datasets demonstrate that SAILOR can significantly improve the tail node representations and outperform the state-of-the-art baselines.", "url": "https://arxiv.org/abs/2308.06801"}, {"metadata": {"arXiv": "2308.06822", "Date": "Sun, 13 Aug 2023 17:40:56 ", "Title": "Approximate and Weighted Data Reconstruction Attack in Federated Learning", "Authors": ["Ziqi Wang", "Yongcun Song", "Enrique Zuazua"], "Categories": "cs.LG cs.AI cs.CR math.OC"}, "abstract": "Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers concerning the neural network structure, with the weights tuned by Bayesian optimization. Finally, experimental results validate the superiority of our proposed approximate and weighted attack (AWA) method over the other state-of-the-art methods, as demonstrated by the substantial improvement in different evaluation metrics for image data reconstructions.", "url": "https://arxiv.org/abs/2308.06822"}, {"metadata": {"arXiv": "2308.06827", "Date": "Sun, 13 Aug 2023 18:12:28 ", "Title": "Reinforcement Graph Clustering with Unknown Cluster Number", "Authors": ["Yue Liu", "Ke Liang", "Jun Xia", "Xihong Yang", "Sihang Zhou", "Meng Liu", "Xinwang Liu", "Stan Z. Li"], "Categories": "cs.LG cs.AI"}, "abstract": "Deep graph clustering, which aims to group nodes into disjoint clusters by neural networks in an unsupervised manner, has attracted great attention in recent years. Although the performance has been largely improved, the excellent performance of the existing methods heavily relies on an accurately predefined cluster number, which is not always available in the real-world scenario. To enable the deep graph clustering algorithms to work without the guidance of the predefined cluster number, we propose a new deep graph clustering method termed Reinforcement Graph Clustering (RGC). In our proposed method, cluster number determination and unsupervised representation learning are unified into a uniform framework by the reinforcement learning mechanism. Concretely, the discriminative node representations are first learned with the contrastive pretext task. Then, to capture the clustering state accurately with both local and global information in the graph, both node and cluster states are considered. Subsequently, at each state, the qualities of different cluster numbers are evaluated by the quality network, and the greedy action is executed to determine the cluster number. In order to conduct feedback actions, the clustering-oriented reward function is proposed to enhance the cohesion of the same clusters and separate the different clusters. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. The source code of RGC is shared at https://github.com/yueliu1999/RGC and a collection (papers, codes and, datasets) of deep graph clustering is shared at https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering on Github.", "url": "https://arxiv.org/abs/2308.06827"}, {"metadata": {"arXiv": "2308.06851", "Date": "Sun, 13 Aug 2023 22:03:35 ", "Title": "Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning", "Authors": ["Eamon Mukhopadhyay"], "Categories": "cs.LG cs.AI", "Comments": ["6 pages", "4 figures"], "MSC-class": "68T01", "ACM-class": "I.2.6"}, "abstract": "Throughout the analytical revolution that has occurred in the NBA, the development of specific metrics and formulas has given teams, coaches, and players a new way to see the game. However - the question arises - how can we verify any metrics? One method would simply be eyeball approximation (trying out many different gameplans) and/or trial and error - an estimation-based and costly approach. Another approach is to try to model already existing metrics with a unique set of features using machine learning techniques. The key to this approach is that with these features that are selected, we can try to gauge the effectiveness of these features combined, rather than using individual analysis in simple metric evaluation. If we have an accurate model, it can particularly help us determine the specifics of gameplan execution. In this paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) was found to have a correlation with different NBA playtypes using both a linear regression model and a neural network regression model, although ultimately, a neural network worked slightly better than linear regression. Using the accuracy of the models as a justification, the next step was to optimize the output of the model with test examples, which would demonstrate the combination of features to best achieve a highly functioning offense.", "url": "https://arxiv.org/abs/2308.06851"}, {"metadata": {"arXiv": "2308.06960", "Date": "Mon, 14 Aug 2023 06:32:02 ", "Title": "Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks", "Authors": ["Zhili Wang", "Shimin Di", "Lei Chen", "Xiaofang Zhou"], "Categories": "cs.LG cs.AI"}, "abstract": "Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search to fine-tune pre-trained graph neural networks for graph-level tasks (S2PGNN), which adaptively design a suitable fine-tuning framework for the given labeled data on the downstream task. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at \\url{https://anonymous.4open.science/r/code_icde2024-A9CB/}.", "url": "https://arxiv.org/abs/2308.06960"}, {"metadata": {"arXiv": "2308.07037", "Date": "Mon, 14 Aug 2023 09:56:35 ", "Title": "Bayesian Flow Networks", "Authors": ["Alex Graves", "Rupesh Kumar Srivastava", "Timothy Atkinson", "Faustino Gomez"], "Categories": "cs.LG cs.AI"}, "abstract": "This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no restrictions on the network architecture. In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task.", "url": "https://arxiv.org/abs/2308.07037"}, {"metadata": {"arXiv": "2308.07061", "Date": "Mon, 14 Aug 2023 10:45:51 ", "Title": "Machine Unlearning: Solutions and Challenges", "Authors": ["Jie Xu", "Zihan Wu", "Cong Wang and Xiaohua Jia"], "Categories": "cs.LG cs.AI"}, "abstract": "Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy violations, security breaches, and performance deterioration. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of machine unlearning research. We categorize existing research into exact unlearning that algorithmically removes data influence entirely and approximate unlearning that efficiently minimizes influence through limited parameter updates. By reviewing the state-of-the-art solutions, we critically discuss their advantages and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning. This paper provides researchers with a roadmap of open problems, encouraging impactful contributions to address real-world needs for selective data removal.", "url": "https://arxiv.org/abs/2308.07061"}, {"metadata": {"arXiv": "2308.07198", "Date": "Mon, 14 Aug 2023 15:07:05 ", "Title": "Explaining Black-Box Models through Counterfactuals", "Authors": ["Patrick Altmeyer and Arie van Deursen and Cynthia C. S. Liem"], "Categories": "cs.LG cs.AI cs.PL", "Comments": ["13 pages", "9 figures", "originally published in The Proceedings of the JuliaCon Conferences (JCON)"], "Journal-ref": "JuliaCon Proceedings, 1(1), 130 (2023)", "DOI": "10.21105/jcon.00130"}, "abstract": "We present CounterfactualExplanations.jl: a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box models in Julia. CE explain how inputs into a model need to change to yield specific model predictions. Explanations that involve realistic and actionable changes can be used to provide AR: a set of proposed actions for individuals to change an undesirable outcome for the better. In this article, we discuss the usefulness of CE for Explainable Artificial Intelligence and demonstrate the functionality of our package. The package is straightforward to use and designed with a focus on customization and extensibility. We envision it to one day be the go-to place for explaining arbitrary predictive models in Julia through a diverse suite of counterfactual generators.", "url": "https://arxiv.org/abs/2308.07198"}, {"metadata": {"arXiv": "2308.07204", "Date": "Mon, 14 Aug 2023 15:16:39 ", "Title": "Algorithms for the Training of Neural Support Vector Machines", "Authors": ["Lars Simon and Manuel Radons"], "Categories": "cs.LG cs.AI", "Comments": ["19 pages", "0 figures"], "MSC-class": "68T99"}, "abstract": "Neural support vector machines (NSVMs) allow for the incorporation of domain knowledge in the design of the model architecture. In this article we introduce a set of training algorithms for NSVMs that leverage the Pegasos algorithm and provide a proof of concept by solving a set of standard machine learning tasks.", "url": "https://arxiv.org/abs/2308.07204"}, {"metadata": {"arXiv": "2308.07247", "Date": "Mon, 14 Aug 2023 16:32:24 ", "Title": "Can we Agree? On the Rash\\=omon Effect and the Reliability of Post-Hoc Explainable AI", "Authors": ["Clement Poiret", "Antoine Grigis", "Justin Thomas", "Marion Noulhiane"], "Categories": "cs.LG cs.AI stat.ML", "Comments": ["13 pages", "6 figures and 6 tables"], "ACM-class": "H.1.1; I.6.4; I.2.1"}, "abstract": "The Rash\\=omon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rash\\=omon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous models.", "url": "https://arxiv.org/abs/2308.07247"}, {"metadata": {"arXiv": "2308.07273", "Date": "Mon, 14 Aug 2023 17:00:13 ", "Title": "Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning", "Authors": ["Youssra Cheriguene", "Wael Jaafar", "Chaker Abdelaziz Kerrache", "Halim Yanikomeroglu", "Fatima Zohra Bousbaa", "and Nasreddine Lagraa"], "Categories": "cs.LG cs.AI cs.NI"}, "abstract": "Unmanned aerial vehicle (UAV)-enabled edge federated learning (FL) has sparked a rise in research interest as a result of the massive and heterogeneous data collected by UAVs, as well as the privacy concerns related to UAV data transmissions to edge servers. However, due to the redundancy of UAV collected data, e.g., imaging data, and non-rigorous FL participant selection, the convergence time of the FL learning process and bias of the FL model may increase. Consequently, we investigate in this paper the problem of selecting UAV participants for edge FL, aiming to improve the FL model's accuracy, under UAV constraints of energy consumption, communication quality, and local datasets' heterogeneity. We propose a novel UAV participant selection scheme, called data-efficient energy-aware participant selection strategy (DEEPS), which consists of selecting the best FL participant in each sub-region based on the structural similarity index measure (SSIM) average score of its local dataset and its power consumption profile. Through experiments, we demonstrate that the proposed selection scheme is superior to the benchmark random selection method, in terms of model accuracy, training time, and UAV energy consumption.", "url": "https://arxiv.org/abs/2308.07273"}, {"metadata": {"arXiv": "2308.06419", "Date": "Fri, 11 Aug 2023 23:58:51 ", "Title": "Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review", "Authors": ["Mahsa Golchoubian", "Moojan Ghafurian", "Kerstin Dautenhahn", "Nasser Lashgarian Azad"], "Categories": "cs.RO cs.AI cs.HC cs.LG cs.SY eess.SY", "Comments": ["Published in IEEE Transactions on Intelligent Transportation Systems"], "DOI": "10.1109/TITS.2023.3291196"}, "abstract": "Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. A total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. An overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. Research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments are discussed.", "url": "https://arxiv.org/abs/2308.06419"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
