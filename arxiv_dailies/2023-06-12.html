<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <script>
        var papers = [{"id": "2306.05844", "date": "Fri, 9 Jun 2023 12:18:14 GMT", "title": "How Object Information Improves Skeleton-based Human Action Recognition\n\u00a0in Assembly Tasks\n", "authors": ["Dustin Aganian", "Mona K\\\"ohler", "Sebastian Baake", "Markus Eisenbach", "and\n\u00a0Horst-Michael Gross\n"], "categories": ["cs.CV", "cs.LG", "cs.RO\nComments:", "IEEE", "International", "Joint", "Conference", "on", "Neural", "Networks", "(IJCNN)", "2023\n"], "abstract": "As the use of collaborative robots (cobots) in industrial manufacturing continues to grow, human action recognition for effective human-robot collaboration becomes increasingly important. This ability is crucial for cobots to act autonomously and assist in assembly tasks. Recently, skeleton-based approaches are often used as they tend to generalize better to different people and environments. However, when processing skeletons alone, information about the objects a human interacts with is lost. Therefore, we present a novel approach of integrating object information into skeleton-based action recognition. We enhance two state-of-the-art methods by treating object centers as further skeleton joints. Our experiments on the assembly dataset IKEA ASM show that our approach improves the performance of these state-of-the-art methods to a large extent when combining skeleton joints with objects predicted by a state-of-the-art instance segmentation model. Our research sheds light on the benefits of combining skeleton joints with object information for human action recognition in assembly tasks. We analyze the effect of the object detector on the combination for action classification and discuss the important factors that must be taken into account.", "link": "https://arxiv.org/abs/2306.05844"}, {"id": "2306.06073", "date": "Wed, 31 May 2023 20:27:10 GMT", "title": "Feature Selection on Sentinel-2 Multi-spectral Imagery for Efficient\n\u00a0Tree Cover Estimation\n", "authors": ["Usman Nazir", "Momin Uppal", "Muhammad Tahir", "Zubair Khalid\n"], "categories": ["cs.CV", "cs.LG", "eess.IV\nComments:", "IEEE", "IGARSS", "2023\n"], "abstract": "This paper proposes a multi-spectral random forest classifier with suitable feature selection and masking for tree cover estimation in urban areas. The key feature of the proposed classifier is filtering out the built-up region using spectral indices followed by random forest classification on the remaining mask with carefully selected features. Using Sentinel-2 satellite imagery, we evaluate the performance of the proposed technique on a specified area (approximately 82 acres) of Lahore University of Management Sciences (LUMS) and demonstrate that our method outperforms a conventional random forest classifier as well as state-of-the-art methods such as European Space Agency (ESA) WorldCover 10m 2020 product as well as a DeepLabv3 deep learning architecture.", "link": "https://arxiv.org/abs/2306.06073"}, {"id": "2306.06076", "date": "Thu, 8 Jun 2023 04:14:32 GMT", "title": "Differentially Private Image Classification by Learning Priors from\n\u00a0Random Processes\n", "authors": ["Xinyu Tang", "Ashwinee Panda", "Vikash Sehwag", "Prateek Mittal\n"], "categories": ["cs.CV", "cs.CR", "cs.LG", "stat.ML\n"], "abstract": "In privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient clipping and noise addition. A recent focus in private learning research is improving the performance of DP-SGD on private data by incorporating priors that are learned on real-world public data. In this work, we explore how we can improve the privacy-utility tradeoff of DP-SGD by learning priors from images generated by random processes and transferring these priors to private data. We propose DP-RandP, a three-phase approach. We attain new state-of-the-art accuracy when training from scratch on CIFAR10, CIFAR100, and MedMNIST for a range of privacy budgets $\\varepsilon \\in [1, 8]$. In particular, we improve the previous best reported accuracy on CIFAR10 from $60.6 \\%$ to $72.3 \\%$ for $\\varepsilon=1$. Our code is available at https://github.com/inspire-group/DP-RandP.", "link": "https://arxiv.org/abs/2306.06076"}, {"id": "2306.06078", "date": "Sat, 27 May 2023 04:03:15 GMT", "title": "Cheating off your neighbors: Improving activity recognition through\n\u00a0corroboration\n", "authors": ["Haoxiang Yu", "Jingyi An", "Evan King", "Edison Thomaz", "Christine Julien\n"], "categories": ["cs.CV", "cs.HC", "cs.LG", "eess.SP\n"], "abstract": "Understanding the complexity of human activities solely through an individual's data can be challenging. However, in many situations, surrounding individuals are likely performing similar activities, while existing human activity recognition approaches focus almost exclusively on individual measurements and largely ignore the context of the activity. Consider two activities: attending a small group meeting and working at an office desk. From solely an individual's perspective, it can be difficult to differentiate between these activities as they may appear very similar, even though they are markedly different. Yet, by observing others nearby, it can be possible to distinguish between these activities. In this paper, we propose an approach to enhance the prediction accuracy of an individual's activities by incorporating insights from surrounding individuals. We have collected a real-world dataset from 20 participants with over 58 hours of data including activities such as attending lectures, having meetings, working in the office, and eating together. Compared to observing a single person in isolation, our proposed approach significantly improves accuracy. We regard this work as a first step in collaborative activity recognition, opening new possibilities for understanding human activity in group settings.", "link": "https://arxiv.org/abs/2306.06078"}, {"id": "2306.05592", "date": "Thu, 8 Jun 2023 23:38:25 GMT", "title": "Evaluating and Incentivizing Diverse Data Contributions in Collaborative\n\u00a0Learning\n", "authors": ["Baihe Huang", "Sai Praneeth Karimireddy", "Michael I. Jordan\n"], "categories": ["cs.GT", "cs.CY", "cs.DC", "cs.LG", "econ.TH\n"], "abstract": "For a federated learning model to perform well, it is crucial to have a diverse and representative dataset. However, the data contributors may only be concerned with the performance on a specific subset of the population, which may not reflect the diversity of the wider population. This creates a tension between the principal (the FL platform designer) who cares about global performance and the agents (the data collectors) who care about local performance. In this work, we formulate this tension as a game between the principal and multiple agents, and focus on the linear experiment design problem to formally study their interaction. We show that the statistical criterion used to quantify the diversity of the data, as well as the choice of the federated learning algorithm used, has a significant effect on the resulting equilibrium. We leverage this to design simple optimal federated learning mechanisms that encourage data collectors to contribute data representative of the global population, thereby maximizing global performance.", "link": "https://arxiv.org/abs/2306.05592"}, {"id": "2306.05437", "date": "Thu, 8 Jun 2023 02:52:24 GMT", "title": "One-step Multi-view Clustering with Diverse Representation\n", "authors": ["Xinhang Wan", "Jiyuan Liu", "Jue Wang", "Xinwang Liu", "Siwei Wang", "Yi Wen,\n\u00a0Tianjiao Wan", "En Zhu\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Multi-view clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-view clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, which limits the expressiveness of the model. Moreover, a range of methods suffer from a two-step process, i.e., multimodal learning and the subsequent $k$-means, inevitably causing a sub-optimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation method, which incorporates multi-view learning and $k$-means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervised manner. Then we directly use the information matrices under diverse dimensions to obtain consensus discrete clustering labels. The unified work of representation learning and clustering boosts the quality of the final results. Furthermore, we develop an efficient optimization algorithm to solve the resultant problem with proven convergence. Comprehensive experiments on various datasets demonstrate the promising clustering performance of our proposed method.", "link": "https://arxiv.org/abs/2306.05437"}, {"id": "2306.05438", "date": "Thu, 8 Jun 2023 06:57:07 GMT", "title": "DynamoRep: Trajectory-Based Population Dynamics for Classification of\n\u00a0Black-box Optimization Problems\n", "authors": ["Gjorgjina Cenikj", "Ga\\v{s}per Petelin", "Carola Doerr", "Peter Koro\\v{s}ec,\n\u00a0Tome Eftimov\n"], "categories": ["cs.LG", "cs.NE\nComments:", "9", "pages,", "5", "figures\nDOI:", "10.1145/3583131.3590401\n"], "abstract": "The application of machine learning (ML) models to the analysis of optimization algorithms requires the representation of optimization problems using numerical features. These features can be used as input for ML models that are trained to select or to configure a suitable algorithm for the problem at hand. Since in pure black-box optimization information about the problem instance can only be obtained through function evaluation, a common approach is to dedicate some function evaluations for feature extraction, e.g., using random sampling. This approach has two key downsides: (1) It reduces the budget left for the actual optimization phase, and (2) it neglects valuable information that could be obtained from a problem-solver interaction. In this paper, we propose a feature extraction method that describes the trajectories of optimization algorithms using simple descriptive statistics. We evaluate the generated features for the task of classifying problem classes from the Black Box Optimization Benchmarking (BBOB) suite. We demonstrate that the proposed DynamoRep features capture enough information to identify the problem class on which the optimization algorithm is running, achieving a mean classification accuracy of 95% across all experiments.", "link": "https://arxiv.org/abs/2306.05438"}, {"id": "2306.05487", "date": "Thu, 8 Jun 2023 18:17:48 GMT", "title": "Boosting with Tempered Exponential Measures\n", "authors": ["Richard Nock", "Ehsan Amid", "Manfred K. Warmuth\n"], "categories": ["cs.LG", "stat.ML\nACM-class:", "I.2.6\n"], "abstract": "One of the most popular ML algorithms, AdaBoost, can be derived from the dual of a relative entropy minimization problem subject to the fact that the positive weights on the examples sum to one. Essentially, harder examples receive higher probabilities. We generalize this setup to the recently introduced {\\it tempered exponential measure}s (TEMs) where normalization is enforced on a specific power of the measure and not the measure itself. TEMs are indexed by a parameter $t$ and generalize exponential families ($t=1$). Our algorithm, $t$-AdaBoost, recovers AdaBoost~as a special case ($t=1$). We show that $t$-AdaBoost retains AdaBoost's celebrated exponential convergence rate when $t\\in [0,1)$ while allowing a slight improvement of the rate's hidden constant compared to $t=1$. $t$-AdaBoost partially computes on a generalization of classical arithmetic over the reals and brings notable properties like guaranteed bounded leveraging coefficients for $t\\in [0,1)$. From the loss that $t$-AdaBoost minimizes (a generalization of the exponential loss), we show how to derive a new family of {\\it tempered} losses for the induction of domain-partitioning classifiers like decision trees. Crucially, strict properness is ensured for all while their boosting rates span the full known spectrum. Experiments using $t$-AdaBoost+trees display that significant leverage can be achieved by tuning $t$.", "link": "https://arxiv.org/abs/2306.05487"}, {"id": "2306.05497", "date": "Thu, 8 Jun 2023 18:38:55 GMT", "title": "Reevaluating Loss Functions: Enhancing Robustness to Label Noise in Deep\n\u00a0Learning Models\n", "authors": ["Max Staats", "Matthias Thamm", "Bernd Rosenow\n"], "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI\nComments:", "13", "pages,", "4", "figures\n"], "abstract": "Large annotated datasets inevitably contain incorrect labels, which poses a major challenge for the training of deep neural networks as they easily fit the labels. Only when training with a robust model that is not easily distracted by the noise, a good generalization performance can be achieved. A simple yet effective way to create a noise robust model is to use a noise robust loss function. However, the number of proposed loss functions is large, they often come with hyperparameters, and may learn slower than the widely used but noise sensitive Cross Entropy loss. By heuristic considerations and extensive numerical experiments, we study in which situations the proposed loss functions are applicable and give suggestions on how to choose an appropriate loss. Additionally, we propose a novel technique to enhance learning with bounded loss functions: the inclusion of an output bias, i.e. a slight increase in the neuron pre-activation corresponding to the correct label. Surprisingly, we find that this not only significantly improves the learning of bounded losses, but also leads to the Mean Absolute Error loss outperforming the Cross Entropy loss on the Cifar-100 dataset - even in the absence of additional label noise. This suggests that training with a bounded loss function can be advantageous even in the presence of minimal label noise. To further strengthen our analysis of the learning behavior of different loss functions, we additionally design and test a novel loss function denoted as Bounded Cross Entropy.", "link": "https://arxiv.org/abs/2306.05497"}, {"id": "2306.05567", "date": "Thu, 8 Jun 2023 21:19:42 GMT", "title": "Intelligent Energy Management with IoT Framework in Smart Cities Using\n\u00a0Intelligent Analysis: An Application of Machine Learning Methods for Complex\n\u00a0Networks and Systems\n", "authors": ["Maryam Nikpour", "Parisa Behvand Yousefi", "Hadi Jafarzadeh", "Kasra Danesh,\n\u00a0Mohsen Ahmadi\n"], "categories": ["cs.LG", "cs.CY", "cs.SY", "eess.SY\n"], "abstract": "Smart buildings are increasingly using Internet of Things (IoT)-based wireless sensing systems to reduce their energy consumption and environmental impact. As a result of their compact size and ability to sense, measure, and compute all electrical properties, Internet of Things devices have become increasingly important in our society. A major contribution of this study is the development of a comprehensive IoT-based framework for smart city energy management, incorporating multiple components of IoT architecture and framework. An IoT framework for intelligent energy management applications that employ intelligent analysis is an essential system component that collects and stores information. Additionally, it serves as a platform for the development of applications by other companies. Furthermore, we have studied intelligent energy management solutions based on intelligent mechanisms. The depletion of energy resources and the increase in energy demand have led to an increase in energy consumption and building maintenance. The data collected is used to monitor, control, and enhance the efficiency of the system.", "link": "https://arxiv.org/abs/2306.05567"}, {"id": "2306.05579", "date": "Thu, 8 Jun 2023 22:37:24 GMT", "title": "Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with\n\u00a0Heterogeneous Rewards\n", "authors": ["Mengfan Xu and Diego Klabjan\n"], "categories": ["cs.LG", "stat.ML\nComments:", "14", "pages,", "under", "review\n"], "abstract": "We study a decentralized multi-agent multi-armed bandit problem in which multiple clients are connected by time dependent random graphs provided by an environment. The reward distributions of each arm vary across clients and rewards are generated independently over time by an environment based on distributions that include both sub-exponential and sub-gaussian distributions. Each client pulls an arm and communicates with neighbors based on the graph provided by the environment. The goal is to minimize the overall regret of the entire system through collaborations. To this end, we introduce a novel algorithmic framework, which first provides robust simulation methods for generating random graphs using rapidly mixing Markov chains or the random graph model, and then combines an averaging-based consensus approach with a newly proposed weighting technique and the upper confidence bound to deliver a UCB-type solution. Our algorithms account for the randomness in the graphs, removing the conventional doubly stochasticity assumption, and only require the knowledge of the number of clients at initialization. We derive optimal instance-dependent regret upper bounds of order $\\log{T}$ in both sub-gaussian and sub-exponential environments, and a nearly optimal mean-gap independent regret upper bound of order $\\sqrt{T}\\log T$ up to a $\\log T$ factor. Importantly, our regret bounds hold with high probability and capture graph randomness, whereas prior works consider expected regret under assumptions and require more stringent reward distributions.", "link": "https://arxiv.org/abs/2306.05579"}, {"id": "2306.05583", "date": "Thu, 8 Jun 2023 22:54:48 GMT", "title": "SGLD-Based Information Criteria and the Over-Parameterized Regime\n", "authors": ["Haobo Chen", "Yuheng Bu and Gregory W. Wornell\n"], "categories": ["cs.LG", "cs.IT", "math.IT\n"], "abstract": "Double-descent refers to the unexpected drop in test loss of a learning algorithm beyond an interpolating threshold with over-parameterization, which is not predicted by information criteria in their classical forms due to the limitations in the standard asymptotic approach. We update these analyses using the information risk minimization framework and provide Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for models learned by stochastic gradient Langevin dynamics (SGLD). Notably, the AIC and BIC penalty terms for SGLD correspond to specific information measures, i.e., symmetrized KL information and KL divergence. We extend this information-theoretic analysis to over-parameterized models by characterizing the SGLD-based BIC for the random feature model in the regime where the number of parameters $p$ and the number of samples $n$ tend to infinity, with $p/n$ fixed. Our experiments demonstrate that the refined SGLD-based BIC can track the double-descent curve, providing meaningful guidance for model selection and revealing new insights into the behavior of SGLD learning algorithms in the over-parameterized regime.", "link": "https://arxiv.org/abs/2306.05583"}, {"id": "2306.05587", "date": "Thu, 8 Jun 2023 23:14:39 GMT", "title": "MC-NN: An End-to-End Multi-Channel Neural Network Approach for\n\u00a0Predicting Influenza A Virus Hosts and Antigenic Types\n", "authors": ["Yanhua Xu and Dominik Wojtczak\n"], "categories": ["cs.LG", "q-bio.QM\nComments:", "Accepted", "version", "submitted", "to", "the", "SN", "Computer", "Science;", "Published", "in\n\u00a0the", "SN", "Computer", "Science", "2023\nDOI:", "10.1007/s42979-023-01839-5\n"], "abstract": "Influenza poses a significant threat to public health, particularly among the elderly, young children, and people with underlying dis-eases. The manifestation of severe conditions, such as pneumonia, highlights the importance of preventing the spread of influenza. An accurate and cost-effective prediction of the host and antigenic sub-types of influenza A viruses is essential to addressing this issue, particularly in resource-constrained regions. In this study, we propose a multi-channel neural network model to predict the host and antigenic subtypes of influenza A viruses from hemagglutinin and neuraminidase protein sequences. Our model was trained on a comprehensive data set of complete protein sequences and evaluated on various test data sets of complete and incomplete sequences. The results demonstrate the potential and practicality of using multi-channel neural networks in predicting the host and antigenic subtypes of influenza A viruses from both full and partial protein sequences.", "link": "https://arxiv.org/abs/2306.05587"}, {"id": "2306.05641", "date": "Fri, 9 Jun 2023 03:00:34 GMT", "title": "Revisiting Permutation Symmetry for Merging Models between Different\n\u00a0Datasets\n", "authors": ["Masanori Yamada", "Tomoya Yamashita", "Shin'ya Yamaguchi", "Daiki Chijiwa\n"], "categories": ["cs.LG", "cs.AI\nComments:", "18", "pages;", "comments", "are", "welcome\n"], "abstract": "Model merging is a new approach to creating a new model by combining the weights of different trained models. Previous studies report that model merging works well for models trained on a single dataset with different random seeds, while model merging between different datasets is difficult. Merging knowledge from different datasets has practical significance, but it has not been well investigated. In this paper, we investigate the properties of merging models between different datasets. Through theoretical and empirical analyses, we find that the accuracy of the merged model decreases more significantly as the datasets diverge more and that the different loss landscapes for each dataset make model merging between different datasets difficult. We also show that merged models require datasets for merging in order to achieve a high accuracy. Furthermore, we show that condensed datasets created by dataset condensation can be used as substitutes for the original datasets when merging models. We conduct experiments for model merging between different datasets. When merging between MNIST and Fashion- MNIST models, the accuracy significantly improves by 28% using the dataset and 25% using the condensed dataset compared with not using the dataset.", "link": "https://arxiv.org/abs/2306.05641"}, {"id": "2306.05655", "date": "Fri, 9 Jun 2023 03:51:45 GMT", "title": "Communication-Efficient Zeroth-Order Distributed Online Optimization:\n\u00a0Algorithm, Theory, and Applications\n", "authors": ["Ege C. Kaya", "M. Berk Sahin and Abolfazl Hashemi\n"], "categories": ["cs.LG", "math.OC\nComments:", "21", "pages,", "5", "figures,", "and", "this", "paper", "has", "been", "accepted", "by", "IEEE", "Access\nDOI:", "10.1109/ACCESS.2023.3284891\n"], "abstract": "This paper focuses on a multi-agent zeroth-order online optimization problem in a federated learning setting for target tracking. The agents only sense their current distances to their targets and aim to maintain a minimum safe distance from each other to prevent collisions. The coordination among the agents and dissemination of collision-prevention information is managed by a central server using the federated learning paradigm. The proposed formulation leads to an instance of distributed online nonconvex optimization problem that is solved via a group of communication-constrained agents. To deal with the communication limitations of the agents, an error feedback-based compression scheme is utilized for agent-to-server communication. The proposed algorithm is analyzed theoretically for the general class of distributed online nonconvex optimization problems. We provide non-asymptotic convergence rates that show the dominant term is independent of the characteristics of the compression scheme. Our theoretical results feature a new approach that employs significantly more relaxed assumptions in comparison to standard literature. The performance of the proposed solution is further analyzed numerically in terms of tracking errors and collisions between agents in two relevant applications.", "link": "https://arxiv.org/abs/2306.05655"}, {"id": "2306.05697", "date": "Fri, 9 Jun 2023 06:34:16 GMT", "title": "Group Equivariant Fourier Neural Operators for Partial Differential\n\u00a0Equations\n", "authors": ["Jacob Helwig", "Xuan Zhang", "Cong Fu", "Jerry Kurtin", "Stephan Wojtowytsch,\n\u00a0Shuiwang Ji\n"], "categories": ["cs.LG", "cs.NA", "math.NA\nComments:", "Proceedings", "of", "the", "40th", "International", "Conference", "on", "Machine", "Learning\n\u00a0https://icml.cc/virtual/2023/poster/23875\n"], "abstract": "We consider solving partial differential equations (PDEs) with Fourier neural operators (FNOs), which operate in the frequency domain. Since the laws of physics do not depend on the coordinate system used to describe them, it is desirable to encode such symmetries in the neural operator architecture for better performance and easier learning. While encoding symmetries in the physical domain using group theory has been studied extensively, how to capture symmetries in the frequency domain is under-explored. In this work, we extend group convolutions to the frequency domain and design Fourier layers that are equivariant to rotations, translations, and reflections by leveraging the equivariance property of the Fourier transform. The resulting $G$-FNO architecture generalizes well across input resolutions and performs well in settings with varying levels of symmetry. Our code is publicly available as part of the AIRS library (https://github.com/divelab/AIRS).", "link": "https://arxiv.org/abs/2306.05697"}, {"id": "2306.05706", "date": "Fri, 9 Jun 2023 06:55:15 GMT", "title": "Understanding How Consistency Works in Federated Learning via Stage-wise\n\u00a0Relaxed Initialization\n", "authors": ["Yan Sun", "Li Shen", "Dacheng Tao\n"], "categories": ["cs.LG", "cs.DC", "math.OC\nComments:", "32", "pages\n"], "abstract": "Federated learning (FL) is a distributed paradigm that coordinates massive local clients to collaboratively train a global model via stage-wise local training processes on the heterogeneous dataset. Previous works have implicitly studied that FL suffers from the ``client-drift'' problem, which is caused by the inconsistent optimum across local clients. However, till now it still lacks solid theoretical analysis to explain the impact of this local inconsistency. To alleviate the negative impact of the ``client drift'' and explore its substance in FL, in this paper, we first design an efficient FL algorithm \\textit{FedInit}, which allows employing the personalized relaxed initialization state at the beginning of each local training stage. Specifically, \\textit{FedInit} initializes the local state by moving away from the current global state towards the reverse direction of the latest local state. This relaxed initialization helps to revise the local divergence and enhance the local consistency level. Moreover, to further understand how inconsistency disrupts performance in FL, we introduce the excess risk analysis and study the divergence term to investigate the test error of the proposed \\textit{FedInit} method. Our studies show that optimization error is not sensitive to this local inconsistency, while it mainly affects the generalization error bound in \\textit{FedInit}. Extensive experiments are conducted to validate this conclusion. Our proposed \\textit{FedInit} could achieve state-of-the-art~(SOTA) results compared to several advanced benchmarks without any additional costs. Meanwhile, stage-wise relaxed initialization could also be incorporated into the current advanced algorithms to achieve higher performance in the FL paradigm.", "link": "https://arxiv.org/abs/2306.05706"}, {"id": "2306.05722", "date": "Fri, 9 Jun 2023 07:38:38 GMT", "title": "Estimation of Ridge Using Nonlinear Transformation on Density Function\n", "authors": ["Zheng Zhai and Hengchao Chen and Zhigang Yao\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "Ridges play a vital role in accurately approximating the underlying structure of manifolds. In this paper, we explore the ridge's variation by applying a concave nonlinear transformation to the density function. Through the derivation of the Hessian matrix, we observe that nonlinear transformations yield a rank-one modification of the Hessian matrix. Leveraging the variational properties of eigenvalue problems, we establish a partial order inclusion relationship among the corresponding ridges. We intuitively discover that the transformation can lead to improved estimation of the tangent space via rank-one modification of the Hessian matrix. To validate our theories, we conduct extensive numerical experiments on synthetic and real-world datasets that demonstrate the superiority of the ridges obtained from our transformed approach in approximating the underlying truth manifold compared to other manifold fitting algorithms.", "link": "https://arxiv.org/abs/2306.05722"}, {"id": "2306.05726", "date": "Fri, 9 Jun 2023 07:46:24 GMT", "title": "In-Sample Policy Iteration for Offline Reinforcement Learning\n", "authors": ["Xiaohan Hu", "Yi Ma", "Chenjun Xiao", "Yan Zheng", "Zhaopeng Meng\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Offline reinforcement learning (RL) seeks to derive an effective control policy from previously collected data. To circumvent errors due to inadequate data coverage, behavior-regularized methods optimize the control policy while concurrently minimizing deviation from the data collection policy. Nevertheless, these methods often exhibit subpar practical performance, particularly when the offline dataset is collected by sub-optimal policies. In this paper, we propose a novel algorithm employing in-sample policy iteration that substantially enhances behavior-regularized methods in offline RL. The core insight is that by continuously refining the policy used for behavior regularization, in-sample policy iteration gradually improves itself while implicitly avoids querying out-of-sample actions to avert catastrophic learning failures. Our theoretical analysis verifies its ability to learn the in-sample optimal policy, exclusively utilizing actions well-covered by the dataset. Moreover, we propose competitive policy improvement, a technique applying two competitive policies, both of which are trained by iteratively improving over the best competitor. We show that this simple yet potent technique significantly enhances learning efficiency when function approximation is applied. Lastly, experimental results on the D4RL benchmark indicate that our algorithm outperforms previous state-of-the-art methods in most tasks.", "link": "https://arxiv.org/abs/2306.05726"}, {"id": "2306.05734", "date": "Fri, 9 Jun 2023 07:55:46 GMT", "title": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework\n", "authors": ["Hua Wang", "Sheng Gao", "Huanyu Zhang", "Weijie J. Su", "Milan Shen\n"], "categories": ["cs.LG", "cs.CR", "cs.DS\n"], "abstract": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best-performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize \"adaptive\" hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for \"adaptive\" private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world and synthetic datasets.", "link": "https://arxiv.org/abs/2306.05734"}, {"id": "2306.05751", "date": "Fri, 9 Jun 2023 08:30:51 GMT", "title": "Advancing Counterfactual Inference through Quantile Regression\n", "authors": ["Shaoan Xie", "Biwei Huang", "Bin Gu", "Tongliang Liu", "Kun Zhang\n"], "categories": ["cs.LG", "stat.ME\n"], "abstract": "The capacity to address counterfactual \"what if\" inquiries is crucial for understanding and making use of causal influences. Traditional counterfactual inference usually assumes a structural causal model is available. However, in practice, such a causal model is often unknown and may not be identifiable. This paper aims to perform reliable counterfactual inference based on the (learned) qualitative causal structure and observational data, without a given causal model or even directly estimating conditional distributions. We re-cast counterfactual reasoning as an extended quantile regression problem using neural networks. The approach is statistically more efficient than existing ones, and further makes it possible to develop the generalization ability of the estimated counterfactual outcome to unseen data and provide an upper bound on the generalization error. Experiment results on multiple datasets strongly support our theoretical claims.", "link": "https://arxiv.org/abs/2306.05751"}, {"id": "2306.05760", "date": "Fri, 9 Jun 2023 08:54:20 GMT", "title": "Efficient GNN Explanation via Learning Removal-based Attribution\n", "authors": ["Yao Rong", "Guanchu Wang", "Qizhang Feng", "Ninghao Liu", "Zirui Liu,\n\u00a0Enkelejda Kasneci", "Xia Hu\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "As Graph Neural Networks (GNNs) have been widely used in real-world applications, model explanations are required not only by users but also by legal regulations. However, simultaneously achieving high fidelity and low computational costs in generating explanations has been a challenge for current methods. In this work, we propose a framework of GNN explanation named LeArn Removal-based Attribution (LARA) to address this problem. Specifically, we introduce removal-based attribution and demonstrate its substantiated link to interpretability fidelity theoretically and experimentally. The explainer in LARA learns to generate removal-based attribution which enables providing explanations with high fidelity. A strategy of subgraph sampling is designed in LARA to improve the scalability of the training process. In the deployment, LARA can efficiently generate the explanation through a feed-forward pass. We benchmark our approach with other state-of-the-art GNN explanation methods on six datasets. Results highlight the effectiveness of our framework regarding both efficiency and fidelity. In particular, LARA is 3.5 times faster and achieves higher fidelity than the state-of-the-art method on the large dataset ogbn-arxiv (more than 160K nodes and 1M edges), showing its great potential in real-world applications. Our source code is available at https://anonymous.4open.science/r/LARA-10D8/README.md.", "link": "https://arxiv.org/abs/2306.05760"}, {"id": "2306.05775", "date": "Fri, 9 Jun 2023 09:33:34 GMT", "title": "Weight Freezing: A Regularization Approach for Fully Connected Layers\n\u00a0with an Application in EEG Classification\n", "authors": ["Zhengqing Miao and Meirong Zhao\n"], "categories": ["cs.LG", "eess.SP\nComments:", "16", "pages,", "5", "figures\n"], "abstract": "In the realm of EEG decoding, enhancing the performance of artificial neural networks (ANNs) carries significant potential. This study introduces a novel approach, termed \"weight freezing\", that is anchored on the principles of ANN regularization and neuroscience prior knowledge. The concept of weight freezing revolves around the idea of reducing certain neurons' influence on the decision-making process for a specific EEG task by freezing specific weights in the fully connected layer during the backpropagation process. This is actualized through the use of a mask matrix and a threshold to determine the proportion of weights to be frozen during backpropagation. Moreover, by setting the masked weights to zero, weight freezing can not only realize sparse connections in networks with a fully connected layer as the classifier but also function as an efficacious regularization method for fully connected layers. Through experiments involving three distinct ANN architectures and three widely recognized EEG datasets, we validate the potency of weight freezing. Our method significantly surpasses previous peak performances in classification accuracy across all examined datasets. Supplementary control experiments offer insights into performance differences pre and post weight freezing implementation and scrutinize the influence of the threshold in the weight freezing process. Our study underscores the superior efficacy of weight freezing compared to traditional fully connected networks for EEG feature classification tasks. With its proven effectiveness, this innovative approach holds substantial promise for contributing to future strides in EEG decoding research.", "link": "https://arxiv.org/abs/2306.05775"}, {"id": "2306.05779", "date": "Fri, 9 Jun 2023 09:46:38 GMT", "title": "Transformer-based Time-to-Event Prediction for Chronic Kidney Disease\n\u00a0Deterioration\n", "authors": ["Moshe Zisser and Dvir Aran\n"], "categories": ["cs.LG", "cs.CL\n"], "abstract": "Deep-learning techniques, particularly the transformer model, have shown great potential in enhancing the prediction performance of longitudinal health records. While previous methods have mainly focused on fixed-time risk prediction, time-to-event prediction (also known as survival analysis) is often more appropriate for clinical scenarios. Here, we present a novel deep-learning architecture we named STRAFE, a generalizable survival analysis transformer-based architecture for electronic health records. The performance of STRAFE was evaluated using a real-world claim dataset of over 130,000 individuals with stage 3 chronic kidney disease (CKD) and was found to outperform other time-to-event prediction algorithms in predicting the exact time of deterioration to stage 5. Additionally, STRAFE was found to outperform binary outcome algorithms in predicting fixed-time risk, possibly due to its ability to train on censored data. We show that STRAFE predictions can improve the positive predictive value of high-risk patients by 3-fold, demonstrating possible usage to improve targeting for intervention programs. Finally, we suggest a novel visualization approach to predictions on a per-patient basis. In conclusion, STRAFE is a cutting-edge time-to-event prediction algorithm that has the potential to enhance risk predictions in large claims datasets.", "link": "https://arxiv.org/abs/2306.05779"}, {"id": "2306.05784", "date": "Fri, 9 Jun 2023 09:55:20 GMT", "title": "Quantitative Ink Analysis: Estimating the Number of Inks in Documents\n\u00a0through Hyperspectral Imaging\n", "authors": ["Aneeqa Abrar", "Hamza Iqbal\n"], "categories": ["cs.LG", "eess.IV\n"], "abstract": "In the field of document forensics, ink analysis plays a crucial role in determining the authenticity of legal and historic documents and detecting forgery. Visual examination alone is insufficient for distinguishing visually similar inks, necessitating the use of advanced scientific techniques. This paper proposes an ink analysis technique based on hyperspectral imaging, which enables the examination of documents in hundreds of narrowly spaced spectral bands, revealing hidden details. The main objective of this study is to identify the number of distinct inks used in a document. Three clustering algorithms, namely k-means, Agglomerative, and c-means, are employed to estimate the number of inks present. The methodology involves data extraction, ink pixel segmentation, and ink number determination. The results demonstrate the effectiveness of the proposed technique in identifying ink clusters and distinguishing between different inks. The analysis of a hyperspectral cube dataset reveals variations in spectral reflectance across different bands and distinct spectral responses among the 12 lines, indicating the presence of multiple inks. The clustering algorithms successfully identify ink clusters, with k-means clustering showing superior classification performance. These findings contribute to the development of reliable methodologies for ink analysis using hyperspectral imaging, enhancing the", "link": "https://arxiv.org/abs/2306.05784"}, {"id": "2306.05815", "date": "Fri, 9 Jun 2023 11:27:35 GMT", "title": "Extending Kernel PCA through Dualization: Sparsity, Robustness and Fast\n\u00a0Algorithms\n", "authors": ["Francesco Tonin", "Alex Lambert", "Panagiotis Patrinos", "Johan A. K.\n\u00a0Suykens\n"], "categories": ["cs.LG", "stat.ML\nComments:", "15", "pages,", "ICML", "2023\n"], "abstract": "The goal of this paper is to revisit Kernel Principal Component Analysis (KPCA) through dualization of a difference of convex functions. This allows to naturally extend KPCA to multiple objective functions and leads to efficient gradient-based algorithms avoiding the expensive SVD of the Gram matrix. Particularly, we consider objective functions that can be written as Moreau envelopes, demonstrating how to promote robustness and sparsity within the same framework. The proposed method is evaluated on synthetic and real-world benchmarks, showing significant speedup in KPCA training time as well as highlighting the benefits in terms of robustness and sparsity.", "link": "https://arxiv.org/abs/2306.05815"}, {"id": "2306.05838", "date": "Fri, 9 Jun 2023 12:12:07 GMT", "title": "Expectation-Complete Graph Representations with Homomorphisms\n", "authors": ["Pascal Welke", "Maximilian Thiessen", "Fabian Jogl", "Thomas G\\\"artner\n"], "categories": ["cs.LG", "cs.DS\nComments:", "accepted", "for", "publication", "at", "ICML", "2023\n"], "abstract": "We investigate novel random graph embeddings that can be computed in expected polynomial time and that are able to distinguish all non-isomorphic graphs in expectation. Previous graph embeddings have limited expressiveness and either cannot distinguish all graphs or cannot be computed efficiently for every graph. To be able to approximate arbitrary functions on graphs, we are interested in efficient alternatives that become arbitrarily expressive with increasing resources. Our approach is based on Lov\\'asz' characterisation of graph isomorphism through an infinite dimensional vector of homomorphism counts. Our empirical evaluation shows competitive results on several benchmark graph learning tasks.", "link": "https://arxiv.org/abs/2306.05838"}, {"id": "2306.05865", "date": "Fri, 9 Jun 2023 12:58:47 GMT", "title": "Faster Discrete Convex Function Minimization with Predictions: The\n\u00a0M-Convex Case\n", "authors": ["Taihei Oki", "Shinsaku Sakaue\n"], "categories": ["cs.LG", "cs.DS\n"], "abstract": "Recent years have seen a growing interest in accelerating optimization algorithms with machine-learned predictions. Sakaue and Oki (NeurIPS 2022) have developed a general framework that warm-starts the L-convex function minimization method with predictions, revealing the idea's usefulness for various discrete optimization problems. In this paper, we present a framework for using predictions to accelerate M-convex function minimization, thus complementing previous research and extending the range of discrete optimization algorithms that can benefit from predictions. Our framework is particularly effective for an important subclass called laminar convex minimization, which appears in many operations research applications. Our methods can improve time complexity bounds upon the best worst-case results by using predictions and even have potential to go beyond a lower-bound result.", "link": "https://arxiv.org/abs/2306.05865"}, {"id": "2306.05880", "date": "Fri, 9 Jun 2023 13:20:04 GMT", "title": "Time Series Continuous Modeling for Imputation and Forecasting with\n\u00a0Implicit Neural Representations\n", "authors": ["Etienne Le Naour", "Louis Serrano", "L\\'eon Migus", "Yuan Yin", "patrick\n\u00a0gallinari", "Ghislain Agoua", "Nicolas Baskiotis", "Vincent Guigue\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.", "link": "https://arxiv.org/abs/2306.05880"}, {"id": "2306.05889", "date": "Fri, 9 Jun 2023 13:35:04 GMT", "title": "C(NN)FD -- a deep learning framework for turbomachinery CFD analysis\n", "authors": ["Giuseppe Bruni", "Sepehr Maleki", "Senthil K. Krishnababu\n"], "categories": ["cs.LG", "cs.CE", "physics.flu-dyn\n"], "abstract": "Deep Learning methods have seen a wide range of successful applications across different industries. Up until now, applications to physical simulations such as CFD (Computational Fluid Dynamics), have been limited to simple test-cases of minor industrial relevance. This paper demonstrates the development of a novel deep learning framework for real-time predictions of the impact of manufacturing and build variations on the overall performance of axial compressors in gas turbines, with a focus on tip clearance variations. The associated scatter in efficiency can significantly increase the $CO_2$ emissions, thus being of great industrial and environmental relevance. The proposed \\textit{C(NN)FD} architecture achieves in real-time accuracy comparable to the CFD benchmark. Predicting the flow field and using it to calculate the corresponding overall performance renders the methodology generalisable, while filtering only relevant parts of the CFD solution makes the methodology scalable to industrial applications.", "link": "https://arxiv.org/abs/2306.05889"}, {"id": "2306.05905", "date": "Fri, 9 Jun 2023 14:01:26 GMT", "title": "TreeDQN: Learning to minimize Branch-and-Bound tree\n", "authors": ["Dmitry Sorokin", "Alexander Kostin\n"], "categories": ["cs.LG", "math.OC\nComments:", "Submitted", "to", "NeurIPS", "2023\n"], "abstract": "Combinatorial optimization problems require an exhaustive search to find the optimal solution. A convenient approach to solving combinatorial optimization tasks in the form of Mixed Integer Linear Programs is Branch-and-Bound. Branch-and-Bound solver splits a task into two parts dividing the domain of an integer variable, then it solves them recursively, producing a tree of nested sub-tasks. The efficiency of the solver depends on the branchning heuristic used to select a variable for splitting. In the present work, we propose a reinforcement learning method that can efficiently learn the branching heuristic. We view the variable selection task as a tree Markov Decision Process, prove that the Bellman operator adapted for the tree Markov Decision Process is contracting in mean, and propose a modified learning objective for the reinforcement learning agent. Our agent requires less training data and produces smaller trees compared to previous reinforcement learning methods.", "link": "https://arxiv.org/abs/2306.05905"}, {"id": "2306.05951", "date": "Fri, 9 Jun 2023 15:05:40 GMT", "title": "Prediction of Transportation Index for Urban Patterns in Small and\n\u00a0Medium-sized Indian Cities using Hybrid RidgeGAN Model\n", "authors": ["Rahisha Thottolil", "Uttam Kumar", "Tanujit Chakraborty\n"], "categories": ["cs.LG", "physics.geo-ph", "stat.AP\n"], "abstract": "The rapid urbanization trend in most developing countries including India is creating a plethora of civic concerns such as loss of green space, degradation of environmental health, clean water availability, air pollution, traffic congestion leading to delays in vehicular transportation, etc. Transportation and network modeling through transportation indices have been widely used to understand transportation problems in the recent past. This necessitates predicting transportation indices to facilitate sustainable urban planning and traffic management. Recent advancements in deep learning research, in particular, Generative Adversarial Networks (GANs), and their modifications in spatial data analysis such as CityGAN, Conditional GAN, and MetroGAN have enabled urban planners to simulate hyper-realistic urban patterns. These synthetic urban universes mimic global urban patterns and evaluating their landscape structures through spatial pattern analysis can aid in comprehending landscape dynamics, thereby enhancing sustainable urban planning. This research addresses several challenges in predicting the urban transportation index for small and medium-sized Indian cities. A hybrid framework based on Kernel Ridge Regression (KRR) and CityGAN is introduced to predict transportation index using spatial indicators of human settlement patterns. This paper establishes a relationship between the transportation index and human settlement indicators and models it using KRR for the selected 503 Indian cities. The proposed hybrid pipeline, we call it RidgeGAN model, can evaluate the sustainability of urban sprawl associated with infrastructure development and transportation systems in sprawling cities. Experimental results show that the two-step pipeline approach outperforms existing benchmarks based on spatial and statistical measures.", "link": "https://arxiv.org/abs/2306.05951"}, {"id": "2306.05952", "date": "Fri, 9 Jun 2023 15:09:16 GMT", "title": "Overcoming Adversarial Attacks for Human-in-the-Loop Applications\n", "authors": ["Ryan McCoppin", "Marla Kennedy", "Platon Lukyanenko", "Sean Kennedy\n"], "categories": ["cs.LG", "cs.CV\nComments:", "New", "Frontiers", "in", "Adversarial", "Machine", "Learning,", "ICML", "2022\n"], "abstract": "Including human analysis has the potential to positively affect the robustness of Deep Neural Networks and is relatively unexplored in the Adversarial Machine Learning literature. Neural network visual explanation maps have been shown to be prone to adversarial attacks. Further research is needed in order to select robust visualizations of explanations for the image analyst to evaluate a given model. These factors greatly impact Human-In-The-Loop (HITL) evaluation tools due to their reliance on adversarial images, including explanation maps and measurements of robustness. We believe models of human visual attention may improve interpretability and robustness of human-machine imagery analysis systems. Our challenge remains, how can HITL evaluation be robust in this adversarial landscape?", "link": "https://arxiv.org/abs/2306.05952"}, {"id": "2306.05955", "date": "Fri, 9 Jun 2023 15:11:49 GMT", "title": "Path Neural Networks: Expressive and Accurate Graph Neural Networks\n", "authors": ["Gaspard Michel", "Giannis Nikolentzos", "Johannes Lutzeyer", "Michalis\n\u00a0Vazirgiannis\n"], "categories": ["cs.LG", "stat.ML\nComments:", "Accepted", "at", "ICML", "2023\n"], "abstract": "Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.", "link": "https://arxiv.org/abs/2306.05955"}, {"id": "2306.05989", "date": "Fri, 9 Jun 2023 15:59:27 GMT", "title": "Quartile-Based Seasonality Decomposition for Time Series Forecasting and\n\u00a0Anomaly Detection\n", "authors": ["Ebenezer RHP Isaac and Bulbul Singh\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "The timely detection of anomalies is essential in the telecom domain as it facilitates the identification and characterization of irregular patterns, abnormal behaviors, and network anomalies, contributing to enhanced service quality and operational efficiency. Precisely forecasting and eliminating predictable time series patterns constitutes a vital component of time series anomaly detection. While the state-of-the-art methods aim to maximize forecasting accuracy, the computational performance takes a hit. In a system composed of a large number of time series variables, e.g., cell Key Performance Indicators (KPIs), the time and space complexity of the forecasting employed is of crucial importance. Quartile-Based Seasonality Decomposition (QBSD) is a live forecasting method proposed in this paper to make an optimal trade-off between computational complexity and forecasting accuracy. This paper compares the performance of QBSD to the state-of-the-art forecasting methods and their applicability to practical anomaly detection. To demonstrate the efficacy of the proposed solution, experimental evaluation was conducted using publicly available datasets as well as a telecom KPI dataset.", "link": "https://arxiv.org/abs/2306.05989"}, {"id": "2306.05998", "date": "Fri, 9 Jun 2023 16:10:26 GMT", "title": "Distributed Consensus Algorithm for Decision-Making in Multi-agent\n\u00a0Multi-armed Bandit\n", "authors": ["Xiaotong Cheng", "Setareh Maghsudi\n"], "categories": ["cs.LG", "cs.MA", "stat.ML\n"], "abstract": "We study a structured multi-agent multi-armed bandit (MAMAB) problem in a dynamic environment. A graph reflects the information-sharing structure among agents, and the arms' reward distributions are piecewise-stationary with several unknown change points. The agents face the identical piecewise-stationary MAB problem. The goal is to develop a decision-making policy for the agents that minimizes the regret, which is the expected total loss of not playing the optimal arm at each time step. Our proposed solution, Restarted Bayesian Online Change Point Detection in Cooperative Upper Confidence Bound Algorithm (RBO-Coop-UCB), involves an efficient multi-agent UCB algorithm as its core enhanced with a Bayesian change point detector. We also develop a simple restart decision cooperation that improves decision-making. Theoretically, we establish that the expected group regret of RBO-Coop-UCB is upper bounded by $\\mathcal{O}(KNM\\log T + K\\sqrt{MT\\log T})$, where K is the number of agents, M is the number of arms, and T is the number of time steps. Numerical experiments on synthetic and real-world datasets demonstrate that our proposed method outperforms the state-of-the-art algorithms.", "link": "https://arxiv.org/abs/2306.05998"}, {"id": "2306.06034", "date": "Fri, 9 Jun 2023 16:55:49 GMT", "title": "RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows\n", "authors": ["Shinjan Ghosh", "Amit Chakraborty", "Georgia Olympia Brikis", "Biswadip Dey\n"], "categories": ["cs.LG", "cs.NA", "math.NA", "physics.flu-dyn\n"], "abstract": "Physics-informed neural networks (PINNs) provide a framework to build surrogate models for dynamical systems governed by differential equations. During the learning process, PINNs incorporate a physics-based regularization term within the loss function to enhance generalization performance. Since simulating dynamics controlled by partial differential equations (PDEs) can be computationally expensive, PINNs have gained popularity in learning parametric surrogates for fluid flow problems governed by Navier-Stokes equations. In this work, we introduce RANS-PINN, a modified PINN framework, to predict flow fields (i.e., velocity and pressure) in high Reynolds number turbulent flow regime. To account for the additional complexity introduced by turbulence, RANS-PINN employs a 2-equation eddy viscosity model based on a Reynolds-averaged Navier-Stokes (RANS) formulation. Furthermore, we adopt a novel training approach that ensures effective initialization and balance among the various components of the loss function. The effectiveness of RANS-PINN framework is then demonstrated using a parametric PINN.", "link": "https://arxiv.org/abs/2306.06034"}, {"id": "2306.06041", "date": "Fri, 9 Jun 2023 17:07:04 GMT", "title": "A Dynamical Graph Prior for Relational Inference\n", "authors": ["Liming Pan", "Cheng Shi", "Ivan Dokmani\\'c\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "Relational inference aims to identify interactions between parts of a dynamical system from the observed dynamics. Current state-of-the-art methods fit a graph neural network (GNN) on a learnable graph to the dynamics. They use one-step message-passing GNNs -- intuitively the right choice since non-locality of multi-step or spectral GNNs may confuse direct and indirect interactions. But the \\textit{effective} interaction graph depends on the sampling rate and it is rarely localized to direct neighbors, leading to local minima for the one-step model. In this work, we propose a \\textit{dynamical graph prior} (DYGR) for relational inference. The reason we call it a prior is that, contrary to established practice, it constructively uses error amplification in high-degree non-local polynomial filters to generate good gradients for graph learning. To deal with non-uniqueness, DYGR simultaneously fits a ``shallow'' one-step model with shared graph topology. Experiments show that DYGR reconstructs graphs far more accurately than earlier methods, with remarkable robustness to under-sampling. Since appropriate sampling rates for unknown dynamical systems are not known a priori, this robustness makes DYGR suitable for real applications in scientific machine learning.", "link": "https://arxiv.org/abs/2306.06041"}, {"id": "2306.06098", "date": "Fri, 9 Jun 2023 17:58:47 GMT", "title": "Error Feedback Can Accurately Compress Preconditioners\n", "authors": ["Ionut-Vlad Modoranu", "Aleksei Kalinov", "Eldar Kurtic", "Dan Alistarh\n"], "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC\n"], "abstract": "Leveraging second-order information at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to medium-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via an efficient and simple-to-implement error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \\emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks for vision show that this approach can compress full-matrix preconditioners by up to two orders of magnitude without impact on accuracy, effectively removing the memory overhead of full-matrix preconditioning for implementations of full-matrix Adagrad (GGT) and natural gradient (M-FAC). Our code is available at https://github.com/IST-DASLab/EFCP.", "link": "https://arxiv.org/abs/2306.06098"}, {"id": "2306.05700", "date": "Fri, 9 Jun 2023 06:39:37 GMT", "title": "Finite-Time Analysis of Minimax Q-Learning for Two-Player Zero-Sum\n\u00a0Markov Games: Switching System Approach\n", "authors": ["Donghwan Lee\n"], "categories": ["eess.SY", "cs.GT", "cs.LG", "cs.SY\nComments:", "arXiv", "admin", "note:", "text", "overlap", "with", "arXiv:2205.05455\n"], "abstract": "The objective of this paper is to investigate the finite-time analysis of a Q-learning algorithm applied to two-player zero-sum Markov games. Specifically, we establish a finite-time analysis of both the minimax Q-learning algorithm and the corresponding value iteration method. To enhance the analysis of both value iteration and Q-learning, we employ the switching system model of minimax Q-learning and the associated value iteration. This approach provides further insights into minimax Q-learning and facilitates a more straightforward and insightful convergence analysis. We anticipate that the introduction of these additional insights has the potential to uncover novel connections and foster collaboration between concepts in the fields of control theory and reinforcement learning communities.", "link": "https://arxiv.org/abs/2306.05700"}, {"id": "2306.05514", "date": "Thu, 8 Jun 2023 19:07:22 GMT", "title": "Robust Brain Age Estimation via Regression Models and MRI-derived\n\u00a0Features\n", "authors": ["Mansoor Ahmed", "Usama Sardar", "Sarwan Ali", "Shafiq Alam", "Murray\n\u00a0Patterson", "Imdad Ullah Khan\n"], "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.NC\nComments:", "Published", "at", "the", "15th", "International", "Conference", "on", "Computational\n\u00a0Collective", "Intelligence\n"], "abstract": "The determination of biological brain age is a crucial biomarker in the assessment of neurological disorders and understanding of the morphological changes that occur during aging. Various machine learning models have been proposed for estimating brain age through Magnetic Resonance Imaging (MRI) of healthy controls. However, developing a robust brain age estimation (BAE) framework has been challenging due to the selection of appropriate MRI-derived features and the high cost of MRI acquisition. In this study, we present a novel BAE framework using the Open Big Healthy Brain (OpenBHB) dataset, which is a new multi-site and publicly available benchmark dataset that includes region-wise feature metrics derived from T1-weighted (T1-w) brain MRI scans of 3965 healthy controls aged between 6 to 86 years. Our approach integrates three different MRI-derived region-wise features and different regression models, resulting in a highly accurate brain age estimation with a Mean Absolute Error (MAE) of 3.25 years, demonstrating the framework's robustness. We also analyze our model's regression-based performance on gender-wise (male and female) healthy test groups. The proposed BAE framework provides a new approach for estimating brain age, which has important implications for the understanding of neurological disorders and age-related brain changes.", "link": "https://arxiv.org/abs/2306.05514"}, {"id": "2306.05812", "date": "Fri, 9 Jun 2023 11:05:09 GMT", "title": "HRTF upsampling with a generative adversarial network using a gnomonic\n\u00a0equiangular projection\n", "authors": ["Aidan O. T. Hogg", "Mads Jenkins", "He Liu", "Isaac Squires", "Samuel J.\n\u00a0Cooper and Lorenzo Picinali\n"], "categories": ["eess.AS", "cs.CV", "cs.HC", "cs.LG", "cs.SD", "eess.SP\nComments:", "13", "pages,", "9", "figures,", "Preprint", "(Submitted", "to", "Transactions", "on", "Audio,\n\u00a0Speech", "and", "Language", "Processing", "on", "the", "24", "Feb", "2023)\n"], "abstract": "An individualised head-related transfer function (HRTF) is essential for creating realistic virtual reality (VR) and augmented reality (AR) environments. However, acoustically measuring high-quality HRTFs requires expensive equipment and an acoustic lab setting. To overcome these limitations and to make this measurement more efficient HRTF upsampling has been exploited in the past where a high-resolution HRTF is created from a low-resolution one. This paper demonstrates how generative adversarial networks (GANs) can be applied to HRTF upsampling. We propose a novel approach that transforms the HRTF data for convenient use with a convolutional super-resolution generative adversarial network (SRGAN). This new approach is benchmarked against two baselines: barycentric upsampling and a HRTF selection approach. Experimental results show that the proposed method outperforms both baselines in terms of log-spectral distortion (LSD) and localisation performance using perceptual models when the input HRTF is sparse.", "link": "https://arxiv.org/abs/2306.05812"}, {"id": "2305.16255", "date": "Thu, 25 May 2023 17:10:54 GMT", "title": "Hierarchical forecasting for aggregated curves with an application to\n\u00a0day-ahead electricity price auctions\n", "authors": ["Paul Ghelasi", "Florian Ziel\n"], "categories": ["stat.AP", "cs.LG", "econ.EM", "q-fin.TR", "stat.ML\nComments:", "34", "pages,", "6", "figures.", "International", "Journal", "of", "Forecasting", "(2022)\nDOI:", "10.1016/j.ijforecast.2022.11.004\n"], "abstract": "Aggregated curves are common structures in economics and finance, and the most prominent examples are supply and demand curves. In this study, we exploit the fact that all aggregated curves have an intrinsic hierarchical structure, and thus hierarchical reconciliation methods can be used to improve the forecast accuracy. We provide an in-depth theory on how aggregated curves can be constructed or deconstructed, and conclude that these methods are equivalent under weak assumptions. We consider multiple reconciliation methods for aggregated curves, including previously established bottom-up, top-down, and linear optimal reconciliation approaches. We also present a new benchmark reconciliation method called 'aggregated-down' with similar complexity to bottom-up and top-down approaches, but it tends to provide better accuracy in this setup. We conducted an empirical forecasting study on the German day-ahead power auction market by predicting the demand and supply curves, where their equilibrium determines the electricity price for the next day. Our results demonstrate that hierarchical reconciliation methods can be used to improve the forecasting accuracy of aggregated curves.", "link": "https://arxiv.org/abs/2305.16255"}, {"id": "2306.05445", "date": "Thu, 8 Jun 2023 17:12:08 GMT", "title": "Towards Predicting Equilibrium Distributions for Molecular Systems with\n\u00a0Deep Learning\n", "authors": ["Shuxin Zheng", "Jiyan He", "Chang Liu", "Yu Shi", "Ziheng Lu", "Weitao Feng,\n\u00a0Fusong Ju", "Jiaxi Wang", "Jianwei Zhu", "Yaosen Min", "He Zhang", "Shidi Tang", "Hongxia\n\u00a0Hao", "Peiran Jin", "Chi Chen", "Frank No\\'e", "Haiguang Liu", "Tie-Yan Liu\n"], "categories": ["physics.chem-ph", "cs.LG", "q-bio.BM\nComments:", "80", "pages,", "11", "figures\n"], "abstract": "Advances in deep learning have greatly improved structure prediction of molecules. However, many macroscopic observations that are important for real-world applications are not functions of a single molecular structure, but rather determined from the equilibrium distribution of structures. Traditional methods for obtaining these distributions, such as molecular dynamics simulation, are computationally expensive and often intractable. In this paper, we introduce a novel deep learning framework, called Distributional Graphormer (DiG), in an attempt to predict the equilibrium distribution of molecular systems. Inspired by the annealing process in thermodynamics, DiG employs deep neural networks to transform a simple distribution towards the equilibrium distribution, conditioned on a descriptor of a molecular system, such as a chemical graph or a protein sequence. This framework enables efficient generation of diverse conformations and provides estimations of state densities. We demonstrate the performance of DiG on several molecular tasks, including protein conformation sampling, ligand structure sampling, catalyst-adsorbate sampling, and property-guided structure generation. DiG presents a significant advancement in methodology for statistically understanding molecular systems, opening up new research opportunities in molecular science.", "link": "https://arxiv.org/abs/2306.05445"}, {"id": "2306.05484", "date": "Thu, 8 Jun 2023 18:10:37 GMT", "title": "Task-specific experimental design for treatment effect estimation\n", "authors": ["Bethany Connolly", "Kim Moore", "Tobias Schwedes", "Alexander Adam", "Gary\n\u00a0Willis", "Ilya Feige", "Christopher Frye\n"], "categories": ["stat.ME", "cs.LG", "stat.ML\nComments:", "To", "appear", "in", "ICML", "2023;", "8", "pages,", "7", "figures,", "4", "appendices\n"], "abstract": "Understanding causality should be a core requirement of any attempt to build real impact through AI. Due to the inherent unobservability of counterfactuals, large randomised trials (RCTs) are the standard for causal inference. But large experiments are generically expensive, and randomisation carries its own costs, e.g. when suboptimal decisions are trialed. Recent work has proposed more sample-efficient alternatives to RCTs, but these are not adaptable to the downstream application for which the causal effect is sought. In this work, we develop a task-specific approach to experimental design and derive sampling strategies customised to particular downstream applications. Across a range of important tasks, real-world datasets, and sample sizes, our method outperforms other benchmarks, e.g. requiring an order-of-magnitude less data to match RCT performance on targeted marketing tasks.", "link": "https://arxiv.org/abs/2306.05484"}, {"id": "2306.05494", "date": "Thu, 8 Jun 2023 18:32:08 GMT", "title": "Adversarial Evasion Attacks Practicality in Networks: Testing the Impact\n\u00a0of Dynamic Learning\n", "authors": ["Mohamed el Shehaby and Ashraf Matrawy\n"], "categories": ["cs.CR", "cs.LG", "cs.NI\n"], "abstract": "Machine Learning (ML) has become ubiquitous, and its deployment in Network Intrusion Detection Systems (NIDS) is inevitable due to its automated nature and high accuracy in processing and classifying large volumes of data. However, ML has been found to have several flaws, on top of them are adversarial attacks, which aim to trick ML models into producing faulty predictions. While most adversarial attack research focuses on computer vision datasets, recent studies have explored the practicality of such attacks against ML-based network security entities, especially NIDS. This paper presents two distinct contributions: a taxonomy of practicality issues associated with adversarial attacks against ML-based NIDS and an investigation of the impact of continuous training on adversarial attacks against NIDS. Our experiments indicate that continuous re-training, even without adversarial training, can reduce the effect of adversarial attacks. While adversarial attacks can harm ML-based NIDSs, our aim is to highlight that there is a significant gap between research and real-world practicality in this domain which requires attention.", "link": "https://arxiv.org/abs/2306.05494"}, {"id": "2306.05666", "date": "Fri, 9 Jun 2023 04:40:38 GMT", "title": "QuestEnvSim: Environment-Aware Simulated Motion Tracking from Sparse\n\u00a0Sensors\n", "authors": ["Sunmin Lee", "Sebastian Starke", "Yuting Ye", "Jungdam Won", "and Alexander\n\u00a0Winkler\n"], "categories": ["cs.GR", "cs.LG", "cs.RO\nACM-class:", "I.3.6\nJournal-ref:", "SIGGRAPH", "23", "Conference", "Proceedings,", "August", "6-10,", "2023,", "Los\n\u00a0Angeles,", "CA,", "USA\nDOI:", "10.1145/3588432.3591504\n"], "abstract": "Replicating a user's pose from only wearable sensors is important for many AR/VR applications. Most existing methods for motion tracking avoid environment interaction apart from foot-floor contact due to their complex dynamics and hard constraints. However, in daily life people regularly interact with their environment, e.g. by sitting on a couch or leaning on a desk. Using Reinforcement Learning, we show that headset and controller pose, if combined with physics simulation and environment observations can generate realistic full-body poses even in highly constrained environments. The physics simulation automatically enforces the various constraints necessary for realistic poses, instead of manually specifying them as in many kinematic approaches. These hard constraints allow us to achieve high-quality interaction motions without typical artifacts such as penetration or contact sliding. We discuss three features, the environment representation, the contact reward and scene randomization, crucial to the performance of the method. We demonstrate the generality of the approach through various examples, such as sitting on chairs, a couch and boxes, stepping over boxes, rocking a chair and turning an office chair. We believe these are some of the highest-quality results achieved for motion tracking from sparse sensor with scene interaction.", "link": "https://arxiv.org/abs/2306.05666"}, {"id": "2306.05708", "date": "Fri, 9 Jun 2023 07:02:43 GMT", "title": "Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion\n", "authors": ["Haogeng Liu", "Tao Wang", "Jie Cao", "Ran He", "Jianhua Tao\n"], "categories": ["cs.SD", "cs.LG", "eess.AS\n"], "abstract": "Denoising Diffusion Probabilistic Models have shown extraordinary ability on various generative tasks. However, their slow inference speed renders them impractical in speech synthesis. This paper proposes a linear diffusion model (LinDiff) based on an ordinary differential equation to simultaneously reach fast inference and high sample quality. Firstly, we employ linear interpolation between the target and noise to design a diffusion sequence for training, while previously the diffusion path that links the noise and target is a curved segment. When decreasing the number of sampling steps (i.e., the number of line segments used to fit the path), the ease of fitting straight lines compared to curves allows us to generate higher quality samples from a random noise with fewer iterations. Secondly, to reduce computational complexity and achieve effective global modeling of noisy speech, LinDiff employs a patch-based processing approach that partitions the input signal into small patches. The patch-wise token leverages Transformer architecture for effective modeling of global information. Adversarial training is used to further improve the sample quality with decreased sampling steps. We test proposed method with speech synthesis conditioned on acoustic feature (Mel-spectrograms). Experimental results verify that our model can synthesize high-quality speech even with only one diffusion step. Both subjective and objective evaluations demonstrate that our model can synthesize speech of a quality comparable to that of autoregressive models with faster synthesis speed (3 diffusion steps).", "link": "https://arxiv.org/abs/2306.05708"}, {"id": "2306.05862", "date": "Fri, 9 Jun 2023 12:53:24 GMT", "title": "Federated Learning You May Communicate Less Often!\n", "authors": ["Milad Sefidgaran", "Romain Chor", "Abdellatif Zaidi", "Yijun Wan\n"], "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT\n"], "abstract": "We investigate the generalization error of statistical learning models in a Federated Learning (FL) setting. Specifically, we study the evolution of the generalization error with the number of communication rounds between the clients and the parameter server, i.e., the effect on the generalization error of how often the local models as computed by the clients are aggregated at the parameter server. We establish PAC-Bayes and rate-distortion theoretic bounds on the generalization error that account explicitly for the effect of the number of rounds, say $ R \\in \\mathbb{N}$, in addition to the number of participating devices $K$ and individual datasets size $n$. The bounds, which apply in their generality for a large class of loss functions and learning algorithms, appear to be the first of their kind for the FL setting. Furthermore, we apply our bounds to FL-type Support Vector Machines (FSVM); and we derive (more) explicit bounds on the generalization error in this case. In particular, we show that the generalization error of FSVM increases with $R$, suggesting that more frequent communication with the parameter server diminishes the generalization power of such learning algorithms. Combined with that the empirical risk generally decreases for larger values of $R$, this indicates that $R$ might be a parameter to optimize in order to minimize the population risk of FL algorithms. Moreover, specialized to the case $R=1$ (sometimes referred to as \"one-shot\" FL or distributed learning) our bounds suggest that the generalization error of the FL setting decreases faster than that of centralized learning by a factor of $\\mathcal{O}(\\sqrt{\\log(K)/K})$, thereby generalizing recent findings in this direction to arbitrary loss functions and algorithms. The results of this paper are also validated on some experiments.", "link": "https://arxiv.org/abs/2306.05862"}, {"id": "2306.05937", "date": "Fri, 9 Jun 2023 14:56:06 GMT", "title": "Robust Data-driven Prescriptiveness Optimization\n", "authors": ["Mehran Poursoltani", "Erick Delage", "Angelos Georghiou\n"], "categories": ["math.OC", "cs.LG", "stat.ME\n"], "abstract": "The abundance of data has led to the emergence of a variety of optimization techniques that attempt to leverage available side information to provide more anticipative decisions. The wide range of methods and contexts of application have motivated the design of a universal unitless measure of performance known as the coefficient of prescriptiveness. This coefficient was designed to quantify both the quality of contextual decisions compared to a reference one and the prescriptive power of side information. To identify policies that maximize the former in a data-driven context, this paper introduces a distributionally robust contextual optimization model where the coefficient of prescriptiveness substitutes for the classical empirical risk minimization objective. We present a bisection algorithm to solve this model, which relies on solving a series of linear programs when the distributional ambiguity set has an appropriate nested form and polyhedral structure. Studying a contextual shortest path problem, we evaluate the robustness of the resulting policies against alternative methods when the out-of-sample dataset is subject to varying amounts of distribution shift.", "link": "https://arxiv.org/abs/2306.05937"}, {"id": "2306.05987", "date": "Fri, 9 Jun 2023 15:56:06 GMT", "title": "Agent market orders representation through a contrastive learning\n\u00a0approach\n", "authors": ["Ruihua Ruan", "Emmanuel Bacry", "Jean-Fran\\c{c}ois Muzy\n"], "categories": ["q-fin.ST", "cs.LG", "stat.ML\n"], "abstract": "Due to the access to the labeled orders on the CAC40 data from Euronext, we are able to analyse agents' behaviours in the market based on their placed orders. In this study, we construct a self-supervised learning model using triplet loss to effectively learn the representation of agent market orders. By acquiring this learned representation, various downstream tasks become feasible. In this work, we utilise the K-means clustering algorithm on the learned representation vectors of agent orders to identify distinct behaviour types within each cluster.", "link": "https://arxiv.org/abs/2306.05987"}, {"id": "2306.06031", "date": "Fri, 9 Jun 2023 16:52:00 GMT", "title": "FinGPT: Open-Source Financial Large Language Models\n", "authors": ["Hongyang Yang", "Xiao-Yang Liu", "Christina Dan Wang\n"], "categories": ["q-fin.ST", "cs.CL", "cs.LG", "q-fin.TR\n"], "abstract": "Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data. In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT} and \\url{https://github.com/AI4Finance-Foundation/FinNLP}", "link": "https://arxiv.org/abs/2306.06031"}, {"id": "2306.06040", "date": "Fri, 9 Jun 2023 17:05:53 GMT", "title": "Reconstructing Human Expressiveness in Piano Performances with a\n\u00a0Transformer Network\n", "authors": ["Jingjing Tang", "Geraint Wiggins", "George Fazekas\n"], "categories": ["cs.SD", "cs.LG", "eess.AS\nComments:", "12", "pages,", "5", "figures,", "submitted", "to", "CMMR", "2023\n"], "abstract": "Capturing intricate and subtle variations in human expressiveness in music performance using computational approaches is challenging. In this paper, we propose a novel approach for reconstructing human expressiveness in piano performance with a multi-layer bi-directional Transformer encoder. To address the needs for large amounts of accurately captured and score-aligned performance data in training neural networks, we use transcribed scores obtained from an existing transcription model to train our model. We integrate pianist identities to control the sampling process and explore the ability of our system to model variations in expressiveness for different pianists. The system is evaluated through statistical analysis of generated expressive performances and a listening test. Overall, the results suggest that our method achieves state-of-the-art in generating human-like piano performances from transcribed scores, while fully and consistently reconstructing human expressiveness poses further challenges.", "link": "https://arxiv.org/abs/2306.06040"}, {"id": "2306.06083", "date": "Tue, 6 Jun 2023 21:13:08 GMT", "title": "Improving Fairness and Robustness in End-to-End Speech Recognition\n\u00a0through unsupervised clustering\n", "authors": ["Irina-Elena Veliche", "Pascale Fung\n"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS\nJournal-ref:", "ICASSP", "2023\n"], "abstract": "The challenge of fairness arises when Automatic Speech Recognition (ASR) systems do not perform equally well for all sub-groups of the population. In the past few years there have been many improvements in overall speech recognition quality, but without any particular focus on advancing Equality and Equity for all user groups for whom systems do not perform well. ASR fairness is therefore also a robustness issue. Meanwhile, data privacy also takes priority in production systems. In this paper, we present a privacy preserving approach to improve fairness and robustness of end-to-end ASR without using metadata, zip codes, or even speaker or utterance embeddings directly in training. We extract utterance level embeddings using a speaker ID model trained on a public dataset, which we then use in an unsupervised fashion to create acoustic clusters. We use cluster IDs instead of speaker utterance embeddings as extra features during model training, which shows improvements for all demographic groups and in particular for different accents.", "link": "https://arxiv.org/abs/2306.06083"}, {"id": "2306.06099", "date": "Fri, 9 Jun 2023 17:59:16 GMT", "title": "NuCLR: Nuclear Co-Learned Representations\n", "authors": ["Ouail Kitouni", "Niklas Nolte", "Sokratis Trifinopoulos", "Subhash\n\u00a0Kantamneni", "Mike Williams\n"], "categories": ["nucl-th", "cs.LG", "nucl-ex\nComments:", "5", "pages,", "3", "figures\n"], "abstract": "We introduce Nuclear Co-Learned Representations (NuCLR), a deep learning model that predicts various nuclear observables, including binding and decay energies, and nuclear charge radii. The model is trained using a multi-task approach with shared representations and obtains state-of-the-art performance, achieving levels of precision that are crucial for understanding fundamental phenomena in nuclear (astro)physics. We also report an intriguing finding that the learned representation of NuCLR exhibits the prominent emergence of crucial aspects of the nuclear shell model, namely the shell structure, including the well-known magic numbers, and the Pauli Exclusion Principle. This suggests that the model is capable of capturing the underlying physical principles and that our approach has the potential to offer valuable insights into nuclear theory.", "link": "https://arxiv.org/abs/2306.06099"}, {"id": "2306.05490", "date": "Thu, 8 Jun 2023 18:22:46 GMT", "title": "Learnability with PAC Semantics for Multi-agent Beliefs\n", "authors": ["Ionela G. Mocanu", "Vaishak Belle and Brendan Juba\n"], "categories": ["cs.AI", "cs.LO\n"], "abstract": "The tension between deduction and induction is perhaps the most fundamental issue in areas such as philosophy, cognition and artificial intelligence. In an influential paper, Valiant recognised that the challenge of learning should be integrated with deduction. In particular, he proposed a semantics to capture the quality possessed by the output of Probably Approximately Correct (PAC) learning algorithms when formulated in a logic. Although weaker than classical entailment, it allows for a powerful model-theoretic framework for answering queries. In this paper, we provide a new technical foundation to demonstrate PAC learning with multi-agent epistemic logics. To circumvent the negative results in the literature on the difficulty of robust learning with the PAC semantics, we consider so-called implicit learning where we are able to incorporate observations to the background theory in service of deciding the entailment of an epistemic query. We prove correctness of the learning procedure and discuss results on the sample complexity, that is how many observations we will need to provably assert that the query is entailed given a user-specified error bound. Finally, we investigate under what circumstances this algorithm can be made efficient. On the last point, given that reasoning in epistemic logics especially in multi-agent epistemic logics is PSPACE-complete, it might seem like there is no hope for this problem. We leverage some recent results on the so-called Representation Theorem explored for single-agent and multi-agent epistemic logics with the only knowing operator to reduce modal reasoning to propositional reasoning.", "link": "https://arxiv.org/abs/2306.05490"}, {"id": "2306.05582", "date": "Thu, 8 Jun 2023 22:46:31 GMT", "title": "A newborn embodied Turing test for view-invariant object recognition\n", "authors": ["Denizhan Pak", "Donsuk Lee", "Samantha M. W. Wood", "Justin N. Wood\n"], "categories": ["cs.AI", "q-bio.NC\nComments:", "7", "Pages.", "4", "figures,", "1", "table.", "This", "paper", "was", "accepted", "to", "the", "CogSci\n\u00a02023", "Conference.", "(https://cognitivesciencesociety.org/)\n"], "abstract": "Recent progress in artificial intelligence has renewed interest in building machines that learn like animals. Almost all of the work comparing learning across biological and artificial systems comes from studies where animals and machines received different training data, obscuring whether differences between animals and machines emerged from differences in learning mechanisms versus training data. We present an experimental approach-a \"newborn embodied Turing Test\"-that allows newborn animals and machines to be raised in the same environments and tested with the same tasks, permitting direct comparison of their learning abilities. To make this platform, we first collected controlled-rearing data from newborn chicks, then performed \"digital twin\" experiments in which machines were raised in virtual environments that mimicked the rearing conditions of the chicks. We found that (1) machines (deep reinforcement learning agents with intrinsic motivation) can spontaneously develop visually guided preference behavior, akin to imprinting in newborn chicks, and (2) machines are still far from newborn-level performance on object recognition tasks. Almost all of the chicks developed view-invariant object recognition, whereas the machines tended to develop view-dependent recognition. The learning outcomes were also far more constrained in the chicks versus machines. Ultimately, we anticipate that this approach will help researchers develop embodied AI systems that learn like newborn animals.", "link": "https://arxiv.org/abs/2306.05582"}, {"id": "2306.05747", "date": "Fri, 9 Jun 2023 08:24:56 GMT", "title": "An End-to-End Reinforcement Learning Approach for Job-Shop Scheduling\n\u00a0Problems Based on Constraint Programming\n", "authors": ["Pierre Tassel", "Martin Gebser", "Konstantin Schekotihin\n"], "categories": ["cs.AI", "cs.LG\nComments:", "To", "be", "published", "at", "ICAPS", "2023\n"], "abstract": "Constraint Programming (CP) is a declarative programming paradigm that allows for modeling and solving combinatorial optimization problems, such as the Job-Shop Scheduling Problem (JSSP). While CP solvers manage to find optimal or near-optimal solutions for small instances, they do not scale well to large ones, i.e., they require long computation times or yield low-quality solutions. Therefore, real-world scheduling applications often resort to fast, handcrafted, priority-based dispatching heuristics to find a good initial solution and then refine it using optimization methods. This paper proposes a novel end-to-end approach to solving scheduling problems by means of CP and Reinforcement Learning (RL). In contrast to previous RL methods, tailored for a given problem by including procedural simulation algorithms, complex feature engineering, or handcrafted reward functions, our neural-network architecture and training algorithm merely require a generic CP encoding of some scheduling problem along with a set of small instances. Our approach leverages existing CP solvers to train an agent learning a Priority Dispatching Rule (PDR) that generalizes well to large instances, even from separate datasets. We evaluate our method on seven JSSP datasets from the literature, showing its ability to find higher-quality solutions for very large instances than obtained by static PDRs and by a CP solver within the same time limit.", "link": "https://arxiv.org/abs/2306.05747"}, {"id": "2306.06067", "date": "Fri, 9 Jun 2023 17:43:49 GMT", "title": "Combining a Meta-Policy and Monte-Carlo Planning for Scalable Type-Based\n\u00a0Reasoning in Partially Observable Environments\n", "authors": ["Jonathon Schwartz", "Hanna Kurniawati", "Marcus Hutter\n"], "categories": ["cs.AI", "cs.MA\nComments:", "24", "pages\n"], "abstract": "The design of autonomous agents that can interact effectively with other agents without prior coordination is a core problem in multi-agent systems. Type-based reasoning methods achieve this by maintaining a belief over a set of potential behaviours for the other agents. However, current methods are limited in that they assume full observability of the state and actions of the other agent or do not scale efficiently to larger problems with longer planning horizons. Addressing these limitations, we propose Partially Observable Type-based Meta Monte-Carlo Planning (POTMMCP) - an online Monte-Carlo Tree Search based planning method for type-based reasoning in large partially observable environments. POTMMCP incorporates a novel meta-policy for guiding search and evaluating beliefs, allowing it to search more effectively to longer horizons using less planning time. We show that our method converges to the optimal solution in the limit and empirically demonstrate that it effectively adapts online to diverse sets of other agents across a range of environments. Comparisons with the state-of-the art method on problems with up to $10^{14}$ states and $10^8$ observations indicate that POTMMCP is able to compute better solutions significantly faster.", "link": "https://arxiv.org/abs/2306.06067"}, {"id": "2306.05493", "date": "Thu, 8 Jun 2023 18:31:56 GMT", "title": "Multi-Modal Classifiers for Open-Vocabulary Object Detection\n", "authors": ["Prannay Kaul", "Weidi Xie", "Andrew Zisserman\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\nComments:", "ICML", "2023,", "project", "page:\n\u00a0https://www.robots.ox.ac.uk/vgg/research/mm-ovod/\nACM-class:", "I.4.6;", "I.4.8;", "I.4.9;", "I.2.10\n"], "abstract": "The goal of this paper is open-vocabulary object detection (OVOD) $\\unicode{x2013}$ building a model that can detect objects beyond the set of categories seen at training, thus enabling the user to specify categories of interest at inference without the need for model retraining. We adopt a standard two-stage object detector architecture, and explore three ways for specifying novel categories: via language descriptions, via image exemplars, or via a combination of the two. We make three contributions: first, we prompt a large language model (LLM) to generate informative language descriptions for object classes, and construct powerful text-based classifiers; second, we employ a visual aggregator on image exemplars that can ingest any number of images as input, forming vision-based classifiers; and third, we provide a simple method to fuse information from language descriptions and image exemplars, yielding a multi-modal classifier. When evaluating on the challenging LVIS open-vocabulary benchmark we demonstrate that: (i) our text-based classifiers outperform all previous OVOD works; (ii) our vision-based classifiers perform as well as text-based classifiers in prior work; (iii) using multi-modal classifiers perform better than either modality alone; and finally, (iv) our text-based and multi-modal classifiers yield better performance than a fully-supervised detector.", "link": "https://arxiv.org/abs/2306.05493"}, {"id": "2306.05642", "date": "Fri, 9 Jun 2023 03:02:36 GMT", "title": "Customizing General-Purpose Foundation Models for Medical Report\n\u00a0Generation\n", "authors": ["Bang Yang", "Asif Raza", "Yuexian Zou", "Tong Zhang\n"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR\nComments:", "14", "pages,", "3", "figures\n"], "abstract": "Medical caption prediction which can be regarded as a task of medical report generation (MRG), requires the automatic generation of coherent and accurate captions for the given medical images. However, the scarcity of labelled medical image-report pairs presents great challenges in the development of deep and large-scale neural networks capable of harnessing the potential artificial general intelligence power like large language models (LLMs). In this work, we propose customizing off-the-shelf general-purpose large-scale pre-trained models, i.e., foundation models (FMs), in computer vision and natural language processing with a specific focus on medical report generation. Specifically, following BLIP-2, a state-of-the-art vision-language pre-training approach, we introduce our encoder-decoder-based MRG model. This model utilizes a lightweight query Transformer to connect two FMs: the giant vision Transformer EVA-ViT-g and a bilingual LLM trained to align with human intentions (referred to as ChatGLM-6B). Furthermore, we conduct ablative experiments on the trainable components of the model to identify the crucial factors for effective transfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn medical image representations, followed by parameter-efficient training of ChatGLM-6B to capture the writing styles of medical reports, is essential for achieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and the 2nd, respectively, out of 13 participating teams, based on the BERTScore and ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction Task competition.", "link": "https://arxiv.org/abs/2306.05642"}, {"id": "2306.05668", "date": "Fri, 9 Jun 2023 04:49:31 GMT", "title": "RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models\n", "authors": ["Xingchen Zhou", "Ying He", "F. Richard Yu", "Jianqiang Li", "You Li\n"], "categories": ["cs.CV", "cs.AI", "cs.GR\nComments:", "IJCAI", "2023", "Accepted", "(Main", "Track)\n"], "abstract": "The emergence of Neural Radiance Fields (NeRF) has promoted the development of synthesized high-fidelity views of the intricate real world. However, it is still a very demanding task to repaint the content in NeRF. In this paper, we propose a novel framework that can take RGB images as input and alter the 3D content in neural scenes. Our work leverages existing diffusion models to guide changes in the designated 3D content. Specifically, we semantically select the target object and a pre-trained diffusion model will guide the NeRF model to generate new 3D objects, which can improve the editability, diversity, and application range of NeRF. Experiment results show that our algorithm is effective for editing 3D objects in NeRF under different text prompts, including editing appearance, shape, and more. We validate our method on both real-world datasets and synthetic-world datasets for these editing tasks. Please visit https://repaintnerf.github.io for a better view of our results.", "link": "https://arxiv.org/abs/2306.05668"}, {"id": "2306.05720", "date": "Fri, 9 Jun 2023 07:34:34 GMT", "title": "Beyond Surface Statistics: Scene Representations in a Latent Diffusion\n\u00a0Model\n", "authors": ["Yida Chen", "Fernanda Vi\\'egas", "Martin Wattenberg\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\nComments:", "17", "pages,", "13", "figures\n"], "abstract": "Latent diffusion models (LDMs) exhibit an impressive ability to produce realistic images, yet the inner workings of these models remain mysterious. Even when trained purely on images without explicit depth information, they typically output coherent pictures of 3D scenes. In this work, we investigate a basic interpretability question: does an LDM create and use an internal representation of simple scene geometry? Using linear probes, we find evidence that the internal activations of the LDM encode linear representations of both 3D depth data and a salient-object / background distinction. These representations appear surprisingly early in the denoising process$-$well before a human can easily make sense of the noisy images. Intervention experiments further indicate these representations play a causal role in image synthesis, and may be used for simple high-level editing of an LDM's output.", "link": "https://arxiv.org/abs/2306.05720"}, {"id": "2306.05963", "date": "Fri, 9 Jun 2023 15:29:54 GMT", "title": "Adaptive Contextual Perception: How to Generalize to New Backgrounds and\n\u00a0Ambiguous Objects\n", "authors": ["Zhuofan Ying", "Peter Hase", "Mohit Bansal\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\nComments:", "21", "pages,", "12", "figures.", "Our", "code", "is", "available", "at\n\u00a0https://github.com/zfying/AdaptiveContext\n"], "abstract": "Biological vision systems make adaptive use of context to recognize objects in new settings with novel contexts as well as occluded or blurry objects in familiar settings. In this paper, we investigate how vision models adaptively use context for out-of-distribution (OOD) generalization and leverage our analysis results to improve model OOD generalization. First, we formulate two distinct OOD settings where the contexts are either irrelevant (Background-Invariance) or beneficial (Object-Disambiguation), reflecting the diverse contextual challenges faced in biological vision. We then analyze model performance in these two different OOD settings and demonstrate that models that excel in one setting tend to struggle in the other. Notably, prior works on learning causal features improve on one setting but hurt in the other. This underscores the importance of generalizing across both OOD settings, as this ability is crucial for both human cognition and robust AI systems. Next, to better understand the model properties contributing to OOD generalization, we use representational geometry analysis and our own probing methods to examine a population of models, and we discover that those with more factorized representations and appropriate feature weighting are more successful in handling Background-Invariance and Object-Disambiguation tests. We further validate these findings through causal intervention on representation factorization and feature weighting to demonstrate their causal effect on performance. Lastly, we propose new augmentation methods to enhance model generalization. These methods outperform strong baselines, yielding improvements in both in-distribution and OOD tests. In conclusion, to replicate the generalization abilities of biological vision, computer vision models must have factorized object vs. background representations and appropriately weight both kinds of features.", "link": "https://arxiv.org/abs/2306.05963"}, {"id": "2306.06077", "date": "Mon, 5 Jun 2023 17:22:54 GMT", "title": "Visually-Grounded Descriptions Improve Zero-Shot Image Classification\n", "authors": ["Michael Ogezi", "Bradley Hauer", "Grzegorz Kondrak\n"], "categories": ["cs.CV", "cs.AI", "cs.CL\n"], "abstract": "Language-vision models like CLIP have made significant progress in zero-shot vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive class descriptions remains a major challenge. Existing approaches suffer from granularity and label ambiguity issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel method leveraging modern language models and semantic knowledge bases to produce visually-grounded class descriptions. We demonstrate V-GLOSS's effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets including ImageNet and STL-10. In addition, we introduce a silver dataset with class descriptions generated by V-GLOSS, and show its usefulness for vision tasks. We make available our code and dataset.", "link": "https://arxiv.org/abs/2306.06077"}, {"id": "2306.06081", "date": "Thu, 25 May 2023 09:04:31 GMT", "title": "CARSO: Counter-Adversarial Recall of Synthetic Observations\n", "authors": ["Emanuele Ballarin", "Alessio Ansuini", "Luca Bortolussi\n"], "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG\nComments:", "20", "pages,", "5", "figures,", "10", "tables\n"], "abstract": "In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/emaballarin/CARSO .", "link": "https://arxiv.org/abs/2306.06081"}, {"id": "2306.06094", "date": "Fri, 9 Jun 2023 17:57:01 GMT", "title": "Leveraging Large Language Models for Scalable Vector Graphics-Driven\n\u00a0Image Understanding\n", "authors": ["Mu Cai", "Zeyi Huang", "Yuheng Li", "Haohan Wang", "Yong Jae Lee\n"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG\n"], "abstract": "Recently, large language models (LLMs) have made significant advancements in natural language understanding and generation. However, their potential in computer vision remains largely unexplored. In this paper, we introduce a new, exploratory approach that enables LLMs to process images using the Scalable Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities, allowing LLMs to directly understand and manipulate images without the need for parameterized visual components. Our method facilitates simple image classification, generation, and in-context learning using only LLM capabilities. We demonstrate the promise of our approach across discriminative and generative tasks, highlighting its (i) robustness against distribution shift, (ii) substantial improvements achieved by tapping into the in-context learning abilities of LLMs, and (iii) image understanding and generation capabilities with human guidance. Our code, data, and models can be found here https://github.com/mu-cai/svg-llm.", "link": "https://arxiv.org/abs/2306.06094"}, {"id": "2306.05562", "date": "Thu, 8 Jun 2023 21:07:15 GMT", "title": "AircraftVerse: A Large-Scale Multimodal Dataset of Aerial Vehicle\n\u00a0Designs\n", "authors": ["Adam D. Cobb", "Anirban Roy", "Daniel Elenius", "F. Michael Heim", "Brian\n\u00a0Swenson", "Sydney Whittington", "James D. Walker", "Theodore Bapty", "Joseph Hite,\n\u00a0Karthik Ramani", "Christopher McComb", "Susmit Jha\n"], "categories": ["cs.RO", "cs.AI", "cs.CE\nComments:", "The", "dataset", "is", "hosted", "at", "https://zenodo.org/record/6525446,", "baseline\n\u00a0models", "and", "code", "at", "https://github.com/SRI-CSL/AircraftVerse,", "and", "the", "dataset\n\u00a0description", "at", "https://aircraftverse.onrender.com/\n"], "abstract": "We present AircraftVerse, a publicly available aerial vehicle design dataset. Aircraft design encompasses different physics domains and, hence, multiple modalities of representation. The evaluation of these cyber-physical system (CPS) designs requires the use of scientific analytical and simulation models ranging from computer-aided design tools for structural and manufacturing analysis, computational fluid dynamics tools for drag and lift computation, battery models for energy estimation, and simulation models for flight control and dynamics. AircraftVerse contains 27,714 diverse air vehicle designs - the largest corpus of engineering designs with this level of complexity. Each design comprises the following artifacts: a symbolic design tree describing topology, propulsion subsystem, battery subsystem, and other design details; a STandard for the Exchange of Product (STEP) model data; a 3D CAD design using a stereolithography (STL) file format; a 3D point cloud for the shape of the design; and evaluation results from high fidelity state-of-the-art physics models that characterize performance metrics such as maximum flight distance and hover-time. We also present baseline surrogate models that use different modalities of design representation to predict design performance metrics, which we provide as part of our dataset release. Finally, we discuss the potential impact of this dataset on the use of learning in aircraft design and, more generally, in CPS. AircraftVerse is accompanied by a data card, and it is released under Creative Commons Attribution-ShareAlike (CC BY-SA) license. The dataset is hosted at https://zenodo.org/record/6525446, baseline models and code at https://github.com/SRI-CSL/AircraftVerse, and the dataset description at https://aircraftverse.onrender.com/.", "link": "https://arxiv.org/abs/2306.05562"}, {"id": "2306.05446", "date": "Thu, 8 Jun 2023 17:28:28 GMT", "title": "Latent Phrase Matching for Dysarthric Speech\n", "authors": ["Colin Lea", "Dianna Yee", "Jaya Narain", "Zifang Huang", "Lauren Tooley,\n\u00a0Jeffrey P. Bigham", "Leah Findlater\n"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG\n"], "abstract": "Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies have emphasized interest in personalized speech models from people with atypical speech patterns. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases.", "link": "https://arxiv.org/abs/2306.05446"}, {"id": "2306.05499", "date": "Thu, 8 Jun 2023 18:43:11 GMT", "title": "Prompt Injection attack against LLM-integrated Applications\n", "authors": ["Yi Liu", "Gelei Deng", "Yuekang Li", "Kailong Wang", "Tianwei Zhang", "Yepang\n\u00a0Liu", "Haoyu Wang", "Yan Zheng and Yang Liu\n"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.SE\n"], "abstract": "Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.", "link": "https://arxiv.org/abs/2306.05499"}, {"id": "2306.05500", "date": "Sat, 3 Jun 2023 21:39:07 GMT", "title": "Word-Level Explanations for Analyzing Bias in Text-to-Image Models\n", "authors": ["Alexander Lin", "Lucas Monteiro Paes", "Sree Harsha Tanneru", "Suraj\n\u00a0Srinivas", "Himabindu Lakkaraju\n"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG\nComments:", "5", "main", "pages,", "3", "pages", "in", "appendix,", "and", "3", "figures\n"], "abstract": "Text-to-image models take a sentence (i.e., prompt) and generate images associated with this input prompt. These models have created award wining-art, videos, and even synthetic datasets. However, text-to-image (T2I) models can generate images that underrepresent minorities based on race and sex. This paper investigates which word in the input prompt is responsible for bias in generated images. We introduce a method for computing scores for each word in the prompt; these scores represent its influence on biases in the model's output. Our method follows the principle of \\emph{explaining by removing}, leveraging masked language models to calculate the influence scores. We perform experiments on Stable Diffusion to demonstrate that our method identifies the replication of societal stereotypes in generated images.", "link": "https://arxiv.org/abs/2306.05500"}, {"id": "2306.05523", "date": "Mon, 22 May 2023 08:29:47 GMT", "title": "FACTIFY3M: A Benchmark for Multimodal Fact Verification with\n\u00a0Explainability through 5W Question-Answering\n", "authors": ["Megha Chakraborty", "Khusbu Pahwa", "Anku Rani", "Adarsh Mahor", "Aditya\n\u00a0Pakala", "Arghya Sarkar", "Harshit Dave", "Ishan Paul", "Janvita Reddy", "Preethi\n\u00a0Gurumurthy", "Ritvik G", "Samahriti Mukherjee", "Shreyas Chatterjee", "Kinjal\n\u00a0Sensharma", "Dwip Dalal", "Suryavardan S", "Shreyash Mishra", "Parth Patwa", "Aman\n\u00a0Chadha", "Amit Sheth", "Amitava Das\n"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM\nComments:", "arXiv", "admin", "note:", "text", "overlap", "with", "arXiv:2305.04329\n"], "abstract": "Combating disinformation is one of the burning societal crises -- about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public opinion, causing disruption in the share market, panic and anxiety in society, and even death during crises. Therefore, disinformation should be identified promptly and, if possible, mitigated. With approximately 3.2 billion images and 720,000 hours of video shared online daily on social media platforms, scalable detection of multimodal disinformation requires efficient fact verification. Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR), the research community lacks substantial effort in multimodal fact verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3 million samples that pushes the boundaries of the domain of fact verification via a multimodal fake news dataset, in addition to offering explainability through the concept of 5W question-answering. Salient features of the dataset include: (i) textual claims, (ii) ChatGPT-generated paraphrased claims, (iii) associated images, (iv) stable diffusion-generated additional images (i.e., visual paraphrases), (v) pixel-level image heatmap to foster image-text explainability of the claim, (vi) 5W QA pairs, and (vii) adversarial fake news stories.", "link": "https://arxiv.org/abs/2306.05523"}, {"id": "2306.05550", "date": "Thu, 8 Jun 2023 20:46:09 GMT", "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and\n\u00a0Downstream Sentiment Classification Tasks\n", "authors": ["Katelyn X. Mei", "Sonia Fereidooni", "Aylin Caliskan\n"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG\nComments:", "20", "pages,12", "figures,2", "tables;", "ACM", "FAccT", "2023\nACM-class:", "K.4;", "I.2.7;", "I.2.0\nDOI:", "10.1145/3593013.3594109", "10.1145/3593013.3594109", "10.1145/3593013.3594109\n"], "abstract": "The rapid deployment of artificial intelligence (AI) models demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is approximately 20 percent higher than when prompts have non-stigmatized conditions. In the sentiment classification tasks, when sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.", "link": "https://arxiv.org/abs/2306.05550"}, {"id": "2306.05554", "date": "Sat, 6 May 2023 10:16:04 GMT", "title": "Simulation and Prediction of Countercurrent Spontaneous Imbibition at\n\u00a0Early and Late Times Using Physics-Informed Neural Networks\n", "authors": ["Jassem Abbasi", "P{\\aa}l {\\O}steb{\\o} Andersen\n"], "categories": ["physics.comp-ph", "cs.AI", "cs.LG\n"], "abstract": "Countercurrent spontaneous imbibition (COUCSI) is a process in porous materials in which a wetting phase displaces non-wetting phase. In this work, we investigate for the first time the application of Physics-Informed Neural Networks (PINNs) in solving the 1D COUCSI problem in both early (ET) and late (LT) times. Also novel, we examine the Change-of-Variables technique for improving the performance of PINNs. We formulated the COUCSI problem in three equivalent forms by changing the independent variables: XT-, XY-, and Z-formulations. The first describes saturation as function of normalized position X and time T; the second as function of X and Y=T^0.5; and the third as a sole function of Z=X/T^0.5 (valid only at ET). The PINN model was generated using a feed-forward neural network and trained based on minimizing a weighted loss function, including the physics-informed loss term and terms corresponding to the initial and boundary conditions. No synthetical or experimental data were involved in the training. All three formulations could closely approximate the correct solutions (obtained by fine-grid numerical simulations), with water saturation mean absolute errors (MAE) around 0.019 and 0.009 for XT and XY formulations and 0.012 for the Z formulation at ET. The Z formulation perfectly captured the self-similarity of the system at ET. This was less captured by XT and XY formulations. The total variation (TV) of saturation was preserved in the Z formulation, and it was better preserved with XY- than XT formulation. It was demonstrated that redefining the problem based on physics-inspired variables reduced the non-linearity of the problem and allowed higher solution accuracies, a higher degree of loss-landscape convexity, a lower number of required collocation points, smaller network sizes, and more computationally efficient solutions.", "link": "https://arxiv.org/abs/2306.05554"}, {"id": "2306.05652", "date": "Fri, 9 Jun 2023 03:37:49 GMT", "title": "Privacy Aware Question-Answering System for Online Mental Health Risk\n\u00a0Assessment\n", "authors": ["Prateek Chhikara", "Ujjwal Pasupulety", "John Marshall", "Dhiraj Chaurasia,\n\u00a0Shweta Kumari\n"], "categories": ["cs.CL", "cs.AI", "cs.HC\nComments:", "5", "pages,", "2", "figures,", "3", "tables\n"], "abstract": "Social media platforms have enabled individuals suffering from mental illnesses to share their lived experiences and find the online support necessary to cope. However, many users fail to receive genuine clinical support, thus exacerbating their symptoms. Screening users based on what they post online can aid providers in administering targeted healthcare and minimize false positives. Pre-trained Language Models (LMs) can assess users' social media data and classify them in terms of their mental health risk. We propose a Question-Answering (QA) approach to assess mental health risk using the Unified-QA model on two large mental health datasets. To protect user data, we extend Unified-QA by anonymizing the model training process using differential privacy. Our results demonstrate the effectiveness of modeling risk assessment as a QA task, specifically for mental health use cases. Furthermore, the model's performance decreases by less than 1% with the inclusion of differential privacy. The proposed system's performance is indicative of a promising research direction that will lead to the development of privacy-aware diagnostic systems.", "link": "https://arxiv.org/abs/2306.05652"}, {"id": "2306.05715", "date": "Fri, 9 Jun 2023 07:19:43 GMT", "title": "Exploring the Responses of Large Language Models to Beginner\n\u00a0Programmers' Help Requests\n", "authors": ["Arto Hellas", "Juho Leinonen", "Sami Sarsa", "Charles Koutcheme", "Lilja\n\u00a0Kujanp\\\"a\\\"a", "Juha Sorva\n"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.SE\nComments:", "13", "pages,", "1", "figure.", "To", "be", "published", "in", "Proceedings", "of", "the", "2023", "ACM\n\u00a0Conference", "on", "International", "Computing", "Education", "Research", "V.1", "(ICER", "'23", "V1)\nDOI:", "10.1145/3568813.3600139\n"], "abstract": "Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence. Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers' help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on. Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students' code and assessed the LLM-generated answers both quantitatively and qualitatively. Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57% of the time). False positives are common (40% chance for GPT-3.5). The advice that the LLMs provide on the issues is often sensible. The LLMs perform better on issues involving program logic rather than on output formatting. Model solutions are frequently provided even when the LLM is prompted not to. LLM responses to prompts in a non-English language are only slightly worse than responses to English prompts. Implications: Our results continue to highlight the utility of LLMs in programming education. At the same time, the results highlight the unreliability of LLMs: LLMs make some of the same mistakes that students do, perhaps especially when formatting output as required by automated assessment systems. Our study informs teachers interested in using LLMs as well as future efforts to customize LLMs for the needs of programming education.", "link": "https://arxiv.org/abs/2306.05715"}, {"id": "2306.05741", "date": "Fri, 9 Jun 2023 08:18:58 GMT", "title": "Challenges and Opportunities for the Design of Smart Speakers\n", "authors": ["Tao Long", "Lydia B. Chilton\n"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.RO\nComments:", "15", "pages,", "7", "figures\n"], "abstract": "Advances in voice technology and voice user interfaces (VUIs) -- such as Alexa, Siri, and Google Home -- have opened up the potential for many new types of interaction. However, despite the potential of these devices reflected by the growing market and body of VUI research, there is a lingering sense that the technology is still underused. In this paper, we conducted a systematic literature review of 35 papers to identify and synthesize 127 VUI design guidelines into five themes. Additionally, we conducted semi-structured interviews with 15 smart speaker users to understand their use and non-use of the technology. From the interviews, we distill four design challenges that contribute the most to non-use. Based on their (non-)use, we identify four opportunity spaces for designers to explore such as focusing on information support while multitasking (cooking, driving, childcare, etc), incorporating users' mental models for smart speakers, and integrating calm design principles.", "link": "https://arxiv.org/abs/2306.05741"}, {"id": "2306.05809", "date": "Fri, 9 Jun 2023 10:48:04 GMT", "title": "Interactive Explanation with Varying Level of Details in an Explainable\n\u00a0Scientific Literature Recommender System\n", "authors": ["Mouadh Guesmi and Mohamed Amine Chatti and Shoeb Joarder and Qurat Ul\n\u00a0Ain and Rawaa Alatrash and Clara Siepmann and Tannaz Vahidi\n"], "categories": ["cs.IR", "cs.AI", "cs.CY", "cs.HC\nComments:", "23", "pages\n"], "abstract": "Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N=14) to investigate the impact of providing interactive explanations with varying level of details on the users' perception of the explainable RS. Our study showed qualitative evidence that fostering interaction and giving users control in deciding which explanation they would like to see can meet the demands of users with different needs, preferences, and goals, and consequently can have positive effects on different crucial aspects in explainable recommendation, including transparency, trust, satisfaction, and user experience.", "link": "https://arxiv.org/abs/2306.05809"}, {"id": "2306.05836", "date": "Fri, 9 Jun 2023 12:09:15 GMT", "title": "Can Large Language Models Infer Causation from Correlation?\n", "authors": ["Zhijing Jin", "Jiarui Liu", "Zhiheng Lyu", "Spencer Poff", "Mrinmaya Sachan,\n\u00a0Rada Mihalcea", "Mona Diab", "Bernhard Sch\\\"olkopf\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\n"], "abstract": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.", "link": "https://arxiv.org/abs/2306.05836"}, {"id": "2306.06022", "date": "Fri, 9 Jun 2023 16:41:34 GMT", "title": "A Dynamic Partial Computation Offloading for the Metaverse in In-Network\n\u00a0Computing\n", "authors": ["Ibrahim Aliyu", "Namseok Ko", "Tai-Won Um", "Jinsul Kim\n"], "categories": ["cs.DC", "cs.AI", "cs.GT\nComments:", "14", "pages,", "9", "figures\n"], "abstract": "The In-Network Computing (COIN) paradigm is a promising solution that leverages unused network resources to perform some tasks to meet up with computation-demanding applications, such as metaverse. In this vein, we consider the metaverse partial computation offloading problem for multiple subtasks in a COIN environment to minimise energy consumption and delay while dynamically adjusting the offloading policy based on the changing computation resources status. We prove that the problem is NP and thus transformed it into two subproblems: task splitting problem (TSP) on the user side and task offloading problem (TOP) on the COIN side. We modelled the TSP as an ordinal potential game (OPG) and proposed a decentralised algorithm to obtain its Nash Equilibrium (NE). Then, we model the TOP as Markov Decision Process (MDP) proposed double deep Q-network (DDQN) to solve for the optimal offloading policy. Unlike the conventional DDQN algorithm, where intelligent agents sample offloading decisions randomly within a certain probability, our COIN agent explores the NE of the TSP and the deep neural network. Finally, simulation results show that our proposed model approach allows the COIN agent to update its policies and make more informed decisions, leading to improved performance over time compared to the traditional baseline.", "link": "https://arxiv.org/abs/2306.06022"}, {"id": "2306.05584", "date": "Thu, 8 Jun 2023 22:55:32 GMT", "title": "Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and\n\u00a0Motion Estimation\n", "authors": ["Jia-Xing Zhong", "Ta-Ying Cheng", "Yuhang He", "Kai Lu", "Kaichen Zhou", "Andrew\n\u00a0Markham", "Niki Trigoni\n"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM\n"], "abstract": "A truly generalizable approach to rigid segmentation and motion estimation is fundamental to 3D understanding of articulated objects and moving scenes. In view of the tightly coupled relationship between segmentation and motion estimates, we present an SE(3) equivariant architecture and a training strategy to tackle this task in an unsupervised manner. Our architecture comprises two lightweight and inter-connected heads that predict segmentation masks using point-level invariant features and motion estimates from SE(3) equivariant features without the prerequisites of category information. Our unified training strategy can be performed online while jointly optimizing the two predictions by exploiting the interrelations among scene flow, segmentation mask, and rigid transformations. We show experiments on four datasets as evidence of the superiority of our method both in terms of model performance and computational efficiency with only 0.25M parameters and 0.92G FLOPs. To the best of our knowledge, this is the first work designed for category-agnostic part-level SE(3) equivariance in dynamic point clouds.", "link": "https://arxiv.org/abs/2306.05584"}, {"id": "2306.05682", "date": "Fri, 9 Jun 2023 05:51:40 GMT", "title": "Lightweight Monocular Depth Estimation via Token-Sharing Transformer\n", "authors": ["Dong-Jae Lee", "Jae Young Lee", "Hyounguk Shon", "Eojindl Yi", "Yeong-Hun\n\u00a0Park", "Sung-Sik Cho", "Junmo Kim\n"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO", "eess.IV\nComments:", "ICRA", "2023\n"], "abstract": "Depth estimation is an important task in various robotics systems and applications. In mobile robotics systems, monocular depth estimation is desirable since a single RGB camera can be deployable at a low cost and compact size. Due to its significant and growing needs, many lightweight monocular depth estimation networks have been proposed for mobile robotics systems. While most lightweight monocular depth estimation methods have been developed using convolution neural networks, the Transformer has been gradually utilized in monocular depth estimation recently. However, massive parameters and large computational costs in the Transformer disturb the deployment to embedded devices. In this paper, we present a Token-Sharing Transformer (TST), an architecture using the Transformer for monocular depth estimation, optimized especially in embedded devices. The proposed TST utilizes global token sharing, which enables the model to obtain an accurate depth prediction with high throughput in embedded devices. Experimental results show that TST outperforms the existing lightweight monocular depth estimation methods. On the NYU Depth v2 dataset, TST can deliver depth maps up to 63.4 FPS in NVIDIA Jetson nano and 142.6 FPS in NVIDIA Jetson TX2, with lower errors than the existing methods. Furthermore, TST achieves real-time depth estimation of high-resolution images on Jetson TX2 with competitive results.", "link": "https://arxiv.org/abs/2306.05682"}, {"id": "2306.05439", "date": "Thu, 8 Jun 2023 07:15:13 GMT", "title": "CLC: Cluster Assignment via Contrastive Representation Learning\n", "authors": ["Fei Ding", "Dan Zhang", "Yin Yang", "Venkat Krovi", "Feng Luo\n"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IR\nComments:", "10", "pages,", "7", "tables,", "4", "figures\n"], "abstract": "Clustering remains an important and challenging task of grouping samples into clusters without manual annotations. Recent works have achieved excellent results on small datasets by performing clustering on feature representations learned from self-supervised learning. However, for datasets with a large number of clusters, such as ImageNet, current methods still can not achieve high clustering performance. In this paper, we propose Contrastive Learning-based Clustering (CLC), which uses contrastive learning to directly learn cluster assignment. We decompose the representation into two parts: one encodes the categorical information under an equipartition constraint, and the other captures the instance-wise factors. We propose a contrastive loss using both parts of the representation. We theoretically analyze the proposed contrastive loss and reveal that CLC sets different weights for the negative samples while learning cluster assignments. Further gradient analysis shows that the larger weights tend to focus more on the hard negative samples. Therefore, the proposed loss has high expressiveness that enables us to efficiently learn cluster assignments. Experimental evaluation shows that CLC achieves overall state-of-the-art or highly competitive clustering performance on multiple benchmark datasets. In particular, we achieve 53.4% accuracy on the full ImageNet dataset and outperform existing methods by large margins (+ 10.2%).", "link": "https://arxiv.org/abs/2306.05439"}, {"id": "2306.05651", "date": "Fri, 9 Jun 2023 03:37:27 GMT", "title": "Differentially Private Sharpness-Aware Training\n", "authors": ["Jinseong Park", "Hoki Kim", "Yujin Choi", "Jaewook Lee\n"], "categories": ["cs.LG", "cs.AI", "cs.CR\nComments:", "ICML", "2023\n"], "abstract": "Training deep learning models with differential privacy (DP) results in a degradation of performance. The training dynamics of models with DP show a significant difference from standard training, whereas understanding the geometric properties of private learning remains largely unexplored. In this paper, we investigate sharpness, a key factor in achieving better generalization, in private learning. We show that flat minima can help reduce the negative effects of per-example gradient clipping and the addition of Gaussian noise. We then verify the effectiveness of Sharpness-Aware Minimization (SAM) for seeking flat minima in private learning. However, we also discover that SAM is detrimental to the privacy budget and computational time due to its two-step optimization. Thus, we propose a new sharpness-aware training method that mitigates the privacy-optimization trade-off. Our experimental results demonstrate that the proposed method improves the performance of deep learning models with DP from both scratch and fine-tuning. Code is available at https://github.com/jinseongP/DPSAT.", "link": "https://arxiv.org/abs/2306.05651"}, {"id": "2306.05670", "date": "Fri, 9 Jun 2023 04:59:24 GMT", "title": "One-Shot Machine Unlearning with Mnemonic Code\n", "authors": ["Tomoya Yamashita and Masanori Yamada and Takashi Shibata\n"], "categories": ["cs.LG", "cs.AI", "cs.CV\nComments:", "14", "pages,", "welcome", "coments\n"], "abstract": "Deep learning has achieved significant improvements in accuracy and has been applied to various fields. With the spread of deep learning, a new problem has also emerged; deep learning models can sometimes have undesirable information from an ethical standpoint. This problem must be resolved if deep learning is to make sensitive decisions such as hiring and prison sentencing. Machine unlearning (MU) is the research area that responds to such demands. MU aims at forgetting about undesirable training data from a trained deep learning model. A naive MU approach is to re-train the whole model with the training data from which the undesirable data has been removed. However, re-training the whole model can take a huge amount of time and consumes significant computer resources. To make MU even more practical, a simple-yet-effective MU method is required. In this paper, we propose a one-shot MU method, which does not need additional training. To design one-shot MU, we add noise to the model parameters that are sensitive to undesirable information. In our proposed method, we use the Fisher information matrix (FIM) to estimate the sensitive model parameters. Training data were usually used to evaluate the FIM in existing methods. In contrast, we avoid the need to retain the training data for calculating the FIM by using class-specific synthetic signals called mnemonic code. Extensive experiments using artificial and natural datasets demonstrate that our method outperforms the existing methods.", "link": "https://arxiv.org/abs/2306.05670"}, {"id": "2306.05764", "date": "Fri, 9 Jun 2023 08:57:14 GMT", "title": "Fair yet Asymptotically Equal Collaborative Learning\n", "authors": ["Xiaoqiang Lin", "Xinyi Xu", "See-Kiong Ng", "Chuan-Sheng Foo", "Bryan Kian\n\u00a0Hsiang Low\n"], "categories": ["cs.LG", "cs.AI", "cs.GT", "cs.MA\nComments:", "Accepted", "to", "40th", "International", "Conference", "on", "Machine", "Learning", "(ICML\n\u00a02023),", "37", "pages\n"], "abstract": "In collaborative learning with streaming data, nodes (e.g., organizations) jointly and continuously learn a machine learning (ML) model by sharing the latest model updates computed from their latest streaming data. For the more resourceful nodes to be willing to share their model updates, they need to be fairly incentivized. This paper explores an incentive design that guarantees fairness so that nodes receive rewards commensurate to their contributions. Our approach leverages an explore-then-exploit formulation to estimate the nodes' contributions (i.e., exploration) for realizing our theoretically guaranteed fair incentives (i.e., exploitation). However, we observe a \"rich get richer\" phenomenon arising from the existing approaches to guarantee fairness and it discourages the participation of the less resourceful nodes. To remedy this, we additionally preserve asymptotic equality, i.e., less resourceful nodes achieve equal performance eventually to the more resourceful/\"rich\" nodes. We empirically demonstrate in two settings with real-world streaming data: federated online incremental learning and federated reinforcement learning, that our proposed approach outperforms existing baselines in fairness and learning performance while remaining competitive in preserving equality.", "link": "https://arxiv.org/abs/2306.05764"}, {"id": "2306.05781", "date": "Fri, 9 Jun 2023 09:49:16 GMT", "title": "Adaptivity Complexity for Causal Graph Discovery\n", "authors": ["Davin Choo", "Kirankumar Shiragur\n"], "categories": ["cs.LG", "cs.AI", "cs.DS", "stat.ME", "stat.ML\nComments:", "Accepted", "into", "UAI", "2023\n"], "abstract": "Causal discovery from interventional data is an important problem, where the task is to design an interventional strategy that learns the hidden ground truth causal graph $G(V,E)$ on $|V| = n$ nodes while minimizing the number of performed interventions. Most prior interventional strategies broadly fall into two categories: non-adaptive and adaptive. Non-adaptive strategies decide on a single fixed set of interventions to be performed while adaptive strategies can decide on which nodes to intervene on sequentially based on past interventions. While adaptive algorithms may use exponentially fewer interventions than their non-adaptive counterparts, there are practical concerns that constrain the amount of adaptivity allowed. Motivated by this trade-off, we study the problem of $r$-adaptivity, where the algorithm designer recovers the causal graph under a total of $r$ sequential rounds whilst trying to minimize the total number of interventions. For this problem, we provide a $r$-adaptive algorithm that achieves $O(\\min\\{r,\\log n\\} \\cdot n^{1/\\min\\{r,\\log n\\}})$ approximation with respect to the verification number, a well-known lower bound for adaptive algorithms. Furthermore, for every $r$, we show that our approximation is tight. Our definition of $r$-adaptivity interpolates nicely between the non-adaptive ($r=1$) and fully adaptive ($r=n$) settings where our approximation simplifies to $O(n)$ and $O(\\log n)$ respectively, matching the best-known approximation guarantees for both extremes. Our results also extend naturally to the bounded size interventions.", "link": "https://arxiv.org/abs/2306.05781"}, {"id": "2306.05813", "date": "Fri, 9 Jun 2023 11:12:55 GMT", "title": "Incorporating Prior Knowledge in Deep Learning Models via Pathway\n\u00a0Activity Autoencoders\n", "authors": ["Pedro Henrique da Costa Avelar", "Min Wu", "Sophia Tsoka\n"], "categories": ["cs.LG", "cs.AI", "cs.NE\n"], "abstract": "Motivation: Despite advances in the computational analysis of high-throughput molecular profiling assays (e.g. transcriptomics), a dichotomy exists between methods that are simple and interpretable, and ones that are complex but with lower degree of interpretability. Furthermore, very few methods deal with trying to translate interpretability in biologically relevant terms, such as known pathway cascades. Biological pathways reflecting signalling events or metabolic conversions are Small improvements or modifications of existing algorithms will generally not be suitable, unless novel biological results have been predicted and verified. Determining which pathways are implicated in disease and incorporating such pathway data as prior knowledge may enhance predictive modelling and personalised strategies for diagnosis, treatment and prevention of disease. Results: We propose a novel prior-knowledge-based deep auto-encoding framework, PAAE, together with its accompanying generative variant, PAVAE, for RNA-seq data in cancer. Through comprehensive comparisons among various learning models, we show that, despite having access to a smaller set of features, our PAAE and PAVAE models achieve better out-of-set reconstruction results compared to common methodologies. Furthermore, we compare our model with equivalent baselines on a classification task and show that they achieve better results than models which have access to the full input gene set. Another result is that using vanilla variational frameworks might negatively impact both reconstruction outputs as well as classification performance. Finally, our work directly contributes by providing comprehensive interpretability analyses on our models on top of improving prognostication for translational medicine.", "link": "https://arxiv.org/abs/2306.05813"}, {"id": "2306.05843", "date": "Fri, 9 Jun 2023 12:17:18 GMT", "title": "Domain-Agnostic Batch Bayesian Optimization with Diverse Constraints via\n\u00a0Bayesian Quadrature\n", "authors": ["Masaki Adachi", "Satoshi Hayakawa", "Xingchen Wan", "Martin J{\\o}rgensen,\n\u00a0Harald Oberhauser", "Michael A. Osborne\n"], "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "stat.CO", "stat.ML\nComments:", "24", "pages,", "5", "figures\nMSC-class:", "62C10,", "62F15\n"], "abstract": "Real-world optimisation problems often feature complex combinations of (1) diverse constraints, (2) discrete and mixed spaces, and are (3) highly parallelisable. (4) There are also cases where the objective function cannot be queried if unknown constraints are not satisfied, e.g. in drug discovery, safety on animal experiments (unknown constraints) must be established before human clinical trials (querying objective function) may proceed. However, most existing works target each of the above three problems in isolation and do not consider (4) unknown constraints with query rejection. For problems with diverse constraints and/or unconventional input spaces, it is difficult to apply these techniques as they are often mutually incompatible. We propose cSOBER, a domain-agnostic prudent parallel active sampler for Bayesian optimisation, based on SOBER of Adachi et al. (2023). We consider infeasibility under unknown constraints as a type of integration error that we can estimate. We propose a theoretically-driven approach that propagates such error as a tolerance in the quadrature precision that automatically balances exploitation and exploration with the expected rejection rate. Moreover, our method flexibly accommodates diverse constraints and/or discrete and mixed spaces via adaptive tolerance, including conventional zero-risk cases. We show that cSOBER outperforms competitive baselines on diverse real-world blackbox-constrained problems, including safety-constrained drug discovery, and human-relationship-aware team optimisation over graph-structured space.", "link": "https://arxiv.org/abs/2306.05843"}, {"id": "2306.05873", "date": "Fri, 9 Jun 2023 13:11:05 GMT", "title": "Detecting Adversarial Directions in Deep Reinforcement Learning to Make\n\u00a0Robust Decisions\n", "authors": ["Ezgi Korkmaz", "Jonah Brown-Cohen\n"], "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ML\nComments:", "Published", "in", "ICML", "2023\n"], "abstract": "Learning in MDPs with highly complex state representations is currently possible due to multiple advancements in reinforcement learning algorithm design. However, this incline in complexity, and furthermore the increase in the dimensions of the observation came at the cost of volatility that can be taken advantage of via adversarial attacks (i.e. moving along worst-case directions in the observation space). To solve this policy instability problem we propose a novel method to detect the presence of these non-robust directions via local quadratic approximation of the deep neural policy loss. Our method provides a theoretical basis for the fundamental cut-off between safe observations and adversarial observations. Furthermore, our technique is computationally efficient, and does not depend on the methods used to produce the worst-case directions. We conduct extensive experiments in the Arcade Learning Environment with several different adversarial attack techniques. Most significantly, we demonstrate the effectiveness of our approach even in the setting where non-robust directions are explicitly optimized to circumvent our proposed method.", "link": "https://arxiv.org/abs/2306.05873"}, {"id": "2306.05879", "date": "Fri, 9 Jun 2023 13:18:50 GMT", "title": "Is Normalization Indispensable for Multi-domain Federated Learning?\n", "authors": ["Weiming Zhuang", "Lingjuan Lyu\n"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC\n"], "abstract": "Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, as opposed to label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated learning Without normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while alternative normalization techniques possess their own limitations. In order to address these issues, FedWon eliminates all normalizations in FL and reparameterizes convolution layers with scaled weight standardization. Through comprehensive experimentation on four datasets and four models, our results demonstrate that FedWon surpasses both FedAvg and the current state-of-the-art method (FedBN) across all experimental setups, achieving notable improvements of over 10% in certain domains. Furthermore, FedWon is versatile for both cross-silo and cross-device FL, exhibiting strong performance even with a batch size as small as 1, thereby catering to resource-constrained devices. Additionally, FedWon effectively tackles the challenge of skewed label distribution.", "link": "https://arxiv.org/abs/2306.05879"}, {"id": "2306.05965", "date": "Fri, 9 Jun 2023 15:33:30 GMT", "title": "Automating Model Comparison in Factor Graphs\n", "authors": ["Bart van Erp", "Wouter W. L. Nuijten", "Thijs van de Laar", "Bert de Vries\n"], "categories": ["cs.LG", "cs.AI", "stat.ML\n"], "abstract": "Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.", "link": "https://arxiv.org/abs/2306.05965"}, {"id": "2306.06087", "date": "Fri, 9 Jun 2023 17:49:56 GMT", "title": "Learning Not to Spoof\n", "authors": ["David Byrd\n"], "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.MA", "q-fin.ST\nDOI:", "10.1145/3533271.3561767\n"], "abstract": "As intelligent trading agents based on reinforcement learning (RL) gain prevalence, it becomes more important to ensure that RL agents obey laws, regulations, and human behavioral expectations. There is substantial literature concerning the aversion of obvious catastrophes like crashing a helicopter or bankrupting a trading account, but little around the avoidance of subtle non-normative behavior for which there are examples, but no programmable definition. Such behavior may violate legal or regulatory, rather than physical or monetary, constraints. In this article, I consider a series of experiments in which an intelligent stock trading agent maximizes profit but may also inadvertently learn to spoof the market in which it participates. I first inject a hand-coded spoofing agent to a multi-agent market simulation and learn to recognize spoofing activity sequences. Then I replace the hand-coded spoofing trader with a simple profit-maximizing RL agent and observe that it independently discovers spoofing as the optimal strategy. Finally, I introduce a method to incorporate the recognizer as normative guide, shaping the agent's perceived rewards and altering its selected actions. The agent remains profitable while avoiding spoofing behaviors that would result in even higher profit. After presenting the empirical results, I conclude with some recommendations. The method should generalize to the reduction of any unwanted behavior for which a recognizer can be learned.", "link": "https://arxiv.org/abs/2306.06087"}, {"id": "2306.06101", "date": "Fri, 9 Jun 2023 17:59:35 GMT", "title": "Prodigy: An Expeditiously Adaptive Parameter-Free Learner\n", "authors": ["Konstantin Mishchenko", "Aaron Defazio\n"], "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML\n"], "abstract": "We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\\sqrt{\\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.", "link": "https://arxiv.org/abs/2306.06101"}, {"id": "2306.05432", "date": "Tue, 6 Jun 2023 15:22:16 GMT", "title": "Towards End-to-end Speech-to-text Summarization\n", "authors": ["Raul Monteiro and Diogo Pernes\n"], "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS\nComments:", "Accepted", "to", "the", "26th", "International", "Conference", "of", "Text,", "Speech", "and\n\u00a0Dialogue", "(TSD2023)\n"], "abstract": "Speech-to-text (S2T) summarization is a time-saving technique for filtering and keeping up with the broadcast news uploaded online on a daily basis. The rise of large language models from deep learning with impressive text generation capabilities has placed the research focus on summarization systems that produce paraphrased compact versions of the document content, also known as abstractive summaries. End-to-end (E2E) modelling of S2T abstractive summarization is a promising approach that offers the possibility of generating rich latent representations that leverage non-verbal and acoustic information, as opposed to the use of only linguistic information from automatically generated transcripts in cascade systems. However, the few literature on E2E modelling of this task fails on exploring different domains, namely broadcast news, which is challenging domain where large and diversified volumes of data are presented to the user every day. We model S2T summarization both with a cascade and an E2E system for a corpus of broadcast news in French. Our novel E2E model leverages external data by resorting to transfer learning from a pre-trained T2T summarizer. Experiments show that both our cascade and E2E abstractive summarizers are stronger than an extractive baseline. However, the performance of the E2E model still lies behind the cascade one, which is object of an extensive analysis that includes future directions to close that gap.", "link": "https://arxiv.org/abs/2306.05432"}, {"id": "2306.05535", "date": "Wed, 24 May 2023 12:09:42 GMT", "title": "Detecting Check-Worthy Claims in Political Debates, Speeches, and\n\u00a0Interviews Using Audio Data\n", "authors": ["Petar Ivanov", "Ivan Koychev", "Momchil Hardalov", "Preslav Nakov\n"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SD", "eess.AS\nComments:", "check-worthy", "claims,", "fake", "news,", "political", "debates,", "audio\nMSC-class:", "68T50\nACM-class:", "F.2.2;", "I.2.7\n"], "abstract": "A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.", "link": "https://arxiv.org/abs/2306.05535"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.link + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
