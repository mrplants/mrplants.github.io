<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <script>
        var papers = [{"id": "2306.04645", "date": "Wed, 31 May 2023 19:27:45 GMT", "title": "Special Session: Approximation and Fault Resiliency of DNN Accelerators\n", "authors": ["Mohammad Hasan Ahmadilivani", "Mario Barbareschi", "Salvatore Barone,\n\u00a0Alberto Bosio", "Masoud Daneshtalab", "Salvatore Della Torca", "Gabriele Gavarini,\n\u00a0Maksim Jenihhin", "Jaan Raik", "Annachiara Ruospo", "Ernesto Sanchez", "and Mahdi\n\u00a0Taheri\n"], "categories": ["cs.LG", "cs.AR", "cs.DC\nComments:", "10", "pages,", "6", "tables,", "9", "figures\n"], "abstract": "Deep Learning, and in particular, Deep Neural Network (DNN) is nowadays widely used in many scenarios, including safety-critical applications such as autonomous driving. In this context, besides energy efficiency and performance, reliability plays a crucial role since a system failure can jeopardize human life. As with any other device, the reliability of hardware architectures running DNNs has to be evaluated, usually through costly fault injection campaigns. This paper explores the approximation and fault resiliency of DNN accelerators. We propose to use approximate (AxC) arithmetic circuits to agilely emulate errors in hardware without performing fault injection on the DNN. To allow fast evaluation of AxC DNN, we developed an efficient GPU-based simulation framework. Further, we propose a fine-grain analysis of fault resiliency by examining fault propagation and masking in networks", "link": "https://arxiv.org/abs/2306.04645"}, {"id": "2306.04646", "date": "Thu, 1 Jun 2023 19:35:13 GMT", "title": "Improve State-Level Wheat Yield Forecasts in Kazakhstan on GEOGLAM's EO\n\u00a0Data by Leveraging A Simple Spatial-Aware Technique\n", "authors": ["Anh Nhat Nhu", "Ritvik Sahajpal", "Christina Justice", "Inbal Becker-Reshef\n"], "categories": ["cs.LG", "cs.CY\nComments:", "Accepted", "to", "(ICLR)", "2023", "Workshop", "on", "Machine", "Learning", "for", "Remote\n\u00a0Sensing\nJournal-ref:", "International", "Conference", "on", "Learning", "Representation", "(ICLR),", "First\n\u00a0Workshop", "on", "Machine", "Learning", "for", "Remote", "Sensing,", "2023\n"], "abstract": "Accurate yield forecasting is essential for making informed policies and long-term decisions for food security. Earth Observation (EO) data and machine learning algorithms play a key role in providing a comprehensive and timely view of crop conditions from field to national scales. However, machine learning algorithms' prediction accuracy is often harmed by spatial heterogeneity caused by exogenous factors not reflected in remote sensing data, such as differences in crop management strategies. In this paper, we propose and investigate a simple technique called state-wise additive bias to explicitly address the cross-region yield heterogeneity in Kazakhstan. Compared to baseline machine learning models (Random Forest, CatBoost, XGBoost), our method reduces the overall RMSE by 8.9\\% and the highest state-wise RMSE by 28.37\\%. The effectiveness of state-wise additive bias indicates machine learning's performance can be significantly improved by explicitly addressing the spatial heterogeneity, motivating future work on spatial-aware machine learning algorithms for yield forecasts as well as for general geospatial forecasting problems.", "link": "https://arxiv.org/abs/2306.04646"}, {"id": "2306.04653", "date": "Tue, 6 Jun 2023 10:22:43 GMT", "title": "From Data to Action: Exploring AI and IoT-driven Solutions for Smarter\n\u00a0Cities\n", "authors": ["Tiago Dias", "Tiago Fonseca", "Jo\\~ao Vitorino", "Andreia Martins", "Sofia\n\u00a0Malpique and Isabel Pra\\c{c}a\n"], "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SY\nComments:", "10", "pages,", "8", "Figures,", "accepted", "for", "DCAI2023\n"], "abstract": "The emergence of smart cities demands harnessing advanced technologies like the Internet of Things (IoT) and Artificial Intelligence (AI) and promises to unlock cities' potential to become more sustainable, efficient, and ultimately livable for their inhabitants. This work introduces an intelligent city management system that provides a data-driven approach to three use cases: (i) analyze traffic information to reduce the risk of traffic collisions and improve driver and pedestrian safety, (ii) identify when and where energy consumption can be reduced to improve cost savings, and (iii) detect maintenance issues like potholes in the city's roads and sidewalks, as well as the beginning of hazards like floods and fires. A case study in Aveiro City demonstrates the system's effectiveness in generating actionable insights that enhance security, energy efficiency, and sustainability, while highlighting the potential of AI and IoT-driven solutions for smart city development.", "link": "https://arxiv.org/abs/2306.04653"}, {"id": "2306.04660", "date": "Wed, 7 Jun 2023 01:16:45 GMT", "title": "Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid\n\u00a0Actor-Critic Reinforcement Learning\n", "authors": ["Ming Xu", "Dongyu Zuo\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Green Light Optimal Speed Advisory (GLOSA) system suggests speeds to vehicles to assist them in passing through intersections during green intervals, thus reducing traffic congestion and fuel consumption by minimizing the number of stops and idle times at intersections. However, previous research has focused on optimizing the GLOSA algorithm, neglecting the frequency of speed advisory by the GLOSA system. Specifically, some studies provide speed advisory profile at each decision step, resulting in redundant advisory, while others calculate the optimal speed for the vehicle only once, which cannot adapt to dynamic traffic. In this paper, we propose an Adaptive Frequency GLOSA (AF-GLOSA) model based on Hybrid Proximal Policy Optimization (H-PPO), which employs an actor-critic architecture with a hybrid actor network. The hybrid actor network consists of a discrete actor that outputs advisory frequency and a continuous actor that outputs acceleration profiles. Additionally, we design a novel reward function that considers both travel efficiency and fuel consumption. The AF-GLOSA model is evaluated in comparison to traditional GLOSA and learning-based GLOSA methods in a three-lane intersection with a traffic signal in SUMO, under three different levels of traffic density. The results demonstrate that the AF-GLOSA model performs best in reducing average stop times, fuel consumption and CO2 emissions.", "link": "https://arxiv.org/abs/2306.04660"}, {"id": "2306.04662", "date": "Wed, 7 Jun 2023 02:32:45 GMT", "title": "Understanding Place Identity with Generative AI\n", "authors": ["Kee Moon Jang and Junda Chen and Yuhao Kang and Junghwan Kim and\n\u00a0Jinhyung Lee and F\\'abio Duarte\n"], "categories": ["cs.LG", "cs.CY", "cs.HC", "cs.SI\nComments:", "6", "pages,", "3", "figures,", "GIScience", "2023\n"], "abstract": "Researchers are constantly leveraging new forms of data with the goal of understanding how people perceive the built environment and build the collective place identity of cities. Latest advancements in generative artificial intelligence (AI) models have enabled the production of realistic representations learned from vast amounts of data. In this study, we aim to test the potential of generative AI as the source of textual and visual information in capturing the place identity of cities assessed by filtered descriptions and images. We asked questions on the place identity of a set of 31 global cities to two generative AI models, ChatGPT and DALL-E2. Since generative AI has raised ethical concerns regarding its trustworthiness, we performed cross-validation to examine whether the results show similar patterns to real urban settings. In particular, we compared the outputs with Wikipedia data for text and images searched from Google for image. Our results indicate that generative AI models have the potential to capture the collective image of cities that can make them distinguishable. This study is among the first attempts to explore the capabilities of generative AI in understanding human perceptions of the built environment. It contributes to urban design literature by discussing future research opportunities and potential limitations.", "link": "https://arxiv.org/abs/2306.04662"}, {"id": "2306.04675", "date": "Wed, 7 Jun 2023 18:00:00 GMT", "title": "Exposing flaws of generative model evaluation metrics and their unfair\n\u00a0treatment of diffusion models\n", "authors": ["George Stein", "Jesse C. Cresswell", "Rasa Hosseinzadeh", "Yi Sui", "Brendan\n\u00a0Leigh Ross", "Valentin Villecroze", "Zhaoyan Liu", "Anthony L. Caterini", "J. Eric T.\n\u00a0Taylor", "Gabriel Loaiza-Ganem\n"], "categories": ["cs.LG", "cs.CV", "stat.ML\nComments:", "50", "pages,", "29", "figures,", "12", "tables,", "code", "at\n\u00a0https://github.com/layer6ai-labs/dgm-eval\n"], "abstract": "We systematically study a wide variety of image-based generative models spanning semantically-diverse datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 16 modern metrics for evaluating the overall performance, fidelity, diversity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization; none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 16 common metrics for 8 different encoders at https://github.com/layer6ai-labs/dgm-eval.", "link": "https://arxiv.org/abs/2306.04675"}, {"id": "2306.04756", "date": "Wed, 7 Jun 2023 20:08:27 GMT", "title": "A Linearly Convergent GAN Inversion-based Algorithm for Reverse\n\u00a0Engineering of Deceptions\n", "authors": ["Darshan Thaker", "Paris Giampouras", "Ren\\'e Vidal\n"], "categories": ["cs.LG", "cs.CR\n"], "abstract": "An important aspect of developing reliable deep learning systems is devising strategies that make these systems robust to adversarial attacks. There is a long line of work that focuses on developing defenses against these attacks, but recently, researchers have began to study ways to reverse engineer the attack process. This allows us to not only defend against several attack models, but also classify the threat model. However, there is still a lack of theoretical guarantees for the reverse engineering process. Current approaches that give any guarantees are based on the assumption that the data lies in a union of linear subspaces, which is not a valid assumption for more complex datasets. In this paper, we build on prior work and propose a novel framework for reverse engineering of deceptions which supposes that the clean data lies in the range of a GAN. To classify the signal and attack, we jointly solve a GAN inversion problem and a block-sparse recovery problem. For the first time in the literature, we provide deterministic linear convergence guarantees for this problem. We also empirically demonstrate the merits of the proposed approach on several nonlinear datasets as compared to state-of-the-art methods.", "link": "https://arxiv.org/abs/2306.04756"}, {"id": "2306.04766", "date": "Wed, 7 Jun 2023 20:27:17 GMT", "title": "Enabling tabular deep learning when $d \\gg n$ with an auxiliary\n\u00a0knowledge graph\n", "authors": ["Camilo Ruiz", "Hongyu Ren", "Kexin Huang", "Jure Leskovec\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Machine learning models exhibit strong performance on datasets with abundant labeled samples. However, for tabular datasets with extremely high $d$-dimensional features but limited $n$ samples (i.e. $d \\gg n$), machine learning models struggle to achieve strong performance due to the risk of overfitting. Here, our key insight is that there is often abundant, auxiliary domain information describing input features which can be structured as a heterogeneous knowledge graph (KG). We propose PLATO, a method that achieves strong performance on tabular data with $d \\gg n$ by using an auxiliary KG describing input features to regularize a multilayer perceptron (MLP). In PLATO, each input feature corresponds to a node in the auxiliary KG. In the MLP's first layer, each input feature also corresponds to a weight vector. PLATO is based on the inductive bias that two input features corresponding to similar nodes in the auxiliary KG should have similar weight vectors in the MLP's first layer. PLATO captures this inductive bias by inferring the weight vector for each input feature from its corresponding node in the KG via a trainable message-passing function. Across 6 $d \\gg n$ datasets, PLATO outperforms 13 state-of-the-art baselines by up to 10.19%.", "link": "https://arxiv.org/abs/2306.04766"}, {"id": "2306.04775", "date": "Wed, 7 Jun 2023 20:48:35 GMT", "title": "Exploiting Observation Bias to Improve Matrix Completion\n", "authors": ["Sean Mann", "Charlotte Park", "Devavrat Shah\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "We consider a variant of matrix completion where entries are revealed in a biased manner, adopting a model akin to that introduced by Ma and Chen. Instead of treating this observation bias as a disadvantage, as is typically the case, our goal is to exploit the shared information between the bias and the outcome of interest to improve predictions. Towards this, we propose a simple two-stage algorithm: (i) interpreting the observation pattern as a fully observed noisy matrix, we apply traditional matrix completion methods to the observation pattern to estimate the distances between the latent factors; (ii) we apply supervised learning on the recovered features to impute missing observations. We establish finite-sample error rates that are competitive with the corresponding supervised learning parametric rates, suggesting that our learning performance is comparable to having access to the unobserved covariates. Empirical evaluation using a real-world dataset reflects similar performance gains, with our algorithm's estimates having 30x smaller mean squared error compared to traditional matrix completion methods.", "link": "https://arxiv.org/abs/2306.04775"}, {"id": "2306.04777", "date": "Wed, 7 Jun 2023 20:52:01 GMT", "title": "Invariant Causal Set Covering Machines\n", "authors": ["Thibaud Godon", "Baptiste Bauvin", "Pascal Germain", "Jacques Corbeil,\n\u00a0Alexandre Drouin\n"], "categories": ["cs.LG", "stat.ME", "stat.ML\n"], "abstract": "Rule-based models, such as decision trees, appeal to practitioners due to their interpretable nature. However, the learning algorithms that produce such models are often vulnerable to spurious associations and thus, they are not guaranteed to extract causally-relevant insights. In this work, we build on ideas from the invariant causal prediction literature to propose Invariant Causal Set Covering Machines, an extension of the classical Set Covering Machine algorithm for conjunctions/disjunctions of binary-valued rules that provably avoids spurious associations. We demonstrate both theoretically and empirically that our method can identify the causal parents of a variable of interest in polynomial time.", "link": "https://arxiv.org/abs/2306.04777"}, {"id": "2306.04778", "date": "Wed, 7 Jun 2023 20:53:50 GMT", "title": "Loss Functions for Behavioral Game Theory\n", "authors": ["Greg d'Eon", "Sophie Greenwood", "Kevin Leyton-Brown", "and James Wright\n"], "categories": ["cs.LG", "cs.GT\nComments:", "17", "pages", "(10", "pages", "body", "+", "references", "and", "appendix).", "Under", "review", "at\n\u00a0NeurIPS", "2023\n"], "abstract": "Behavioral game theorists all use experimental data to evaluate predictive models of human behavior. However, they differ greatly in their choice of loss function for these evaluations, with error rate, negative log-likelihood, cross-entropy, Brier score, and L2 error all being common choices. We attempt to offer a principled answer to the question of which loss functions make sense for this task, formalizing desiderata that we argue loss functions should satisfy. We construct a family of loss functions, which we dub \"diagonal bounded Bregman divergences\", that satisfy all of these axioms and includes the squared L2 error. In fact, the squared L2 error is the only acceptable loss that is relatively commonly used in practice; we thus recommend its continued use to behavioral game theorists.", "link": "https://arxiv.org/abs/2306.04778"}, {"id": "2306.04785", "date": "Wed, 7 Jun 2023 21:08:09 GMT", "title": "Interpretable Deep Clustering\n", "authors": ["Jonathan Svirsky", "Ofir Lindenbaum\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "Clustering is a fundamental learning task widely used as a first step in data analysis. For example, biologists often use cluster assignments to analyze genome sequences, medical records, or images. Since downstream analysis is typically performed at the cluster level, practitioners seek reliable and interpretable clustering models. We propose a new deep-learning framework that predicts interpretable cluster assignments at the instance and cluster levels. First, we present a self-supervised procedure to identify a subset of informative features from each data point. Then, we design a model that predicts cluster assignments and a gate matrix that leads to cluster-level feature selection. We show that the proposed method can reliably predict cluster assignments using synthetic and real data. Furthermore, we verify that our model leads to interpretable results at a sample and cluster level.", "link": "https://arxiv.org/abs/2306.04785"}, {"id": "2306.04791", "date": "Wed, 7 Jun 2023 21:25:32 GMT", "title": "XInsight: Revealing Model Insights for GNNs with Flow-based Explanations\n", "authors": ["Eli Laird", "Ayesh Madushanka", "Elfi Kraka", "Corey Clark\n"], "categories": ["cs.LG", "cs.AI\nComments:", "eXplainable", "Artificial", "Intelligence.", "1st", "World", "Conference", "on\n\u00a0eXplainable", "Artificial", "Intelligence,", "xAI-2023,", "Lisbon,", "Portugal\n"], "abstract": "Progress in graph neural networks has grown rapidly in recent years, with many new developments in drug discovery, medical diagnosis, and recommender systems. While this progress is significant, many networks are `black boxes' with little understanding of the `what' exactly the network is learning. Many high-stakes applications, such as drug discovery, require human-intelligible explanations from the models so that users can recognize errors and discover new knowledge. Therefore, the development of explainable AI algorithms is essential for us to reap the benefits of AI. We propose an explainability algorithm for GNNs called eXplainable Insight (XInsight) that generates a distribution of model explanations using GFlowNets. Since GFlowNets generate objects with probabilities proportional to a reward, XInsight can generate a diverse set of explanations, compared to previous methods that only learn the maximum reward sample. We demonstrate XInsight by generating explanations for GNNs trained on two graph classification tasks: classifying mutagenic compounds with the MUTAG dataset and classifying acyclic graphs with a synthetic dataset that we have open-sourced. We show the utility of XInsight's explanations by analyzing the generated compounds using QSAR modeling, and we find that XInsight generates compounds that cluster by lipophilicity, a known correlate of mutagenicity. Our results show that XInsight generates a distribution of explanations that uncovers the underlying relationships demonstrated by the model. They also highlight the importance of generating a diverse set of explanations, as it enables us to discover hidden relationships in the model and provides valuable guidance for further analysis.", "link": "https://arxiv.org/abs/2306.04791"}, {"id": "2306.04793", "date": "Wed, 7 Jun 2023 21:35:26 GMT", "title": "On the Joint Interaction of Models, Data, and Features\n", "authors": ["Yiding Jiang", "Christina Baek", "J. Zico Kolter\n"], "categories": ["cs.LG", "stat.ML\n"], "abstract": "Learning features from data is one of the defining characteristics of deep learning, but our theoretical understanding of the role features play in deep learning is still rudimentary. To address this gap, we introduce a new tool, the interaction tensor, for empirically analyzing the interaction between data and model through features. With the interaction tensor, we make several key observations about how features are distributed in data and how models with different random seeds learn different features. Based on these observations, we propose a conceptual framework for feature learning. Under this framework, the expected accuracy for a single hypothesis and agreement for a pair of hypotheses can both be derived in closed-form. We demonstrate that the proposed framework can explain empirically observed phenomena, including the recently discovered Generalization Disagreement Equality (GDE) that allows for estimating the generalization error with only unlabeled data. Further, our theory also provides explicit construction of natural data distributions that break the GDE. Thus, we believe this work provides valuable new insight into our understanding of feature learning.", "link": "https://arxiv.org/abs/2306.04793"}, {"id": "2306.04803", "date": "Wed, 7 Jun 2023 21:53:14 GMT", "title": "Privately generating tabular data using language models\n", "authors": ["Alexandre Sablayrolles", "Yue Wang", "Brian Karrer\n"], "categories": ["cs.LG", "cs.CL", "cs.CR\nComments:", "9", "pages,", "3", "figures\n"], "abstract": "Privately generating synthetic data from a table is an important brick of a privacy-first world. We propose and investigate a simple approach of treating each row in a table as a sentence and training a language model with differential privacy. We show this approach obtains competitive results in modelling tabular data across multiple datasets, even at small scales that favor alternative methods based on marginal distributions.", "link": "https://arxiv.org/abs/2306.04803"}, {"id": "2306.04847", "date": "Thu, 8 Jun 2023 00:50:16 GMT", "title": "Embedding stochastic differential equations into neural networks via\n\u00a0dual processes\n", "authors": ["Naoki Sughishita and Jun Ohkubo\n"], "categories": ["cs.LG", "physics.data-an\nComments:", "13", "pages,", "4", "figures\n"], "abstract": "We propose a new approach to constructing a neural network for predicting expectations of stochastic differential equations. The proposed method does not need data sets of inputs and outputs; instead, the information obtained from the time-evolution equations, i.e., the corresponding dual process, is directly compared with the weights in the neural network. As a demonstration, we construct neural networks for the Ornstein-Uhlenbeck process and the noisy van der Pol system. The remarkable feature of learned networks with the proposed method is the accuracy of inputs near the origin. Hence, it would be possible to avoid the overfitting problem because the learned network does not depend on training data sets.", "link": "https://arxiv.org/abs/2306.04847"}, {"id": "2306.04848", "date": "Thu, 8 Jun 2023 00:56:33 GMT", "title": "Interpreting and Improving Diffusion Models Using the Euclidean Distance\n\u00a0Function\n", "authors": ["Frank Permenter and Chenyang Yuan\n"], "categories": ["cs.LG", "cs.CV", "math.OC", "stat.ML\nComments:", "18", "pages,", "6", "figures,", "2", "tables\n"], "abstract": "Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.", "link": "https://arxiv.org/abs/2306.04848"}, {"id": "2306.04862", "date": "Thu, 8 Jun 2023 01:26:22 GMT", "title": "A Systematic Literature Review on Client Selection in Federated Learning\n", "authors": ["Carl Smestad (1) and Jingyue Li (2) ((1) Norwegian University of\n\u00a0Science and Technology", "(2) Norwegian University of Science and Technology)\n"], "categories": ["cs.LG", "cs.AI\nDOI:", "10.1145/3593434.3593438\n"], "abstract": "With the arising concerns of privacy within machine learning, federated learning (FL) was invented in 2017, in which the clients, such as mobile devices, train a model and send the update to the centralized server. Choosing clients randomly for FL can harm learning performance due to different reasons. Many studies have proposed approaches to address the challenges of client selection of FL. However, no systematic literature review (SLR) on this topic existed. This SLR investigates the state of the art of client selection in FL and answers the challenges, solutions, and metrics to evaluate the solutions. We systematically reviewed 47 primary studies. The main challenges found in client selection are heterogeneity, resource allocation, communication costs, and fairness. The client selection schemes aim to improve the original random selection algorithm by focusing on one or several of the aforementioned challenges. The most common metric used is testing accuracy versus communication rounds, as testing accuracy measures the successfulness of the learning and preferably in as few communication rounds as possible, as they are very expensive. Although several possible improvements can be made with the current state of client selection, the most beneficial ones are evaluating the impact of unsuccessful clients and gaining a more theoretical understanding of the impact of fairness in FL.", "link": "https://arxiv.org/abs/2306.04862"}, {"id": "2306.04891", "date": "Thu, 8 Jun 2023 02:38:23 GMT", "title": "In-Context Learning through the Bayesian Prism\n", "authors": ["Kabir Ahuja", "Madhur Panwar", "Navin Goyal\n"], "categories": ["cs.LG", "cs.CL\n"], "abstract": "In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs $(x, f(x))$ from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learner across different linear and non-linear function classes. We also extend the previous setups to work in the multitask setting and verify that transformers can do in-context learning in this setup as well and the Bayesian perspective sheds light on this setting also. Finally, via the example of learning Fourier series, we study the inductive bias for in-context learning. We find that in-context learning may or may not have simplicity bias depending on the pretraining data distribution.", "link": "https://arxiv.org/abs/2306.04891"}, {"id": "2306.04898", "date": "Thu, 8 Jun 2023 03:00:10 GMT", "title": "Understanding Masked Autoencoders via Hierarchical Latent Variable\n\u00a0Models\n", "authors": ["Lingjing Kong", "Martin Q. Ma", "Guangyi Chen", "Eric P. Xing", "Yuejie Chi,\n\u00a0Louis-Philippe Morency", "Kun Zhang\n"], "categories": ["cs.LG", "cs.CV\nComments:", "CVPR", "2023", "Highlight\n"], "abstract": "Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical insights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masking-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.", "link": "https://arxiv.org/abs/2306.04898"}, {"id": "2306.04904", "date": "Thu, 8 Jun 2023 03:16:21 GMT", "title": "An adaptive augmented Lagrangian method for training physics and\n\u00a0equality constrained artificial neural networks\n", "authors": ["Shamsulhaq Basir", "Inanc Senocak\n"], "categories": ["cs.LG", "physics.comp-ph", "physics.flu-dyn\nMSC-class:", "35Qxx,", "35Exx,", "76Dxx,", "68Wxx,", "65Mxx,", "65Kxx,", "49Mxx,", "49Kxx,\nACM-class:", "G.1.6;", "G.1.8;", "G.1.10;", "J.2;", "I.2;", "I.6\n"], "abstract": "Physics and equality constrained artificial neural networks (PECANN) are grounded in methods of constrained optimization to properly constrain the solution of partial differential equations (PDEs) with their boundary and initial conditions and any high-fidelity data that may be available. To this end, adoption of the augmented Lagrangian method within the PECANN framework is paramount for learning the solution of PDEs without manually balancing the individual loss terms in the objective function used for determining the parameters of the neural network. Generally speaking, ALM combines the merits of the penalty and Lagrange multiplier methods while avoiding the ill conditioning and convergence issues associated singly with these methods . In the present work, we apply our PECANN framework to solve forward and inverse problems that have an expanded and diverse set of constraints. We show that ALM with its conventional formulation to update its penalty parameter and Lagrange multipliers stalls for such challenging problems. To address this issue, we propose an adaptive ALM in which each constraint is assigned a unique penalty parameter that evolve adaptively according to a rule inspired by the adaptive subgradient method. Additionally, we revise our PECANN formulation for improved computational efficiency and savings which allows for mini-batch training. We demonstrate the efficacy of our proposed approach by solving several forward and PDE-constrained inverse problems with noisy data, including simulation of incompressible fluid flows with a primitive-variables formulation of the Navier-Stokes equations up to a Reynolds number of 1000.", "link": "https://arxiv.org/abs/2306.04904"}, {"id": "2306.04922", "date": "Thu, 8 Jun 2023 03:47:33 GMT", "title": "Efficient and Equivariant Graph Networks for Predicting Quantum\n\u00a0Hamiltonian\n", "authors": ["Haiyang Yu", "Zhao Xu", "Xiaofeng Qian", "Xiaoning Qian", "Shuiwang Ji\n"], "categories": ["cs.LG", "physics.comp-ph\n"], "abstract": "We consider the prediction of the Hamiltonian matrix, which finds use in quantum chemistry and condensed matter physics. Efficiency and equivariance are two important, but conflicting factors. In this work, we propose a SE(3)-equivariant network, named QHNet, that achieves efficiency and equivariance. Our key advance lies at the innovative design of QHNet architecture, which not only obeys the underlying symmetries, but also enables the reduction of number of tensor products by 92\\%. In addition, QHNet prevents the exponential growth of channel dimension when more atom types are involved. We perform experiments on MD17 datasets, including four molecular systems. Experimental results show that our QHNet can achieve comparable performance to the state of the art methods at a significantly faster speed. Besides, our QHNet consumes 50\\% less memory due to its streamlined architecture. Our code is publicly available as part of the AIRS library (\\url{https://github.com/divelab/AIRS}).", "link": "https://arxiv.org/abs/2306.04922"}, {"id": "2306.04923", "date": "Thu, 8 Jun 2023 03:55:33 GMT", "title": "Unconstrained Online Learning with Unbounded Losses\n", "authors": ["Andrew Jacobsen", "Ashok Cutkosky\n"], "categories": ["cs.LG", "stat.ML\nComments:", "41", "pages;", "ICML", "2023\n"], "abstract": "Algorithms for online learning typically require one or more boundedness assumptions: that the domain is bounded, that the losses are Lipschitz, or both. In this paper, we develop a new setting for online learning with unbounded domains and non-Lipschitz losses. For this setting we provide an algorithm which guarantees $R_{T}(u)\\le \\tilde O(G\\|u\\|\\sqrt{T}+L\\|u\\|^{2}\\sqrt{T})$ regret on any problem where the subgradients satisfy $\\|g_{t}\\|\\le G+L\\|w_{t}\\|$, and show that this bound is unimprovable without further assumptions. We leverage this algorithm to develop new saddle-point optimization algorithms that converge in duality gap in unbounded domains, even in the absence of meaningful curvature. Finally, we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound. The regret of our dynamic regret algorithm automatically improves to a novel $L^{*}$ bound when the losses are smooth.", "link": "https://arxiv.org/abs/2306.04923"}, {"id": "2306.04924", "date": "Thu, 8 Jun 2023 04:00:00 GMT", "title": "Exact Optimality of Communication-Privacy-Utility Tradeoffs in\n\u00a0Distributed Mean Estimation\n", "authors": ["Berivan Isik", "Wei-Ning Chen", "Ayfer Ozgur", "Tsachy Weissman", "Albert No\n"], "categories": ["cs.LG", "cs.CR", "cs.DC", "cs.IT", "math.IT", "stat.ML\n"], "abstract": "We study the mean estimation problem under communication and local differential privacy constraints. While previous work has proposed \\emph{order}-optimal algorithms for the same problem (i.e., asymptotically optimal as we spend more bits), \\emph{exact} optimality (in the non-asymptotic setting) still has not been achieved. In this work, we take a step towards characterizing the \\emph{exact}-optimal approach in the presence of shared randomness (a random variable shared between the server and the user) and identify several necessary conditions for \\emph{exact} optimality. We prove that one of the necessary conditions is to utilize a rotationally symmetric shared random codebook. Based on this, we propose a randomization mechanism where the codebook is a randomly rotated simplex -- satisfying the necessary properties of the \\emph{exact}-optimal codebook. The proposed mechanism is based on a $k$-closest encoding which we prove to be \\emph{exact}-optimal for the randomly rotated simplex codebook.", "link": "https://arxiv.org/abs/2306.04924"}, {"id": "2306.04940", "date": "Thu, 8 Jun 2023 05:13:34 GMT", "title": "Layer-level activation mechanism\n", "authors": ["Yoon Kihyuk and Lim Chiehyeon\n"], "categories": ["cs.LG", "cs.CV", "cs.NE\nComments:", "9", "pages,", "4", "figures,", "4", "tables", "except", "appendix\n"], "abstract": "In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.", "link": "https://arxiv.org/abs/2306.04940"}, {"id": "2306.04948", "date": "Thu, 8 Jun 2023 05:41:42 GMT", "title": "ShuttleSet: A Human-Annotated Stroke-Level Singles Dataset for Badminton\n\u00a0Tactical Analysis\n", "authors": ["Wei-Yao Wang", "Yung-Chang Huang", "Tsi-Ui Ik", "Wen-Chih Peng\n"], "categories": ["cs.LG", "cs.AI\nComments:", "KDD", "2023.", "Project", "page:", "https://github.com/wywyWang/CoachAI-Projects\n"], "abstract": "With the recent progress in sports analytics, deep learning approaches have demonstrated the effectiveness of mining insights into players' tactics for improving performance quality and fan engagement. This is attributed to the availability of public ground-truth datasets. While there are a few available datasets for turn-based sports for action detection, these datasets severely lack structured source data and stroke-level records since these require high-cost labeling efforts from domain experts and are hard to detect using automatic techniques. Consequently, the development of artificial intelligence approaches is significantly hindered when existing models are applied to more challenging structured turn-based sequences. In this paper, we present ShuttleSet, the largest publicly-available badminton singles dataset with annotated stroke-level records. It contains 104 sets, 3,685 rallies, and 36,492 strokes in 44 matches between 2018 and 2021 with 27 top-ranking men's singles and women's singles players. ShuttleSet is manually annotated with a computer-aided labeling tool to increase the labeling efficiency and effectiveness of selecting the shot type with a choice of 18 distinct classes, the corresponding hitting locations, and the locations of both players at each stroke. In the experiments, we provide multiple benchmarks (i.e., stroke influence, stroke forecasting, and movement forecasting) with baselines to illustrate the practicability of using ShuttleSet for turn-based analytics, which is expected to stimulate both academic and sports communities. Over the past two years, a visualization platform has been deployed to illustrate the variability of analysis cases from ShuttleSet for coaches to delve into players' tactical preferences with human-interactive interfaces, which was also used by national badminton teams during multiple international high-ranking matches.", "link": "https://arxiv.org/abs/2306.04948"}, {"id": "2306.04961", "date": "Thu, 8 Jun 2023 06:35:47 GMT", "title": "Recovering Simultaneously Structured Data via Non-Convex Iteratively\n\u00a0Reweighted Least Squares\n", "authors": ["Christian K\\\"ummerle and Johannes Maly\n"], "categories": ["cs.LG", "cs.IT", "math.IT", "math.OC\nComments:", "35", "pages,", "6", "figures\n"], "abstract": "We propose a new algorithm for the problem of recovering data that adheres to multiple, heterogeneous low-dimensional structures from linear observations. Focusing on data matrices that are simultaneously row-sparse and low-rank, we propose and analyze an iteratively reweighted least squares (IRLS) algorithm that is able to leverage both structures. In particular, it optimizes a combination of non-convex surrogates for row-sparsity and rank, a balancing of which is built into the algorithm. We prove locally quadratic convergence of the iterates to a simultaneously structured data matrix in a regime of minimal sample complexity (up to constants and a logarithmic factor), which is known to be impossible for a combination of convex surrogates. In experiments, we show that the IRLS method exhibits favorable empirical convergence, identifying simultaneously row-sparse and low-rank matrices from fewer measurements than state-of-the-art methods.", "link": "https://arxiv.org/abs/2306.04961"}, {"id": "2306.04974", "date": "Thu, 8 Jun 2023 07:05:36 GMT", "title": "Conservative Prediction via Data-Driven Confidence Minimization\n", "authors": ["Caroline Choi and Fahim Tajwar and Yoonho Lee and Huaxiu Yao and\n\u00a0Ananya Kumar and Chelsea Finn\n"], "categories": ["cs.LG", "cs.AI\nComments:", "Preprint.", "Under", "review\n"], "abstract": "Errors of machine learning models are costly, especially in safety-critical domains such as healthcare, where such mistakes can prevent the deployment of machine learning altogether. In these settings, conservative models -- models which can defer to human judgment when they are likely to make an error -- may offer a solution. However, detecting unusual or difficult examples is notably challenging, as it is impossible to anticipate all potential inputs at test time. To address this issue, prior work has proposed to minimize the model's confidence on an auxiliary pseudo-OOD dataset. We theoretically analyze the effect of confidence minimization and show that the choice of auxiliary dataset is critical. Specifically, if the auxiliary dataset includes samples from the OOD region of interest, confidence minimization provably separates ID and OOD inputs by predictive confidence. Taking inspiration from this result, we present data-driven confidence minimization (DCM), which minimizes confidence on an uncertainty dataset containing examples that the model is likely to misclassify at test time. Our experiments show that DCM consistently outperforms state-of-the-art OOD detection methods on 8 ID-OOD dataset pairs, reducing FPR (at TPR 95%) by 6.3% and 58.1% on CIFAR-10 and CIFAR-100, and outperforms existing selective classification approaches on 4 datasets in conditions of distribution shift.", "link": "https://arxiv.org/abs/2306.04974"}, {"id": "2306.04979", "date": "Thu, 8 Jun 2023 07:10:35 GMT", "title": "CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive\n\u00a0Graph Classification\n", "authors": ["Nan Yin", "Li Shen", "Mengzhu Wang", "Long Lan", "Zeyu Ma", "Chong Chen,\n\u00a0Xian-Sheng Hua", "Xiao Luo\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \\underline{Co}upled \\underline{Co}ntrastive Graph Representation Learning (\\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \\method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which not only incorporates graph representations learned from complementary views for enhanced understanding, but also encourages the similarity between cross-domain example pairs with the same semantics for domain alignment. Extensive experiments on various popular datasets show that \\method{} outperforms these competing baselines by 5.7\\% to 21.0\\% generally.", "link": "https://arxiv.org/abs/2306.04979"}, {"id": "2306.05021", "date": "Thu, 8 Jun 2023 08:16:38 GMT", "title": "Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific\n\u00a0Tensor Decomposition\n", "authors": ["Zhewen Yu", "Christos-Savvas Bouganis\n"], "categories": ["cs.LG", "cs.AR\n"], "abstract": "Neural Network designs are quite diverse, from VGG-style to ResNet-style, and from Convolutional Neural Networks to Transformers. Towards the design of efficient accelerators, many works have adopted a dataflow-based, inter-layer pipelined architecture, with a customised hardware towards each layer, achieving ultra high throughput and low latency. The deployment of neural networks to such dataflow architecture accelerators is usually hindered by the available on-chip memory as it is desirable to preload the weights of neural networks on-chip to maximise the system performance. To address this, networks are usually compressed before the deployment through methods such as pruning, quantization and tensor decomposition. In this paper, a framework for mapping CNNs onto FPGAs based on a novel tensor decomposition method called Mixed-TD is proposed. The proposed method applies layer-specific Singular Value Decomposition (SVD) and Canonical Polyadic Decomposition (CPD) in a mixed manner, achieving 1.73x to 10.29x throughput per DSP to state-of-the-art CNNs. Our work is open-sourced: https://github.com/Yu-Zhewen/Mixed-TD", "link": "https://arxiv.org/abs/2306.05021"}, {"id": "2306.05079", "date": "Thu, 8 Jun 2023 10:02:04 GMT", "title": "Enhancing Robustness of AI Offensive Code Generators via Data\n\u00a0Augmentation\n", "authors": ["Cristina Improta", "Pietro Liguori", "Roberto Natella", "Bojan Cukic and\n\u00a0Domenico Cotroneo\n"], "categories": ["cs.LG", "cs.CL", "cs.CR\n"], "abstract": "In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.", "link": "https://arxiv.org/abs/2306.05079"}, {"id": "2306.05108", "date": "Thu, 8 Jun 2023 11:15:34 GMT", "title": "Hybrid Graph: A Unified Graph Representation with Datasets and\n\u00a0Benchmarks for Complex Graphs\n", "authors": ["Zehui Li", "Xiangyu Zhao", "Mingzhu Shen", "Guy-Bart Stan", "Pietro Li\\`o,\n\u00a0Yiren Zhao\n"], "categories": ["cs.LG", "cs.SI\nComments:", "Preprint.", "Under", "review.", "16", "pages,", "5", "figures,", "11", "tables\n"], "abstract": "Graphs are widely used to encapsulate a variety of data formats, but real-world networks often involve complex node relations beyond only being pairwise. While hypergraphs and hierarchical graphs have been developed and employed to account for the complex node relations, they cannot fully represent these complexities in practice. Additionally, though many Graph Neural Networks (GNNs) have been proposed for representation learning on higher-order graphs, they are usually only evaluated on simple graph datasets. Therefore, there is a need for a unified modelling of higher-order graphs, and a collection of comprehensive datasets with an accessible evaluation framework to fully understand the performance of these algorithms on complex graphs. In this paper, we introduce the concept of hybrid graphs, a unified definition for higher-order graphs, and present the Hybrid Graph Benchmark (HGB). HGB contains 23 real-world hybrid graph datasets across various domains such as biology, social media, and e-commerce. Furthermore, we provide an extensible evaluation framework and a supporting codebase to facilitate the training and evaluation of GNNs on HGB. Our empirical study of existing GNNs on HGB reveals various research opportunities and gaps, including (1) evaluating the actual performance improvement of hypergraph GNNs over simple graph GNNs; (2) comparing the impact of different sampling strategies on hybrid graph learning methods; and (3) exploring ways to integrate simple graph and hypergraph information. We make our source code and full datasets publicly available at https://zehui127.github.io/hybrid-graph-benchmark/.", "link": "https://arxiv.org/abs/2306.05108"}, {"id": "2306.05143", "date": "Thu, 8 Jun 2023 12:10:13 GMT", "title": "Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D\n\u00a0Shifted Window Transformer\n", "authors": ["Zehui Li", "Akashaditya Das", "William A V Beardall", "Yiren Zhao", "Guy-Bart\n\u00a0Stan\n"], "categories": ["cs.LG", "q-bio.GN\n"], "abstract": "Given the increasing volume and quality of genomics data, extracting new insights requires interpretable machine-learning models. This work presents Genomic Interpreter: a novel architecture for genomic assay prediction. This model outperforms the state-of-the-art models for genomic assay prediction tasks. Our model can identify hierarchical dependencies in genomic sites. This is achieved through the integration of 1D-Swin, a novel Transformer-based block designed by us for modelling long-range hierarchical data. Evaluated on a dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter demonstrates superior performance in chromatin accessibility and gene expression prediction and unmasks the underlying `syntax' of gene regulation.", "link": "https://arxiv.org/abs/2306.05143"}, {"id": "2306.05172", "date": "Thu, 8 Jun 2023 13:11:20 GMT", "title": "FLEdge: Benchmarking Federated Machine Learning Applications in Edge\n\u00a0Computing Systems\n", "authors": ["Herbert Woisetschl\\\"ager", "Alexander Isenko", "Ruben Mayer", "Hans-Arno\n\u00a0Jacobsen\n"], "categories": ["cs.LG", "cs.DC\nComments:", "Preprint.", "Under", "Review\nACM-class:", "I.2.11;", "C.2.4;", "C.4;", "D.2.8\n"], "abstract": "Federated Machine Learning (FL) has received considerable attention in recent years. FL benchmarks are predominantly explored in either simulated systems or data center environments, neglecting the setups of real-world systems, which are often closely linked to edge computing. We close this research gap by introducing FLEdge, a benchmark targeting FL workloads in edge computing systems. We systematically study hardware heterogeneity, energy efficiency during training, and the effect of various differential privacy levels on training in FL systems. To make this benchmark applicable to real-world scenarios, we evaluate the impact of client dropouts on state-of-the-art FL strategies with failure rates as high as 50%. FLEdge provides new insights, such as that training state-of-the-art FL workloads on older GPU-accelerated embedded devices is up to 3x more energy efficient than on modern server-grade GPUs.", "link": "https://arxiv.org/abs/2306.05172"}, {"id": "2306.05175", "date": "Thu, 8 Jun 2023 13:14:35 GMT", "title": "Large-scale Dataset Pruning with Dynamic Uncertainty\n", "authors": ["Muyang He", "Shuo Yang", "Tiejun Huang", "Bo Zhao\n"], "categories": ["cs.LG", "cs.CV\n"], "abstract": "The state of the art of many learning tasks, e.g., image classification, is advanced by collecting larger datasets and then training larger models on them. As the outcome, the increasing computational cost is becoming unaffordable. In this paper, we investigate how to prune the large-scale datasets, and thus produce an informative subset for training sophisticated deep models with negligible performance drop. We propose a simple yet effective dataset pruning method by exploring both the prediction uncertainty and training dynamics. To our knowledge, this is the first work to study dataset pruning on large-scale datasets, i.e., ImageNet-1K and ImageNet-21K, and advanced models, i.e., Swin Transformer and ConvNeXt. Extensive experimental results indicate that our method outperforms the state of the art and achieves 75% lossless compression ratio on both ImageNet-1K and ImageNet-21K. The code and pruned datasets are available at https://github.com/BAAI-DCAI/Dataset-Pruning.", "link": "https://arxiv.org/abs/2306.05175"}, {"id": "2306.05255", "date": "Thu, 8 Jun 2023 14:52:56 GMT", "title": "Toward more accurate and generalizable brain deformation estimators for\n\u00a0traumatic brain injury detection with unsupervised domain adaptation\n", "authors": ["Xianghao Zhan", "Jiawei Sun", "Yuzhe Liu", "Nicholas J. Cecchi", "Enora Le\n\u00a0Flao", "Olivier Gevaert", "Michael M. Zeineh", "David B. Camarillo\n"], "categories": ["cs.LG", "eess.SP", "physics.bio-ph", "q-bio.QM", "stat.AP\n"], "abstract": "Machine learning head models (MLHMs) are developed to estimate brain deformation for early detection of traumatic brain injury (TBI). However, the overfitting to simulated impacts and the lack of generalizability caused by distributional shift of different head impact datasets hinders the broad clinical applications of current MLHMs. We propose brain deformation estimators that integrates unsupervised domain adaptation with a deep neural network to predict whole-brain maximum principal strain (MPS) and MPS rate (MPSR). With 12,780 simulated head impacts, we performed unsupervised domain adaptation on on-field head impacts from 302 college football (CF) impacts and 457 mixed martial arts (MMA) impacts using domain regularized component analysis (DRCA) and cycle-GAN-based methods. The new model improved the MPS/MPSR estimation accuracy, with the DRCA method significantly outperforming other domain adaptation methods in prediction accuracy (p<0.001): MPS RMSE: 0.027 (CF) and 0.037 (MMA); MPSR RMSE: 7.159 (CF) and 13.022 (MMA). On another two hold-out test sets with 195 college football impacts and 260 boxing impacts, the DRCA model significantly outperformed the baseline model without domain adaptation in MPS and MPSR estimation accuracy (p<0.001). The DRCA domain adaptation reduces the MPS/MPSR estimation error to be well below TBI thresholds, enabling accurate brain deformation estimation to detect TBI in future clinical applications.", "link": "https://arxiv.org/abs/2306.05255"}, {"id": "2306.05257", "date": "Thu, 8 Jun 2023 14:54:50 GMT", "title": "Comprehensive evaluation of deep and graph learning on drug-drug\n\u00a0interactions prediction\n", "authors": ["Xuan Lin", "Lichang Dai", "Yafang Zhou", "Zu-Guo Yu", "Wen Zhang", "Jian-Yu Shi,\n\u00a0Dong-Sheng Cao", "Li Zeng", "Haowen Chen", "Bosheng Song", "Philip S. Yu and\n\u00a0Xiangxiang Zeng\n"], "categories": ["cs.LG", "q-bio.QM\nComments:", "Accepted", "by", "Briefings", "in", "Bioinformatics\nDOI:", "10.1093/bib/bbad235\n"], "abstract": "Recent advances and achievements of artificial intelligence (AI) as well as deep and graph learning models have established their usefulness in biomedical applications, especially in drug-drug interactions (DDIs). DDIs refer to a change in the effect of one drug to the presence of another drug in the human body, which plays an essential role in drug discovery and clinical research. DDIs prediction through traditional clinical trials and experiments is an expensive and time-consuming process. To correctly apply the advanced AI and deep learning, the developer and user meet various challenges such as the availability and encoding of data resources, and the design of computational methods. This review summarizes chemical structure based, network based, NLP based and hybrid methods, providing an updated and accessible guide to the broad researchers and development community with different domain knowledge. We introduce widely-used molecular representation and describe the theoretical frameworks of graph neural network models for representing molecular structures. We present the advantages and disadvantages of deep and graph learning methods by performing comparative experiments. We discuss the potential technical challenges and highlight future directions of deep and graph learning models for accelerating DDIs prediction.", "link": "https://arxiv.org/abs/2306.05257"}, {"id": "2306.05275", "date": "Thu, 8 Jun 2023 15:21:47 GMT", "title": "Federated Linear Contextual Bandits with User-level Differential Privacy\n", "authors": ["Ruiquan Huang", "Huanyu Zhang", "Luca Melis", "Milan Shen", "Meisam Hajzinia,\n\u00a0Jing Yang\n"], "categories": ["cs.LG", "cs.CR", "stat.ML\nComments:", "Accepted", "by", "ICML", "2023\n"], "abstract": "This paper studies federated linear contextual bandits under the notion of user-level differential privacy (DP). We first introduce a unified federated bandits framework that can accommodate various definitions of DP in the sequential decision-making setting. We then formally introduce user-level central DP (CDP) and local DP (LDP) in the federated bandits framework, and investigate the fundamental trade-offs between the learning regrets and the corresponding DP guarantees in a federated linear contextual bandits model. For CDP, we propose a federated algorithm termed as \\robin and show that it is near-optimal in terms of the number of clients $M$ and the privacy budget $\\varepsilon$ by deriving nearly-matching upper and lower regret bounds when user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating that learning under user-level $(\\varepsilon,\\delta)$-LDP must suffer a regret blow-up factor at least {$\\min\\{1/\\varepsilon,M\\}$ or $\\min\\{1/\\sqrt{\\varepsilon},\\sqrt{M}\\}$} under different conditions.", "link": "https://arxiv.org/abs/2306.05275"}, {"id": "2306.05300", "date": "Thu, 8 Jun 2023 15:45:57 GMT", "title": "Correlated Noise in Epoch-Based Stochastic Gradient Descent:\n\u00a0Implications for Weight Variances\n", "authors": ["Marcel K\\\"uhn", "Bernd Rosenow\n"], "categories": ["cs.LG", "cond-mat.dis-nn\nComments:", "25", "pages,", "7", "figures\n"], "abstract": "Stochastic gradient descent (SGD) has become a cornerstone of neural network optimization, yet the noise introduced by SGD is often assumed to be uncorrelated over time, despite the ubiquity of epoch-based training. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum, limited to a quadratic loss. Our main contributions are twofold: first, we calculate the exact autocorrelation of the noise for training in epochs under the assumption that the noise is independent of small fluctuations in the weight vector; second, we explore the influence of correlations introduced by the epoch-based learning scheme on SGD dynamics. We find that for directions with a curvature greater than a hyperparameter-dependent crossover value, the results for uncorrelated noise are recovered. However, for relatively flat directions, the weight variance is significantly reduced. We provide an intuitive explanation for these results based on a crossover between correlation times, contributing to a deeper understanding of the dynamics of SGD in the presence of epoch-based noise correlations.", "link": "https://arxiv.org/abs/2306.05300"}, {"id": "2306.05344", "date": "Thu, 8 Jun 2023 16:46:11 GMT", "title": "A Crystal-Specific Pre-Training Framework for Crystal Material Property\n\u00a0Prediction\n", "authors": ["Haomin Yu", "Yanru Song", "Jilin Hu", "Chenjuan Guo", "Bin Yang\n"], "categories": ["cs.LG", "cond-mat.mtrl-sci\n"], "abstract": "Crystal property prediction is a crucial aspect of developing novel materials. However, there are two technical challenges to be addressed for speeding up the investigation of crystals. First, labeling crystal properties is intrinsically difficult due to the high cost and time involved in physical simulations or lab experiments. Second, crystals adhere to a specific quantum chemical principle known as periodic invariance, which is often not captured by existing machine learning methods. To overcome these challenges, we propose the crystal-specific pre-training framework for learning crystal representations with self-supervision. The framework designs a mutex mask strategy for enhancing representation learning so as to alleviate the limited labels available for crystal property prediction. Moreover, we take into account the specific periodic invariance in crystal structures by developing a periodic invariance multi-graph module and periodic attribute learning within our framework. This framework has been tested on eight different tasks. The experimental results on these tasks show that the framework achieves promising prediction performance and is able to outperform recent strong baselines.", "link": "https://arxiv.org/abs/2306.05344"}, {"id": "2306.05401", "date": "Thu, 8 Jun 2023 17:52:34 GMT", "title": "RDumb: A simple approach that questions our progress in continual\n\u00a0test-time adaptation\n", "authors": ["Ori Press", "Steffen Schneider", "Matthias K\\\"ummerer", "Matthias Bethge\n"], "categories": ["cs.LG", "cs.CV\n"], "abstract": "Test-Time Adaptation (TTA) allows to update pretrained models to changing data distributions at deployment time. While early work tested these algorithms for individual fixed distribution shifts, recent work proposed and applied methods for continual adaptation over long timescales. To examine the reported progress in the field, we propose the Continuously Changing Corruptions (CCC) benchmark to measure asymptotic performance of TTA techniques. We find that eventually all but one state-of-the-art methods collapse and perform worse than a non-adapting model, including models specifically proposed to be robust to performance collapse. In addition, we introduce a simple baseline, \"RDumb\", that periodically resets the model to its pretrained state. RDumb performs better or on par with the previously proposed state-of-the-art in all considered benchmarks. Our results show that previous TTA approaches are neither effective at regularizing adaptation to avoid collapse nor able to outperform a simplistic resetting strategy.", "link": "https://arxiv.org/abs/2306.05401"}, {"id": "2306.05412", "date": "Thu, 8 Jun 2023 17:56:46 GMT", "title": "Offline Prioritized Experience Replay\n", "authors": ["Yang Yue", "Bingyi Kang", "Xiao Ma", "Gao Huang", "Shiji Song", "Shuicheng Yan\n"], "categories": ["cs.LG", "cs.AI\nComments:", "preprint\n"], "abstract": "Offline reinforcement learning (RL) is challenged by the distributional shift problem. To address this problem, existing works mainly focus on designing sophisticated policy constraints between the learned policy and the behavior policy. However, these constraints are applied equally to well-performing and inferior actions through uniform sampling, which might negatively affect the learned policy. To alleviate this issue, we propose Offline Prioritized Experience Replay (OPER), featuring a class of priority functions designed to prioritize highly-rewarding transitions, making them more frequently visited during training. Through theoretical analysis, we show that this class of priority functions induce an improved behavior policy, and when constrained to this improved policy, a policy-constrained offline RL algorithm is likely to yield a better solution. We develop two practical strategies to obtain priority weights by estimating advantages based on a fitted value network (OPER-A) or utilizing trajectory returns (OPER-R) for quick computation. OPER is a plug-and-play component for offline RL algorithms. As case studies, we evaluate OPER on five different algorithms, including BC, TD3+BC, Onestep RL, CQL, and IQL. Extensive experiments demonstrate that both OPER-A and OPER-R significantly improve the performance for all baseline methods. Codes and priority weights are availiable at https://github.com/sail-sg/OPER.", "link": "https://arxiv.org/abs/2306.05412"}, {"id": "2306.05420", "date": "Thu, 8 Jun 2023 17:59:08 GMT", "title": "Scaling Spherical CNNs\n", "authors": ["Carlos Esteves", "Jean-Jacques Slotine", "Ameesh Makadia\n"], "categories": ["cs.LG", "cs.CV\nComments:", "Accepted", "to", "ICML'23\n"], "abstract": "Spherical CNNs generalize CNNs to functions on the sphere, by using spherical convolutions as the main linear operation. The most accurate and efficient way to compute spherical convolutions is in the spectral domain (via the convolution theorem), which is still costlier than the usual planar convolutions. For this reason, applications of spherical CNNs have so far been limited to small problems that can be approached with low model capacity. In this work, we show how spherical CNNs can be scaled for much larger problems. To achieve this, we make critical improvements including novel variants of common model components, an implementation of core operations to exploit hardware accelerator characteristics, and application-specific input representations that exploit the properties of our model. Experiments show our larger spherical CNNs reach state-of-the-art on several targets of the QM9 molecular benchmark, which was previously dominated by equivariant graph neural networks, and achieve competitive performance on multiple weather forecasting tasks. Our code is available at https://github.com/google-research/spherical-cnn.", "link": "https://arxiv.org/abs/2306.05420"}, {"id": "2306.05426", "date": "Thu, 8 Jun 2023 17:59:58 GMT", "title": "SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling\n\u00a0with Backtracking\n", "authors": ["Chris Cundy", "Stefano Ermon\n"], "categories": ["cs.LG", "cs.AI\n"], "abstract": "In many domains, autoregressive models can achieve low log-likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compounding error problem by allowing the model to revert a sampled token if it takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented without adversarial training or major architectural changes. We identify the SequenceMatch-$\\chi^2$ divergence as a more suitable training objective for autoregressive models which are used for generation. We show that empirically, SequenceMatch training leads to improvements over MLE on text generation with language models.", "link": "https://arxiv.org/abs/2306.05426"}, {"id": "2306.04781", "date": "Wed, 7 Jun 2023 21:02:20 GMT", "title": "Learning to Navigate in Turbulent Flows with Aerial Robot Swarms: A\n\u00a0Cooperative Deep Reinforcement Learning Approach\n", "authors": ["Diego Pati\\~no and Siddharth Mayya and Juan Calderon and Kostas\n\u00a0Daniilidis and David Salda\\~na\n"], "categories": ["cs.RO", "cs.LG", "cs.MA\nDOI:", "10.1109/LRA.2023.3280806\n"], "abstract": "Aerial operation in turbulent environments is a challenging problem due to the chaotic behavior of the flow. This problem is made even more complex when a team of aerial robots is trying to achieve coordinated motion in turbulent wind conditions. In this paper, we present a novel multi-robot controller to navigate in turbulent flows, decoupling the trajectory-tracking control from the turbulence compensation via a nested control architecture. Unlike previous works, our method does not learn to compensate for the air-flow at a specific time and space. Instead, our method learns to compensate for the flow based on its effect on the team. This is made possible via a deep reinforcement learning approach, implemented via a Graph Convolutional Neural Network (GCNN)-based architecture, which enables robots to achieve better wind compensation by processing the spatial-temporal correlation of wind flows across the team. Our approach scales well to large robot teams -- as each robot only uses information from its nearest neighbors -- , and generalizes well to robot teams larger than seen in training. Simulated experiments demonstrate how information sharing improves turbulence compensation in a team of aerial robots and demonstrate the flexibility of our method over different team configurations.", "link": "https://arxiv.org/abs/2306.04781"}, {"id": "2306.04647", "date": "Mon, 5 Jun 2023 01:29:24 GMT", "title": "Compressed Sensing: A Discrete Optimization Approach\n", "authors": ["Dimitris Bertsimas and Nicholas Johnson\n"], "categories": ["eess.SP", "cs.LG", "stat.ML\n"], "abstract": "We study the Compressed Sensing (CS) problem, which is the problem of finding the most sparse vector that satisfies a set of linear measurements up to some numerical tolerance. CS is a central problem in Statistics, Operations Research and Machine Learning which arises in applications such as signal processing, data compression and image reconstruction. We introduce an $\\ell_2$ regularized formulation of CS which we reformulate as a mixed integer second order cone program. We derive a second order cone relaxation of this problem and show that under mild conditions on the regularization parameter, the resulting relaxation is equivalent to the well studied basis pursuit denoising problem. We present a semidefinite relaxation that strengthens the second order cone relaxation and develop a custom branch-and-bound algorithm that leverages our second order cone relaxation to solve instances of CS to certifiable optimality. Our numerical results show that our approach produces solutions that are on average $6.22\\%$ more sparse than solutions returned by state of the art benchmark methods on synthetic data in minutes. On real world ECG data, for a given $\\ell_2$ reconstruction error our approach produces solutions that are on average $9.95\\%$ more sparse than benchmark methods, while for a given sparsity level our approach produces solutions that have on average $10.77\\%$ lower reconstruction error than benchmark methods in minutes.", "link": "https://arxiv.org/abs/2306.04647"}, {"id": "2306.04655", "date": "Tue, 6 Jun 2023 16:14:15 GMT", "title": "Modulation Classification Through Deep Learning Using Resolution\n\u00a0Transformed Spectrograms\n", "authors": ["Muhammad Waqas", "Muhammad Ashraf", "Muhammad Zakwan\n"], "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS\nComments:", "15", "pages,", "12", "figures\n"], "abstract": "Modulation classification is an essential step of signal processing and has been regularly applied in the field of tele-communication. Since variations of frequency with respect to time remains a vital distinction among radio signals having different modulation formats, these variations can be used for feature extraction by converting 1-D radio signals into frequency domain. In this paper, we propose a scheme for Automatic Modulation Classification (AMC) using modern architectures of Convolutional Neural Networks (CNN), through generating spectrum images of eleven different modulation types. Additionally, we perform resolution transformation of spectrograms that results up to 99.61% of computational load reduction and 8x faster conversion from the received I/Q data. This proposed AMC is implemented on CPU and GPU, to recognize digital as well as analogue signal modulation schemes on signals. The performance is evaluated on existing CNN models including SqueezeNet, Resnet-50, InceptionResnet-V2, Inception-V3, VGG-16 and Densenet-201. Best results of 91.2% are achieved in presence of AWGN and other noise impairments in the signals, stating that the transformed spectrogram-based AMC has good classification accuracy as the spectral features are highly discriminant, and CNN based models have capability to extract these high-dimensional features. The spectrograms were created under different SNRs ranging from 5 to 30db with a step size of 5db to observe the experimental results at various SNR levels. The proposed methodology is efficient to be applied in wireless communication networks for real-time applications.", "link": "https://arxiv.org/abs/2306.04655"}, {"id": "2306.04689", "date": "Wed, 7 Jun 2023 18:00:06 GMT", "title": "Multiscale Flow for Robust and Optimal Cosmological Analysis\n", "authors": ["Biwei Dai and Uros Seljak\n"], "categories": ["astro-ph.CO", "cs.LG", "physics.data-an\nComments:", "12", "pages,", "7", "figures.", "Comments", "welcome\n"], "abstract": "We propose Multiscale Flow, a generative Normalizing Flow that creates samples and models the field-level likelihood of two-dimensional cosmological data such as weak lensing. Multiscale Flow uses hierarchical decomposition of cosmological fields via a wavelet basis, and then models different wavelet components separately as Normalizing Flows. The log-likelihood of the original cosmological field can be recovered by summing over the log-likelihood of each wavelet term. This decomposition allows us to separate the information from different scales and identify distribution shifts in the data such as unknown scale-dependent systematics. The resulting likelihood analysis can not only identify these types of systematics, but can also be made optimal, in the sense that the Multiscale Flow can learn the full likelihood at the field without any dimensionality reduction. We apply Multiscale Flow to weak lensing mock datasets for cosmological inference, and show that it significantly outperforms traditional summary statistics such as power spectrum and peak counts, as well as novel Machine Learning based summary statistics such as scattering transform and convolutional neural networks. We further show that Multiscale Flow is able to identify distribution shifts not in the training data such as baryonic effects. Finally, we demonstrate that Multiscale Flow can be used to generate realistic samples of weak lensing data.", "link": "https://arxiv.org/abs/2306.04689"}, {"id": "2306.04712", "date": "Wed, 7 Jun 2023 18:23:38 GMT", "title": "Differentiable Earth Mover's Distance for Data Compression at the\n\u00a0High-Luminosity LHC\n", "authors": ["Rohan Shenoy and Javier Duarte and Christian Herwig and James\n\u00a0Hirschauer and Daniel Noonan and Maurizio Pierini and Nhan Tran and Cristina\n\u00a0Mantilla Suarez\n"], "categories": ["hep-ex", "cs.LG", "physics.ins-det\nComments:", "15", "pages,", "7", "figures,", "submitted", "to", "Machine", "Learning:", "Science", "and\n\u00a0Technology\nReport-no:", "FERMILAB-PUB-23-288-CMS-CSAID\n"], "abstract": "The Earth mover's distance (EMD) is a useful metric for image recognition and classification, but its usual implementations are not differentiable or too slow to be used as a loss function for training other algorithms via gradient descent. In this paper, we train a convolutional neural network (CNN) to learn a differentiable, fast approximation of the EMD and demonstrate that it can be used as a substitute for computing-intensive EMD implementations. We apply this differentiable approximation in the training of an autoencoder-inspired neural network (encoder NN) for data compression at the high-luminosity LHC at CERN. The goal of this encoder NN is to compress the data while preserving the information related to the distribution of energy deposits in particle detectors. We demonstrate that the performance of our encoder NN trained using the differentiable EMD CNN surpasses that of training with loss functions based on mean squared error.", "link": "https://arxiv.org/abs/2306.04712"}, {"id": "2306.04730", "date": "Wed, 7 Jun 2023 18:49:19 GMT", "title": "Stochastic Natural Thresholding Algorithms\n", "authors": ["Rachel Grotheer", "Shuang Li", "Anna Ma", "Deanna Needell", "and Jing Qin\n"], "categories": ["eess.SP", "cs.LG", "cs.NA", "math.NA", "math.OC", "stat.ML\n"], "abstract": "Sparse signal recovery is one of the most fundamental problems in various applications, including medical imaging and remote sensing. Many greedy algorithms based on the family of hard thresholding operators have been developed to solve the sparse signal recovery problem. More recently, Natural Thresholding (NT) has been proposed with improved computational efficiency. This paper proposes and discusses convergence guarantees for stochastic natural thresholding algorithms by extending the NT from the deterministic version with linear measurements to the stochastic version with a general objective function. We also conduct various numerical experiments on linear and nonlinear measurements to demonstrate the performance of StoNT.", "link": "https://arxiv.org/abs/2306.04730"}, {"id": "2306.04746", "date": "Wed, 7 Jun 2023 19:49:41 GMT", "title": "Using Large Language Model Annotations for Valid Downstream Statistical\n\u00a0Inference in Social Science: Design-Based Semi-Supervised Learning\n", "authors": ["Naoki Egami", "Musashi Jacobs-Harukawa", "Brandon M. Stewart", "Hanying Wei\n"], "categories": ["stat.ME", "cs.CL", "cs.LG", "stat.ML\n"], "abstract": "In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised learning (DSL) estimator. DSL employs a doubly-robust procedure to combine surrogate labels with a smaller number of gold-standard labels. Our approach guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased, without requiring stringent assumptions, by controlling the probability of sampling documents for gold-standard labeling. Both our theoretical analysis and experimental results show that DSL provides valid statistical inference while achieving root mean squared errors comparable to existing alternatives that focus only on prediction without statistical guarantees.", "link": "https://arxiv.org/abs/2306.04746"}, {"id": "2306.04810", "date": "Wed, 7 Jun 2023 22:14:33 GMT", "title": "Correlative Information Maximization: A Biologically Plausible Approach\n\u00a0to Supervised Deep Neural Networks without Weight Symmetry\n", "authors": ["Bariscan Bozkurt", "Cengiz Pehlevan", "Alper T Erdogan\n"], "categories": ["cs.NE", "cs.IT", "cs.LG", "math.IT", "q-bio.NC\nComments:", "Preprint,", "31", "pages\n"], "abstract": "The backpropagation algorithm has experienced remarkable success in training large-scale artificial neural networks, however, its biological-plausibility is disputed, and it remains an open question whether the brain employs supervised learning mechanisms akin to it. Here, we propose correlative information maximization between layer activations as an alternative normative approach to describe the signal propagation in biological neural networks in both forward and backward directions. This new framework addresses many concerns about the biological-plausibility of conventional artificial neural networks and the backpropagation algorithm. The coordinate descent-based optimization of the corresponding objective, combined with the mean square error loss function for fitting labeled supervision data, gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Furthermore, our approach provides a natural resolution to the weight symmetry problem between forward and backward signal propagation paths, a significant critique against the plausibility of the conventional backpropagation algorithm. This is achieved by leveraging two alternative, yet equivalent forms of the correlative mutual information objective. These alternatives intrinsically lead to forward and backward prediction networks without weight symmetry issues, providing a compelling solution to this long-standing challenge.", "link": "https://arxiv.org/abs/2306.04810"}, {"id": "2306.04836", "date": "Wed, 7 Jun 2023 23:55:12 GMT", "title": "$K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic\n\u00a0Control\n", "authors": ["Michael Giegrich", "Roel Oomen", "Christoph Reisinger\n"], "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.ME", "stat.TH\n"], "abstract": "We propose a novel $K$-nearest neighbor resampling procedure for estimating the performance of a policy from historical data containing realized episodes of a decision process generated under a different policy. We focus on feedback policies that depend deterministically on the current state in environments with continuous state-action spaces and system-inherent stochasticity effected by chosen actions. Such settings are common in a wide range of high-stake applications and are actively investigated in the context of stochastic control. Our procedure exploits that similar state/action pairs (in a metric sense) are associated with similar rewards and state transitions. This enables our resampling procedure to tackle the counterfactual estimation problem underlying off-policy evaluation (OPE) by simulating trajectories similarly to Monte Carlo methods. Compared to other OPE methods, our algorithm does not require optimization, can be efficiently implemented via tree-based nearest neighbor search and parallelization and does not explicitly assume a parametric model for the environment's dynamics. These properties make the proposed resampling algorithm particularly useful for stochastic control environments. We prove that our method is statistically consistent in estimating the performance of a policy in the OPE setting under weak assumptions and for data sets containing entire episodes rather than independent transitions. To establish the consistency, we generalize Stone's Theorem, a well-known result in nonparametric statistics on local averaging, to include episodic data and the counterfactual estimation underlying OPE. Numerical experiments demonstrate the effectiveness of the algorithm in a variety of stochastic control settings including a linear quadratic regulator, trade execution in limit order books and online stochastic bin packing.", "link": "https://arxiv.org/abs/2306.04836"}, {"id": "2306.04843", "date": "Thu, 8 Jun 2023 00:31:27 GMT", "title": "Classical Verification of Quantum Learning\n", "authors": ["Matthias C. Caro", "Marcel Hinsche", "Marios Ioannou", "Alexander Nietner,\n\u00a0Ryan Sweke\n"], "categories": ["quant-ph", "cs.CC", "cs.LG", "stat.ML\nComments:", "12", "+", "43", "+", "22", "pages,", "1", "table,", "1", "figure\n"], "abstract": "Quantum data access and quantum processing can make certain classically intractable learning tasks feasible. However, quantum capabilities will only be available to a select few in the near future. Thus, reliable schemes that allow classical clients to delegate learning to untrusted quantum servers are required to facilitate widespread access to quantum learning advantages. Building on a recently introduced framework of interactive proof systems for classical machine learning, we develop a framework for classical verification of quantum learning. We exhibit learning problems that a classical learner cannot efficiently solve on their own, but that they can efficiently and reliably solve when interacting with an untrusted quantum prover. Concretely, we consider the problems of agnostic learning parities and Fourier-sparse functions with respect to distributions with uniform input marginal. We propose a new quantum data access model that we call \"mixture-of-superpositions\" quantum examples, based on which we give efficient quantum learning algorithms for these tasks. Moreover, we prove that agnostic quantum parity and Fourier-sparse learning can be efficiently verified by a classical verifier with only random example or statistical query access. Finally, we showcase two general scenarios in learning and verification in which quantum mixture-of-superpositions examples do not lead to sample complexity improvements over classical data. Our results demonstrate that the potential power of quantum data for learning tasks, while not unlimited, can be utilized by classical agents through interaction with untrusted quantum entities.", "link": "https://arxiv.org/abs/2306.04843"}, {"id": "2306.04884", "date": "Thu, 8 Jun 2023 02:29:37 GMT", "title": "Faster Approximation Algorithms for Parameterized Graph Clustering and\n\u00a0Edge Labeling\n", "authors": ["Vedangi Bengali", "Nate Veldt\n"], "categories": ["cs.DS", "cs.DM", "cs.LG", "cs.SI\nComments:", "12", "pages,4", "figures\n"], "abstract": "Graph clustering is a fundamental task in network analysis where the goal is to detect sets of nodes that are well-connected to each other but sparsely connected to the rest of the graph. We present faster approximation algorithms for an NP-hard parameterized clustering framework called LambdaCC, which is governed by a tunable resolution parameter and generalizes many other clustering objectives such as modularity, sparsest cut, and cluster deletion. Previous LambdaCC algorithms are either heuristics with no approximation guarantees, or computationally expensive approximation algorithms. We provide fast new approximation algorithms that can be made purely combinatorial. These rely on a new parameterized edge labeling problem we introduce that generalizes previous edge labeling problems that are based on the principle of strong triadic closure and are of independent interest in social network analysis. Our methods are orders of magnitude more scalable than previous approximation algorithms and our lower bounds allow us to obtain a posteriori approximation guarantees for previous heuristics that have no approximation guarantees of their own.", "link": "https://arxiv.org/abs/2306.04884"}, {"id": "2306.04902", "date": "Thu, 8 Jun 2023 03:09:49 GMT", "title": "A Cover Time Study of a non-Markovian Algorithm\n", "authors": ["Guanhua Fang", "Gennady Samorodnitsky", "Zhiqiang Xu\n"], "categories": ["cs.DS", "cs.LG", "math.ST", "stat.TH\nComments:", "22", "pages\n"], "abstract": "Given a traversal algorithm, cover time is the expected number of steps needed to visit all nodes in a given graph. A smaller cover time means a higher exploration efficiency of traversal algorithm. Although random walk algorithms have been studied extensively in the existing literature, there has been no cover time result for any non-Markovian method. In this work, we stand on a theoretical perspective and show that the negative feedback strategy (a count-based exploration method) is better than the naive random walk search. In particular, the former strategy can locally improve the search efficiency for an arbitrary graph. It also achieves smaller cover times for special but important graphs, including clique graphs, tree graphs, etc. Moreover, we make connections between our results and reinforcement learning literature to give new insights on why classical UCB and MCTS algorithms are so useful. Various numerical results corroborate our theoretical findings.", "link": "https://arxiv.org/abs/2306.04902"}, {"id": "2306.04930", "date": "Thu, 8 Jun 2023 04:24:24 GMT", "title": "When to Show a Suggestion? Integrating Human Feedback in AI-Assisted\n\u00a0Programming\n", "authors": ["Hussein Mozannar", "Gagan Bansal", "Adam Fourney", "Eric Horvitz\n"], "categories": ["cs.HC", "cs.LG", "cs.SE\nComments:", "arXiv", "admin", "note:", "text", "overlap", "with", "arXiv:2210.14306\n"], "abstract": "AI powered code-recommendation systems, such as Copilot and CodeWhisperer, provide code suggestions inside a programmer's environment (e.g., an IDE) with the aim to improve their productivity. Since, in these scenarios, programmers accept and reject suggestions, ideally, such a system should use this feedback in furtherance of this goal. In this work we leverage prior data of programmers interacting with Copilot to develop interventions that can save programmer time. We propose a utility theory framework, which models this interaction with programmers and decides when and which suggestions to display. Our framework Conditional suggestion Display from Human Feedback (CDHF) is based on predictive models of programmer actions. Using data from 535 programmers we build models that predict the likelihood of suggestion acceptance. In a retrospective evaluation on real-world programming tasks solved with AI-assisted programming, we find that CDHF can achieve favorable tradeoffs. Our findings show the promise of integrating human feedback to improve interaction with large language models in scenarios such as programming and possibly writing tasks.", "link": "https://arxiv.org/abs/2306.04930"}, {"id": "2306.04933", "date": "Thu, 8 Jun 2023 04:31:48 GMT", "title": "InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural\n\u00a0Language Understanding\n", "authors": ["Junda Wu", "Tong Yu", "Rui Wang", "Zhao Song", "Ruiyi Zhang", "Handong Zhao,\n\u00a0Chaochao Lu", "Shuai Li", "Ricardo Henao\n"], "categories": ["cs.CL", "cs.LG", "stat.ML\n"], "abstract": "Soft prompt tuning achieves superior performances across a wide range of few-shot tasks. However, the performances of prompt tuning can be highly sensitive to the initialization of the prompts. We also empirically observe that conventional prompt tuning methods cannot encode and learn sufficient task-relevant information from prompt tokens. In this work, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing mutual information between prompts and other model parameters (or encoded representations). This novel view helps us to develop a more efficient, accurate and robust soft prompt tuning method InfoPrompt. With this framework, we develop two novel mutual information based loss functions, to (i) discover proper prompt initialization for the downstream tasks and learn sufficient task-relevant information from prompt tokens and (ii) encourage the output representation from the pretrained language model to be more aware of the task-relevant information captured in the learnt prompt. Extensive experiments validate that InfoPrompt can significantly accelerate the convergence of the prompt tuning and outperform traditional prompt tuning methods. Finally, we provide a formal theoretical result for showing to show that gradient descent type algorithm can be used to train our mutual information loss.", "link": "https://arxiv.org/abs/2306.04933"}, {"id": "2306.04956", "date": "Thu, 8 Jun 2023 06:06:42 GMT", "title": "Adaptive Fake Audio Detection with Low-Rank Model Squeezing\n", "authors": ["Xiaohui Zhang", "Jiangyan Yi", "Jianhua Tao", "Chenlong Wang", "Le Xu and\n\u00a0Ruibo Fu\n"], "categories": ["cs.SD", "cs.LG", "eess.AS\n"], "abstract": "The rapid advancement of spoofing algorithms necessitates the development of robust detection methods capable of accurately identifying emerging fake audio. Traditional approaches, such as finetuning on new datasets containing these novel spoofing algorithms, are computationally intensive and pose a risk of impairing the acquired knowledge of known fake audio types. To address these challenges, this paper proposes an innovative approach that mitigates the limitations associated with finetuning. We introduce the concept of training low-rank adaptation matrices tailored specifically to the newly emerging fake audio types. During the inference stage, these adaptation matrices are combined with the existing model to generate the final prediction output. Extensive experimentation is conducted to evaluate the efficacy of the proposed method. The results demonstrate that our approach effectively preserves the prediction accuracy of the existing model for known fake audio types. Furthermore, our approach offers several advantages, including reduced storage memory requirements and lower equal error rates compared to conventional finetuning methods, particularly on specific spoofing algorithms.", "link": "https://arxiv.org/abs/2306.04956"}, {"id": "2306.05014", "date": "Thu, 8 Jun 2023 08:07:54 GMT", "title": "Learning Closed-form Equations for Subgrid-scale Closures from\n\u00a0High-fidelity Data: Promises and Challenges\n", "authors": ["Karan Jakhar", "Yifei Guan", "Rambod Mojgani", "Ashesh Chattopadhyay", "Pedram\n\u00a0Hassanzadeh and Laura Zanna\n"], "categories": ["physics.flu-dyn", "cs.LG", "physics.ao-ph\nComments:", "40", "pages,", "4", "figures.", "The", "codes", "and", "data", "used", "in", "this", "work", "can", "be\n\u00a0found", "at", "https://github.com/jakharkaran/EqsDiscovery_2D-FHIT_RBC", "and\n\u00a0https://doi.org/10.5281/zenodo.7500647,", "respectively\nMSC-class:", "76F65", "(Primary)", "86A08,", "68T01,", "76F05,", "76F35", "(Secondary)\nACM-class:", "J.2;", "I.2.0;", "G.1.8\n"], "abstract": "There is growing interest in discovering interpretable, closed-form equations for subgrid-scale (SGS) closures/parameterizations of complex processes in Earth system. Here, we apply a common equation-discovery technique with expansive libraries to learn closures from filtered direct numerical simulations of 2D forced turbulence and Rayleigh-B\\'enard convection (RBC). Across common filters, we robustly discover closures of the same form for momentum and heat fluxes. These closures depend on nonlinear combinations of gradients of filtered variables (velocity, temperature), with constants that are independent of the fluid/flow properties and only depend on filter type/size. We show that these closures are the nonlinear gradient model (NGM), which is derivable analytically using Taylor-series expansions. In fact, we suggest that with common (physics-free) equation-discovery algorithms, regardless of the system/physics, discovered closures are always consistent with the Taylor-series. Like previous studies, we find that large-eddy simulations with NGM closures are unstable, despite significant similarities between the true and NGM-predicted fluxes (pattern correlations $> 0.95$). We identify two shortcomings as reasons for these instabilities: in 2D, NGM produces zero kinetic energy transfer between resolved and subgrid scales, lacking both diffusion and backscattering. In RBC, backscattering of potential energy is poorly predicted. Moreover, we show that SGS fluxes diagnosed from data, presumed the \"truth\" for discovery, depend on filtering procedures and are not unique. Accordingly, to learn accurate, stable closures from high-fidelity data in future work, we propose several ideas around using physics-informed libraries, loss functions, and metrics. These findings are relevant beyond turbulence to closure modeling of any multi-scale system.", "link": "https://arxiv.org/abs/2306.05014"}, {"id": "2306.05088", "date": "Thu, 8 Jun 2023 10:42:44 GMT", "title": "The ART of Conversation: Measuring Phonetic Convergence and Deliberate\n\u00a0Imitation in L2-Speech with a Siamese RNN\n", "authors": ["Zheng Yuan (1 and 2)", "Aldo Pastore (1 and 2)", "Dorina de Jong (1 and\n\u00a02)", "Hao Xu (3)", "Luciano Fadiga (1 and 2)", "Alessandro D'Ausilio (1 and 2) ((1)\n\u00a0Istituto Italiano di Tecnologia", "Italy", "(2) Universit\\`a degli Studi di\n\u00a0Ferrara", "Italy", "(3) University of California San Diego", "USA)\n"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS\nComments:", "Accepted", "at", "INTERSPEECH", "2023\n"], "abstract": "Phonetic convergence describes the automatic and unconscious speech adaptation of two interlocutors in a conversation. This paper proposes a Siamese recurrent neural network (RNN) architecture to measure the convergence of the holistic spectral characteristics of speech sounds in an L2-L2 interaction. We extend an alternating reading task (the ART) dataset by adding 20 native Slovak L2 English speakers. We train and test the Siamese RNN model to measure phonetic convergence of L2 English speech from three different native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10 dyads). Our results indicate that the Siamese RNN model effectively captures the dynamics of phonetic convergence and the speaker's imitation ability. Moreover, this text-independent model is scalable and capable of handling L1-induced speaker variability.", "link": "https://arxiv.org/abs/2306.05088"}, {"id": "2306.05100", "date": "Thu, 8 Jun 2023 10:58:46 GMT", "title": "Communication-Efficient Gradient Descent-Accent Methods for Distributed\n\u00a0Variational Inequalities: Unified Analysis and Local Updates\n", "authors": ["Siqi Zhang", "Sayantan Choudhury", "Sebastian U Stich", "Nicolas Loizou\n"], "categories": ["math.OC", "cs.LG", "stat.ML\n"], "abstract": "Distributed and federated learning algorithms and techniques associated primarily with minimization problems. However, with the increase of minimax optimization and variational inequality problems in machine learning, the necessity of designing efficient distributed/federated learning approaches for these problems is becoming more apparent. In this paper, we provide a unified convergence analysis of communication-efficient local training methods for distributed variational inequality problems (VIPs). Our approach is based on a general key assumption on the stochastic estimates that allows us to propose and analyze several novel local training algorithms under a single framework for solving a class of structured non-monotone VIPs. We present the first local gradient descent-accent algorithms with provable improved communication complexity for solving distributed variational inequalities on heterogeneous data. The general algorithmic framework recovers state-of-the-art algorithms and their sharp convergence guarantees when the setting is specialized to minimization or minimax optimization problems. Finally, we demonstrate the strong performance of the proposed algorithms compared to state-of-the-art methods when solving federated minimax optimization problems.", "link": "https://arxiv.org/abs/2306.05100"}, {"id": "2306.05245", "date": "Thu, 8 Jun 2023 14:44:23 GMT", "title": "Matching Latent Encoding for Audio-Text based Keyword Spotting\n", "authors": ["Kumari Nishu", "Minsik Cho", "Devang Naik\n"], "categories": ["eess.AS", "cs.LG", "cs.SD\n"], "abstract": "Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown high-quality results, but the key challenge of how to semantically align two embeddings for multi-word keywords of different sequence lengths remains largely unsolved. In this paper, we propose an audio-text-based end-to-end model architecture for flexible keyword spotting (KWS), which builds upon learned audio and text embeddings. Our architecture uses a novel dynamic programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally partition the audio sequence into the same length as the word-based text sequence using the monotonic alignment of spoken content. Our proposed model consists of an encoder block to get audio and text embeddings, a projector block to project individual embeddings to a common latent space, and an audio-text aligner containing a novel DSP algorithm, which aligns the audio and text embeddings to determine if the spoken content is the same as the text. Experimental results show that our DSP is more effective than other partitioning schemes, and the proposed architecture outperformed the state-of-the-art results on the public dataset in terms of Area Under the ROC Curve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively.", "link": "https://arxiv.org/abs/2306.05245"}, {"id": "2306.05294", "date": "Wed, 7 Jun 2023 08:18:56 GMT", "title": "Deep Learning with Partially Labeled Data for Radio Map Reconstruction\n", "authors": ["Alkesandra Malkova and Massih-Reza Amini and Benoit Denis and\n\u00a0Christophe Villien\n"], "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT\nComments:", "42", "pages,", "39", "figures\n"], "abstract": "In this paper, we address the problem of Received Signal Strength map reconstruction based on location-dependent radio measurements and utilizing side knowledge about the local region; for example, city plan, terrain height, gateway position. Depending on the quantity of such prior side information, we employ Neural Architecture Search to find an optimized Neural Network model with the best architecture for each of the supposed settings. We demonstrate that using additional side information enhances the final accuracy of the Received Signal Strength map reconstruction on three datasets that correspond to three major cities, particularly in sub-areas near the gateways where larger variations of the average received signal power are typically observed.", "link": "https://arxiv.org/abs/2306.05294"}, {"id": "2306.05307", "date": "Thu, 8 Jun 2023 15:56:57 GMT", "title": "Are fairness metric scores enough to assess discrimination biases in\n\u00a0machine learning?\n", "authors": ["Fanny Jourdan", "Laurent Risser", "Jean-Michel Loubes", "Nicholas Asher\n"], "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.ML\nComments:", "Accepted", "for", "publication", "at", "Third", "Workshop", "on", "Trustworthy", "Natural\n\u00a0Language", "Processing,", "ACL", "2023\n"], "abstract": "This paper presents novel experiments shedding light on the shortcomings of current metrics for assessing biases of gender discrimination made by machine learning algorithms on textual data. We focus on the Bios dataset, and our learning task is to predict the occupation of individuals, based on their biography. Such prediction tasks are common in commercial Natural Language Processing (NLP) applications such as automatic job recommendations. We address an important limitation of theoretical discussions dealing with group-wise fairness metrics: they focus on large datasets, although the norm in many industrial NLP applications is to use small to reasonably large linguistic datasets for which the main practical constraint is to get a good prediction accuracy. We then question how reliable are different popular measures of bias when the size of the training set is simply sufficient to learn reasonably accurate predictions. Our experiments sample the Bios dataset and learn more than 200 models on different sample sizes. This allows us to statistically study our results and to confirm that common gender bias indices provide diverging and sometimes unreliable results when applied to relatively small training and test samples. This highlights the crucial importance of variance calculations for providing sound results in this field.", "link": "https://arxiv.org/abs/2306.05307"}, {"id": "2306.05321", "date": "Thu, 8 Jun 2023 16:13:29 GMT", "title": "Real-time whole-heart electromechanical simulations using Latent Neural\n\u00a0Ordinary Differential Equations\n", "authors": ["Matteo Salvador", "Marina Strocchi", "Francesco Regazzoni", "Luca Dede',\n\u00a0Steven Niederer", "Alfio Quarteroni\n"], "categories": ["math.NA", "cs.LG", "cs.NA\n"], "abstract": "Cardiac digital twins provide a physics and physiology informed framework to deliver predictive and personalized medicine. However, high-fidelity multi-scale cardiac models remain a barrier to adoption due to their extensive computational costs and the high number of model evaluations needed for patient-specific personalization. Artificial Intelligence-based methods can make the creation of fast and accurate whole-heart digital twins feasible. In this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to learn the temporal pressure-volume dynamics of a heart failure patient. Our surrogate model based on LNODEs is trained from 400 3D-0D whole-heart closed-loop electromechanical simulations while accounting for 43 model parameters, describing single cell through to whole organ and cardiovascular hemodynamics. The trained LNODEs provides a compact and efficient representation of the 3D-0D model in a latent space by means of a feedforward fully-connected Artificial Neural Network that retains 3 hidden layers with 13 neurons per layer and allows for 300x real-time numerical simulations of the cardiac function on a single processor of a standard laptop. This surrogate model is employed to perform global sensitivity analysis and robust parameter estimation with uncertainty quantification in 3 hours of computations, still on a single processor. We match pressure and volume time traces unseen by the LNODEs during the training phase and we calibrate 4 to 11 model parameters while also providing their posterior distribution. This paper introduces the most advanced surrogate model of cardiac function available in the literature and opens new important venues for parameter calibration in cardiac digital twins.", "link": "https://arxiv.org/abs/2306.05321"}, {"id": "2306.05363", "date": "Thu, 8 Jun 2023 17:07:24 GMT", "title": "Subject clustering by IF-PCA and several recent methods\n", "authors": ["Dieyi Chen", "Jiashun Jin", "Zheng Tracy Ke\n"], "categories": ["stat.ME", "cs.LG", "math.ST", "stat.AP", "stat.TH\n"], "abstract": "Subject clustering (i.e., the use of measured features to cluster subjects, such as patients or cells, into multiple groups) is a problem of great interest. In recent years, many approaches were proposed, among which unsupervised deep learning (UDL) has received a great deal of attention. Two interesting questions are (a) how to combine the strengths of UDL and other approaches, and (b) how these approaches compare to one other. We combine Variational Auto-Encoder (VAE), a popular UDL approach, with the recent idea of Influential Feature PCA (IF-PCA), and propose IF-VAE as a new method for subject clustering. We study IF-VAE and compare it with several other methods (including IF-PCA, VAE, Seurat, and SC3) on $10$ gene microarray data sets and $8$ single-cell RNA-seq data sets. We find that IF-VAE significantly improves over VAE, but still underperforms IF-PCA. We also find that IF-PCA is quite competitive, which slightly outperforms Seurat and SC3 over the $8$ single-cell data sets. IF-PCA is conceptually simple and permits delicate analysis. We demonstrate that IF-PCA is capable of achieving the phase transition in a Rare/Weak model. Comparatively, Seurat and SC3 are more complex and theoretically difficult to analyze (for these reasons, their optimality remains unclear).", "link": "https://arxiv.org/abs/2306.05363"}, {"id": "2306.04765", "date": "Wed, 7 Jun 2023 20:24:43 GMT", "title": "The HCI Aspects of Public Deployment of Research Chatbots: A User Study,\n\u00a0Design Recommendations, and Open Challenges\n", "authors": ["Morteza Behrooz", "William Ngan", "Joshua Lane", "Giuliano Morse", "Benjamin\n\u00a0Babcock", "Kurt Shuster", "Mojtaba Komeili", "Moya Chen", "Melanie Kambadur", "Y-Lan\n\u00a0Boureau", "Jason Weston\n"], "categories": ["cs.AI", "cs.CL\n"], "abstract": "Publicly deploying research chatbots is a nuanced topic involving necessary risk-benefit analyses. While there have recently been frequent discussions on whether it is responsible to deploy such models, there has been far less focus on the interaction paradigms and design approaches that the resulting interfaces should adopt, in order to achieve their goals more effectively. We aim to pose, ground, and attempt to answer HCI questions involved in this scope, by reporting on a mixed-methods user study conducted on a recent research chatbot. We find that abstract anthropomorphic representation for the agent has a significant effect on user's perception, that offering AI explainability may have an impact on feedback rates, and that two (diegetic and extradiegetic) levels of the chat experience should be intentionally designed. We offer design recommendations and areas of further focus for the research community.", "link": "https://arxiv.org/abs/2306.04765"}, {"id": "2306.04887", "date": "Thu, 8 Jun 2023 02:30:55 GMT", "title": "Big-data-driven and AI-based framework to enable personalization in\n\u00a0wireless networks\n", "authors": ["Rawan Alkurd", "Ibrahim Abualhaol", "and Halim Yanikomeroglu\n"], "categories": ["cs.AI", "cs.NI\nJournal-ref:", "IEEE", "Communications", "Magazine", "(", "Volume:", "58,", "Issue:", "3,", "March", "2020)\n"], "abstract": "Current communication networks use design methodologies that prevent the realization of maximum network efficiency. In the first place, while users' perception of satisfactory service diverges widely, current networks are designed to be a \"universal fit,\" where they are generally over-engineered to deliver services appealing to all types of users. Also, current networks lack user-level data cognitive intelligence that would enable fast personalized network decisions and actions through automation. Thus, in this article, we propose the utilization of AI, big data analytics, and real-time non-intrusive user feedback in order to enable the personalization of wireless networks. Based on each user's actual QoS requirements and context, a multi-objective formulation enables the network to micro-manage and optimize the provided QoS and user satisfaction levels simultaneously. Moreover, in order to enable user feedback tracking and measurement, we propose a user satisfaction model based on the zone of tolerance concept. Furthermore, we propose a big-data-driven and AI-based personalization framework to integrate personalization into wireless networks. Finally, we implement a personalized network prototype to demonstrate the proposed personalization concept and its potential benefits through a case study. The case study shows how personalization can be realized to enable the efficient optimization of network resources such that certain requirement levels of user satisfaction and revenue in the form of saved resources are achieved.", "link": "https://arxiv.org/abs/2306.04887"}, {"id": "2306.04962", "date": "Thu, 8 Jun 2023 06:37:04 GMT", "title": "arXiv4TGC: Large-Scale Datasets for Temporal Graph Clustering\n", "authors": ["Meng Liu", "Ke Liang", "Yue Liu", "Siwei Wang", "Sihang Zhou", "Xinwang Liu\n"], "categories": ["cs.AI", "cs.LG\n"], "abstract": "Temporal graph clustering (TGC) is a crucial task in temporal graph learning. Its focus is on node clustering on temporal graphs, and it offers greater flexibility for large-scale graph structures due to the mechanism of temporal graph methods. However, the development of TGC is currently constrained by a significant problem: the lack of suitable and reliable large-scale temporal graph datasets to evaluate clustering performance. In other words, most existing temporal graph datasets are in small sizes, and even large-scale datasets contain only a limited number of available node labels. It makes evaluating models for large-scale temporal graph clustering challenging. To address this challenge, we build arXiv4TGC, a set of novel academic datasets (including arXivAI, arXivCS, arXivMath, arXivPhy, and arXivLarge) for large-scale temporal graph clustering. In particular, the largest dataset, arXivLarge, contains 1.3 million labeled available nodes and 10 million temporal edges. We further compare the clustering performance with typical temporal graph learning models on both previous classic temporal graph datasets and the new datasets proposed in this paper. The clustering performance on arXiv4TGC can be more apparent for evaluating different models, resulting in higher clustering confidence and more suitable for large-scale temporal graph clustering. The arXiv4TGC datasets are publicly available at: https://github.com/MGitHubL/arXiv4TGC.", "link": "https://arxiv.org/abs/2306.04962"}, {"id": "2306.05082", "date": "Thu, 8 Jun 2023 10:20:08 GMT", "title": "The Importance of Time in Causal Algorithmic Recourse\n", "authors": ["Isacco Beretta and Martina Cinquini\n"], "categories": ["cs.AI", "cs.LG\nComments:", "Accepted", "for", "xAI", "Conference", "2023\n"], "abstract": "The application of Algorithmic Recourse in decision-making is a promising field that offers practical solutions to reverse unfavorable decisions. However, the inability of these methods to consider potential dependencies among variables poses a significant challenge due to the assumption of feature independence. Recent advancements have incorporated knowledge of causal dependencies, thereby enhancing the quality of the recommended recourse actions. Despite these improvements, the inability to incorporate the temporal dimension remains a significant limitation of these approaches. This is particularly problematic as identifying and addressing the root causes of undesired outcomes requires understanding time-dependent relationships between variables. In this work, we motivate the need to integrate the temporal dimension into causal algorithmic recourse methods to enhance recommendations' plausibility and reliability. The experimental evaluation highlights the significance of the role of time in this field.", "link": "https://arxiv.org/abs/2306.05082"}, {"id": "2306.05112", "date": "Thu, 8 Jun 2023 11:20:00 GMT", "title": "FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving\n\u00a0Federated Learning with Byzantine Users\n", "authors": ["Yogachandran Rahulamathavan", "Charuka Herath", "Xiaolan Liu,\n\u00a0Sangarapillai Lambotharan and Carsten Maple\n"], "categories": ["cs.AI", "cs.CR\n"], "abstract": "The federated learning (FL) technique was initially developed to mitigate data privacy issues that can arise in the traditional machine learning paradigm. While FL ensures that a user's data always remain with the user, the gradients of the locally trained models must be communicated with the centralized server to build the global model. This results in privacy leakage, where the server can infer private information of the users' data from the shared gradients. To mitigate this flaw, the next-generation FL architectures proposed encryption and anonymization techniques to protect the model updates from the server. However, this approach creates other challenges, such as a malicious user might sabotage the global model by sharing false gradients. Since the gradients are encrypted, the server is unable to identify and eliminate rogue users which would protect the global model. Therefore, to mitigate both attacks, this paper proposes a novel fully homomorphic encryption (FHE) based scheme suitable for FL. We modify the one-to-one single-key Cheon-Kim-Kim-Song (CKKS)-based FHE scheme into a distributed multi-key additive homomorphic encryption scheme that supports model aggregation in FL. We employ a novel aggregation scheme within the encrypted domain, utilizing users' non-poisoning rates, to effectively address data poisoning attacks while ensuring privacy is preserved by the proposed encryption scheme. Rigorous security, privacy, convergence, and experimental analyses have been provided to show that FheFL is novel, secure, and private, and achieves comparable accuracy at reasonable computational cost.", "link": "https://arxiv.org/abs/2306.05112"}, {"id": "2306.05331", "date": "Thu, 8 Jun 2023 16:31:47 GMT", "title": "Actively learning a Bayesian matrix fusion model with deep side\n\u00a0information\n", "authors": ["Yangyang Yu", "Jordan W. Suchow\n"], "categories": ["cs.AI", "cs.CE\n"], "abstract": "High-dimensional deep neural network representations of images and concepts can be aligned to predict human annotations of diverse stimuli. However, such alignment requires the costly collection of behavioral responses, such that, in practice, the deep-feature spaces are only ever sparsely sampled. Here, we propose an active learning approach to adaptively sampling experimental stimuli to efficiently learn a Bayesian matrix factorization model with deep side information. We observe a significant efficiency gain over a passive baseline. Furthermore, with a sequential batched sampling strategy, the algorithm is applicable not only to small datasets collected from traditional laboratory experiments but also to settings where large-scale crowdsourced data collection is needed to accurately align the high-dimensional deep feature representations derived from pre-trained networks.", "link": "https://arxiv.org/abs/2306.05331"}, {"id": "2306.05353", "date": "Thu, 8 Jun 2023 16:57:12 GMT", "title": "Negotiated Reasoning: On Provably Addressing Relative\n\u00a0Over-Generalization\n", "authors": ["Junjie Sheng", "Wenhao Li", "Bo Jin", "Hongyuan Zha", "Jun Wang", "Xiangfeng\n\u00a0Wang\n"], "categories": ["cs.AI", "cs.MA\nComments:", "21", "pages\n"], "abstract": "Over-generalization is a thorny issue in cognitive science, where people may become overly cautious due to past experiences. Agents in multi-agent reinforcement learning (MARL) also have been found to suffer relative over-generalization (RO) as people do and stuck to sub-optimal cooperation. Recent methods have shown that assigning reasoning ability to agents can mitigate RO algorithmically and empirically, but there has been a lack of theoretical understanding of RO, let alone designing provably RO-free methods. This paper first proves that RO can be avoided when the MARL method satisfies a consistent reasoning requirement under certain conditions. Then we introduce a novel reasoning framework, called negotiated reasoning, that first builds the connection between reasoning and RO with theoretical justifications. After that, we propose an instantiated algorithm, Stein variational negotiated reasoning (SVNR), which uses Stein variational gradient descent to derive a negotiation policy that provably avoids RO in MARL under maximum entropy policy iteration. The method is further parameterized with neural networks for amortized learning, making computation efficient. Numerical experiments on many RO-challenged environments demonstrate the superiority and efficiency of SVNR compared to state-of-the-art methods in addressing RO.", "link": "https://arxiv.org/abs/2306.05353"}, {"id": "2306.04641", "date": "Thu, 25 May 2023 08:24:22 GMT", "title": "Generalizable Low-Resource Activity Recognition with Diverse and\n\u00a0Discriminative Representation Learning\n", "authors": ["Xin Qin", "Jindong Wang", "Shuo Ma", "Wang Lu", "Yongchun Zhu", "Xing Xie,\n\u00a0Yiqiang Chen\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\nComments:", "Accepted", "by", "SIGKDD", "2023", "Research", "track;", "12", "pages\n"], "abstract": "Human activity recognition (HAR) is a time series classification task that focuses on identifying the motion patterns from human sensor readings. Adequate data is essential but a major bottleneck for training a generalizable HAR model, which assists customization and optimization of online web applications. However, it is costly in time and economy to collect large-scale labeled data in reality, i.e., the low-resource challenge. Meanwhile, data collected from different persons have distribution shifts due to different living habits, body shapes, age groups, etc. The low-resource and distribution shift challenges are detrimental to HAR when applying the trained model to new unseen subjects. In this paper, we propose a novel approach called Diverse and Discriminative representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn simultaneously considers diversity and discrimination learning. With the constructed self-supervised learning task, DDLearn enlarges the data diversity and explores the latent activity properties. Then, we propose a diversity preservation module to preserve the diversity of learned features by enlarging the distribution divergence between the original and augmented domains. Meanwhile, DDLearn also enhances semantic discrimination by learning discriminative representations with supervised contrastive learning. Extensive experiments on three public HAR datasets demonstrate that our method significantly outperforms state-of-art methods by an average accuracy improvement of 9.5% under the low-resource distribution shift scenarios, while being a generic, explainable, and flexible framework.", "link": "https://arxiv.org/abs/2306.04641"}, {"id": "2306.04717", "date": "Wed, 7 Jun 2023 18:28:21 GMT", "title": "AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment\n", "authors": ["Chunyi Li", "Zicheng Zhang", "Haoning Wu", "Wei Sun", "Xiongkuo Min", "Xiaohong\n\u00a0Liu", "Guangtao Zhai", "Weisi Lin\n"], "categories": ["cs.CV", "cs.AI", "eess.IV\nComments:", "12", "pages,", "11", "figures\n"], "abstract": "With the rapid advancements of the text-to-image generative model, AI-generated images (AGIs) have been widely applied to entertainment, education, social media, etc. However, considering the large quality variance among different AGIs, there is an urgent need for quality models that are consistent with human subjective ratings. To address this issue, we extensively consider various popular AGI models, generated AGI through different prompts and model parameters, and collected subjective scores at the perceptual quality and text-to-image alignment, thus building the most comprehensive AGI subjective quality database AGIQA-3K so far. Furthermore, we conduct a benchmark experiment on this database to evaluate the consistency between the current Image Quality Assessment (IQA) model and human perception, while proposing StairReward that significantly improves the assessment performance of subjective text-to-image alignment. We believe that the fine-grained subjective scores in AGIQA-3K will inspire subsequent AGI quality models to fit human subjective perception mechanisms at both perception and alignment levels and to optimize the generation result of future AGI models. The database is released on \\url{https://github.com/lcysyzxdxc/AGIQA-3k-Database}.", "link": "https://arxiv.org/abs/2306.04717"}, {"id": "2306.04955", "date": "Thu, 8 Jun 2023 06:02:39 GMT", "title": "Degraded Polygons Raise Fundamental Questions of Neural Network\n\u00a0Perception\n", "authors": ["Leonard Tang", "Dan Ley\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\n"], "abstract": "It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize and recover such degraded shapes when initialized with different priors. Ultimately, we find that neural networks' behavior on this simple task conflicts with human behavior, raising a fundamental question of the robustness and learning capabilities of modern computer vision models.", "link": "https://arxiv.org/abs/2306.04955"}, {"id": "2306.05045", "date": "Thu, 8 Jun 2023 08:55:16 GMT", "title": "Spain on Fire: A novel wildfire risk assessment model based on image\n\u00a0satellite processing and atmospheric information\n", "authors": ["Helena Liz-L\\'opez", "Javier Huertas-Tato", "Jorge P\\'erez-Aracil", "Carlos\n\u00a0Casanova-Mateo", "Julia Sanz-Justo", "David Camacho\n"], "categories": ["cs.CV", "cs.AI", "eess.IV\n"], "abstract": "Each year, wildfires destroy larger areas of Spain, threatening numerous ecosystems. Humans cause 90% of them (negligence or provoked) and the behaviour of individuals is unpredictable. However, atmospheric and environmental variables affect the spread of wildfires, and they can be analysed by using deep learning. In order to mitigate the damage of these events we proposed the novel Wildfire Assessment Model (WAM). Our aim is to anticipate the economic and ecological impact of a wildfire, assisting managers resource allocation and decision making for dangerous regions in Spain, Castilla y Le\\'on and Andaluc\\'ia. The WAM uses a residual-style convolutional network architecture to perform regression over atmospheric variables and the greenness index, computing necessary resources, the control and extinction time, and the expected burnt surface area. It is first pre-trained with self-supervision over 100,000 examples of unlabelled data with a masked patch prediction objective and fine-tuned using 311 samples of wildfires. The pretraining allows the model to understand situations, outclassing baselines with a 1,4%, 3,7% and 9% improvement estimating human, heavy and aerial resources; 21% and 10,2% in expected extinction and control time; and 18,8% in expected burnt area. Using the WAM we provide an example assessment map of Castilla y Le\\'on, visualizing the expected resources over an entire region.", "link": "https://arxiv.org/abs/2306.05045"}, {"id": "2306.05357", "date": "Thu, 8 Jun 2023 17:02:15 GMT", "title": "Unsupervised Compositional Concepts Discovery with Text-to-Image\n\u00a0Generative Models\n", "authors": ["Nan Liu", "Yilun Du", "Shuang Li", "Joshua B. Tenenbaum", "Antonio Torralba\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\nComments:", "Project", "Webpage:\n\u00a0https://energy-based-model.github.io/unsupervised-concept-discovery/\n"], "abstract": "Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem -- given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.", "link": "https://arxiv.org/abs/2306.05357"}, {"id": "2306.05419", "date": "Thu, 8 Jun 2023 17:58:57 GMT", "title": "TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem\n\u00a0via Transformer-Based Architecture\n", "authors": ["M. Esat Kalfaoglu", "Halil Ibrahim Ozturk", "Ozsel Kilinc", "Alptekin\n\u00a0Temizel\n"], "categories": ["cs.CV", "cs.AI", "cs.LG\nComments:", "4th", "in", "OLS", "and", "2nd", "in", "the", "F1-score", "in", "OpenLane", "Topology", "Challenge\n\u00a02023\n"], "abstract": "Driving scene understanding task involves detecting static elements such as lanes, traffic signs, and traffic lights, and their relationships with each other. To facilitate the development of comprehensive scene understanding solutions using multiple camera views, a new dataset called Road Genome (OpenLane-V2) has been released. This dataset allows for the exploration of complex road connections and situations where lane markings may be absent. Instead of using traditional lane markings, the lanes in this dataset are represented by centerlines, which offer a more suitable representation of lanes and their connections. In this study, we have introduced a new approach called TopoMask for predicting centerlines in road topology. Unlike existing approaches in the literature that rely on keypoints or parametric methods, TopoMask utilizes an instance-mask based formulation with a transformer-based architecture and, in order to enrich the mask instances with flow information, a direction label representation is proposed. TopoMask have ranked 4th in the OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction in OpenLane Topology Challenge 2023. In comparison to the current state-of-the-art method, TopoNet, the proposed method has achieved similar performance in Frechet-based lane detection and outperformed TopoNet in Chamfer-based lane detection without utilizing its scene graph neural network.", "link": "https://arxiv.org/abs/2306.05419"}, {"id": "2306.05425", "date": "Thu, 8 Jun 2023 17:59:56 GMT", "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning\n", "authors": ["Bo Li", "Yuanhan Zhang", "Liangyu Chen", "Jinghao Wang", "Fanyi Pu", "Jingkang\n\u00a0Yang", "Chunyuan Li", "Ziwei Liu\n"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC\nComments:", "Project", "page:", "https://otter-ntu.github.io/", "Dataset", "&", "code:\n\u00a0https://github.com/Luodian/otter", "Initial", "release,", "work", "in", "progress\n"], "abstract": "High-quality instructions and responses are essential for the zero-shot performance of large language models on interactive natural language tasks. For interactive vision-language tasks involving intricate visual scenes, a large quantity of diverse and creative instruction-response pairs should be imperative to tune vision-language models (VLMs). Nevertheless, the current availability of vision-language instruction-response pairs in terms of quantity, diversity, and creativity remains limited, posing challenges to the generalization of interactive VLMs. Here we present MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos. Each pair is accompanied by multi-modal in-context information, forming conversational contexts aimed at empowering VLMs in perception, reasoning, and planning. The instruction-response collection process, dubbed as Syphus, is scaled using an automatic annotation pipeline that combines human expertise with GPT's capabilities. Using the MIMIC-IT dataset, we train a large VLM named Otter. Based on extensive evaluations conducted on vision-language benchmarks, it has been observed that Otter demonstrates remarkable proficiency in multi-modal perception, reasoning, and in-context learning. Human evaluation reveals it effectively aligns with the user's intentions. We release the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and the Otter model.", "link": "https://arxiv.org/abs/2306.05425"}, {"id": "2306.04643", "date": "Thu, 25 May 2023 15:12:14 GMT", "title": "Abnormal Trading Detection in the NFT Market\n", "authors": ["Mingxiao Song and Yunsong Liu and Agam Shah and Sudheer Chava\n"], "categories": ["q-fin.TR", "cs.AI", "q-fin.CP\n"], "abstract": "The Non-Fungible-Token (NFT) market has experienced explosive growth in recent years. According to DappRadar, the total transaction volume on OpenSea, the largest NFT marketplace, reached 34.7 billion dollars in February 2023. However, the NFT market is mostly unregulated and there are significant concerns about money laundering, fraud and wash trading. Amateur traders and retail investors comprise a significant fraction of the NFT market. Hence it is important that researchers highlight the relevant risks involved in NFT trading. In this paper, we attempt to uncover common fraudulent behaviors such as wash trading that could mislead other traders. Using market data, we design quantitative features from the network, monetary, and temporal perspectives that are fed into K-means clustering unsupervised learning algorithm to sort traders into groups. Lastly, we discuss the clustering results' significance and how regulations can reduce undesired behaviors. Our work can potentially help regulators narrow down their search space for bad actors in the market as well as provide insights for amateur traders to protect themselves from unforeseen frauds.", "link": "https://arxiv.org/abs/2306.04643"}, {"id": "2306.04735", "date": "Wed, 7 Jun 2023 19:11:25 GMT", "title": "Soft-prompt Tuning for Large Language Models to Evaluate Bias\n", "authors": ["Jacob-Junqi Tian", "David Emerson", "Sevil Zanjani Miyandoab", "Deval\n\u00a0Pandya", "Laleh Seyyed-Kalantari", "Faiza Khan Khattak\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\n"], "abstract": "Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, it is crucial to identify the biases before deploying these models in practice. We open-source our pipeline and encourage industry researchers to adapt our work to their use cases.", "link": "https://arxiv.org/abs/2306.04735"}, {"id": "2306.04743", "date": "Wed, 7 Jun 2023 19:37:55 GMT", "title": "ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural\n\u00a0Language to SQL Systems\n", "authors": ["Yi Zhang", "Jan Deriu", "George Katsogiannis-Meimarakis", "Catherine Kosten,\n\u00a0Georgia Koutrika", "Kurt Stockinger\n"], "categories": ["cs.DB", "cs.AI", "cs.CL\nComments:", "12", "pages,", "2", "figures,", "5", "tables\nACM-class:", "H.2.4;", "I.2.7\n"], "abstract": "Natural Language to SQL systems (NL-to-SQL) have recently shown a significant increase in accuracy for natural language to SQL query translation. This improvement is due to the emergence of transformer-based language models, and the popularity of the Spider benchmark - the de-facto standard for evaluating NL-to-SQL systems. The top NL-to-SQL systems reach accuracies of up to 85\\%. However, Spider mainly contains simple databases with few tables, columns, and entries, which does not reflect a realistic setting. Moreover, complex real-world databases with domain-specific content have little to no training data available in the form of NL/SQL-pairs leading to poor performance of existing NL-to-SQL systems. In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQL benchmark for three real-world, highly domain-specific databases. For this new benchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for each domain. To garner more data, we extended the small amount of human-generated data with synthetic data generated using GPT-3. We show that our benchmark is highly challenging, as the top performing systems on Spider achieve a very low performance on our benchmark. Thus, the challenge is many-fold: creating NL-to-SQL systems for highly complex domains with a small amount of hand-made training data augmented with synthetic data. To our knowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed with complex real-world scientific databases, containing challenging training and test data carefully validated by domain experts.", "link": "https://arxiv.org/abs/2306.04743"}, {"id": "2306.04758", "date": "Wed, 7 Jun 2023 20:16:08 GMT", "title": "SKG: A Versatile Information Retrieval and Analysis Framework for\n\u00a0Academic Papers with Semantic Knowledge Graphs\n", "authors": ["Yamei Tu", "Rui Qiu", "Han-Wei Shen\n"], "categories": ["cs.IR", "cs.AI", "cs.GR", "cs.HC\n"], "abstract": "The number of published research papers has experienced exponential growth in recent years, which makes it crucial to develop new methods for efficient and versatile information extraction and knowledge discovery. To address this need, we propose a Semantic Knowledge Graph (SKG) that integrates semantic concepts from abstracts and other meta-information to represent the corpus. The SKG can support various semantic queries in academic literature thanks to the high diversity and rich information content stored within. To extract knowledge from unstructured text, we develop a Knowledge Extraction Module that includes a semi-supervised pipeline for entity extraction and entity normalization. We also create an ontology to integrate the concepts with other meta information, enabling us to build the SKG. Furthermore, we design and develop a dataflow system that demonstrates how to conduct various semantic queries flexibly and interactively over the SKG. To demonstrate the effectiveness of our approach, we conduct the research based on the visualization literature and provide real-world use cases to show the usefulness of the SKG. The dataset and codes for this work are available at https://osf.io/aqv8p/?view_only=2c26b36e3e3941ce999df47e4616207f.", "link": "https://arxiv.org/abs/2306.04758"}, {"id": "2306.04874", "date": "Thu, 8 Jun 2023 02:07:49 GMT", "title": "Expanding Scope: Adapting English Adversarial Attacks to Chinese\n", "authors": ["Hanyu Liu", "Chengyuan Cai", "Yanjun Qi\n"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG\nComments:", "11", "pages;", "in", "ACL23", "TrustNLP", "2023:", "TrustNLP:", "Third", "Workshop", "on\n\u00a0Trustworthy", "Natural", "Language", "Processing", "Colocated", "with", "the", "Annual", "Conference\n\u00a0of", "the", "Association", "for", "Computational", "Linguistics", "(ACL", "2023)\n"], "abstract": "Recent studies have revealed that NLP predictive models are vulnerable to adversarial attacks. Most existing studies focused on designing attacks to evaluate the robustness of NLP models in the English language alone. Literature has seen an increasing need for NLP solutions for other languages. We, therefore, ask one natural question: whether state-of-the-art (SOTA) attack methods generalize to other languages. This paper investigates how to adapt SOTA adversarial attack algorithms in English to the Chinese language. Our experiments show that attack methods previously applied to English NLP can generate high-quality adversarial examples in Chinese when combined with proper text segmentation and linguistic constraints. In addition, we demonstrate that the generated adversarial examples can achieve high fluency and semantic consistency by focusing on the Chinese language's morphology and phonology, which in turn can be used to improve the adversarial robustness of Chinese NLP models.", "link": "https://arxiv.org/abs/2306.04874"}, {"id": "2306.04926", "date": "Thu, 8 Jun 2023 04:08:32 GMT", "title": "covLLM: Large Language Models for COVID-19 Biomedical Literature\n", "authors": ["Yousuf A. Khan", "Clarisse Hokia", "Jennifer Xu", "Ben Ehlert\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\n"], "abstract": "The COVID-19 pandemic led to 1.1 million deaths in the United States, despite the explosion of coronavirus research. These new findings are slow to translate to clinical interventions, leading to poorer patient outcomes and unnecessary deaths. One reason is that clinicians, overwhelmed by patients, struggle to keep pace with the rate of new coronavirus literature. A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing. LLMs can be used to summarize and extract user-specified information. The greater availability and advancement of LLMs and pre-processed coronavirus literature databases provide the opportunity to assist clinicians in evaluating coronavirus literature through a coronavirus literature specific LLM (covLLM), a tool that directly takes an inputted research article and a user query to return an answer. Using the COVID-19 Open Research Dataset (CORD-19), we produced two datasets: (1) synCovid, which uses a combination of handwritten prompts and synthetic prompts generated using OpenAI, and (2) real abstracts, which contains abstract and title pairs. covLLM was trained with LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real abstract datasets. These models were evaluated by two human evaluators and ChatGPT. Results demonstrate that training covLLM on the synCovid and abstract pairs datasets performs competitively with ChatGPT and outperforms covLLM trained primarily using the Alpaca dataset.", "link": "https://arxiv.org/abs/2306.04926"}, {"id": "2306.05004", "date": "Thu, 8 Jun 2023 07:48:01 GMT", "title": "VIFS: An End-to-End Variational Inference for Foley Sound Synthesis\n", "authors": ["Junhyeok Lee", "Hyeonuk Nam", "Yong-Hwa Park\n"], "categories": ["eess.AS", "cs.AI", "cs.SD\nComments:", "DCASE", "2023", "Challenge", "Task", "7\n"], "abstract": "The goal of DCASE 2023 Challenge Task 7 is to generate various sound clips for Foley sound synthesis (FSS) by \"category-to-sound\" approach. \"Category\" is expressed by a single index while corresponding \"sound\" covers diverse and different sound examples. To generate diverse sounds for a given category, we adopt VITS, a text-to-speech (TTS) model with variational inference. In addition, we apply various techniques from speech synthesis including PhaseAug and Avocodo. Different from TTS models which generate short pronunciation from phonemes and speaker identity, the category-to-sound problem requires generating diverse sounds just from a category index. To compensate for the difference while maintaining consistency within each audio clip, we heavily modified the prior encoder to enhance consistency with posterior latent variables. This introduced additional Gaussian on the prior encoder which promotes variance within the category. With these modifications, we propose VIFS, variational inference for end-to-end Foley sound synthesis, which generates diverse high-quality sounds.", "link": "https://arxiv.org/abs/2306.05004"}, {"id": "2306.05077", "date": "Thu, 8 Jun 2023 10:00:19 GMT", "title": "Improving Language Model Integration for Neural Machine Translation\n", "authors": ["Christian Herold and Yingbo Gao and Mohammad Zeineldeen and Hermann\n\u00a0Ney\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\nComments:", "accepted", "at", "ACL2023", "(Findings)\n"], "abstract": "The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time. Recently, some works on automatic speech recognition have demonstrated that, if the implicit language model is neutralized in decoding, further improvements can be gained when integrating an external language model. In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data - namely back-translation. We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is still outperformed by back-translation.", "link": "https://arxiv.org/abs/2306.05077"}, {"id": "2306.05116", "date": "Thu, 8 Jun 2023 11:30:43 GMT", "title": "On Search Strategies for Document-Level Neural Machine Translation\n", "authors": ["Christian Herold and Hermann Ney\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\nComments:", "Accepted", "to", "ACL", "2023", "(Findings)\n"], "abstract": "Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input. There are many works on document-level NMT, mostly focusing on modifying the model architecture or training strategy to better accommodate the additional context-input. On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes not mentioned at all. In this work, we aim to answer the question how to best utilize a context-aware translation model in decoding. We start with the most popular document-level NMT approach and compare different decoding schemes, some from the literature and others proposed by us. In the comparison, we are using both, standard automatic metrics, as well as specific linguistic phenomena on three standard document-level translation benchmarks. We find that most commonly used decoding strategies perform similar to each other and that higher quality context information has the potential to further improve the translation.", "link": "https://arxiv.org/abs/2306.05116"}, {"id": "2306.05183", "date": "Thu, 8 Jun 2023 13:28:48 GMT", "title": "Improving Long Context Document-Level Machine Translation\n", "authors": ["Christian Herold and Hermann Ney\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\nComments:", "accepted", "at", "CODI", "2023", "(ACL", "workshop)\n"], "abstract": "Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena. Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information. This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation. When increasing the context size beyond just the local context, there are two challenges: (i) the~memory usage increases exponentially (ii) the translation performance starts to degrade. We argue that the widely-used attention mechanism is responsible for both issues. Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneously reducing the memory consumption. For evaluation, we utilize targeted test sets in combination with novel evaluation techniques to analyze the translations in regards to specific discourse-related phenomena. We find that our approach is a good compromise between sentence-level NMT vs attending to the full context, especially in low resource scenarios.", "link": "https://arxiv.org/abs/2306.05183"}, {"id": "2306.05240", "date": "Thu, 8 Jun 2023 14:39:24 GMT", "title": "Dealing with Semantic Underspecification in Multimodal NLP\n", "authors": ["Sandro Pezzelle\n"], "categories": ["cs.CL", "cs.AI", "cs.CV\nComments:", "To", "appear", "in", "the", "Proceedings", "of", "ACL", "2023", "(main", "conference).", "13", "pages,\n\u00a03", "figures\n"], "abstract": "Intelligent systems that aim at mastering language as humans do must deal with its semantic underspecification, namely, the possibility for a linguistic signal to convey only part of the information needed for communication to succeed. Consider the usages of the pronoun they, which can leave the gender and number of its referent(s) underspecified. Semantic underspecification is not a bug but a crucial language feature that boosts its storage and processing efficiency. Indeed, human speakers can quickly and effortlessly integrate semantically-underspecified linguistic signals with a wide range of non-linguistic information, e.g., the multimodal context, social or cultural conventions, and shared knowledge. Standard NLP models have, in principle, no or limited access to such extra information, while multimodal systems grounding language into other modalities, such as vision, are naturally equipped to account for this phenomenon. However, we show that they struggle with it, which could negatively affect their performance and lead to harmful consequences when used for applications. In this position paper, we argue that our community should be aware of semantic underspecification if it aims to develop language technology that can successfully interact with human users. We discuss some applications where mastering it is crucial and outline a few directions toward achieving this goal.", "link": "https://arxiv.org/abs/2306.05240"}, {"id": "2306.05281", "date": "Tue, 30 May 2023 12:24:41 GMT", "title": "Fault Identification of Rotating Machinery Based on Dynamic Feature\n\u00a0Reconstruction Signal Graph\n", "authors": ["Wenbin He", "Jianxu Mao", "Zhe Li", "Yaonan Wang", "Qiu Fang", "Haotian Wu\n"], "categories": ["eess.SP", "cs.AI", "cs.RO\n"], "abstract": "To improve the performance in identifying the faults under strong noise for rotating machinery, this paper presents a dynamic feature reconstruction signal graph method, which plays the key role of the proposed end-to-end fault diagnosis model. Specifically, the original mechanical signal is first decomposed by wavelet packet decomposition (WPD) to obtain multiple subbands including coefficient matrix. Then, with originally defined two feature extraction factors MDD and DDD, a dynamic feature selection method based on L2 energy norm (DFSL) is proposed, which can dynamically select the feature coefficient matrix of WPD based on the difference in the distribution of norm energy, enabling each sub-signal to take adaptive signal reconstruction. Next the coefficient matrices of the optimal feature sub-bands are reconstructed and reorganized to obtain the feature signal graphs. Finally, deep features are extracted from the feature signal graphs by 2D-Convolutional neural network (2D-CNN). Experimental results on a public data platform of a bearing and our laboratory platform of robot grinding show that this method is better than the existing methods under different noise intensities.", "link": "https://arxiv.org/abs/2306.05281"}, {"id": "2306.05291", "date": "Wed, 31 May 2023 14:52:42 GMT", "title": "One shot learning based drivers head movement identification using a\n\u00a0millimetre wave radar sensor\n", "authors": ["Hong Nhung Nguyen", "Seongwook Lee", "Tien Tung Nguyen", "Yong Hwa Kim\n"], "categories": ["eess.SP", "cs.AI", "cs.LG\n"], "abstract": "Concentration of drivers on traffic is a vital safety issue; thus, monitoring a driver being on road becomes an essential requirement. The key purpose of supervision is to detect abnormal behaviours of the driver and promptly send warnings to him her for avoiding incidents related to traffic accidents. In this paper, to meet the requirement, based on radar sensors applications, the authors first use a small sized millimetre wave radar installed at the steering wheel of the vehicle to collect signals from different head movements of the driver. The received signals consist of the reflection patterns that change in response to the head movements of the driver. Then, in order to distinguish these different movements, a classifier based on the measured signal of the radar sensor is designed. However, since the collected data set is not large, in this paper, the authors propose One shot learning to classify four cases of driver's head movements. The experimental results indicate that the proposed method can classify the four types of cases according to the various head movements of the driver with a high accuracy reaching up to 100. In addition, the classification performance of the proposed method is significantly better than that of the convolutional neural network model.", "link": "https://arxiv.org/abs/2306.05291"}, {"id": "2306.05323", "date": "Thu, 8 Jun 2023 16:15:46 GMT", "title": "Advancing Italian Biomedical Information Extraction with Large Language\n\u00a0Models: Methodological Insights and Multicenter Practical Application\n", "authors": ["Claudio Crema", "Tommaso Mario Buonocore", "Silvia Fostinelli", "Enea\n\u00a0Parimbelli", "Federico Verde", "Cira Fundar\\`o", "Marina Manera", "Matteo Cotta\n\u00a0Ramusino", "Marco Capelli", "Alfredo Costa", "Giuliano Binetti", "Riccardo Bellazzi\n\u00a0and Alberto Redolfi\n"], "categories": ["cs.CL", "cs.AI", "cs.LG\nACM-class:", "I.2.7;", "J.3\n"], "abstract": "The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a \"few-shot\" approach. This allowed us to establish methodological guidelines that pave the way for future implementations in this field and allow Italian hospitals to tap into important research opportunities.", "link": "https://arxiv.org/abs/2306.05323"}, {"id": "2306.05360", "date": "Thu, 8 Jun 2023 17:05:38 GMT", "title": "The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher\n\u00a0Responses in Educational Dialogues\n", "authors": ["Adaeze Adigwe (1)", "Zheng Yuan (2 and 3)((1) University of Edinburgh,\n\u00a0United Kingdom", "(2) Istituto Italiano di Tecnologia", "Italy", "(3) Universit\\`a\n\u00a0di Ferrara", "Italy)\n"], "categories": ["cs.CL", "cs.AI", "cs.CY\nComments:", "Accepted", "in", "the", "BEA", "workshop", "at", "ACL", "2023\n"], "abstract": "This paper presents the ADAIO team's system entry in the Building Educational Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. The task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses within a student-teacher dialogue. Our system comprises evaluating various baseline models using OpenAI GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response generation. After the challenge, our system achieved second place by employing a few-shot prompt-based approach with the OpenAI text-davinci-003 model. The results highlight the few-shot learning capabilities of large-language models, particularly OpenAI's GPT-3, in the role of AI teachers.", "link": "https://arxiv.org/abs/2306.05360"}, {"id": "2306.04802", "date": "Wed, 7 Jun 2023 21:51:56 GMT", "title": "A Survey on Knowledge Graphs for Healthcare: Resources, Applications,\n\u00a0and Promises\n", "authors": ["Hejie Cui", "Jiaying Lu", "Shiyu Wang", "Ran Xu", "Wenjing Ma", "Shaojun Yu", "Yue\n\u00a0Yu", "Xuan Kan", "Chen Ling", "Joyce Ho", "Fei Wang", "Carl Yang\n"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SI\n"], "abstract": "Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various healthcare domains, spanning from fine-grained basic science research to high-level clinical decision support. Lastly, we shed light on the opportunities for creating comprehensive and accurate HKGs in the era of large language models, presenting the potential to revolutionize healthcare delivery and enhance the interpretability and reliability of clinical prediction.", "link": "https://arxiv.org/abs/2306.04802"}, {"id": "2306.05066", "date": "Thu, 8 Jun 2023 09:31:18 GMT", "title": "Causal Fairness for Outcome Control\n", "authors": ["Drago Plecko", "Elias Bareinboim\n"], "categories": ["cs.AI", "cs.LG", "stat.ML\n"], "abstract": "As society transitions towards an AI-based decision-making infrastructure, an ever-increasing number of decisions once under control of humans are now delegated to automated systems. Even though such developments make various parts of society more efficient, a large body of evidence suggests that a great deal of care needs to be taken to make such automated decision-making systems fair and equitable, namely, taking into account sensitive attributes such as gender, race, and religion. In this paper, we study a specific decision-making task called outcome control in which an automated system aims to optimize an outcome variable $Y$ while being fair and equitable. The interest in such a setting ranges from interventions related to criminal justice and welfare, all the way to clinical decision-making and public health. In this paper, we first analyze through causal lenses the notion of benefit, which captures how much a specific individual would benefit from a positive decision, counterfactually speaking, when contrasted with an alternative, negative one. We introduce the notion of benefit fairness, which can be seen as the minimal fairness requirement in decision-making, and develop an algorithm for satisfying it. We then note that the benefit itself may be influenced by the protected attribute, and propose causal tools which can be used to analyze this. Finally, if some of the variations of the protected attribute in the benefit are considered as discriminatory, the notion of benefit fairness may need to be strengthened, which leads us to articulating a notion of causal benefit fairness. Using this notion, we develop a new optimization procedure capable of maximizing $Y$ while ascertaining causal fairness in the decision process.", "link": "https://arxiv.org/abs/2306.05066"}, {"id": "2306.04719", "date": "Wed, 7 Jun 2023 18:31:39 GMT", "title": "Don't trust your eyes: on the (un)reliability of feature visualizations\n", "authors": ["Robert Geirhos", "Roland S. Zimmermann", "Blair Bilodeau", "Wieland Brendel,\n\u00a0Been Kim\n"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "q-bio.NC\n"], "abstract": "How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to \"explain\" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extremely small and does not include general black-box neural networks. Therefore, a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations.", "link": "https://arxiv.org/abs/2306.04719"}, {"id": "2306.04648", "date": "Mon, 5 Jun 2023 13:57:23 GMT", "title": "On training locally adaptive CP\n", "authors": ["Nicolo Colombo\n"], "categories": ["cs.LG", "cs.AI", "stat.ML\nComments:", "15", "pages,", "1", "table,", "1", "figure\n"], "abstract": "We address the problem of making Conformal Prediction (CP) intervals locally adaptive. Most existing methods focus on approximating the object-conditional validity of the intervals by partitioning or re-weighting the calibration set. Our strategy is new and conceptually different. Instead of re-weighting the calibration data, we redefine the conformity measure through a trainable change of variables, $A \\to \\phi_X(A)$, that depends explicitly on the object attributes, $X$. Under certain conditions and if $\\phi_X$ is monotonic in $A$ for any $X$, the transformations produce prediction intervals that are guaranteed to be marginally valid and have $X$-dependent sizes. We describe how to parameterize and train $\\phi_X$ to maximize the interval efficiency. Contrary to other CP-aware training methods, the objective function is smooth and can be minimized through standard gradient methods without approximations.", "link": "https://arxiv.org/abs/2306.04648"}, {"id": "2306.04835", "date": "Wed, 7 Jun 2023 23:40:18 GMT", "title": "Empowering Counterfactual Reasoning over Graph Neural Networks through\n\u00a0Inductivity\n", "authors": ["Samidha Verma", "Burouj Armgaan", "Sourav Medya", "Sayan Ranu\n"], "categories": ["cs.LG", "cs.AI", "cs.SI\n"], "abstract": "Graph neural networks (GNNs) have various practical applications, such as drug discovery, recommendation engines, and chip design. However, GNNs lack transparency as they cannot provide understandable explanations for their predictions. To address this issue, counterfactual reasoning is used. The main goal is to make minimal changes to the input graph of a GNN in order to alter its prediction. While several algorithms have been proposed for counterfactual explanations of GNNs, most of them have two main drawbacks. Firstly, they only consider edge deletions as perturbations. Secondly, the counterfactual explanation models are transductive, meaning they do not generalize to unseen data. In this study, we introduce an inductive algorithm called INDUCE, which overcomes these limitations. By conducting extensive experiments on several datasets, we demonstrate that incorporating edge additions leads to better counterfactual results compared to the existing methods. Moreover, the inductive modeling approach allows INDUCE to directly predict counterfactual perturbations without requiring instance-specific training. This results in significant computational speed improvements compared to baseline methods and enables scalable counterfactual analysis for GNNs.", "link": "https://arxiv.org/abs/2306.04835"}, {"id": "2306.05041", "date": "Thu, 8 Jun 2023 08:49:42 GMT", "title": "Energy-Efficient Downlink Semantic Generative Communication with\n\u00a0Text-to-Image Generators\n", "authors": ["Hyein Lee", "Jihong Park", "Sooyoung Kim", "Jinho Choi\n"], "categories": ["cs.LG", "cs.AI", "cs.DC\nComments:", "6", "pages,", "7", "figures.", "arXiv", "admin", "note:", "text", "overlap", "with\n\u00a0arXiv:2302.02498\n"], "abstract": "In this paper, we introduce a novel semantic generative communication (SGC) framework, where generative users leverage text-to-image (T2I) generators to create images locally from downloaded text prompts, while non-generative users directly download images from a base station (BS). Although generative users help reduce downlink transmission energy at the BS, they consume additional energy for image generation and for uploading their generator state information (GSI). We formulate the problem of minimizing the total energy consumption of the BS and the users, and devise a generative user selection algorithm. Simulation results corroborate that our proposed algorithm reduces total energy by up to 54% compared to a baseline with all non-generative users.", "link": "https://arxiv.org/abs/2306.05041"}, {"id": "2306.05052", "date": "Thu, 8 Jun 2023 09:12:28 GMT", "title": "Interpretable Medical Diagnostics with Structured Data Extraction by\n\u00a0Large Language Models\n", "authors": ["Aleksa Bisercic", "Mladen Nikolic", "Mihaela van der Schaar", "Boris\n\u00a0Delibasic", "Pietro Lio", "Andrija Petrovic\n"], "categories": ["cs.LG", "cs.AI", "cs.CL\n"], "abstract": "Tabular data is often hidden in text, particularly in medical diagnostic reports. Traditional machine learning (ML) models designed to work with tabular data, cannot effectively process information in such form. On the other hand, large language models (LLMs) which excel at textual tasks, are probably not the best tool for modeling tabular data. Therefore, we propose a novel, simple, and effective methodology for extracting structured tabular data from textual medical reports, called TEMED-LLM. Drawing upon the reasoning capabilities of LLMs, TEMED-LLM goes beyond traditional extraction techniques, accurately inferring tabular features, even when their names are not explicitly mentioned in the text. This is achieved by combining domain-specific reasoning guidelines with a proposed data validation and reasoning correction feedback loop. By applying interpretable ML models such as decision trees and logistic regression over the extracted and validated data, we obtain end-to-end interpretable predictions. We demonstrate that our approach significantly outperforms state-of-the-art text classification models in medical diagnostics. Given its predictive performance, simplicity, and interpretability, TEMED-LLM underscores the potential of leveraging LLMs to improve the performance and trustworthiness of ML models in medical applications.", "link": "https://arxiv.org/abs/2306.05052"}, {"id": "2306.05058", "date": "Thu, 8 Jun 2023 09:23:09 GMT", "title": "Neuro-Symbolic Approaches for Context-Aware Human Activity Recognition\n", "authors": ["Luca Arrotta", "Gabriele Civitarese", "Claudio Bettini\n"], "categories": ["cs.LG", "cs.AI", "eess.SP\n"], "abstract": "Deep Learning models are a standard solution for sensor-based Human Activity Recognition (HAR), but their deployment is often limited by labeled data scarcity and models' opacity. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate these issues by infusing knowledge about context information into HAR deep learning classifiers. However, existing NeSy methods for context-aware HAR require computationally expensive symbolic reasoners during classification, making them less suitable for deployment on resource-constrained devices (e.g., mobile devices). Additionally, NeSy approaches for context-aware HAR have never been evaluated on in-the-wild datasets, and their generalization capabilities in real-world scenarios are questionable. In this work, we propose a novel approach based on a semantic loss function that infuses knowledge constraints in the HAR model during the training phase, avoiding symbolic reasoning during classification. Our results on scripted and in-the-wild datasets show the impact of different semantic loss functions in outperforming a purely data-driven model. We also compare our solution with existing NeSy methods and analyze each approach's strengths and weaknesses. Our semantic loss remains the only NeSy solution that can be deployed as a single DNN without the need for symbolic reasoning modules, reaching recognition rates close (and better in some cases) to existing approaches.", "link": "https://arxiv.org/abs/2306.05058"}, {"id": "2306.05067", "date": "Thu, 8 Jun 2023 09:31:28 GMT", "title": "Improving Visual Prompt Tuning for Self-supervised Vision Transformers\n", "authors": ["Seungryong Yoo", "Eunji Kim", "Dahuin Jung", "Jungbeom Lee", "Sungroh Yoon\n"], "categories": ["cs.LG", "cs.AI", "cs.CV\nComments:", "International", "Conference", "on", "Machine", "Learning", "(ICML)", "2023\n"], "abstract": "Visual Prompt Tuning (VPT) is an effective tuning method for adapting pretrained Vision Transformers (ViTs) to downstream tasks. It leverages extra learnable tokens, known as prompts, which steer the frozen pretrained ViTs. Although VPT has demonstrated its applicability with supervised vision transformers, it often underperforms with self-supervised ones. Through empirical observations, we deduce that the effectiveness of VPT hinges largely on the ViT blocks with which the prompt tokens interact. Specifically, VPT shows improved performance on image classification tasks for MAE and MoCo v3 when the prompt tokens are inserted into later blocks rather than the first block. These observations suggest that there exists an optimal location of blocks for the insertion of prompt tokens. Unfortunately, identifying the optimal blocks for prompts within each self-supervised ViT for diverse future scenarios is a costly process. To mitigate this problem, we propose a simple yet effective method that learns a gate for each ViT block to adjust its intervention into the prompt tokens. With our method, prompt tokens are selectively influenced by blocks that require steering for task adaptation. Our method outperforms VPT variants in FGVC and VTAB image classification and ADE20K semantic segmentation. The code is available at https://github.com/ryongithub/GatedPromptTuning.", "link": "https://arxiv.org/abs/2306.05067"}, {"id": "2306.05068", "date": "Thu, 8 Jun 2023 09:34:20 GMT", "title": "Shedding light on underrepresentation and Sampling Bias in machine\n\u00a0learning\n", "authors": ["Sami Zhioua", "R\\=uta Binkyt\\.e\n"], "categories": ["cs.LG", "cs.AI", "cs.CY\n"], "abstract": "Accurately measuring discrimination is crucial to faithfully assessing fairness of trained machine learning (ML) models. Any bias in measuring discrimination leads to either amplification or underestimation of the existing disparity. Several sources of bias exist and it is assumed that bias resulting from machine learning is born equally by different groups (e.g. females vs males, whites vs blacks, etc.). If, however, bias is born differently by different groups, it may exacerbate discrimination against specific sub-populations. Sampling bias, is inconsistently used in the literature to describe bias due to the sampling procedure. In this paper, we attempt to disambiguate this term by introducing clearly defined variants of sampling bias, namely, sample size bias (SSB) and underrepresentation bias (URB). We show also how discrimination can be decomposed into variance, bias, and noise. Finally, we challenge the commonly accepted mitigation approach that discrimination can be addressed by collecting more samples of the underrepresented group.", "link": "https://arxiv.org/abs/2306.05068"}, {"id": "2306.05150", "date": "Thu, 8 Jun 2023 12:18:18 GMT", "title": "Bayesian Optimization of Expensive Nested Grey-Box Functions\n", "authors": ["Wenjie Xu", "Yuning Jiang", "Bratislav Svetozarevic", "Colin N. Jones\n"], "categories": ["cs.LG", "cs.AI", "math.OC\n"], "abstract": "We consider the problem of optimizing a grey-box objective function, i.e., nested function composed of both black-box and white-box functions. A general formulation for such grey-box problems is given, which covers the existing grey-box optimization formulations as special cases. We then design an optimism-driven algorithm to solve it. Under certain regularity assumptions, our algorithm achieves similar regret bound as that for the standard black-box Bayesian optimization algorithm, up to a constant multiplicative term depending on the Lipschitz constants of the functions considered. We further extend our method to the constrained case and discuss several special cases. For the commonly used kernel functions, the regret bounds allow us to derive a convergence rate to the optimal solution. Experimental results show that our grey-box optimization method empirically improves the speed of finding the global optimal solution significantly, as compared to the standard black-box optimization algorithm.", "link": "https://arxiv.org/abs/2306.05150"}, {"id": "2306.05256", "date": "Thu, 8 Jun 2023 14:53:02 GMT", "title": "Unscented Autoencoder\n", "authors": ["Faris Janjo\\v{s}", "Lars Rosenbaum", "Maxim Dolgov", "J. Marius Z\\\"ollner\n"], "categories": ["cs.LG", "cs.AI", "cs.CV\n"], "abstract": "The Variational Autoencoder (VAE) is a seminal approach in deep generative modeling with latent variables. Interpreting its reconstruction process as a nonlinear transformation of samples from the latent posterior distribution, we apply the Unscented Transform (UT) -- a well-known distribution approximation used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite set of statistics called sigma points, sampled deterministically, provides a more informative and lower-variance posterior representation than the ubiquitous noise-scaling of the reparameterization trick, while ensuring higher-quality reconstruction. We further boost the performance by replacing the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric that allows for a sharper posterior. Inspired by the two components, we derive a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder (UAE), trained purely with regularization-like terms on the per-sample posterior. We empirically show competitive performance in Fr\\'echet Inception Distance (FID) scores over closely-related models, in addition to a lower training variance than the VAE.", "link": "https://arxiv.org/abs/2306.05256"}, {"id": "2306.05268", "date": "Thu, 8 Jun 2023 15:17:04 GMT", "title": "Factorized Contrastive Learning: Going Beyond Multi-view Redundancy\n", "authors": ["Paul Pu Liang", "Zihao Deng", "Martin Ma", "James Zou", "Louis-Philippe\n\u00a0Morency", "Ruslan Salakhutdinov\n"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM\nComments:", "Code", "available", "at:", "https://github.com/pliang279/FactorCL\n"], "abstract": "In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks.", "link": "https://arxiv.org/abs/2306.05268"}, {"id": "2306.05304", "date": "Thu, 8 Jun 2023 15:50:35 GMT", "title": "Bayesian Optimisation of Functions on Graphs\n", "authors": ["Xingchen Wan", "Pierre Osselin", "Henry Kenlay", "Binxin Ru", "Michael A.\n\u00a0Osborne", "Xiaowen Dong\n"], "categories": ["cs.LG", "cs.AI", "stat.ML\nComments:", "10", "pages,", "9", "figures,", "1", "table", "(23", "pages,", "24", "figures,", "1", "table", "including\n\u00a0references", "and", "appendices)\n"], "abstract": "The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has been scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.", "link": "https://arxiv.org/abs/2306.05304"}, {"id": "2306.05415", "date": "Thu, 8 Jun 2023 17:58:05 GMT", "title": "Causal normalizing flows: from theory to practice\n", "authors": ["Adri\\'an Javaloy", "Pablo S\\'anchez-Mart\\'in and Isabel Valera\n"], "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML\nComments:", "31", "pages,", "15", "figures.", "Under", "submission\n"], "abstract": "In this work, we deepen on the use of normalizing flows for causal reasoning. Specifically, we first leverage recent results on non-linear ICA to show that causal models are identifiable from observational data given a causal ordering, and thus can be recovered using autoregressive normalizing flows (NFs). Second, we analyze different design and learning choices for causal normalizing flows to capture the underlying causal data-generating process. Third, we describe how to implement the do-operator in causal NFs, and thus, how to answer interventional and counterfactual questions. Finally, in our experiments, we validate our design and training choices through a comprehensive ablation study; compare causal NFs to other approaches for approximating causal models; and empirically demonstrate that causal NFs can be used to address real-world problems, where the presence of mixed discrete-continuous data and partial knowledge on the causal graph is the norm. The code for this work can be found at https://github.com/psanch21/causal-flows.", "link": "https://arxiv.org/abs/2306.05415"}, {"id": "2306.05059", "date": "Thu, 8 Jun 2023 09:23:22 GMT", "title": "Reconciling Predictive and Statistical Parity: A Causal Approach\n", "authors": ["Drago Plecko", "Elias Bareinboim\n"], "categories": ["cs.CY", "cs.AI", "cs.LG", "stat.ML\n"], "abstract": "Since the rise of fair machine learning as a critical field of inquiry, many different notions on how to quantify and measure discrimination have been proposed in the literature. Some of these notions, however, were shown to be mutually incompatible. Such findings make it appear that numerous different kinds of fairness exist, thereby making a consensus on the appropriate measure of fairness harder to reach, hindering the applications of these tools in practice. In this paper, we investigate one of these key impossibility results that relates the notions of statistical and predictive parity. Specifically, we derive a new causal decomposition formula for the fairness measures associated with predictive parity, and obtain a novel insight into how this criterion is related to statistical parity through the legal doctrines of disparate treatment, disparate impact, and the notion of business necessity. Our results show that through a more careful causal analysis, the notions of statistical and predictive parity are not really mutually exclusive, but complementary and spanning a spectrum of fairness notions through the concept of business necessity. Finally, we demonstrate the importance of our findings on a real-world example.", "link": "https://arxiv.org/abs/2306.05059"}, {"id": "2306.05071", "date": "Thu, 8 Jun 2023 09:40:28 GMT", "title": "A Causal Framework for Decomposing Spurious Variations\n", "authors": ["Drago Plecko", "Elias Bareinboim\n"], "categories": ["stat.ME", "cs.AI", "cs.LG", "stat.ML\n"], "abstract": "One of the fundamental challenges found throughout the data sciences is to explain why things happen in specific ways, or through which mechanisms a certain variable $X$ exerts influences over another variable $Y$. In statistics and machine learning, significant efforts have been put into developing machinery to estimate correlations across variables efficiently. In causal inference, a large body of literature is concerned with the decomposition of causal effects under the rubric of mediation analysis. However, many variations are spurious in nature, including different phenomena throughout the applied sciences. Despite the statistical power to estimate correlations and the identification power to decompose causal effects, there is still little understanding of the properties of spurious associations and how they can be decomposed in terms of the underlying causal mechanisms. In this manuscript, we develop formal tools for decomposing spurious variations in both Markovian and Semi-Markovian models. We prove the first results that allow a non-parametric decomposition of spurious effects and provide sufficient conditions for the identification of such decompositions. The described approach has several applications, ranging from explainable and fair AI to questions in epidemiology and medicine, and we empirically demonstrate its use on a real-world dataset.", "link": "https://arxiv.org/abs/2306.05071"}, {"id": "2306.05284", "date": "Thu, 8 Jun 2023 15:31:05 GMT", "title": "Simple and Controllable Music Generation\n", "authors": ["Jade Copet", "Felix Kreuk", "Itai Gat", "Tal Remez", "David Kant", "Gabriel\n\u00a0Synnaeve", "Yossi Adi", "Alexandre D\\'efossez\n"], "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS\n"], "abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft.", "link": "https://arxiv.org/abs/2306.05284"}, {"id": "2306.05358", "date": "Tue, 30 May 2023 00:57:51 GMT", "title": "Trustworthy Sensor Fusion against Inaudible Command Attacks in Advanced\n\u00a0Driver-Assistance System\n", "authors": ["Jiwei Guan", "Lei Pan", "Chen Wang", "Shui Yu", "Longxiang Gao", "Xi Zheng\n"], "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS\n"], "abstract": "There are increasing concerns about malicious attacks on autonomous vehicles. In particular, inaudible voice command attacks pose a significant threat as voice commands become available in autonomous driving systems. How to empirically defend against these inaudible attacks remains an open question. Previous research investigates utilizing deep learning-based multimodal fusion for defense, without considering the model uncertainty in trustworthiness. As deep learning has been applied to increasingly sensitive tasks, uncertainty measurement is crucial in helping improve model robustness, especially in mission-critical scenarios. In this paper, we propose the Multimodal Fusion Framework (MFF) as an intelligent security system to defend against inaudible voice command attacks. MFF fuses heterogeneous audio-vision modalities using VGG family neural networks and achieves the detection accuracy of 92.25% in the comparative fusion method empirical study. Additionally, extensive experiments on audio-vision tasks reveal the model's uncertainty. Using Expected Calibration Errors, we measure calibration errors and Monte-Carlo Dropout to estimate the predictive distribution for the proposed models. Our findings show empirically to train robust multimodal models, improve standard accuracy and provide a further step toward interpretability. Finally, we discuss the pros and cons of our approach and its applicability for Advanced Driver Assistance Systems.", "link": "https://arxiv.org/abs/2306.05358"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.link + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
