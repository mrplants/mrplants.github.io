<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2311.00807", "Date": "Wed, 01 Nov 2023 19:43:56 ", "Title": "VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization", "Authors": ["Suraj Jyothi Unni", "Raha Moraffah", "Huan Liu"], "Categories": "cs.CV cs.LG"}, "abstract": "Visual question answering (VQA) models are designed to demonstrate visual-textual reasoning capabilities. However, their real-world applicability is hindered by a lack of comprehensive benchmark datasets. Existing domain generalization datasets for VQA exhibit a unilateral focus on textual shifts while VQA being a multi-modal task contains shifts across both visual and textual domains. We propose VQA-GEN, the first ever multi-modal benchmark dataset for distribution shift generated through a shift induced pipeline. Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing methods to joint multi-modal distribution shifts. validating that comprehensive multi-modal shifts are critical for robust VQA generalization. Models trained on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming the value of VQA-GEN. Further, we analyze the importance of each shift technique of our pipeline contributing to the generalization of the model.", "url": "https://arxiv.org/abs/2311.00807"}, {"metadata": {"arXiv": "2311.01064", "Date": "Thu, 02 Nov 2023 08:32:00 ", "Title": "Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images", "Authors": ["Zalan Fabian", "Zhongqi Miao", "Chunyuan Li", "Yuanhan Zhang", "Ziwei Liu", "Andr\\'es Hern\\'andez", "Andr\\'es Montes-Rojas", "Rafael Escucha", "Laura Siabatto", "Andr\\'es Link", "Pablo Arbel\\'aez", "Rahul Dodhia", "Juan Lavista Ferres"], "Categories": "cs.CV cs.LG", "Comments": ["18 pages", "9 figures"]}, "abstract": "Due to deteriorating environmental conditions and increasing human activity, conservation efforts directed towards wildlife is crucial. Motion-activated camera traps constitute an efficient tool for tracking and monitoring wildlife populations across the globe. Supervised learning techniques have been successfully deployed to analyze such imagery, however training such techniques requires annotations from experts. Reducing the reliance on costly labelled data therefore has immense potential in developing large-scale wildlife tracking solutions with markedly less human labor. In this work we propose WildMatch, a novel zero-shot species classification framework that leverages multimodal foundation models. In particular, we instruction tune vision-language models to generate detailed visual descriptions of camera trap images using similar terminology to experts. Then, we match the generated caption to an external knowledge base of descriptions in order to determine the species in a zero-shot manner. We investigate techniques to build instruction tuning datasets for detailed animal description generation and propose a novel knowledge augmentation technique to enhance caption quality. We demonstrate the performance of WildMatch on a new camera trap dataset collected in the Magdalena Medio region of Colombia.", "url": "https://arxiv.org/abs/2311.01064"}, {"metadata": {"arXiv": "2311.01111", "Date": "Thu, 02 Nov 2023 09:36:20 ", "Title": "H-NeXt: The next step towards roto-translation invariant networks", "Authors": ["Tomas Karella", "Filip Sroubek", "Jan Flusser", "Jan Blazek", "Vasek Kosik"], "Categories": "cs.CV cs.LG", "Comments": ["Appears in British Machine Vision Conference 2023 (BMVC 2023)"]}, "abstract": "The widespread popularity of equivariant networks underscores the significance of parameter efficient models and effective use of training data. At a time when robustness to unseen deformations is becoming increasingly important, we present H-NeXt, which bridges the gap between equivariance and invariance. H-NeXt is a parameter-efficient roto-translation invariant network that is trained without a single augmented image in the training set. Our network comprises three components: an equivariant backbone for learning roto-translation independent features, an invariant pooling layer for discarding roto-translation information, and a classification layer. H-NeXt outperforms the state of the art in classification on unaugmented training sets and augmented test sets of MNIST and CIFAR-10.", "url": "https://arxiv.org/abs/2311.01111"}, {"metadata": {"arXiv": "2311.01130", "Date": "Thu, 02 Nov 2023 10:25:47 ", "Title": "A deep learning experiment for semantic segmentation of overlapping characters in palimpsests", "Authors": ["Michela Perino", "Michele Ginolfi", "Anna Candida Felici", "Michela Rosellini"], "Categories": "cs.CV cs.LG"}, "abstract": "Palimpsests refer to historical manuscripts where erased writings have been partially covered by the superimposition of a second writing. By employing imaging techniques, e.g., multispectral imaging, it becomes possible to identify features that are imperceptible to the naked eye, including faded and erased inks. When dealing with overlapping inks, Artificial Intelligence techniques can be utilized to disentangle complex nodes of overlapping letters. In this work, we propose deep learning-based semantic segmentation as a method for identifying and segmenting individual letters in overlapping characters. The experiment was conceived as a proof of concept, focusing on the palimpsests of the Ars Grammatica by Prisciano as a case study. Furthermore, caveats and prospects of our approach combined with multispectral imaging are also discussed.", "url": "https://arxiv.org/abs/2311.01130"}, {"metadata": {"arXiv": "2311.01138", "Date": "Thu, 02 Nov 2023 10:41:42 ", "Title": "AeroPath: An airway segmentation benchmark dataset with challenging pathology", "Authors": ["Karen-Helene St{\\o}verud", "David Bouget", "Andre Pedersen", "H{\\aa}kon Olav Leira", "Thomas Lang{\\o}", "and Erlend Fagertun Hofstad"], "Categories": "cs.CV cs.LG", "Comments": ["13 pages", "5 figures", "submitted to Scientific Reports"]}, "abstract": "To improve the prognosis of patients suffering from pulmonary diseases, such as lung cancer, early diagnosis and treatment are crucial. The analysis of CT images is invaluable for diagnosis, whereas high quality segmentation of the airway tree are required for intervention planning and live guidance during bronchoscopy. Recently, the Multi-domain Airway Tree Modeling (ATM'22) challenge released a large dataset, both enabling training of deep-learning based models and bringing substantial improvement of the state-of-the-art for the airway segmentation task. However, the ATM'22 dataset includes few patients with severe pathologies affecting the airway tree anatomy. In this study, we introduce a new public benchmark dataset (AeroPath), consisting of 27 CT images from patients with pathologies ranging from emphysema to large tumors, with corresponding trachea and bronchi annotations. Second, we present a multiscale fusion design for automatic airway segmentation. Models were trained on the ATM'22 dataset, tested on the AeroPath dataset, and further evaluated against competitive open-source methods. The same performance metrics as used in the ATM'22 challenge were used to benchmark the different considered approaches. Lastly, an open web application is developed, to easily test the proposed model on new data. The results demonstrated that our proposed architecture predicted topologically correct segmentations for all the patients included in the AeroPath dataset. The proposed method is robust and able to handle various anomalies, down to at least the fifth airway generation. In addition, the AeroPath dataset, featuring patients with challenging pathologies, will contribute to development of new state-of-the-art methods. The AeroPath dataset and the web application are made openly available.", "url": "https://arxiv.org/abs/2311.01138"}, {"metadata": {"arXiv": "2311.01410", "Date": "Thu, 02 Nov 2023 17:23:14 ", "Title": "The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing", "Authors": ["Shen Nie", "Hanzhong Allan Guo", "Cheng Lu", "Yuhao Zhou", "Chenyu Zheng", "Chongxuan Li"], "Categories": "cs.CV cs.LG"}, "abstract": "We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE). Instead, it defines a corresponding SDE or ODE for editing. In the formulation, we prove that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing. Inspired by it, we provide the SDE counterparts for widely used ODE baselines in various tasks including inpainting and image-to-image translation, where SDE shows a consistent and substantial improvement. Moreover, we propose SDE-Drag -- a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set natural, art, and AI-generated images for evaluation. A user study on DragBench indicates that SDE-Drag significantly outperforms our ODE baseline, existing diffusion-based methods, and the renowned DragGAN. Our results demonstrate the superiority and versatility of SDE in image editing and push the boundary of diffusion-based editing methods.", "url": "https://arxiv.org/abs/2311.01410"}, {"metadata": {"arXiv": "2311.01447", "Date": "Thu, 02 Nov 2023 17:56:59 ", "Title": "CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation", "Authors": ["Jingkang Wang", "Sivabalan Manivasagam", "Yun Chen", "Ze Yang", "Ioan Andrei B\\^arsan", "Anqi Joyce Yang", "Wei-Chiu Ma", "Raquel Urtasun"], "Categories": "cs.CV cs.LG cs.RO", "Comments": ["CoRL 2022. Project page: https://waabi.ai/cadsim/"]}, "abstract": "Realistic simulation is key to enabling safe and scalable development of % self-driving vehicles. A core component is simulating the sensors so that the entire autonomy system can be tested in simulation. Sensor simulation involves modeling traffic participants, such as vehicles, with high quality appearance and articulated geometry, and rendering them in real time. The self-driving industry has typically employed artists to build these assets. However, this is expensive, slow, and may not reflect reality. Instead, reconstructing assets automatically from sensor data collected in the wild would provide a better path to generating a diverse and large set with good real-world coverage. Nevertheless, current reconstruction approaches struggle on in-the-wild sensor data, due to its sparsity and noise. To tackle these issues, we present CADSim, which combines part-aware object-class priors via a small set of CAD models with differentiable rendering to automatically reconstruct vehicle geometry, including articulated wheels, with high-quality appearance. Our experiments show our method recovers more accurate shapes from sparse data compared to existing approaches. Importantly, it also trains and renders efficiently. We demonstrate our reconstructed vehicles in several applications, including accurate testing of autonomy perception systems.", "url": "https://arxiv.org/abs/2311.01447"}, {"metadata": {"arXiv": "2311.01448", "Date": "Thu, 02 Nov 2023 17:57:03 ", "Title": "UltraLiDAR: Learning Compact Representations for LiDAR Completion and Generation", "Authors": ["Yuwen Xiong", "Wei-Chiu Ma", "Jingkang Wang", "Raquel Urtasun"], "Categories": "cs.CV cs.LG cs.RO", "Comments": ["CVPR 2023. Project page: https://waabi.ai/ultralidar/"]}, "abstract": "LiDAR provides accurate geometric measurements of the 3D world. Unfortunately, dense LiDARs are very expensive and the point clouds captured by low-beam LiDAR are often sparse. To address these issues, we present UltraLiDAR, a data-driven framework for scene-level LiDAR completion, LiDAR generation, and LiDAR manipulation. The crux of UltraLiDAR is a compact, discrete representation that encodes the point cloud's geometric structure, is robust to noise, and is easy to manipulate. We show that by aligning the representation of a sparse point cloud to that of a dense point cloud, we can densify the sparse point clouds as if they were captured by a real high-density LiDAR, drastically reducing the cost. Furthermore, by learning a prior over the discrete codebook, we can generate diverse, realistic LiDAR point clouds for self-driving. We evaluate the effectiveness of UltraLiDAR on sparse-to-dense LiDAR completion and LiDAR generation. Experiments show that densifying real-world point clouds with our approach can significantly improve the performance of downstream perception systems. Compared to prior art on LiDAR generation, our approach generates much more realistic point clouds. According to A/B test, over 98.5\\% of the time human participants prefer our results over those of previous methods.", "url": "https://arxiv.org/abs/2311.01448"}, {"metadata": {"arXiv": "2311.01458", "Date": "Thu, 02 Nov 2023 17:59:31 ", "Title": "Detecting Deepfakes Without Seeing Any", "Authors": ["Tal Reiss", "Bar Cavia", "Yedid Hoshen"], "Categories": "cs.CV cs.LG", "Comments": ["Our code is available at https://github.com/talreiss/FACTOR"]}, "abstract": "Deepfake attacks, malicious manipulation of media containing people, are a serious concern for society. Conventional deepfake detection methods train supervised classifiers to distinguish real media from previously encountered deepfakes. Such techniques can only detect deepfakes similar to those previously seen, but not zero-day (previously unseen) attack types. As current deepfake generation techniques are changing at a breathtaking pace, new attack types are proposed frequently, making this a major issue. Our main observations are that: i) in many effective deepfake attacks, the fake media must be accompanied by false facts i.e. claims about the identity, speech, motion, or appearance of the person. For instance, when impersonating Obama, the attacker explicitly or implicitly claims that the fake media show Obama; ii) current generative techniques cannot perfectly synthesize the false facts claimed by the attacker. We therefore introduce the concept of \"fact checking\", adapted from fake news detection, for detecting zero-day deepfake attacks. Fact checking verifies that the claimed facts (e.g. identity is Obama), agree with the observed media (e.g. is the face really Obama's?), and thus can differentiate between real and fake media. Consequently, we introduce FACTOR, a practical recipe for deepfake fact checking and demonstrate its power in critical attack settings: face swapping and audio-visual synthesis. Although it is training-free, relies exclusively on off-the-shelf features, is very easy to implement, and does not see any deepfakes, it achieves better than state-of-the-art accuracy.", "url": "https://arxiv.org/abs/2311.01458"}, {"metadata": {"arXiv": "2311.01462", "Date": "Thu, 02 Nov 2023 17:59:55 ", "Title": "Idempotent Generative Network", "Authors": ["Assaf Shocher", "Amil Dravid", "Yossi Gandelsman", "Inbar Mosseri", "Michael Rubinstein", "Alexei A. Efros"], "Categories": "cs.CV cs.LG"}, "abstract": "We propose a new approach for generative modeling based on training a neural network to be idempotent. An idempotent operator is one that can be applied sequentially without changing the result beyond the initial application, namely $f(f(z))=f(z)$. The proposed model $f$ is trained to map a source distribution (e.g, Gaussian noise) to a target distribution (e.g. realistic images) using the following objectives: (1) Instances from the target distribution should map to themselves, namely $f(x)=x$. We define the target manifold as the set of all instances that $f$ maps to themselves. (2) Instances that form the source distribution should map onto the defined target manifold. This is achieved by optimizing the idempotence term, $f(f(z))=f(z)$ which encourages the range of $f(z)$ to be on the target manifold. Under ideal assumptions such a process provably converges to the target distribution. This strategy results in a model capable of generating an output in one step, maintaining a consistent latent space, while also allowing sequential applications for refinement. Additionally, we find that by processing inputs from both target and source distributions, the model adeptly projects corrupted or modified data back to the target manifold. This work is a first step towards a ``global projector'' that enables projecting any input into a target data distribution.", "url": "https://arxiv.org/abs/2311.01462"}, {"metadata": {"arXiv": "2311.00731", "Date": "Wed, 01 Nov 2023 06:12:02 ", "Title": "Enhancing Clustering Representations with Positive Proximity and Cluster Dispersion Learning", "Authors": ["Abhishek Kumar and Dong-Gyu Lee"], "Categories": "cs.LG cs.CV"}, "abstract": "Contemporary deep clustering approaches often rely on either contrastive or non-contrastive techniques to acquire effective representations for clustering tasks. Contrastive methods leverage negative pairs to achieve homogenous representations but can introduce class collision issues, potentially compromising clustering performance. On the contrary, non-contrastive techniques prevent class collisions but may produce non-uniform representations that lead to clustering collapse. In this work, we propose a novel end-to-end deep clustering approach named PIPCDR, designed to harness the strengths of both approaches while mitigating their limitations. PIPCDR incorporates a positive instance proximity loss and a cluster dispersion regularizer. The positive instance proximity loss ensures alignment between augmented views of instances and their sampled neighbors, enhancing within-cluster compactness by selecting genuinely positive pairs within the embedding space. Meanwhile, the cluster dispersion regularizer maximizes inter-cluster distances while minimizing within-cluster compactness, promoting uniformity in the learned representations. PIPCDR excels in producing well-separated clusters, generating uniform representations, avoiding class collision issues, and enhancing within-cluster compactness. We extensively validate the effectiveness of PIPCDR within an end-to-end Majorize-Minimization framework, demonstrating its competitive performance on moderate-scale clustering benchmark datasets and establishing new state-of-the-art results on large-scale datasets.", "url": "https://arxiv.org/abs/2311.00731"}, {"metadata": {"arXiv": "2311.00735", "Date": "Wed, 01 Nov 2023 12:04:33 ", "Title": "PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network", "Authors": ["Bohui Shen", "Wei Zhang", "Xubiao Liu", "Pengfei Yu", "Shirui Jiang", "Xinchong Shi", "Xiangsong Zhang", "Xiaoyu Zhou", "Weirui Zhang", "Bingxuan Li", "Qiegen Liu"], "Categories": "cs.LG cs.CV", "MSC-class": "68T01"}, "abstract": "Positron emission tomography (PET), as an imaging technique with high biochemical sensitivity, has been widely used in diagnosis of encephalopathy and brain science research used in brain disease diagnosis and brain science research. Since different tracers present different effects on the same focal area, the choice of tracers is getting more significant for PET imaging. Nowadays, with the wide application of PET imaging in neuropsychiatric treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine (DOPA) has been found to be more effective than 18F-labeled fluorine-2-deoxyglucose (FDG) in this field. However, due to the complexity of its preparation and other limitations, DOPA is far less widely used than FDG. To address this issue, a tracer conversion invertible neural network (TC-INN) for image projection is developed to map FDG images to DOPA images through deep learning. More diagnostic information is obtained by generating PET images from FDG to DOPA. Specifically, the proposed TC-INN consists of two separate phases, one for training the traceable data, the other for re-building the new data. The reference DOPA PET image is used as the learning target for the corresponding network during the training process of tracer conversion. Mean-while, the invertible network iteratively estimates the resultant DOPA PET data and compares it to the reference DOPA PET data. Notably, the reversible model employed variable enhancement techniques to achieve better power generation. Moreover, image registration needs to be performed before training due to the angular deviation of the acquired FDG and DOPA data information. Experimental results show generative ability in mapping be-tween FDG images and DOPA images. It demonstrates great potential for PET image conversion in the case of limited tracer applications.", "url": "https://arxiv.org/abs/2311.00735"}, {"metadata": {"arXiv": "2311.00737", "Date": "Wed, 01 Nov 2023 13:57:33 ", "Title": "Real-Time Magnetic Tracking and Diagnosis of COVID-19 via Machine Learning", "Authors": ["Dang Nguyen", "Phat K. Huynh", "Vinh Duc An Bui", "Kee Young Hwang", "Nityanand Jain", "Chau Nguyen", "Le Huu Nhat Minh", "Le Van Truong", "Xuan Thanh Nguyen", "Dinh Hoang Nguyen", "Le Tien Dung", "Trung Q. Le", "and Manh-Huong Phan"], "Categories": "cs.LG physics.ins-det physics.med-ph"}, "abstract": "The COVID-19 pandemic underscored the importance of reliable, noninvasive diagnostic tools for robust public health interventions. In this work, we fused magnetic respiratory sensing technology (MRST) with machine learning (ML) to create a diagnostic platform for real-time tracking and diagnosis of COVID-19 and other respiratory diseases. The MRST precisely captures breathing patterns through three specific breath testing protocols: normal breath, holding breath, and deep breath. We collected breath data from both COVID-19 patients and healthy subjects in Vietnam using this platform, which then served to train and validate ML models. Our evaluation encompassed multiple ML algorithms, including support vector machines and deep learning models, assessing their ability to diagnose COVID-19. Our multi-model validation methodology ensures a thorough comparison and grants the adaptability to select the most optimal model, striking a balance between diagnostic precision with model interpretability. The findings highlight the exceptional potential of our diagnostic tool in pinpointing respiratory anomalies, achieving over 90% accuracy. This innovative sensor technology can be seamlessly integrated into healthcare settings for patient monitoring, marking a significant enhancement for the healthcare infrastructure.", "url": "https://arxiv.org/abs/2311.00737"}, {"metadata": {"arXiv": "2311.00768", "Date": "Wed, 01 Nov 2023 18:23:12 ", "Title": "Language Model Training Paradigms for Clinical Feature Embeddings", "Authors": ["Yurong Hu", "Manuel Burger", "Gunnar R\\\"atsch", "Rita Kuznetsova"], "Categories": "cs.LG cs.CL", "Comments": ["Poster at \"NeurIPS 2023 Workshop: Self-Supervised Learning - Theory and Practice\""]}, "abstract": "In research areas with scarce data, representation learning plays a significant role. This work aims to enhance representation learning for clinical time series by deriving universal embeddings for clinical features, such as heart rate and blood pressure. We use self-supervised training paradigms for language models to learn high-quality clinical feature embeddings, achieving a finer granularity than existing time-step and patient-level representation learning. We visualize the learnt embeddings via unsupervised dimension reduction techniques and observe a high degree of consistency with prior clinical knowledge. We also evaluate the model performance on the MIMIC-III benchmark and demonstrate the effectiveness of using clinical feature embeddings. We publish our code online for replication.", "url": "https://arxiv.org/abs/2311.00768"}, {"metadata": {"arXiv": "2311.00774", "Date": "Wed, 01 Nov 2023 18:37:07 ", "Title": "Conformalized Deep Splines for Optimal and Efficient Prediction Sets", "Authors": ["Nathaniel Diamant", "Ehsan Hajiramezanali", "Tommaso Biancalani", "Gabriele Scalia"], "Categories": "cs.LG"}, "abstract": "Uncertainty estimation is critical in high-stakes machine learning applications. One effective way to estimate uncertainty is conformal prediction, which can provide predictive inference with statistical coverage guarantees. We present a new conformal regression method, Spline Prediction Intervals via Conformal Estimation (SPICE), that estimates the conditional density using neural-network-parameterized splines. We prove universal approximation and optimality results for SPICE, which are empirically validated by our experiments. SPICE is compatible with two different efficient-to-compute conformal scores, one oracle-optimal for marginal coverage (SPICE-ND) and the other asymptotically optimal for conditional coverage (SPICE-HPD). Results on benchmark datasets demonstrate SPICE-ND models achieve the smallest average prediction set sizes, including average size reductions of nearly 50% for some datasets compared to the next best baseline. SPICE-HPD models achieve the best conditional coverage compared to baselines. The SPICE implementation is made available.", "url": "https://arxiv.org/abs/2311.00774"}, {"metadata": {"arXiv": "2311.00801", "Date": "Wed, 01 Nov 2023 19:35:18 ", "Title": "GIST: Generated Inputs Sets Transferability in Deep Learning", "Authors": ["Florian Tambon", "Foutse Khomh", "Giuliano Antoniol"], "Categories": "cs.LG cs.SE", "Comments": ["submitted to the \"ACM Transactions on Software Engineering and Methodology\" journal"]}, "abstract": "As the demand for verifiability and testability of neural networks continues to rise, an increasing number of methods for generating test sets are being developed. However, each of these techniques tends to emphasize specific testing aspects and can be quite time-consuming. A straightforward solution to mitigate this issue is to transfer test sets between some benchmarked models and a new model under test, based on a desirable property one wishes to transfer. This paper introduces GIST (Generated Inputs Sets Transferability), a novel approach for the efficient transfer of test sets among Deep Learning models. Given a property of interest that a user wishes to transfer (e.g., coverage criterion), GIST enables the selection of good test sets from the point of view of this property among available ones from a benchmark. We empirically evaluate GIST on fault types coverage property with two modalities and different test set generation procedures to demonstrate the approach's feasibility. Experimental results show that GIST can select an effective test set for the given property to transfer it to the model under test. Our results suggest that GIST could be applied to transfer other properties and could generalize to different test sets' generation procedures and modalities", "url": "https://arxiv.org/abs/2311.00801"}, {"metadata": {"arXiv": "2311.00808", "Date": "Wed, 01 Nov 2023 19:46:40 ", "Title": "Mahalanobis-Aware Training for Out-of-Distribution Detection", "Authors": ["Connor Mclaughlin", "Jason Matterer", "Michael Yee"], "Categories": "cs.LG", "Comments": ["2 pages", "2 figures. Presented at AAAI Fall Symposium Series `23"]}, "abstract": "While deep learning models have seen widespread success in controlled environments, there are still barriers to their adoption in open-world settings. One critical task for safe deployment is the detection of anomalous or out-of-distribution samples that may require human intervention. In this work, we present a novel loss function and recipe for training networks with improved density-based out-of-distribution sensitivity. We demonstrate the effectiveness of our method on CIFAR-10, notably reducing the false-positive rate of the relative Mahalanobis distance method on far-OOD tasks by over 50%.", "url": "https://arxiv.org/abs/2311.00808"}, {"metadata": {"arXiv": "2311.00858", "Date": "Wed, 01 Nov 2023 21:24:37 ", "Title": "SmoothHess: ReLU Network Feature Interactions via Stein's Lemma", "Authors": ["Max Torop", "Aria Masoomi", "Davin Hill", "Kivanc Kose", "Stratis Ioannidis", "Jennifer Dy"], "Categories": "cs.LG", "Comments": ["Accepted to NeurIPS 2023 as a conference paper"]}, "abstract": "Several recent methods for interpretability model feature interactions by looking at the Hessian of a neural network. This poses a challenge for ReLU networks, which are piecewise-linear and thus have a zero Hessian almost everywhere. We propose SmoothHess, a method of estimating second-order interactions through Stein's Lemma. In particular, we estimate the Hessian of the network convolved with a Gaussian through an efficient sampling algorithm, requiring only network gradient calls. SmoothHess is applied post-hoc, requires no modifications to the ReLU network architecture, and the extent of smoothing can be controlled explicitly. We provide a non-asymptotic bound on the sample complexity of our estimation procedure. We validate the superior ability of SmoothHess to capture interactions on benchmark datasets and a real-world medical spirometry dataset.", "url": "https://arxiv.org/abs/2311.00858"}, {"metadata": {"arXiv": "2311.00866", "Date": "Wed, 01 Nov 2023 21:36:15 ", "Title": "Generalizing Nonlinear ICA Beyond Structural Sparsity", "Authors": ["Yujia Zheng", "Kun Zhang"], "Categories": "cs.LG eess.SP stat.ML"}, "abstract": "Nonlinear independent component analysis (ICA) aims to uncover the true latent sources from their observable nonlinear mixtures. Despite its significance, the identifiability of nonlinear ICA is known to be impossible without additional assumptions. Recent advances have proposed conditions on the connective structure from sources to observed variables, known as Structural Sparsity, to achieve identifiability in an unsupervised manner. However, the sparsity constraint may not hold universally for all sources in practice. Furthermore, the assumptions of bijectivity of the mixing process and independence among all sources, which arise from the setting of ICA, may also be violated in many real-world scenarios. To address these limitations and generalize nonlinear ICA, we propose a set of new identifiability results in the general settings of undercompleteness, partial sparsity and source dependence, and flexible grouping structures. Specifically, we prove identifiability when there are more observed variables than sources (undercomplete), and when certain sparsity and/or source independence assumptions are not met for some changing sources. Moreover, we show that even in cases with flexible grouping structures (e.g., part of the sources can be divided into irreducible independent groups with various sizes), appropriate identifiability results can also be established. Theoretical claims are supported empirically on both synthetic and real-world datasets.", "url": "https://arxiv.org/abs/2311.00866"}, {"metadata": {"arXiv": "2311.00871", "Date": "Wed, 01 Nov 2023 21:41:08 ", "Title": "Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models", "Authors": ["Steve Yadlowsky", "Lyric Doshi", "Nilesh Tripuraneni"], "Categories": "cs.LG cs.CL stat.ML"}, "abstract": "Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of $(x, f(x))$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.", "url": "https://arxiv.org/abs/2311.00871"}, {"metadata": {"arXiv": "2311.00875", "Date": "Wed, 01 Nov 2023 22:02:08 ", "Title": "Learning Collective Behaviors from Observation", "Authors": ["Jinchao Feng and Ming Zhong"], "Categories": "cs.LG cs.MA math.DS"}, "abstract": "We present a review of a series of learning methods used to identify the structure of dynamical systems, aiming to understand emergent behaviors in complex systems of interacting agents. These methods not only offer theoretical guarantees of convergence but also demonstrate computational efficiency in handling high-dimensional observational data. They can manage observation data from both first- and second-order dynamical systems, accounting for observation/stochastic noise, complex interaction rules, missing interaction features, and real-world observations of interacting agent systems. The essence of developing such a series of learning methods lies in designing appropriate loss functions using the variational inverse problem approach, which inherently provides dimension reduction capabilities to our learning methods.", "url": "https://arxiv.org/abs/2311.00875"}, {"metadata": {"arXiv": "2311.00886", "Date": "Wed, 01 Nov 2023 22:38:14 ", "Title": "COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning", "Authors": ["Chuizheng Meng", "Yihe Dong", "Sercan \\\"O. Ar{\\i}k", "Yan Liu", "Tomas Pfister"], "Categories": "cs.LG"}, "abstract": "Estimation of temporal counterfactual outcomes from observed history is crucial for decision-making in many domains such as healthcare and e-commerce, particularly when randomized controlled trials (RCTs) suffer from high cost or impracticality. For real-world datasets, modeling time-dependent confounders is challenging due to complex dynamics, long-range dependencies and both past treatments and covariates affecting the future outcomes. In this paper, we introduce COunterfactual Self-supervised TrAnsformeR (COSTAR), a novel approach that integrates self-supervised learning for improved historical representations. The proposed framework combines temporal and feature-wise attention with a component-wise contrastive loss tailored for temporal treatment outcome observations, yielding superior performance in estimation accuracy and generalization to out-of-distribution data compared to existing models, as validated by empirical results on both synthetic and real-world datasets.", "url": "https://arxiv.org/abs/2311.00886"}, {"metadata": {"arXiv": "2311.00923", "Date": "Thu, 02 Nov 2023 01:31:42 ", "Title": "A Review and Roadmap of Deep Causal Model from Different Causal Structures and Representations", "Authors": ["Hang Chen and Keqing Du and Chenguang Li and Xinyu Yang"], "Categories": "cs.LG stat.ME", "Comments": ["under review"]}, "abstract": "The fusion of causal models with deep learning introducing increasingly intricate data sets, such as the causal associations within images or between textual components, has surfaced as a focal research area. Nonetheless, the broadening of original causal concepts and theories to such complex, non-statistical data has been met with serious challenges. In response, our study proposes redefinitions of causal data into three distinct categories from the standpoint of causal structure and representation: definite data, semi-definite data, and indefinite data. Definite data chiefly pertains to statistical data used in conventional causal scenarios, while semi-definite data refers to a spectrum of data formats germane to deep learning, including time-series, images, text, and others. Indefinite data is an emergent research sphere inferred from the progression of data forms by us. To comprehensively present these three data paradigms, we elaborate on their formal definitions, differences manifested in datasets, resolution pathways, and development of research. We summarize key tasks and achievements pertaining to definite and semi-definite data from myriad research undertakings, present a roadmap for indefinite data, beginning with its current research conundrums. Lastly, we classify and scrutinize the key datasets presently utilized within these three paradigms.", "url": "https://arxiv.org/abs/2311.00923"}, {"metadata": {"arXiv": "2311.00931", "Date": "Thu, 02 Nov 2023 01:51:43 ", "Title": "Learning Defect Prediction from Unrealistic Data", "Authors": ["Kamel Alrashedy", "Vincent J. Hellendoorn", "Alessandro Orso"], "Categories": "cs.LG"}, "abstract": "Pretrained models of code, such as CodeBERT and CodeT5, have become popular choices for code understanding and generation tasks. Such models tend to be large and require commensurate volumes of training data, which are rarely available for downstream tasks. Instead, it has become popular to train models with far larger but less realistic datasets, such as functions with artificially injected bugs. Models trained on such data, however, tend to only perform well on similar data, while underperforming on real world programs. In this paper, we conjecture that this discrepancy stems from the presence of distracting samples that steer the model away from the real-world task distribution. To investigate this conjecture, we propose an approach for identifying the subsets of these large yet unrealistic datasets that are most similar to examples in real-world datasets based on their learned representations. Our approach extracts high-dimensional embeddings of both real-world and artificial programs using a neural model and scores artificial samples based on their distance to the nearest real-world sample. We show that training on only the nearest, representationally most similar samples while discarding samples that are not at all similar in representations yields consistent improvements across two popular pretrained models of code on two code understanding tasks. Our results are promising, in that they show that training models on a representative subset of an unrealistic dataset can help us harness the power of large-scale synthetic data generation while preserving downstream task performance. Finally, we highlight the limitations of applying AI models for predicting vulnerabilities and bugs in real-world applications", "url": "https://arxiv.org/abs/2311.00931"}, {"metadata": {"arXiv": "2311.00936", "Date": "Thu, 02 Nov 2023 02:00:27 ", "Title": "SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data", "Authors": ["M\\'elisande Teng", "Amna Elmustafa", "Benjamin Akera", "Yoshua Bengio", "Hager Radi Abdelwahed", "Hugo Larochelle", "David Rolnick"], "Categories": "cs.LG cs.CV q-bio.PE", "Comments": ["37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks"]}, "abstract": "Biodiversity is declining at an unprecedented rate, impacting ecosystem services necessary to ensure food, water, and human health and well-being. Understanding the distribution of species and their habitats is crucial for conservation policy planning. However, traditional methods in ecology for species distribution models (SDMs) generally focus either on narrow sets of species or narrow geographical areas and there remain significant knowledge gaps about the distribution of species. A major reason for this is the limited availability of data traditionally used, due to the prohibitive amount of effort and expertise required for traditional field monitoring. The wide availability of remote sensing data and the growing adoption of citizen science tools to collect species observations data at low cost offer an opportunity for improving biodiversity monitoring and enabling the modelling of complex ecosystems. We introduce a novel task for mapping bird species to their habitats by predicting species encounter rates from satellite images, and present SatBird, a satellite dataset of locations in the USA with labels derived from presence-absence observation data from the citizen science database eBird, considering summer (breeding) and winter seasons. We also provide a dataset in Kenya representing low-data regimes. We additionally provide environmental data and species range maps for each location. We benchmark a set of baselines on our dataset, including SOTA models for remote sensing tasks. SatBird opens up possibilities for scalably modelling properties of ecosystems worldwide.", "url": "https://arxiv.org/abs/2311.00936"}, {"metadata": {"arXiv": "2311.00959", "Date": "Thu, 02 Nov 2023 03:05:40 ", "Title": "Dynamic Fair Federated Learning Based on Reinforcement Learning", "Authors": ["Weikang Chen", "Junping Du", "Yingxia Shao", "Jia Wang", "and Yangxi Zhou"], "Categories": "cs.LG cs.CY cs.IT math.IT"}, "abstract": "Federated learning enables a collaborative training and optimization of global models among a group of devices without sharing local data samples. However, the heterogeneity of data in federated learning can lead to unfair representation of the global model across different devices. To address the fairness issue in federated learning, we propose a dynamic q fairness federated learning algorithm with reinforcement learning, called DQFFL. DQFFL aims to mitigate the discrepancies in device aggregation and enhance the fairness of treatment for all groups involved in federated learning. To quantify fairness, DQFFL leverages the performance of the global federated model on each device and incorporates {\\alpha}-fairness to transform the preservation of fairness during federated aggregation into the distribution of client weights in the aggregation process. Considering the sensitivity of parameters in measuring fairness, we propose to utilize reinforcement learning for dynamic parameters during aggregation. Experimental results demonstrate that our DQFFL outperforms the state-of-the-art methods in terms of overall performance, fairness and convergence speed.", "url": "https://arxiv.org/abs/2311.00959"}, {"metadata": {"arXiv": "2311.00964", "Date": "Thu, 02 Nov 2023 03:18:40 ", "Title": "On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications", "Authors": ["Chengyao Wen", "Yin Lou"], "Categories": "cs.LG q-fin.ST"}, "abstract": "Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of rules. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diversity of the initial rule set and we empirically find that SpectralRules further improves the quality of the found Pareto front. On two real application scenarios within Alipay, we demonstrate the advantages of our proposed methodology compared to existing work.", "url": "https://arxiv.org/abs/2311.00964"}, {"metadata": {"arXiv": "2311.00966", "Date": "Thu, 02 Nov 2023 03:24:55 ", "Title": "Invariant-Feature Subspace Recovery: A New Class of Provable Domain Generalization Algorithms", "Authors": ["Haoxiang Wang", "Gargi Balasubramaniam", "Haozhe Si", "Bo Li", "Han Zhao"], "Categories": "cs.LG stat.ML", "Comments": ["Submitted to JMLR. This journal version significantly extends our ICML 2022 paper", "arXiv:2201.12919"]}, "abstract": "Domain generalization asks for models trained over a set of training environments to generalize well in unseen test environments. Recently, a series of algorithms such as Invariant Risk Minimization (IRM) have been proposed for domain generalization. However, Rosenfeld et al. (2021) shows that in a simple linear data model, even if non-convexity issues are ignored, IRM and its extensions cannot generalize to unseen environments with less than $d_s+1$ training environments, where $d_s$ is the dimension of the spurious-feature subspace. In this work, we propose Invariant-feature Subspace Recovery (ISR): a new class of algorithms to achieve provable domain generalization across the settings of classification and regression problems. First, in the binary classification setup of Rosenfeld et al. (2021), we show that our first algorithm, ISR-Mean, can identify the subspace spanned by invariant features from the first-order moments of the class-conditional distributions, and achieve provable domain generalization with $d_s+1$ training environments. Our second algorithm, ISR-Cov, further reduces the required number of training environments to $O(1)$ using the information of second-order moments. Notably, unlike IRM, our algorithms bypass non-convexity issues and enjoy global convergence guarantees. Next, we extend ISR-Mean to the more general setting of multi-class classification and propose ISR-Multiclass, which leverages class information and provably recovers the invariant-feature subspace with $\\lceil d_s/k\\rceil+1$ training environments for $k$-class classification. Finally, for regression problems, we propose ISR-Regression that can identify the invariant-feature subspace with $d_s+1$ training environments. Empirically, we demonstrate the superior performance of our ISRs on synthetic benchmarks. Further, ISR can be used as post-processing methods for feature extractors such as neural nets.", "url": "https://arxiv.org/abs/2311.00966"}, {"metadata": {"arXiv": "2311.00973", "Date": "Thu, 02 Nov 2023 03:41:58 ", "Title": "Federated Linear Bandits with Finite Adversarial Actions", "Authors": ["Li Fan", "Ruida Zhou", "Chao Tian", "Cong Shen"], "Categories": "cs.LG cs.IT math.IT stat.ML", "Comments": ["Accepted to NeurIPS 2023", "camera-ready version"]}, "abstract": "We study a federated linear bandits model, where $M$ clients communicate with a central server to solve a linear contextual bandits problem with finite adversarial action sets that may be different across clients. To address the unique challenges of adversarial finite action sets, we propose the FedSupLinUCB algorithm, which extends the principles of SupLinUCB and OFUL algorithms in linear contextual bandits. We prove that FedSupLinUCB achieves a total regret of $\\tilde{O}(\\sqrt{d T})$, where $T$ is the total number of arm pulls from all clients, and $d$ is the ambient dimension of the linear model. This matches the minimax lower bound and thus is order-optimal (up to polylog terms). We study both asynchronous and synchronous cases and show that the communication cost can be controlled as $O(d M^2 \\log(d)\\log(T))$ and $O(\\sqrt{d^3 M^3} \\log(d))$, respectively. The FedSupLinUCB design is further extended to two scenarios: (1) variance-adaptive, where a total regret of $\\tilde{O} (\\sqrt{d \\sum \\nolimits_{t=1}^{T} \\sigma_t^2})$ can be achieved with $\\sigma_t^2$ being the noise variance of round $t$; and (2) adversarial corruption, where a total regret of $\\tilde{O}(\\sqrt{dT} + d C_p)$ can be achieved with $C_p$ being the total corruption budget. Experiment results corroborate the theoretical analysis and demonstrate the effectiveness of FedSupLinUCB on both synthetic and real-world datasets.", "url": "https://arxiv.org/abs/2311.00973"}, {"metadata": {"arXiv": "2311.00993", "Date": "Thu, 02 Nov 2023 04:46:32 ", "Title": "Scalable Probabilistic Forecasting in Retail with Gradient Boosted Trees: A Practitioner's Approach", "Authors": ["Xueying Long", "Quang Bui", "Grady Oktavian", "Daniel F. Schmidt", "Christoph Bergmeir", "Rakshitha Godahewa", "Seong Per Lee", "Kaifeng Zhao", "Paul Condylis"], "Categories": "cs.LG"}, "abstract": "The recent M5 competition has advanced the state-of-the-art in retail forecasting. However, we notice important differences between the competition challenge and the challenges we face in a large e-commerce company. The datasets in our scenario are larger (hundreds of thousands of time series), and e-commerce can afford to have a larger assortment than brick-and-mortar retailers, leading to more intermittent data. To scale to larger dataset sizes with feasible computational effort, firstly, we investigate a two-layer hierarchy and propose a top-down approach to forecasting at an aggregated level with less amount of series and intermittency, and then disaggregating to obtain the decision-level forecasts. Probabilistic forecasts are generated under distributional assumptions. Secondly, direct training at the lower level with subsamples can also be an alternative way of scaling. Performance of modelling with subsets is evaluated with the main dataset. Apart from a proprietary dataset, the proposed scalable methods are evaluated using the Favorita dataset and the M5 dataset. We are able to show the differences in characteristics of the e-commerce and brick-and-mortar retail datasets. Notably, our top-down forecasting framework enters the top 50 of the original M5 competition, even with models trained at a higher level under a much simpler setting.", "url": "https://arxiv.org/abs/2311.00993"}, {"metadata": {"arXiv": "2311.01010", "Date": "Thu, 02 Nov 2023 06:09:24 ", "Title": "Exploring Unified Perspective For Fast Shapley Value Estimation", "Authors": ["Borui Zhang", "Baotong Tian", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "Categories": "cs.LG cs.CV"}, "abstract": "Shapley values have emerged as a widely accepted and trustworthy tool, grounded in theoretical axioms, for addressing challenges posed by black-box models like deep neural networks. However, computing Shapley values encounters exponential complexity in the number of features. Various approaches, including ApproSemivalue, KernelSHAP, and FastSHAP, have been explored to expedite the computation. We analyze the consistency of existing works and conclude that stochastic estimators can be unified as the linear transformation of importance sampling of feature subsets. Based on this, we investigate the possibility of designing simple amortized estimators and propose a straightforward and efficient one, SimSHAP, by eliminating redundant techniques. Extensive experiments conducted on tabular and image datasets validate the effectiveness of our SimSHAP, which significantly accelerates the computation of accurate Shapley values.", "url": "https://arxiv.org/abs/2311.01010"}, {"metadata": {"arXiv": "2311.01011", "Date": "Thu, 02 Nov 2023 06:13:36 ", "Title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game", "Authors": ["Sam Toyer", "Olivia Watkins", "Ethan Adrian Mendes", "Justin Svegliato", "Luke Bailey", "Tiffany Wang", "Isaac Ong", "Karim Elmaaroufi", "Pieter Abbeel", "Trevor Darrell", "Alan Ritter", "Stuart Russell"], "Categories": "cs.LG cs.CR"}, "abstract": "While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based \"defenses\" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release all data and source code at https://tensortrust.ai/paper", "url": "https://arxiv.org/abs/2311.01011"}, {"metadata": {"arXiv": "2311.01038", "Date": "Thu, 02 Nov 2023 07:09:59 ", "Title": "Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks", "Authors": ["Jiarong Xu", "Renhong Huang", "Xin Jiang", "Yuxuan Cao", "Carl Yang", "Chunping Wang", "Yang Yang"], "Categories": "cs.LG cs.SI"}, "abstract": "Pre-training on graph neural networks (GNNs) aims to learn transferable knowledge for downstream tasks with unlabeled data, and it has recently become an active research area. The success of graph pre-training models is often attributed to the massive amount of input data. In this paper, however, we identify the curse of big data phenomenon in graph pre-training: more training data do not necessarily lead to better downstream performance. Motivated by this observation, we propose a better-with-less framework for graph pre-training: fewer, but carefully chosen data are fed into a GNN model to enhance pre-training. The proposed pre-training pipeline is called the data-active graph pre-training (APT) framework, and is composed of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on the inherent properties of graphs as well as predictive uncertainty. The proposed predictive uncertainty, as feedback from the pre-training model, measures the confidence level of the model in the data. When fed with the chosen data, on the other hand, the pre-training model grasps an initial understanding of the new, unseen data, and at the same time attempts to remember the knowledge learned from previous data. Therefore, the integration and interaction between these two components form a unified framework (APT), in which graph pre-training is performed in a progressive and iterative way. Experiment results show that the proposed APT is able to obtain an efficient pre-training model with fewer training data and better downstream performance.", "url": "https://arxiv.org/abs/2311.01038"}, {"metadata": {"arXiv": "2311.01046", "Date": "Thu, 02 Nov 2023 07:42:23 ", "Title": "Time-Independent Information-Theoretic Generalization Bounds for SGLD", "Authors": ["Futoshi Futami", "Masahiro Fujisawa"], "Categories": "cs.LG stat.ML", "Comments": ["Accepted by the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS2023)", "29 pages"]}, "abstract": "We provide novel information-theoretic generalization bounds for stochastic gradient Langevin dynamics (SGLD) under the assumptions of smoothness and dissipativity, which are widely used in sampling and non-convex optimization studies. Our bounds are time-independent and decay to zero as the sample size increases, regardless of the number of iterations and whether the step size is fixed. Unlike previous studies, we derive the generalization error bounds by focusing on the time evolution of the Kullback--Leibler divergence, which is related to the stability of datasets and is the upper bound of the mutual information between output parameters and an input dataset. Additionally, we establish the first information-theoretic generalization bound when the training and test loss are the same by showing that a loss function of SGLD is sub-exponential. This bound is also time-independent and removes the problematic step size dependence in existing work, leading to an improved excess risk bound by combining our analysis with the existing non-convex optimization error bounds.", "url": "https://arxiv.org/abs/2311.01046"}, {"metadata": {"arXiv": "2311.01047", "Date": "Thu, 02 Nov 2023 07:47:42 ", "Title": "Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective", "Authors": ["Bhagyashree Puranik", "Ahmad Beirami", "Yao Qin", "Upamanyu Madhow"], "Categories": "cs.LG cs.IT eess.SP math.IT"}, "abstract": "State-of-the-art techniques for enhancing robustness of deep networks mostly rely on empirical risk minimization with suitable data augmentation. In this paper, we propose a complementary approach motivated by communication theory, aimed at enhancing the signal-to-noise ratio at the output of a neural network layer via neural competition during learning and inference. In addition to minimization of a standard end-to-end cost, neurons compete to sparsely represent layer inputs by maximization of a tilted exponential (TEXP) objective function for the layer. TEXP learning can be interpreted as maximum likelihood estimation of matched filters under a Gaussian model for data noise. Inference in a TEXP layer is accomplished by replacing batch norm by a tilted softmax, which can be interpreted as computation of posterior probabilities for the competing signaling hypotheses represented by each neuron. After providing insights via simplified models, we show, by experimentation on standard image datasets, that TEXP learning and inference enhances robustness against noise and other common corruptions, without requiring data augmentation. Further cumulative gains in robustness against this array of distortions can be obtained by appropriately combining TEXP with data augmentation techniques.", "url": "https://arxiv.org/abs/2311.01047"}, {"metadata": {"arXiv": "2311.01061", "Date": "Thu, 02 Nov 2023 08:26:29 ", "Title": "Deep Learning for real-time neural decoding of grasp", "Authors": ["Paolo Viviani and Ilaria Gesmundo and Elios Ghinato and Andres Agudelo-Toro and Chiara Vercellino and Giacomo Vitali and Letizia Bergamasco and Alberto Scionti and Marco Ghislieri and Valentina Agostini and Olivier Terzo and Hansj\\\"org Scherberger"], "Categories": "cs.LG q-bio.NC", "DOI": "10.1007/978-3-031-43427-3_23"}, "abstract": "Neural decoding involves correlating signals acquired from the brain to variables in the physical world like limb movement or robot control in Brain Machine Interfaces. In this context, this work starts from a specific pre-existing dataset of neural recordings from monkey motor cortex and presents a Deep Learning-based approach to the decoding of neural signals for grasp type classification. Specifically, we propose here an approach that exploits LSTM networks to classify time series containing neural data (i.e., spike trains) into classes representing the object being grasped. The main goal of the presented approach is to improve over state-of-the-art decoding accuracy without relying on any prior neuroscience knowledge, and leveraging only the capability of deep learning models to extract correlations from data. The paper presents the results achieved for the considered dataset and compares them with previous works on the same dataset, showing a significant improvement in classification accuracy, even if considering simulated real-time decoding.", "url": "https://arxiv.org/abs/2311.01061"}, {"metadata": {"arXiv": "2311.01075", "Date": "Thu, 02 Nov 2023 08:41:00 ", "Title": "Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning", "Authors": ["Siming Lan", "Rui Zhang", "Qi Yi", "Jiaming Guo", "Shaohui Peng", "Yunkai Gao", "Fan Wu", "Ruizhi Chen", "Zidong Du", "Xing Hu", "Xishan Zhang", "Ling Li", "Yunji Chen"], "Categories": "cs.LG", "Comments": ["This paper has been accepted at NeurIPS 2023 as a poster"]}, "abstract": "In the field of multi-task reinforcement learning, the modular principle, which involves specializing functionalities into different modules and combining them appropriately, has been widely adopted as a promising approach to prevent the negative transfer problem that performance degradation due to conflicts between tasks. However, most of the existing multi-task RL methods only combine shared modules at the task level, ignoring that there may be conflicts within the task. In addition, these methods do not take into account that without constraints, some modules may learn similar functions, resulting in restricting the model's expressiveness and generalization capability of modular methods. In this paper, we propose the Contrastive Modules with Temporal Attention(CMTA) method to address these limitations. CMTA constrains the modules to be different from each other by contrastive learning and combining shared modules at a finer granularity than the task level with temporal attention, alleviating the negative transfer within the task and improving the generalization ability and the performance for multi-task RL. We conducted the experiment on Meta-World, a multi-task RL benchmark containing various robotics manipulation tasks. Experimental results show that CMTA outperforms learning each task individually for the first time and achieves substantial performance improvements over the baselines.", "url": "https://arxiv.org/abs/2311.01075"}, {"metadata": {"arXiv": "2311.01106", "Date": "Thu, 02 Nov 2023 09:15:52 ", "Title": "In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer", "Authors": ["Yuzhou Cao", "Hussein Mozannar", "Lei Feng", "Hongxin Wei", "Bo An"], "Categories": "cs.LG", "Comments": ["NeurIPS 2023"]}, "abstract": "Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness. We further analyze the non-asymptotic properties of our method and empirically validate its performance and calibration on benchmark datasets.", "url": "https://arxiv.org/abs/2311.01106"}, {"metadata": {"arXiv": "2311.01118", "Date": "Thu, 02 Nov 2023 09:47:27 ", "Title": "AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning", "Authors": ["Mohammadamin Tavakoli", "Yin Ting T.Chiu", "Alexander Shmakov", "Ann Marie Carlton", "David Van Vranken", "Pierre Baldi"], "Categories": "cs.LG physics.chem-ph"}, "abstract": "Deep learning-based reaction predictors have undergone significant architectural evolution. However, their reliance on reactions from the US Patent Office results in a lack of interpretable predictions and limited generalization capability to other chemistry domains, such as radical and atmospheric chemistry. To address these challenges, we introduce a new reaction predictor system, RMechRP, that leverages contrastive learning in conjunction with mechanistic pathways, the most interpretable representation of chemical reactions. Specifically designed for radical reactions, RMechRP provides different levels of interpretation of chemical reactions. We develop and train multiple deep-learning models using RMechDB, a public database of radical reactions, to establish the first benchmark for predicting radical reactions. Our results demonstrate the effectiveness of RMechRP in providing accurate and interpretable predictions of radical reactions, and its potential for various applications in atmospheric chemistry.", "url": "https://arxiv.org/abs/2311.01118"}, {"metadata": {"arXiv": "2311.01135", "Date": "Thu, 02 Nov 2023 10:31:20 ", "Title": "Generating QM1B with PySCF$_{\\text{IPU}}$", "Authors": ["Alexander Mathiasen", "Hatem Helal", "Kerstin Klaser", "Paul Balanca", "Josef Dean", "Carlo Luschi", "Dominique Beaini", "Andrew Fitzgibbon", "Dominic Masters"], "Categories": "cs.LG physics.chem-ph", "Comments": ["15 pages", "7 figures. NeurIPS 2023 Track Datasets and Benchmarks"], "ACM-class": "I.2.6; J.2"}, "abstract": "The emergence of foundation models in Computer Vision and Natural Language Processing have resulted in immense progress on downstream tasks. This progress was enabled by datasets with billions of training examples. Similar benefits are yet to be unlocked for quantum chemistry, where the potential of deep learning is constrained by comparatively small datasets with 100k to 20M training examples. These datasets are limited in size because the labels are computed using the accurate (but computationally demanding) predictions of Density Functional Theory (DFT). Notably, prior DFT datasets were created using CPU supercomputers without leveraging hardware acceleration. In this paper, we take a first step towards utilising hardware accelerators by introducing the data generator PySCF$_{\\text{IPU}}$ using Intelligence Processing Units (IPUs). This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms. We demonstrate that a simple baseline neural network (SchNet 9M) improves its performance by simply increasing the amount of training data without additional inductive biases. To encourage future researchers to use QM1B responsibly, we highlight several limitations of QM1B and emphasise the low-resolution of our DFT options, which also serves as motivation for even larger, more accurate datasets. Code and dataset are available on Github: http://github.com/graphcore-research/pyscf-ipu", "url": "https://arxiv.org/abs/2311.01135"}, {"metadata": {"arXiv": "2311.01139", "Date": "Thu, 02 Nov 2023 10:42:35 ", "Title": "Add and Thin: Diffusion for Temporal Point Processes", "Authors": ["David L\\\"udke", "Marin Bilo\\v{s}", "Oleksandr Shchur", "Marten Lienen", "Stephan G\\\"unnemann"], "Categories": "cs.LG stat.ML"}, "abstract": "Autoregressive neural networks within the temporal point process (TPP) framework have become the standard for modeling continuous-time event data. Even though these models can expressively capture event sequences in a one-step-ahead fashion, they are inherently limited for long-term forecasting applications due to the accumulation of errors caused by their sequential nature. To overcome these limitations, we derive ADD-THIN, a principled probabilistic denoising diffusion model for TPPs that operates on entire event sequences. Unlike existing diffusion approaches, ADD-THIN naturally handles data with discrete and continuous components. In experiments on synthetic and real-world datasets, our model matches the state-of-the-art TPP models in density estimation and strongly outperforms them in forecasting.", "url": "https://arxiv.org/abs/2311.01139"}, {"metadata": {"arXiv": "2311.01196", "Date": "Thu, 02 Nov 2023 12:47:49 ", "Title": "Combating Bilateral Edge Noise for Robust Link Prediction", "Authors": ["Zhanke Zhou", "Jiangchao Yao", "Jiaxu Liu", "Xiawei Guo", "Quanming Yao", "Li He", "Liang Wang", "Bo Zheng", "Bo Han"], "Categories": "cs.LG cs.SI", "Comments": ["Accepted by NeurIPS 2023"]}, "abstract": "Although link prediction on graphs has achieved great success with the development of graph neural networks (GNNs), the potential robustness under the edge noise is still less investigated. To close this gap, we first conduct an empirical study to disclose that the edge noise bilaterally perturbs both input topology and target label, yielding severe performance degradation and representation collapse. To address this dilemma, we propose an information-theory-guided principle, Robust Graph Information Bottleneck (RGIB), to extract reliable supervision signals and avoid representation collapse. Different from the basic information bottleneck, RGIB further decouples and balances the mutual dependence among graph topology, target labels, and representation, building new learning objectives for robust representation against the bilateral noise. Two instantiations, RGIB-SSL and RGIB-REP, are explored to leverage the merits of different methodologies, i.e., self-supervised learning and data reparameterization, for implicit and explicit data denoising, respectively. Extensive experiments on six datasets and three GNNs with diverse noisy scenarios verify the effectiveness of our RGIB instantiations. The code is publicly available at: https://github.com/tmlr-group/RGIB.", "url": "https://arxiv.org/abs/2311.01196"}, {"metadata": {"arXiv": "2311.01198", "Date": "Thu, 02 Nov 2023 12:49:14 ", "Title": "Gaussian Processes on Cellular Complexes", "Authors": ["Mathieu Alain", "So Takao", "Brooks Paige", "Marc Peter Deisenroth"], "Categories": "cs.LG stat.ML"}, "abstract": "In recent years, there has been considerable interest in developing machine learning models on graphs in order to account for topological inductive biases. In particular, recent attention was given to Gaussian processes on such structures since they can additionally account for uncertainty. However, graphs are limited to modelling relations between two vertices. In this paper, we go beyond this dyadic setting and consider polyadic relations that include interactions between vertices, edges and one of their generalisations, known as cells. Specifically, we propose Gaussian processes on cellular complexes, a generalisation of graphs that captures interactions between these higher-order cells. One of our key contributions is the derivation of two novel kernels, one that generalises the graph Mat\\'ern kernel and one that additionally mixes information of different cell types.", "url": "https://arxiv.org/abs/2311.01198"}, {"metadata": {"arXiv": "2311.01252", "Date": "Thu, 02 Nov 2023 14:10:14 ", "Title": "Sanitized Clustering against Confounding Bias", "Authors": ["Yinghua Yao", "Yuangang Pan", "Jing Li", "Ivor W. Tsang", "Xin Yao"], "Categories": "cs.LG", "Comments": ["Machine Learning", "in press"]}, "abstract": "Real-world datasets inevitably contain biases that arise from different sources or conditions during data collection. Consequently, such inconsistency itself acts as a confounding factor that disturbs the cluster analysis. Existing methods eliminate the biases by projecting data onto the orthogonal complement of the subspace expanded by the confounding factor before clustering. Therein, the interested clustering factor and the confounding factor are coarsely considered in the raw feature space, where the correlation between the data and the confounding factor is ideally assumed to be linear for convenient solutions. These approaches are thus limited in scope as the data in real applications is usually complex and non-linearly correlated with the confounding factor. This paper presents a new clustering framework named Sanitized Clustering Against confounding Bias (SCAB), which removes the confounding factor in the semantic latent space of complex data through a non-linear dependence measure. To be specific, we eliminate the bias information in the latent space by minimizing the mutual information between the confounding factor and the latent representation delivered by Variational Auto-Encoder (VAE). Meanwhile, a clustering module is introduced to cluster over the purified latent representations. Extensive experiments on complex datasets demonstrate that our SCAB achieves a significant gain in clustering performance by removing the confounding bias. The code is available at \\url{https://github.com/EvaFlower/SCAB}.", "url": "https://arxiv.org/abs/2311.01252"}, {"metadata": {"arXiv": "2311.01276", "Date": "Thu, 02 Nov 2023 14:44:50 ", "Title": "Long-Range Neural Atom Learning for Molecular Graphs", "Authors": ["Xuan Li", "Zhanke Zhou", "Jiangchao Yao", "Yu Rong", "Lu Zhang", "Bo Han"], "Categories": "cs.LG q-bio.QM"}, "abstract": "Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs are mainly good at leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method that implicitly projects all original atoms into a few Neural Atoms, which abstracts the collective information of atomic groups within a molecule. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms' representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection with the traditional LRI calculation method, Ewald Summation. We conduct extensive experiments on three long-range graph benchmarks, covering both graph-level and link-level tasks on molecular graphs. We empirically justify that our method can be equipped with an arbitrary GNN and help to capture LRI.", "url": "https://arxiv.org/abs/2311.01276"}, {"metadata": {"arXiv": "2311.01282", "Date": "Thu, 02 Nov 2023 14:57:03 ", "Title": "FlashDecoding++: Faster Large Language Model Inference on GPUs", "Authors": ["Ke Hong", "Guohao Dai", "Jiaming Xu", "Qiuli Mao", "Xiuhong Li", "Jun Liu", "Kangdi Chen", "Hanyu Dong", "Yu Wang"], "Categories": "cs.LG cs.CL"}, "abstract": "As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference. We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++ creatively proposes: (1) Asynchronized softmax with unified max value. FlashDecoding++ introduces a unified max value technique for different partial softmax computations to avoid synchronization. (2) Flat GEMM optimization with double buffering. FlashDecoding++ points out that flat GEMMs with different shapes face varied bottlenecks. Then, techniques like double buffering are introduced. (3) Heuristic dataflow with hardware resource adaptation. FlashDecoding++ heuristically optimizes dataflow using different hardware resource considering input dynamics. Due to the versatility of optimizations in FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on both NVIDIA and AMD GPUs compared to Hugging Face implementations. FlashDecoding++ also achieves an average speedup of 1.37x compared to state-of-the-art LLM inference engines on mainstream LLMs.", "url": "https://arxiv.org/abs/2311.01282"}, {"metadata": {"arXiv": "2311.01295", "Date": "Thu, 02 Nov 2023 15:12:12 ", "Title": "DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning", "Authors": ["Wenxuan Bao", "Francesco Pittaluga", "Vijay Kumar B G", "Vincent Bindschaedler"], "Categories": "cs.LG cs.CR cs.CV", "Comments": ["17 pages", "2 figures", "to be published in Neural Information Processing Systems 2023"]}, "abstract": "Data augmentation techniques, such as simple image transformations and combinations, are highly effective at improving the generalization of computer vision models, especially when training data is limited. However, such techniques are fundamentally incompatible with differentially private learning approaches, due to the latter's built-in assumption that each training image's contribution to the learned model is bounded. In this paper, we investigate why naive applications of multi-sample data augmentation techniques, such as mixup, fail to achieve good performance and propose two novel data augmentation techniques specifically designed for the constraints of differentially private learning. Our first technique, DP-Mix_Self, achieves SoTA classification performance across a range of datasets and settings by performing mixup on self-augmented data. Our second technique, DP-Mix_Diff, further improves performance by incorporating synthetic data from a pre-trained diffusion model into the mixup process. We open-source the code at https://github.com/wenxuan-Bao/DP-Mix.", "url": "https://arxiv.org/abs/2311.01295"}, {"metadata": {"arXiv": "2311.01323", "Date": "Thu, 02 Nov 2023 15:35:58 ", "Title": "Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly", "Authors": ["Qizhang Li", "Yiwen Guo", "Wangmeng Zuo", "Hao Chen"], "Categories": "cs.LG cs.CR cs.CV", "Comments": ["Accepted by NeurIPS 2023"]}, "abstract": "The adversarial vulnerability of deep neural networks (DNNs) has drawn great attention due to the security risk of applying these models in real-world applications. Based on transferability of adversarial examples, an increasing number of transfer-based methods have been developed to fool black-box DNN models whose architecture and parameters are inaccessible. Although tremendous effort has been exerted, there still lacks a standardized benchmark that could be taken advantage of to compare these methods systematically, fairly, and practically. Our investigation shows that the evaluation of some methods needs to be more reasonable and more thorough to verify their effectiveness, to avoid, for example, unfair comparison and insufficient consideration of possible substitute/victim models. Therefore, we establish a transfer-based attack benchmark (TA-Bench) which implements 30+ methods. In this paper, we evaluate and compare them comprehensively on 25 popular substitute/victim models on ImageNet. New insights about the effectiveness of these methods are gained and guidelines for future evaluations are provided. Code at: https://github.com/qizhangli/TA-Bench.", "url": "https://arxiv.org/abs/2311.01323"}, {"metadata": {"arXiv": "2311.01327", "Date": "Thu, 02 Nov 2023 15:40:33 ", "Title": "High-dimensional Linear Bandits with Knapsacks", "Authors": ["Wanteng Ma", "Dong Xia and Jiashuo Jiang"], "Categories": "cs.LG cs.DS stat.ML"}, "abstract": "We study the contextual bandits with knapsack (CBwK) problem under the high-dimensional setting where the dimension of the feature is large. The reward of pulling each arm equals the multiplication of a sparse high-dimensional weight vector and the feature of the current arrival, with additional random noise. In this paper, we investigate how to exploit this sparsity structure to achieve improved regret for the CBwK problem. To this end, we first develop an online variant of the hard thresholding algorithm that performs the sparse estimation in an online manner. We further combine our online estimator with a primal-dual framework, where we assign a dual variable to each knapsack constraint and utilize an online learning algorithm to update the dual variable, thereby controlling the consumption of the knapsack capacity. We show that this integrated approach allows us to achieve a sublinear regret that depends logarithmically on the feature dimension, thus improving the polynomial dependency established in the previous literature. We also apply our framework to the high-dimension contextual bandit problem without the knapsack constraint and achieve optimal regret in both the data-poor regime and the data-rich regime. We finally conduct numerical experiments to show the efficient empirical performance of our algorithms under the high dimensional setting.", "url": "https://arxiv.org/abs/2311.01327"}, {"metadata": {"arXiv": "2311.01349", "Date": "Thu, 02 Nov 2023 15:59:00 ", "Title": "Unreading Race: Purging Protected Features from Chest X-ray Embeddings", "Authors": ["Tobias Weber", "Michael Ingrisch", "Bernd Bischl", "David R\\\"ugamer"], "Categories": "cs.LG cs.CY stat.ML"}, "abstract": "Purpose: To analyze and remove protected feature effects in chest radiograph embeddings of deep learning models. Materials and Methods: An orthogonalization is utilized to remove the influence of protected features (e.g., age, sex, race) in chest radiograph embeddings, ensuring feature-independent results. To validate the efficacy of the approach, we retrospectively study the MIMIC and CheXpert datasets using three pre-trained models, namely a supervised contrastive, a self-supervised contrastive, and a baseline classifier model. Our statistical analysis involves comparing the original versus the orthogonalized embeddings by estimating protected feature influences and evaluating the ability to predict race, age, or sex using the two types of embeddings. Results: Our experiments reveal a significant influence of protected features on predictions of pathologies. Applying orthogonalization removes these feature effects. Apart from removing any influence on pathology classification, while maintaining competitive predictive performance, orthogonalized embeddings further make it infeasible to directly predict protected attributes and mitigate subgroup disparities. Conclusion: The presented work demonstrates the successful application and evaluation of the orthogonalization technique in the domain of chest X-ray classification.", "url": "https://arxiv.org/abs/2311.01349"}, {"metadata": {"arXiv": "2311.01375", "Date": "Thu, 02 Nov 2023 16:33:35 ", "Title": "Monotone Generative Modeling via a Gromov-Monge Embedding", "Authors": ["Wonjun Lee", "Yifei Yang", "Dongmian Zou", "Gilad Lerman"], "Categories": "cs.LG cs.NA math.NA", "Comments": ["29 pages including main text and appendix"]}, "abstract": "Generative Adversarial Networks (GANs) are powerful tools for creating new content, but they face challenges such as sensitivity to starting conditions and mode collapse. To address these issues, we propose a deep generative model that utilizes the Gromov-Monge embedding (GME). It helps identify the low-dimensional structure of the underlying measure of the data and then maps it, while preserving its geometry, into a measure in a low-dimensional latent space, which is then optimally transported to the reference measure. We guarantee the preservation of the underlying geometry by the GME and $c$-cyclical monotonicity of the generative map, where $c$ is an intrinsic embedding cost employed by the GME. The latter property is a first step in guaranteeing better robustness to initialization of parameters and mode collapse. Numerical experiments demonstrate the effectiveness of our approach in generating high-quality images, avoiding mode collapse, and exhibiting robustness to different starting conditions.", "url": "https://arxiv.org/abs/2311.01375"}, {"metadata": {"arXiv": "2311.01409", "Date": "Thu, 02 Nov 2023 17:22:22 ", "Title": "A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference", "Authors": ["Mert Ketenci and Adler Perotte and No\\'emie Elhadad and I\\~nigo Urteaga"], "Categories": "cs.LG stat.ML"}, "abstract": "We present a novel stochastic variational Gaussian process ($\\mathcal{GP}$) inference method, based on a posterior over a learnable set of weighted pseudo input-output points (coresets). Instead of a free-form variational family, the proposed coreset-based, variational tempered family for $\\mathcal{GP}$s (CVTGP) is defined in terms of the $\\mathcal{GP}$ prior and the data-likelihood; hence, accommodating the modeling inductive biases. We derive CVTGP's lower bound for the log-marginal likelihood via marginalization of the proposed posterior over latent $\\mathcal{GP}$ coreset variables, and show it is amenable to stochastic optimization. CVTGP reduces the learnable parameter size to $\\mathcal{O}(M)$, enjoys numerical stability, and maintains $\\mathcal{O}(M^3)$ time- and $\\mathcal{O}(M^2)$ space-complexity, by leveraging a coreset-based tempered posterior that, in turn, provides sparse and explainable representations of the data. Results on simulated and real-world regression problems with Gaussian observation noise validate that CVTGP provides better evidence lower-bound estimates and predictive root mean squared error than alternative stochastic $\\mathcal{GP}$ inference methods.", "url": "https://arxiv.org/abs/2311.01409"}, {"metadata": {"arXiv": "2311.01420", "Date": "Thu, 02 Nov 2023 17:35:16 ", "Title": "Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data", "Authors": ["Cheng-Hao Tu", "Hong-You Chen", "Zheda Mai", "Jike Zhong", "Vardaan Pahuja", "Tanya Berger-Wolf", "Song Gao", "Charles Stewart", "Yu Su", "Wei-Lun Chao"], "Categories": "cs.LG", "Comments": ["Accepted to NeurIPS 2023 main track"]}, "abstract": "We propose a learning problem involving adapting a pre-trained source model to the target domain for classifying all classes that appeared in the source data, using target data that covers only a partial label space. This problem is practical, as it is unrealistic for the target end-users to collect data for all classes prior to adaptation. However, it has received limited attention in the literature. To shed light on this issue, we construct benchmark datasets and conduct extensive experiments to uncover the inherent challenges. We found a dilemma -- on the one hand, adapting to the new target domain is important to claim better performance; on the other hand, we observe that preserving the classification accuracy of classes missing in the target adaptation data is highly challenging, let alone improving them. To tackle this, we identify two key directions: 1) disentangling domain gradients from classification gradients, and 2) preserving class relationships. We present several effective solutions that maintain the accuracy of the missing classes and enhance the overall performance, establishing solid baselines for holistic transfer of pre-trained models with partial target data.", "url": "https://arxiv.org/abs/2311.01420"}, {"metadata": {"arXiv": "2311.01428", "Date": "Thu, 02 Nov 2023 17:44:28 ", "Title": "Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods", "Authors": ["Md Gulzar Hussain", "Ye Shiren"], "Categories": "cs.LG eess.IV"}, "abstract": "Dementia, a prevalent neurodegenerative condition, is a major manifestation of Alzheimer's disease (AD). As the condition progresses from mild to severe, it significantly impairs the individual's ability to perform daily tasks independently, necessitating the need for timely and accurate AD classification. Machine learning or deep learning models have emerged as effective tools for this purpose. In this study, we suggested an approach for classifying the four stages of dementia using RF, SVM, and CNN algorithms, augmented with watershed segmentation for feature extraction from MRI images. Our results reveal that SVM with watershed features achieves an impressive accuracy of 96.25%, surpassing other classification methods. The ADNI dataset is utilized to evaluate the effectiveness of our method, and we observed that the inclusion of watershed segmentation contributes to the enhanced performance of the models.", "url": "https://arxiv.org/abs/2311.01428"}, {"metadata": {"arXiv": "2311.01435", "Date": "Thu, 02 Nov 2023 17:51:10 ", "Title": "Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time", "Authors": ["Xinyuan Cao", "Santosh S. Vempala"], "Categories": "cs.LG math.PR stat.ML", "Comments": ["Preliminary version in NeurIPS 2023"]}, "abstract": "We give a polynomial-time algorithm for learning high-dimensional halfspaces with margins in $d$-dimensional space to within desired TV distance when the ambient distribution is an unknown affine transformation of the $d$-fold product of an (unknown) symmetric one-dimensional logconcave distribution, and the halfspace is introduced by deleting at least an $\\epsilon$ fraction of the data in one of the component distributions. Notably, our algorithm does not need labels and establishes the unique (and efficient) identifiability of the hidden halfspace under this distributional assumption. The sample and time complexity of the algorithm are polynomial in the dimension and $1/\\epsilon$. The algorithm uses only the first two moments of suitable re-weightings of the empirical distribution, which we call contrastive moments; its analysis uses classical facts about generalized Dirichlet polynomials and relies crucially on a new monotonicity property of the moment ratio of truncations of logconcave distributions. Such algorithms, based only on first and second moments were suggested in earlier work, but hitherto eluded rigorous guarantees. Prior work addressed the special case when the underlying distribution is Gaussian via Non-Gaussian Component Analysis. We improve on this by providing polytime guarantees based on Total Variation (TV) distance, in place of existing moment-bound guarantees that can be super-polynomial. Our work is also the first to go beyond Gaussians in this setting.", "url": "https://arxiv.org/abs/2311.01435"}, {"metadata": {"arXiv": "2311.01442", "Date": "Thu, 02 Nov 2023 17:55:41 ", "Title": "Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models", "Authors": ["Valentino Assandri", "Sam Heshmati", "Burhaneddin Yaman", "Anton Iakovlev", "Ariel Emiliano Repetur"], "Categories": "cs.LG"}, "abstract": "Deep learning models, particularly Transformers, have achieved impressive results in various domains, including time series forecasting. While existing time series literature primarily focuses on model architecture modifications and data augmentation techniques, this paper explores the training schema of deep learning models for time series; how models are trained regardless of their architecture. We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets. We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs. Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested. This suggests that many models in the literature may possess untapped potential. Additionally, we introduce a taxonomy for classifying training schema modifications, covering data augmentation, model inputs, model targets, time series per model, and computational budget.", "url": "https://arxiv.org/abs/2311.01442"}, {"metadata": {"arXiv": "2311.00802", "Date": "Wed, 01 Nov 2023 19:36:56 ", "Title": "Neural Field Dynamics Model for Granular Object Piles Manipulation", "Authors": ["Shangjie Xue", "Shuo Cheng", "Pujith Kachana and Danfei Xu"], "Categories": "cs.RO cs.LG"}, "abstract": "We present a learning-based dynamics model for granular material manipulation. Inspired by the Eulerian approach commonly used in fluid dynamics, our method adopts a fully convolutional neural network that operates on a density field-based representation of object piles and pushers, allowing it to exploit the spatial locality of inter-object interactions as well as the translation equivariance through convolution operations. Furthermore, our differentiable action rendering module makes the model fully differentiable and can be directly integrated with a gradient-based trajectory optimization algorithm. We evaluate our model with a wide array of piles manipulation tasks both in simulation and real-world experiments and demonstrate that it significantly exceeds existing latent or particle-based methods in both accuracy and computation efficiency, and exhibits zero-shot generalization capabilities across various environments and tasks.", "url": "https://arxiv.org/abs/2311.00802"}, {"metadata": {"arXiv": "2311.01059", "Date": "Thu, 02 Nov 2023 08:22:28 ", "Title": "Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment", "Authors": ["Annie S. Chen", "Govind Chada", "Laura Smith", "Archit Sharma", "Zipeng Fu", "Sergey Levine", "Chelsea Finn"], "Categories": "cs.RO cs.LG", "Comments": ["19 pages", "6 figures"]}, "abstract": "To succeed in the real world, robots must cope with situations that differ from those seen during training. We study the problem of adapting on-the-fly to such novel scenarios during deployment, by drawing upon a diverse repertoire of previously learned behaviors. Our approach, RObust Autonomous Modulation (ROAM), introduces a mechanism based on the perceived value of pre-trained behaviors to select and adapt pre-trained behaviors to the situation at hand. Crucially, this adaptation process all happens within a single episode at test time, without any human supervision. We provide theoretical analysis of our selection mechanism and demonstrate that ROAM enables a robot to adapt rapidly to changes in dynamics both in simulation and on a real Go1 quadruped, even successfully moving forward with roller skates on its feet. Our approach adapts over 2x as efficiently compared to existing methods when facing a variety of out-of-distribution situations during deployment by effectively choosing and adapting relevant behaviors on-the-fly.", "url": "https://arxiv.org/abs/2311.01059"}, {"metadata": {"arXiv": "2311.01394", "Date": "Thu, 02 Nov 2023 16:55:23 ", "Title": "Learning Realistic Traffic Agents in Closed-loop", "Authors": ["Chris Zhang", "James Tu", "Lunjun Zhang", "Kelvin Wong", "Simon Suo", "Raquel Urtasun"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["CORL 2023"]}, "abstract": "Realistic traffic simulation is crucial for developing self-driving software in a safe and scalable manner prior to real-world deployment. Typically, imitation learning (IL) is used to learn human-like traffic agents directly from real-world observations collected offline, but without explicit specification of traffic rules, agents trained from IL alone frequently display unrealistic infractions like collisions and driving off the road. This problem is exacerbated in out-of-distribution and long-tail scenarios. On the other hand, reinforcement learning (RL) can train traffic agents to avoid infractions, but using RL alone results in unhuman-like driving behaviors. We propose Reinforcing Traffic Rules (RTR), a holistic closed-loop learning objective to match expert demonstrations under a traffic compliance constraint, which naturally gives rise to a joint IL + RL approach, obtaining the best of both worlds. Our method learns in closed-loop simulations of both nominal scenarios from real-world datasets as well as procedurally generated long-tail scenarios. Our experiments show that RTR learns more realistic and generalizable traffic simulation policies, achieving significantly better tradeoffs between human-like driving and traffic compliance in both nominal and long-tail scenarios. Moreover, when used as a data generation tool for training prediction models, our learned traffic policy leads to considerably improved downstream prediction metrics compared to baseline traffic agents. For more information, visit the project website: https://waabi.ai/rtr", "url": "https://arxiv.org/abs/2311.01394"}, {"metadata": {"arXiv": "2311.01405", "Date": "Thu, 02 Nov 2023 17:19:18 ", "Title": "Learning to See Physical Properties with Active Sensing Motor Policies", "Authors": ["Gabriel B. Margolis", "Xiang Fu", "Yandong Ji", "Pulkit Agrawal"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["In CoRL 2023. Website: https://gmargo11.github.io/active-sensing-loco/"]}, "abstract": "Knowledge of terrain's physical properties inferred from color images can aid in making efficient robotic locomotion plans. However, unlike image classification, it is unintuitive for humans to label image patches with physical properties. Without labeled data, building a vision system that takes as input the observed terrain and predicts physical properties remains challenging. We present a method that overcomes this challenge by self-supervised labeling of images captured by robots during real-world traversal with physical property estimators trained in simulation. To ensure accurate labeling, we introduce Active Sensing Motor Policies (ASMP), which are trained to explore locomotion behaviors that increase the accuracy of estimating physical parameters. For instance, the quadruped robot learns to swipe its foot against the ground to estimate the friction coefficient accurately. We show that the visual system trained with a small amount of real-world traversal data accurately predicts physical parameters. The trained system is robust and works even with overhead images captured by a drone despite being trained on data collected by cameras attached to a quadruped robot walking on the ground.", "url": "https://arxiv.org/abs/2311.01405"}, {"metadata": {"arXiv": "2311.01446", "Date": "Thu, 02 Nov 2023 17:56:44 ", "Title": "Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation", "Authors": ["Jay Sarva", "Jingkang Wang", "James Tu", "Yuwen Xiong", "Sivabalan Manivasagam", "Raquel Urtasun"], "Categories": "cs.RO cs.CV cs.LG", "Comments": ["CoRL 2023. Project page: https://waabi.ai/adv3d/"]}, "abstract": "Self-driving vehicles (SDVs) must be rigorously tested on a wide range of scenarios to ensure safe deployment. The industry typically relies on closed-loop simulation to evaluate how the SDV interacts on a corpus of synthetic and real scenarios and verify it performs properly. However, they primarily only test the system's motion planning module, and only consider behavior variations. It is key to evaluate the full autonomy system in closed-loop, and to understand how variations in sensor data based on scene appearance, such as the shape of actors, affect system performance. In this paper, we propose a framework, Adv3D, that takes real world scenarios and performs closed-loop sensor simulation to evaluate autonomy performance, and finds vehicle shapes that make the scenario more challenging, resulting in autonomy failures and uncomfortable SDV maneuvers. Unlike prior works that add contrived adversarial shapes to vehicle roof-tops or roadside to harm perception only, we optimize a low-dimensional shape representation to modify the vehicle shape itself in a realistic manner to degrade autonomy performance (e.g., perception, prediction, and motion planning). Moreover, we find that the shape variations found with Adv3D optimized in closed-loop are much more effective than those in open-loop, demonstrating the importance of finding scene appearance variations that affect autonomy in the interactive setting.", "url": "https://arxiv.org/abs/2311.01446"}, {"metadata": {"arXiv": "2311.00738", "Date": "Wed, 01 Nov 2023 15:13:49 ", "Title": "Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?", "Authors": ["Yuwei Bao", "Keunwoo Peter Yu", "Yichi Zhang", "Shane Storks", "Itamar Bar-Yossef", "Alexander De La Iglesia", "Megan Su", "Xiao Lin Zheng", "Joyce Chai"], "Categories": "cs.AI cs.HC", "Comments": ["Accepted to EMNLP 2023 Findings"]}, "abstract": "Despite tremendous advances in AI, it remains a significant challenge to develop interactive task guidance systems that can offer situated, personalized guidance and assist humans in various tasks. These systems need to have a sophisticated understanding of the user as well as the environment, and make timely accurate decisions on when and what to say. To address this issue, we created a new multimodal benchmark dataset, Watch, Talk and Guide (WTaG) based on natural interaction between a human user and a human instructor. We further proposed two tasks: User and Environment Understanding, and Instructor Decision Making. We leveraged several foundation models to study to what extent these models can be quickly adapted to perceptually enabled task guidance. Our quantitative, qualitative, and human evaluation results show that these models can demonstrate fair performances in some cases with no task-specific training, but a fast and reliable adaptation remains a significant challenge. Our benchmark and baselines will provide a stepping stone for future work on situated task guidance.", "url": "https://arxiv.org/abs/2311.00738"}, {"metadata": {"arXiv": "2311.00767", "Date": "Wed, 01 Nov 2023 18:18:09 ", "Title": "Hand Gesture Classification on Praxis Dataset: Trading Accuracy for Expense", "Authors": ["Rahat Islam", "Kenneth Lai", "and Svetlana Yanushkevich"], "Categories": "cs.AI", "Comments": ["8 pages", "6 figures"], "Journal-ref": "2022 International Joint Conference on Neural Networks (IJCNN), Padua, pp. 1-8", "DOI": "10.1109/IJCNN55064.2022.9892631"}, "abstract": "In this paper, we investigate hand gesture classifiers that rely upon the abstracted 'skeletal' data recorded using the RGB-Depth sensor. We focus on 'skeletal' data represented by the body joint coordinates, from the Praxis dataset. The PRAXIS dataset contains recordings of patients with cortical pathologies such as Alzheimer's disease, performing a Praxis test under the direction of a clinician. In this paper, we propose hand gesture classifiers that are more effective with the PRAXIS dataset than previously proposed models. Body joint data offers a compressed form of data that can be analyzed specifically for hand gesture recognition. Using a combination of windowing techniques with deep learning architecture such as a Recurrent Neural Network (RNN), we achieved an overall accuracy of 70.8% using only body joint data. In addition, we investigated a long-short-term-memory (LSTM) to extract and analyze the movement of the joints through time to recognize the hand gestures being performed and achieved a gesture recognition rate of 74.3% and 67.3% for static and dynamic gestures, respectively. The proposed approach contributed to the task of developing an automated, accurate, and inexpensive approach to diagnosing cortical pathologies for multiple healthcare applications.", "url": "https://arxiv.org/abs/2311.00767"}, {"metadata": {"arXiv": "2311.00772", "Date": "Wed, 01 Nov 2023 18:36:28 ", "Title": "SAGE: Smart home Agent with Grounded Execution", "Authors": ["Dmitriy Rivkin", "Francois Hogan", "Amal Feriani", "Abhisek Konar", "Adam Sigal", "Steve Liu", "Greg Dudek"], "Categories": "cs.AI cs.HC cs.RO"}, "abstract": "This article introduces SAGE (Smart home Agent with Grounded Execution), a framework designed to maximize the flexibility of smart home assistants by replacing manually-defined inference logic with an LLM-powered autonomous agent system. SAGE integrates information about user preferences, device states, and external factors (such as weather and TV schedules) through the orchestration of a collection of tools. SAGE's capabilities include learning user preferences from natural-language utterances, interacting with devices by reading their API documentation, writing code to continuously monitor devices, and understanding natural device references. To evaluate SAGE, we develop a benchmark of 43 highly challenging smart home tasks, where SAGE successfully achieves 23 tasks, significantly outperforming existing LLM-enabled baselines (5/43).", "url": "https://arxiv.org/abs/2311.00772"}, {"metadata": {"arXiv": "2311.00855", "Date": "Wed, 01 Nov 2023 21:19:35 ", "Title": "A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan", "Authors": ["Dinesh Sharma", "Ankit Shah", "Chaitra Gopalappa"], "Categories": "cs.AI"}, "abstract": "Human immunodeficiency virus (HIV) is a major public health concern in the United States, with about 1.2 million people living with HIV and 35,000 newly infected each year. There are considerable geographical disparities in HIV burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE) initiative aims to reduce new infections by 90% by 2030, by improving coverage of diagnoses, treatment, and prevention interventions and prioritizing jurisdictions with high HIV prevalence. Identifying optimal scale-up of intervention combinations will help inform resource allocation. Existing HIV decision analytic models either evaluate specific cities or the overall national population, thus overlooking jurisdictional interactions or differences. In this paper, we propose a multi-agent reinforcement learning (MARL) model, that enables jurisdiction-specific decision analyses but in an environment with cross-jurisdictional epidemiological interactions. In experimental analyses, conducted on jurisdictions within California and Florida, optimal policies from MARL were significantly different than those generated from single-agent RL, highlighting the influence of jurisdictional variations and interactions. By using comprehensive modeling of HIV and formulations of state space, action space, and reward functions, this work helps demonstrate the strengths and applicability of MARL for informing public health policies, and provides a framework for expanding to the national-level to inform the EHE.", "url": "https://arxiv.org/abs/2311.00855"}, {"metadata": {"arXiv": "2311.01043", "Date": "Thu, 02 Nov 2023 07:23:33 ", "Title": "A Survey of Large Language Models for Autonomous Driving", "Authors": ["Zhenjie Yang", "Xiaosong Jia", "Hongyang Li", "Junchi Yan"], "Categories": "cs.AI", "Comments": ["GitHub Repo: https://github.com/Thinklab-SJTU/Awesome-LLM4AD"]}, "abstract": "Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their ``black box\" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about \\textit{Large Language Models for Autonomous Driving (LLM4AD)}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.", "url": "https://arxiv.org/abs/2311.01043"}, {"metadata": {"arXiv": "2311.01193", "Date": "Thu, 02 Nov 2023 12:39:22 ", "Title": "Contextual Confidence and Generative AI", "Authors": ["Shrey Jain", "Zo\\\"e Hitzig", "Pamela Mishkin"], "Categories": "cs.AI"}, "abstract": "Generative AI models perturb the foundations of effective human communication. They present new challenges to contextual confidence, disrupting participants' ability to identify the authentic context of communication and their ability to protect communication from reuse and recombination outside its intended context. In this paper, we describe strategies--tools, technologies and policies--that aim to stabilize communication in the face of these challenges. The strategies we discuss fall into two broad categories. Containment strategies aim to reassert context in environments where it is currently threatened--a reaction to the context-free expectations and norms established by the internet. Mobilization strategies, by contrast, view the rise of generative AI as an opportunity to proactively set new and higher expectations around privacy and authenticity in mediated communication.", "url": "https://arxiv.org/abs/2311.01193"}, {"metadata": {"arXiv": "2311.01258", "Date": "Thu, 02 Nov 2023 14:18:43 ", "Title": "Formal Methods for Autonomous Systems", "Authors": ["Tichakorn Wongpiromsarn", "Mahsa Ghasemi", "Murat Cubuktepe", "Georgios Bakirtzis", "Steven Carr", "Mustafa O. Karabag", "Cyrus Neary", "Parham Gohari", "Ufuk Topcu"], "Categories": "cs.AI cs.LO cs.SY eess.SY", "DOI": "10.1561/2600000029"}, "abstract": "Formal methods refer to rigorous, mathematical approaches to system development and have played a key role in establishing the correctness of safety-critical systems. The main building blocks of formal methods are models and specifications, which are analogous to behaviors and requirements in system design and give us the means to verify and synthesize system behaviors with formal guarantees. This monograph provides a survey of the current state of the art on applications of formal methods in the autonomous systems domain. We consider correct-by-construction synthesis under various formulations, including closed systems, reactive, and probabilistic settings. Beyond synthesizing systems in known environments, we address the concept of uncertainty and bound the behavior of systems that employ learning using formal methods. Further, we examine the synthesis of systems with monitoring, a mitigation technique for ensuring that once a system deviates from expected behavior, it knows a way of returning to normalcy. We also show how to overcome some limitations of formal methods themselves with learning. We conclude with future directions for formal methods in reinforcement learning, uncertainty, privacy, explainability of formal methods, and regulation and certification.", "url": "https://arxiv.org/abs/2311.01258"}, {"metadata": {"arXiv": "2311.00729", "Date": "Wed, 01 Nov 2023 00:17:37 ", "Title": "ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection", "Authors": ["Thinh Phan", "Khoa Vo", "Duy Le", "Gianfranco Doretto", "Donald Adjeroh", "Ngan Le"], "Categories": "cs.CV cs.AI"}, "abstract": "Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising of open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationships between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance discriminative capability on unseen classes by minimally updating the frozen CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and ActivityNet-1.3 datasets demonstrate our approach's superior performance in zero-shot TAD and effective knowledge transfer from ViL models to unseen action categories.", "url": "https://arxiv.org/abs/2311.00729"}, {"metadata": {"arXiv": "2311.00800", "Date": "Wed, 01 Nov 2023 19:34:45 ", "Title": "Beyond Still Images: Robust Multi-Stream Spatiotemporal Networks", "Authors": ["AmirHosein Fadaei", "Mohammad-Reza A. Dehaqani"], "Categories": "cs.CV cs.AI", "Comments": ["12 pages", "9 figures"], "ACM-class": "I.2.10; I.5.1; I.4.8"}, "abstract": "A defining characteristic of natural vision is its ability to withstand a variety of input alterations, resulting in the creation of an invariant representation of the surroundings. While convolutional neural networks exhibit resilience to certain forms of spatial input variation, modifications in the spatial and temporal aspects can significantly affect the representations of video content in deep neural networks. Inspired by the resilience of natural vision to input variations, we employ a simple multi-stream model to explore its potential to address spatiotemporal changes by including temporal features. Our primary goal is to introduce a video-trained model and evaluate its robustness to diverse image and video inputs, with a particular focus on exploring the role of temporal features in invariant recognition. Results show that including videos and the temporal stream during training mitigates the decline in accuracy and mAP in image and video understanding tasks by 1.36% and 3.14%, respectively.", "url": "https://arxiv.org/abs/2311.00800"}, {"metadata": {"arXiv": "2311.01001", "Date": "Thu, 02 Nov 2023 05:35:49 ", "Title": "Fully Quantized Always-on Face Detector Considering Mobile Image Sensors", "Authors": ["Haechang Lee", "Wongi Jeong", "Dongil Ryu", "Hyunwoo Je", "Albert No", "Kijeong Kim", "Se Young Chun"], "Categories": "cs.CV cs.AI", "Comments": ["Accepted to ICCV 2023 Workshop on Low-Bit Quantized Neural Networks (LBQNN)", "Oral"]}, "abstract": "Despite significant research on lightweight deep neural networks (DNNs) designed for edge devices, the current face detectors do not fully meet the requirements for \"intelligent\" CMOS image sensors (iCISs) integrated with embedded DNNs. These sensors are essential in various practical applications, such as energy-efficient mobile phones and surveillance systems with always-on capabilities. One noteworthy limitation is the absence of suitable face detectors for the always-on scenario, a crucial aspect of image sensor-level applications. These detectors must operate directly with sensor RAW data before the image signal processor (ISP) takes over. This gap poses a significant challenge in achieving optimal performance in such scenarios. Further research and development are necessary to bridge this gap and fully leverage the potential of iCIS applications. In this study, we aim to bridge the gap by exploring extremely low-bit lightweight face detectors, focusing on the always-on face detection scenario for mobile image sensor applications. To achieve this, our proposed model utilizes sensor-aware synthetic RAW inputs, simulating always-on face detection processed \"before\" the ISP chain. Our approach employs ternary (-1, 0, 1) weights for potential implementations in image sensors, resulting in a relatively simple network architecture with shallow layers and extremely low-bitwidth. Our method demonstrates reasonable face detection performance and excellent efficiency in simulation studies, offering promising possibilities for practical always-on face detectors in real-world applications.", "url": "https://arxiv.org/abs/2311.01001"}, {"metadata": {"arXiv": "2311.01004", "Date": "Thu, 02 Nov 2023 05:44:13 ", "Title": "Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning", "Authors": ["Gaoang Wang", "Zhenyu Zhang", "Benlu Wang", "Weijie Liang", "Yizhi Li", "Xuechen Guo", "Guanhong Wang", "Shiyan Li"], "Categories": "cs.CV cs.AI"}, "abstract": "With the development of multimodality and large language models, the deep learning-based technique for medical image captioning holds the potential to offer valuable diagnostic recommendations. However, current generic text and image pre-trained models do not yield satisfactory results when it comes to describing intricate details within medical images. In this paper, we present a novel medical image captioning method guided by the segment anything model (SAM) to enable enhanced encoding with both general and detailed feature extraction. In addition, our approach employs a distinctive pre-training strategy with mixed semantic learning to simultaneously capture both the overall information and finer details within medical images. We demonstrate the effectiveness of this approach, as it outperforms the pre-trained BLIP2 model on various evaluation metrics for generating descriptions of medical images.", "url": "https://arxiv.org/abs/2311.01004"}, {"metadata": {"arXiv": "2311.01009", "Date": "Thu, 02 Nov 2023 06:08:49 ", "Title": "Revamping AI Models in Dermatology: Overcoming Critical Challenges for Enhanced Skin Lesion Diagnosis", "Authors": ["Deval Mehta", "Brigid Betz-Stablein", "Toan D Nguyen", "Yaniv Gal", "Adrian Bowling", "Martin Haskett", "Maithili Sashindranath", "Paul Bonnington", "Victoria Mar", "H Peter Soyer", "Zongyuan Ge"], "Categories": "cs.CV cs.AI"}, "abstract": "The surge in developing deep learning models for diagnosing skin lesions through image analysis is notable, yet their clinical black faces challenges. Current dermatology AI models have limitations: limited number of possible diagnostic outputs, lack of real-world testing on uncommon skin lesions, inability to detect out-of-distribution images, and over-reliance on dermoscopic images. To address these, we present an All-In-One \\textbf{H}ierarchical-\\textbf{O}ut of Distribution-\\textbf{C}linical Triage (HOT) model. For a clinical image, our model generates three outputs: a hierarchical prediction, an alert for out-of-distribution images, and a recommendation for dermoscopy if clinical image alone is insufficient for diagnosis. When the recommendation is pursued, it integrates both clinical and dermoscopic images to deliver final diagnosis. Extensive experiments on a representative cutaneous lesion dataset demonstrate the effectiveness and synergy of each component within our framework. Our versatile model provides valuable decision support for lesion diagnosis and sets a promising precedent for medical AI applications.", "url": "https://arxiv.org/abs/2311.01009"}, {"metadata": {"arXiv": "2311.01022", "Date": "Thu, 02 Nov 2023 06:29:53 ", "Title": "NeuroWrite: Predictive Handwritten Digit Classification using Deep Neural Networks", "Authors": ["Kottakota Asish", "P. Sarath Teja", "R. Kishan Chander", "Dr. D. Deva Hema"], "Categories": "cs.CV cs.AI", "Comments": ["6 pages", "10 figures"], "MSC-class": "68T10, 68T45, 68T60", "ACM-class": "I.4.8; I.5.2; J.4"}, "abstract": "The rapid evolution of deep neural networks has revolutionized the field of machine learning, enabling remarkable advancements in various domains. In this article, we introduce NeuroWrite, a unique method for predicting the categorization of handwritten digits using deep neural networks. Our model exhibits outstanding accuracy in identifying and categorising handwritten digits by utilising the strength of convolutional neural networks (CNNs) and recurrent neural networks (RNNs).In this article, we give a thorough examination of the data preparation methods, network design, and training methods used in NeuroWrite. By implementing state-of-the-art techniques, we showcase how NeuroWrite can achieve high classification accuracy and robust generalization on handwritten digit datasets, such as MNIST. Furthermore, we explore the model's potential for real-world applications, including digit recognition in digitized documents, signature verification, and automated postal code recognition. NeuroWrite is a useful tool for computer vision and pattern recognition because of its performance and adaptability.The architecture, training procedure, and evaluation metrics of NeuroWrite are covered in detail in this study, illustrating how it can improve a number of applications that call for handwritten digit classification. The outcomes show that NeuroWrite is a promising method for raising the bar for deep neural network-based handwritten digit recognition.", "url": "https://arxiv.org/abs/2311.01022"}, {"metadata": {"arXiv": "2311.01023", "Date": "Thu, 02 Nov 2023 06:31:08 ", "Title": "Augmentation is AUtO-Net: Augmentation-Driven Contrastive Multiview Learning for Medical Image Segmentation", "Authors": ["Yanming Guo"], "Categories": "cs.CV cs.AI"}, "abstract": "The utilisation of deep learning segmentation algorithms that learn complex organs and tissue patterns and extract essential regions of interest from the noisy background to improve the visual ability for medical image diagnosis has achieved impressive results in Medical Image Computing (MIC). This thesis focuses on retinal blood vessel segmentation tasks, providing an extensive literature review of deep learning-based medical image segmentation approaches while comparing the methodologies and empirical performances. The work also examines the limitations of current state-of-the-art methods by pointing out the two significant existing limitations: data size constraints and the dependency on high computational resources. To address such problems, this work proposes a novel efficient, simple multiview learning framework that contrastively learns invariant vessel feature representation by comparing with multiple augmented views by various transformations to overcome data shortage and improve generalisation ability. Moreover, the hybrid network architecture integrates the attention mechanism into a Convolutional Neural Network to further capture complex continuous curvilinear vessel structures. The result demonstrates the proposed method validated on the CHASE-DB1 dataset, attaining the highest F1 score of 83.46% and the highest Intersection over Union (IOU) score of 71.62% with UNet structure, surpassing existing benchmark UNet-based methods by 1.95% and 2.8%, respectively. The combination of the metrics indicates the model detects the vessel object accurately with a highly coincidental location with the ground truth. Moreover, the proposed approach could be trained within 30 minutes by consuming less than 3 GB GPU RAM, and such characteristics support the efficient implementation for real-world applications and deployments.", "url": "https://arxiv.org/abs/2311.01023"}, {"metadata": {"arXiv": "2311.01057", "Date": "Thu, 02 Nov 2023 08:01:49 ", "Title": "Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO", "Authors": ["Julian Moosmann", "Pietro Bonazzi", "Yawei Li", "Sizhen Bian", "Philipp Mayer", "Luca Benini", "Michele Magno"], "Categories": "cs.CV cs.AI cs.RO"}, "abstract": "Smart glasses are rapidly gaining advanced functionality thanks to cutting-edge computing technologies, accelerated hardware architectures, and tiny AI algorithms. Integrating AI into smart glasses featuring a small form factor and limited battery capacity is still challenging when targeting full-day usage for a satisfactory user experience. This paper illustrates the design and implementation of tiny machine-learning algorithms exploiting novel low-power processors to enable prolonged continuous operation in smart glasses. We explore the energy- and latency-efficient of smart glasses in the case of real-time object detection. To this goal, we designed a smart glasses prototype as a research platform featuring two microcontrollers, including a novel milliwatt-power RISC-V parallel processor with a hardware accelerator for visual AI, and a Bluetooth low-power module for communication. The smart glasses integrate power cycling mechanisms, including image and audio sensing interfaces. Furthermore, we developed a family of novel tiny deep-learning models based on YOLO with sub-million parameters customized for microcontroller-based inference dubbed TinyissimoYOLO v1.3, v5, and v8, aiming at benchmarking object detection with smart glasses for energy and latency. Evaluations on the prototype of the smart glasses demonstrate TinyissimoYOLO's 17ms inference latency and 1.59mJ energy consumption per inference while ensuring acceptable detection accuracy. Further evaluation reveals an end-to-end latency from image capturing to the algorithm's prediction of 56ms or equivalently 18 fps, with a total power consumption of 62.9mW, equivalent to a 9.3 hours of continuous run time on a 154mAh battery. These results outperform MCUNet (TinyNAS+TinyEngine), which runs a simpler task (image classification) at just 7.3 fps per second.", "url": "https://arxiv.org/abs/2311.01057"}, {"metadata": {"arXiv": "2311.01185", "Date": "Thu, 02 Nov 2023 12:32:25 ", "Title": "Revolutionizing Healthcare Image Analysis in Pandemic-Based Fog-Cloud Computing Architectures", "Authors": ["Al Zahraa Elsayed", "Khalil Mohamed", "Hany Harb"], "Categories": "cs.CV cs.AI"}, "abstract": "The emergence of pandemics has significantly emphasized the need for effective solutions in healthcare data analysis. One particular challenge in this domain is the manual examination of medical images, such as X-rays and CT scans. This process is time-consuming and involves the logistical complexities of transferring these images to centralized cloud computing servers. Additionally, the speed and accuracy of image analysis are vital for efficient healthcare image management. This research paper introduces an innovative healthcare architecture that tackles the challenges of analysis efficiency and accuracy by harnessing the capabilities of Artificial Intelligence (AI). Specifically, the proposed architecture utilizes fog computing and presents a modified Convolutional Neural Network (CNN) designed specifically for image analysis. Different architectures of CNN layers are thoroughly explored and evaluated to optimize overall performance. To demonstrate the effectiveness of the proposed approach, a dataset of X-ray images is utilized for analysis and evaluation. Comparative assessments are conducted against recent models such as VGG16, VGG19, MobileNet, and related research papers. Notably, the proposed approach achieves an exceptional accuracy rate of 99.88% in classifying normal cases, accompanied by a validation rate of 96.5%, precision and recall rates of 100%, and an F1 score of 100%. These results highlight the immense potential of fog computing and modified CNNs in revolutionizing healthcare image analysis and diagnosis, not only during pandemics but also in the future. By leveraging these technologies, healthcare professionals can enhance the efficiency and accuracy of medical image analysis, leading to improved patient care and outcomes.", "url": "https://arxiv.org/abs/2311.01185"}, {"metadata": {"arXiv": "2311.01197", "Date": "Thu, 02 Nov 2023 12:48:43 ", "Title": "AiluRus: A Scalable ViT Framework for Dense Prediction", "Authors": ["Jin Li", "Yaoming Wang", "Xiaopeng Zhang", "Bowen Shi", "Dongsheng Jiang", "Chenglin Li", "Wenrui Dai", "Hongkai Xiong", "Qi Tian"], "Categories": "cs.CV cs.AI", "Comments": ["Accepted by NeurIPS 2023"]}, "abstract": "Vision transformers (ViTs) have emerged as a prevalent architecture for vision tasks owing to their impressive performance. However, when it comes to handling long token sequences, especially in dense prediction tasks that require high-resolution input, the complexity of ViTs increases significantly. Notably, dense prediction tasks, such as semantic segmentation or object detection, emphasize more on the contours or shapes of objects, while the texture inside objects is less informative. Motivated by this observation, we propose to apply adaptive resolution for different regions in the image according to their importance. Specifically, at the intermediate layer of the ViT, we utilize a spatial-aware density-based clustering algorithm to select representative tokens from the token sequence. Once the representative tokens are determined, we proceed to merge other tokens into their closest representative token. Consequently, semantic similar tokens are merged together to form low-resolution regions, while semantic irrelevant tokens are preserved independently as high-resolution regions. This strategy effectively reduces the number of tokens, allowing subsequent layers to handle a reduced token sequence and achieve acceleration. We evaluate our proposed method on three different datasets and observe promising performance. For example, the \"Segmenter ViT-L\" model can be accelerated by 48% FPS without fine-tuning, while maintaining the performance. Additionally, our method can be applied to accelerate fine-tuning as well. Experimental results demonstrate that we can save 52% training time while accelerating 2.46 times FPS with only a 0.09% performance drop. The code is available at https://github.com/caddyless/ailurus/tree/main.", "url": "https://arxiv.org/abs/2311.01197"}, {"metadata": {"arXiv": "2311.01202", "Date": "Thu, 02 Nov 2023 12:56:47 ", "Title": "Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration", "Authors": ["Yifan Xie", "Jihua Zhu", "Shiqi Li and Pengcheng Shi"], "Categories": "cs.CV cs.AI", "Comments": ["8 pages", "accepted by RAL 2023"]}, "abstract": "The majority of point cloud registration methods currently rely on extracting features from points. However, these methods are limited by their dependence on information obtained from a single modality of points, which can result in deficiencies such as inadequate perception of global features and a lack of texture information. Actually, humans can employ visual information learned from 2D images to comprehend the 3D world. Based on this fact, we present a novel Cross-Modal Information-Guided Network (CMIGNet), which obtains global shape perception through cross-modal information to achieve precise and robust point cloud registration. Specifically, we first incorporate the projected images from the point clouds and fuse the cross-modal features using the attention mechanism. Furthermore, we employ two contrastive learning strategies, namely overlapping contrastive learning and cross-modal contrastive learning. The former focuses on features in overlapping regions, while the latter emphasizes the correspondences between 2D and 3D features. Finally, we propose a mask prediction module to identify keypoints in the point clouds. Extensive experiments on several benchmark datasets demonstrate that our network achieves superior registration performance.", "url": "https://arxiv.org/abs/2311.01202"}, {"metadata": {"arXiv": "2311.01212", "Date": "Thu, 02 Nov 2023 13:06:03 ", "Title": "Multi-view Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification", "Authors": ["Chun Liu", "Longwei Yang", "Zheng Li", "Wei Yang", "Zhigang Han", "Jianzhong Guo", "Junyong Yu"], "Categories": "cs.CV cs.AI"}, "abstract": "Cross-domain few-shot hyperspectral image classification focuses on learning prior knowledge from a large number of labeled samples from source domain and then transferring the knowledge to the tasks which contain only few labeled samples in target domains. Following the metric-based manner, many current methods first extract the features of the query and support samples, and then directly predict the classes of query samples according to their distance to the support samples or prototypes. The relations between samples have not been fully explored and utilized. Different from current works, this paper proposes to learn sample relations from different views and take them into the model learning process, to improve the cross-domain few-shot hyperspectral image classification. Building on current DCFSL method which adopts a domain discriminator to deal with domain-level distribution difference, the proposed method applys contrastive learning to learn the class-level sample relations to obtain more discriminable sample features. In addition, it adopts a transformer based cross-attention learning module to learn the set-level sample relations and acquire the attentions from query samples to support samples. Our experimental results have demonstrated the contribution of the multi-view relation learning mechanism for few-shot hyperspectral image classification when compared with the state of the art methods.", "url": "https://arxiv.org/abs/2311.01212"}, {"metadata": {"arXiv": "2311.01233", "Date": "Thu, 02 Nov 2023 13:36:11 ", "Title": "Long Story Short: a Summarize-then-Search Method for Long Video Question Answering", "Authors": ["Jiwan Chung", "Youngjae Yu"], "Categories": "cs.CV cs.AI", "Comments": ["Published in BMVC 2023"]}, "abstract": "Large language models such as GPT-3 have demonstrated an impressive capability to adapt to new tasks without requiring task-specific training data. This capability has been particularly effective in settings such as narrative question answering, where the diversity of tasks is immense, but the available supervision data is small. In this work, we investigate if such language models can extend their zero-shot reasoning abilities to long multimodal narratives in multimedia content such as drama, movies, and animation, where the story plays an essential role. We propose Long Story Short, a framework for narrative video QA that first summarizes the narrative of the video to a short plot and then searches parts of the video relevant to the question. We also propose to enhance visual matching with CLIPCheck. Our model outperforms state-of-the-art supervised models by a large margin, highlighting the potential of zero-shot QA for long videos.", "url": "https://arxiv.org/abs/2311.01233"}, {"metadata": {"arXiv": "2311.01240", "Date": "Thu, 02 Nov 2023 13:57:43 ", "Title": "FacadeNet: Conditional Facade Synthesis via Selective Editing", "Authors": ["Yiangos Georgiou and Marios Loizou and Tom Kelly and Melinos Averkiou"], "Categories": "cs.CV cs.AI"}, "abstract": "We introduce FacadeNet, a deep learning approach for synthesizing building facade images from diverse viewpoints. Our method employs a conditional GAN, taking a single view of a facade along with the desired viewpoint information and generates an image of the facade from the distinct viewpoint. To precisely modify view-dependent elements like windows and doors while preserving the structure of view-independent components such as walls, we introduce a selective editing module. This module leverages image embeddings extracted from a pre-trained vision transformer. Our experiments demonstrated state-of-the-art performance on building facade generation, surpassing alternative methods.", "url": "https://arxiv.org/abs/2311.01240"}, {"metadata": {"arXiv": "2311.01373", "Date": "Thu, 02 Nov 2023 16:31:49 ", "Title": "Recognize Any Regions", "Authors": ["Haosen Yang", "Chuofan Ma", "Bin Wen", "Yi Jiang", "Zehuan Yuan", "Xiatian Zhu"], "Categories": "cs.CV cs.AI"}, "abstract": "Understanding the semantics of individual regions or patches within unconstrained images, such as in open-world object detection, represents a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, and deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient region recognition architecture, named RegionSpot, designed to integrate position-aware localization knowledge from a localization foundation model (e.g., SAM) with semantic information extracted from a ViL model (e.g., CLIP). To fully exploit pretrained knowledge while minimizing training overhead, we keep both foundation models frozen, focusing optimization efforts solely on a lightweight attention-based knowledge integration module. Through extensive experiments in the context of open-world object recognition, our RegionSpot demonstrates significant performance improvements over prior alternatives, while also providing substantial computational savings. For instance, training our model with 3 million data in a single day using 8 V100 GPUs. Our model outperforms GLIP by 6.5 % in mean average precision (mAP), with an even larger margin by 14.8 % for more challenging and rare categories.", "url": "https://arxiv.org/abs/2311.01373"}, {"metadata": {"arXiv": "2311.00837", "Date": "Wed, 01 Nov 2023 20:40:10 ", "Title": "Constant-time Motion Planning with Anytime Refinement for Manipulation", "Authors": ["Itamar Mishani", "Hayden Feddock", "Maxim Likhachev"], "Categories": "cs.RO cs.AI", "Comments": ["Under review for publication at the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024). Copyright may be transferred without notice", "after which this version may no longer be accessible"]}, "abstract": "Robotic manipulators are essential for future autonomous systems, yet limited trust in their autonomy has confined them to rigid, task-specific systems. The intricate configuration space of manipulators, coupled with the challenges of obstacle avoidance and constraint satisfaction, often makes motion planning the bottleneck for achieving reliable and adaptable autonomy. Recently, a class of constant-time motion planners (CTMP) was introduced. These planners employ a preprocessing phase to compute data structures that enable online planning provably guarantee the ability to generate motion plans, potentially sub-optimal, within a user defined time bound. This framework has been demonstrated to be effective in a number of time-critical tasks. However, robotic systems often have more time allotted for planning than the online portion of CTMP requires, time that can be used to improve the solution. To this end, we propose an anytime refinement approach that works in combination with CTMP algorithms. Our proposed framework, as it operates as a constant time algorithm, rapidly generates an initial solution within a user-defined time threshold. Furthermore, functioning as an anytime algorithm, it iteratively refines the solution's quality within the allocated time budget. This enables our approach to strike a balance between guaranteed fast plan generation and the pursuit of optimization over time. We support our approach by elucidating its analytical properties, showing the convergence of the anytime component towards optimal solutions. Additionally, we provide empirical validation through simulation and real-world demonstrations on a 6 degree-of-freedom robot manipulator, applied to an assembly domain.", "url": "https://arxiv.org/abs/2311.00837"}, {"metadata": {"arXiv": "2311.00924", "Date": "Thu, 02 Nov 2023 01:33:00 ", "Title": "The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning", "Authors": ["Carmelo Sferrazza", "Younggyo Seo", "Hao Liu", "Youngwoon Lee", "Pieter Abbeel"], "Categories": "cs.RO cs.AI"}, "abstract": "Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We evaluate M3L on three simulated environments with both visual and tactile observations: robotic insertion, door opening, and dexterous in-hand manipulation, demonstrating the benefits of learning a multimodal policy. Code and videos of the experiments are available at https://sferrazza.cc/m3l_site.", "url": "https://arxiv.org/abs/2311.00924"}, {"metadata": {"arXiv": "2311.00926", "Date": "Thu, 02 Nov 2023 01:42:52 ", "Title": "M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place", "Authors": ["Wentao Yuan", "Adithyavairavan Murali", "Arsalan Mousavian", "Dieter Fox"], "Categories": "cs.RO cs.AI cs.CV", "Comments": ["12 pages", "8 figures", "accepted by CoRL 2023"]}, "abstract": "With the advent of large language models and large-scale robotic datasets, there has been tremendous progress in high-level decision-making for object manipulation. These generic models are able to interpret complex tasks using language commands, but they often have difficulties generalizing to out-of-distribution objects due to the inability of low-level action primitives. In contrast, existing task-specific models excel in low-level manipulation of unknown objects, but only work for a single type of action. To bridge this gap, we present M2T2, a single model that supplies different types of low-level actions that work robustly on arbitrary objects in cluttered scenes. M2T2 is a transformer model which reasons about contact points and predicts valid gripper poses for different action modes given a raw point cloud of the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2 achieves zero-shot sim2real transfer on the real robot, outperforming the baseline system with state-of-the-art task-specific models by about 19% in overall performance and 37.5% in challenging scenes where the object needs to be re-oriented for collision-free placement. M2T2 also achieves state-of-the-art results on a subset of language conditioned tasks in RLBench. Videos of robot experiments on unseen objects in both real world and simulation are available on our project website https://m2-t2.github.io.", "url": "https://arxiv.org/abs/2311.00926"}, {"metadata": {"arXiv": "2311.00967", "Date": "Thu, 02 Nov 2023 03:32:30 ", "Title": "Vision-Language Interpreter for Robot Task Planning", "Authors": ["Keisuke Shirai", "Cristian C. Beltran-Hernandez", "Masashi Hamaya", "Atsushi Hashimoto", "Shohei Tanaka", "Kento Kawaharazuka", "Kazutoshi Tanaka", "Yoshitaka Ushiku", "Shinsuke Mori"], "Categories": "cs.RO cs.AI cs.CL"}, "abstract": "Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated with four new evaluation metrics. Experimental results show that ViLaIn can generate syntactically correct problems with more than 99% accuracy and valid plans with more than 58% accuracy.", "url": "https://arxiv.org/abs/2311.00967"}, {"metadata": {"arXiv": "2311.01107", "Date": "Thu, 02 Nov 2023 09:16:15 ", "Title": "GREEMA: Proposal and Experimental Verification of Growing Robot by Eating Environmental MAterial for Landslide Disaster", "Authors": ["Yusuke Tsunoda", "Yuya Sato", "and Koichi Osuka"], "Categories": "cs.RO cs.AI"}, "abstract": "In areas that are inaccessible to humans, such as the lunar surface and landslide sites, there is a need for multiple autonomous mobile robot systems that can replace human workers. In particular, at landslide sites such as river channel blockages, robots are required to remove water and sediment from the site as soon as possible. Conventionally, several construction machines have been deployed to the site for civil engineering work. However, because of the large size and weight of conventional construction equipment, it is difficult to move multiple units of construction equipment to the site, resulting in significant transportation costs and time. To solve such problems, this study proposes a novel growing robot by eating environmental material called GREEMA, which is lightweight and compact during transportation, but can function by eating on environmental materials once it arrives at the site. GREEMA actively takes in environmental materials such as water and sediment, uses them as its structure, and removes them by moving itself. In this paper, we developed and experimentally verified two types of GREEMAs. First, we developed a fin-type swimming robot that passively takes water into its body using a water-absorbing polymer and forms a body to express its swimming function. Second, we constructed an arm-type robot that eats soil to increase the rigidity of its body. We discuss the results of these two experiments from the viewpoint of Explicit-Implicit control and describe the design theory of GREEMA.", "url": "https://arxiv.org/abs/2311.01107"}, {"metadata": {"arXiv": "2311.01267", "Date": "Thu, 02 Nov 2023 14:25:10 ", "Title": "UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding", "Authors": ["Han Xue", "Yutong Li", "Wenqiang Xu", "Huanyu Li", "Dongzhe Zheng", "Cewu Lu"], "Categories": "cs.RO cs.AI cs.CV", "Comments": ["CoRL 2023"]}, "abstract": "This paper explores the development of UniFolding, a sample-efficient, scalable, and generalizable robotic system for unfolding and folding various garments. UniFolding employs the proposed UFONet neural network to integrate unfolding and folding decisions into a single policy model that is adaptable to different garment types and states. The design of UniFolding is based on a garment's partial point cloud, which aids in generalization and reduces sensitivity to variations in texture and shape. The training pipeline prioritizes low-cost, sample-efficient data collection. Training data is collected via a human-centric process with offline and online stages. The offline stage involves human unfolding and folding actions via Virtual Reality, while the online stage utilizes human-in-the-loop learning to fine-tune the model in a real-world setting. The system is tested on two garment types: long-sleeve and short-sleeve shirts. Performance is evaluated on 20 shirts with significant variations in textures, shapes, and materials. More experiments and videos can be found in the supplementary materials and on the website: https://unifolding.robotflow.ai", "url": "https://arxiv.org/abs/2311.01267"}, {"metadata": {"arXiv": "2311.01454", "Date": "Thu, 02 Nov 2023 17:59:06 ", "Title": "NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities", "Authors": ["Ruohan Zhang", "Sharon Lee", "Minjune Hwang", "Ayano Hiranaka", "Chen Wang", "Wensi Ai", "Jin Jie Ryan Tan", "Shreya Gupta", "Yilun Hao", "Gabrael Levine", "Ruohan Gao", "Anthony Norcia", "Li Fei-Fei", "Jiajun Wu"], "Categories": "cs.RO cs.AI"}, "abstract": "We present Neural Signal Operated Intelligent Robots (NOIR), a general-purpose, intelligent brain-robot interface system that enables humans to command robots to perform everyday activities through brain signals. Through this interface, humans communicate their intended objects of interest and actions to the robots using electroencephalography (EEG). Our novel system demonstrates success in an expansive array of 20 challenging, everyday household activities, including cooking, cleaning, personal care, and entertainment. The effectiveness of the system is improved by its synergistic integration of robot learning algorithms, allowing for NOIR to adapt to individual users and predict their intentions. Our work enhances the way humans interact with robots, replacing traditional channels of interaction with direct, neural communication. Project website: https://noir-corl.github.io/.", "url": "https://arxiv.org/abs/2311.01454"}, {"metadata": {"arXiv": "2311.01457", "Date": "Thu, 02 Nov 2023 17:59:30 ", "Title": "Conformal Policy Learning for Sensorimotor Control Under Distribution Shifts", "Authors": ["Huang Huang", "Satvik Sharma", "Antonio Loquercio", "Anastasios Angelopoulos", "Ken Goldberg", "Jitendra Malik"], "Categories": "cs.RO cs.AI", "Comments": ["Conformal Policy Learning"]}, "abstract": "This paper focuses on the problem of detecting and reacting to changes in the distribution of a sensorimotor controller's observables. The key idea is the design of switching policies that can take conformal quantiles as input, which we define as conformal policy learning, that allows robots to detect distribution shifts with formal statistical guarantees. We show how to design such policies by using conformal quantiles to switch between base policies with different characteristics, e.g. safety or speed, or directly augmenting a policy observation with a quantile and training it with reinforcement learning. Theoretically, we show that such policies achieve the formal convergence guarantees in finite time. In addition, we thoroughly evaluate their advantages and limitations on two compelling use cases: simulated autonomous driving and active perception with a physical quadruped. Empirical results demonstrate that our approach outperforms five baselines. It is also the simplest of the baseline strategies besides one ablation. Being easy to use, flexible, and with formal guarantees, our work demonstrates how conformal prediction can be an effective tool for sensorimotor learning under uncertainty.", "url": "https://arxiv.org/abs/2311.01457"}, {"metadata": {"arXiv": "2311.00750", "Date": "Wed, 01 Nov 2023 18:00:03 ", "Title": "Are These the Same Apple? Comparing Images Based on Object Intrinsics", "Authors": ["Klemen Kotar", "Stephen Tian", "Hong-Xing Yu", "Daniel L.K. Yamins", "Jiajun Wu"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["First two authors contributed equally. Accepted at NeurIPS Datasets and Benchmarks Track 2023"]}, "abstract": "The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of $18,000$ images of $180$ objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics well, we find that combining deep features learned from contrastive self-supervised learning with foreground filtering is a simple yet effective approach to approximating the similarity. We conduct an extensive survey of pre-trained features and foreground extraction methods to arrive at a strong baseline that best measures intrinsic object-centric image similarity among current methods. Finally, we demonstrate that our approach can aid in downstream applications such as acting as an analog for human subjects and improving generalizable re-identification. Please see our project website at https://s-tian.github.io/projects/cute/ for visualizations of the data and demos of our metric.", "url": "https://arxiv.org/abs/2311.00750"}, {"metadata": {"arXiv": "2311.01017", "Date": "Thu, 02 Nov 2023 06:21:56 ", "Title": "Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion", "Authors": ["Lunjun Zhang", "Yuwen Xiong", "Ze Yang", "Sergio Casas", "Rui Hu", "Raquel Urtasun"], "Categories": "cs.CV cs.AI cs.LG cs.RO"}, "abstract": "Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, and more than 50% for 3s prediction, across NuScenes, KITTI Odometry, and Argoverse2 datasets. Our results demonstrate that discrete diffusion on tokenized agent experience can unlock the power of GPT-like unsupervised learning for robotic agents.", "url": "https://arxiv.org/abs/2311.01017"}, {"metadata": {"arXiv": "2311.01310", "Date": "Thu, 02 Nov 2023 15:24:23 ", "Title": "Scattering Vision Transformer: Spectral Mixing Matters", "Authors": ["Badri N. Patro and Vijay Srinivas Agneeswaran"], "Categories": "cs.CV cs.AI cs.LG eess.IV eess.SP", "Comments": ["Accepted @NeurIPS 2023,"]}, "abstract": "Vision transformers have gained significant attention and achieved state-of-the-art performance in various computer vision tasks, including image classification, instance segmentation, and object detection. However, challenges remain in addressing attention complexity and effectively capturing fine-grained information within images. Existing solutions often resort to down-sampling operations, such as pooling, to reduce computational cost. Unfortunately, such operations are non-invertible and can result in information loss. In this paper, we present a novel approach called Scattering Vision Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally scattering network that enables the capture of intricate image details. SVT overcomes the invertibility issue associated with down-sampling operations by separating low-frequency and high-frequency components. Furthermore, SVT introduces a unique spectral gating network utilizing Einstein multiplication for token and channel mixing, effectively reducing complexity. We show that SVT achieves state-of-the-art performance on the ImageNet dataset with a significant reduction in a number of parameters and FLOPS. SVT shows 2\\% improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\\% top-1 accuracy, while SVT-H-B reaches 85.2\\% (state-of-art for base versions) and SVT-H-L reaches 85.7\\% (again state-of-art for large versions). SVT also shows comparable results in other vision tasks such as instance segmentation. SVT also outperforms other transformers in transfer learning on standard datasets such as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. The project page is available on this webpage.\\url{https://badripatro.github.io/svt/}.", "url": "https://arxiv.org/abs/2311.01310"}, {"metadata": {"arXiv": "2311.00724", "Date": "Tue, 31 Oct 2023 05:47:35 ", "Title": "Fraud Analytics Using Machine-learning & Engineering on Big Data (FAME) for Telecom", "Authors": ["Sudarson Roy Pratihar", "Subhadip Paul", "Pranab Kumar Dash", "Amartya Kumar Das"], "Categories": "cs.LG cs.AI cs.DC", "Comments": ["Presented in International Conference in Indian Institute of Management", "Bangalore", "India"]}, "abstract": "Telecom industries lose globally 46.3 Billion USD due to fraud. Data mining and machine learning techniques (apart from rules oriented approach) have been used in past, but efficiency has been low as fraud pattern changes very rapidly. This paper presents an industrialized solution approach with self adaptive data mining technique and application of big data technologies to detect fraud and discover novel fraud patterns in accurate, efficient and cost effective manner. Solution has been successfully demonstrated to detect International Revenue Share Fraud with <5% false positive. More than 1 Terra Bytes of Call Detail Record from a reputed wholesale carrier and overseas telecom transit carrier has been used to conduct this study.", "url": "https://arxiv.org/abs/2311.00724"}, {"metadata": {"arXiv": "2311.00727", "Date": "Tue, 31 Oct 2023 12:52:00 ", "Title": "Investigating Relative Performance of Transfer and Meta Learning", "Authors": ["Benji Alwis"], "Categories": "cs.LG cs.AI"}, "abstract": "Over the past decade, the field of machine learning has experienced remarkable advancements. While image recognition systems have achieved impressive levels of accuracy, they continue to rely on extensive training datasets. Additionally, a significant challenge has emerged in the form of poor out-of-distribution performance, which necessitates retraining neural networks when they encounter conditions that deviate from their training data. This limitation has notably contributed to the slow progress in self-driving car technology. These pressing issues have sparked considerable interest in methods that enable neural networks to learn effectively from limited data. This paper presents the outcomes of an extensive investigation designed to compare two distinct approaches, transfer learning and meta learning, as potential solutions to this problem. The overarching objective was to establish a robust criterion for selecting the most suitable method in diverse machine learning scenarios. Building upon prior research, I expanded the comparative analysis by introducing a new meta learning method into the investigation. Subsequently, I assessed whether the findings remained consistent under varying conditions. Finally, I delved into the impact of altering the size of the training dataset on the relative performance of these methods. This comprehensive exploration has yielded insights into the conditions favoring each approach, thereby facilitating the development of a criterion for selecting the most appropriate method in any given situation", "url": "https://arxiv.org/abs/2311.00727"}, {"metadata": {"arXiv": "2311.00797", "Date": "Wed, 01 Nov 2023 19:33:03 ", "Title": "Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling", "Authors": ["Nikolaos Evangelou", "Tianqi Cui", "Juan M. Bello-Rivas", "Alexei Makeev", "Ioannis G. Kevrekidis"], "Categories": "cs.LG cs.AI math.DS q-bio.PE", "Comments": ["23 pages", "13 figures"]}, "abstract": "We study the tipping point collective dynamics of an adaptive susceptible-infected-susceptible (SIS) epidemiological network in a data-driven, machine learning-assisted manner. We identify a parameter-dependent effective stochastic differential equation (eSDE) in terms of physically meaningful coarse mean-field variables through a deep-learning ResNet architecture inspired by numerical stochastic integrators. We construct an approximate effective bifurcation diagram based on the identified drift term of the eSDE and contrast it with the mean-field SIS model bifurcation diagram. We observe a subcritical Hopf bifurcation in the evolving network's effective SIS dynamics, that causes the tipping point behavior; this takes the form of large amplitude collective oscillations that spontaneously -- yet rarely -- arise from the neighborhood of a (noisy) stationary state. We study the statistics of these rare events both through repeated brute force simulations and by using established mathematical/computational tools exploiting the right-hand-side of the identified SDE. We demonstrate that such a collective SDE can also be identified (and the rare events computations also performed) in terms of data-driven coarse observables, obtained here via manifold learning techniques, in particular Diffusion Maps. The workflow of our study is straightforwardly applicable to other complex dynamics problems exhibiting tipping point dynamics.", "url": "https://arxiv.org/abs/2311.00797"}, {"metadata": {"arXiv": "2311.00859", "Date": "Wed, 01 Nov 2023 21:28:02 ", "Title": "Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems", "Authors": ["Ziqing Lu", "Guanlin Liu", "Lifeng Cai", "Weiyu Xu"], "Categories": "cs.LG cs.AI cs.CR cs.MA", "Comments": ["Submitted to ICCASP2024"]}, "abstract": "Finding optimal adversarial attack strategies is an important topic in reinforcement learning and the Markov decision process. Previous studies usually assume one all-knowing coordinator (attacker) for whom attacking different recipient (victim) agents incurs uniform costs. However, in reality, instead of using one limitless central attacker, the attacks often need to be performed by distributed attack agents. We formulate the problem of performing optimal adversarial agent-to-agent attacks using distributed attack agents, in which we impose distinct cost constraints on each different attacker-victim pair. We propose an optimal method integrating within-step static constrained attack-resource allocation optimization and between-step dynamic programming to achieve the optimal adversarial attack in a multi-agent system. Our numerical results show that the proposed attacks can significantly reduce the rewards received by the attacked agents.", "url": "https://arxiv.org/abs/2311.00859"}, {"metadata": {"arXiv": "2311.00860", "Date": "Wed, 01 Nov 2023 21:28:24 ", "Title": "Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning", "Authors": ["Kuangdai Leng", "Mallikarjun Shankar", "Jeyan Thiyagalingam"], "Categories": "cs.LG cs.AI cs.NA math.NA physics.comp-ph", "Comments": ["19 pages"]}, "abstract": "Automatic differentiation (AD) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates. In this paper, we present a novel and lightweight algorithm to conduct such AD for physics-informed operator learning, as we call the trick of Zero Coordinate Shift (ZCS). Instead of making all sampled coordinates leaf variables, ZCS introduces only one scalar-valued leaf variable for each spatial or temporal dimension, leading to a game-changing performance leap by simplifying the wanted derivatives from \"many-roots-many-leaves\" to \"one-root-many-leaves\". ZCS is easy to implement with current deep learning libraries; our own implementation is by extending the DeepXDE package. We carry out a comprehensive benchmark analysis and several case studies, training physics-informed DeepONets to solve partial differential equations (PDEs) without data. The results show that ZCS has persistently brought down GPU memory consumption and wall time for training by an order of magnitude, with the savings increasing with problem scale (i.e., number of functions, number of points and order of PDE). As a low-level optimisation, ZCS entails no restrictions on data, physics (PDEs) or network architecture and does not compromise training results from any aspect.", "url": "https://arxiv.org/abs/2311.00860"}, {"metadata": {"arXiv": "2311.00863", "Date": "Wed, 01 Nov 2023 21:32:51 ", "Title": "Training Dynamics of Contextual N-Grams in Language Models", "Authors": ["Lucia Quirke", "Lovis Heindrich", "Wes Gurnee", "Neel Nanda"], "Categories": "cs.LG cs.AI cs.CL", "Comments": ["Accepted workshop paper at ATTRIB 2023 (@ NeurIPS)"]}, "abstract": "Prior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually rather than in a sudden phase transition. We further present a range of anomalous observations such as a simultaneous phase transition in many tasks coinciding with the learning rate warm-up, and evidence that many context neurons form simultaneously early in training but are later unlearned.", "url": "https://arxiv.org/abs/2311.00863"}, {"metadata": {"arXiv": "2311.00865", "Date": "Wed, 01 Nov 2023 21:35:32 ", "Title": "Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning", "Authors": ["Matthias Gerstgrasser", "Tom Danino", "Sarah Keren"], "Categories": "cs.LG cs.AI cs.MA cs.RO", "Comments": ["to be published at NeurIPS 2023"]}, "abstract": "We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation of our algorithm is available at https://github.com/mgerstgrasser/super.", "url": "https://arxiv.org/abs/2311.00865"}, {"metadata": {"arXiv": "2311.00880", "Date": "Wed, 01 Nov 2023 22:12:50 ", "Title": "SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization", "Authors": ["Jaafar Mhamed and Shangding Gu"], "Categories": "cs.LG cs.AI"}, "abstract": "Incorporating safety is an essential prerequisite for broadening the practical applications of reinforcement learning in real-world scenarios. To tackle this challenge, Constrained Markov Decision Processes (CMDPs) are leveraged, which introduce a distinct cost function representing safety violations. In CMDPs' settings, Lagrangian relaxation technique has been employed in previous algorithms to convert constrained optimization problems into unconstrained dual problems. However, these algorithms may inaccurately predict unsafe behavior, resulting in instability while learning the Lagrange multiplier. This study introduces a novel safe reinforcement learning algorithm, Safety Critic Policy Optimization (SCPO). In this study, we define the safety critic, a mechanism that nullifies rewards obtained through violating safety constraints. Furthermore, our theoretical analysis indicates that the proposed algorithm can automatically balance the trade-off between adhering to safety constraints and maximizing rewards. The effectiveness of the SCPO algorithm is empirically validated by benchmarking it against strong baselines.", "url": "https://arxiv.org/abs/2311.00880"}, {"metadata": {"arXiv": "2311.00938", "Date": "Thu, 02 Nov 2023 02:03:12 ", "Title": "Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance", "Authors": ["Niket Patel", "Luis Salamanca", "Luis Barba"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Accepted at NeurIPS Diffusion Workshop 2023"]}, "abstract": "Diffusion models have emerged as a pivotal advancement in generative models, setting new standards to the quality of the generated instances. In the current paper we aim to underscore a discrepancy between conventional training methods and the desired conditional sampling behavior of these models. While the prevalent classifier-free guidance technique works well, it's not without flaws. At higher values for the guidance scale parameter $w$, we often get out of distribution samples and mode collapse, whereas at lower values for $w$ we may not get the desired specificity. To address these challenges, we introduce an updated loss function that better aligns training objectives with sampling behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our method's ability to produce higher quality samples with fewer sampling timesteps, and be more robust to the choice of guidance scale $w$. We also experiment with fine-tuning Stable Diffusion on the proposed loss, to provide early evidence that large diffusion models may also benefit from this refined loss function.", "url": "https://arxiv.org/abs/2311.00938"}, {"metadata": {"arXiv": "2311.00941", "Date": "Thu, 02 Nov 2023 02:05:38 ", "Title": "Gaussian Mixture Solvers for Diffusion Models", "Authors": ["Hanzhong Guo", "Cheng Lu", "Fan Bao", "Tianyu Pang", "Shuicheng Yan", "Chao Du", "Chongxuan Li"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["NeurIPS 2023"]}, "abstract": "Recently, diffusion models have achieved great success in generative tasks. Sampling from diffusion models is equivalent to solving the reverse diffusion stochastic differential equations (SDEs) or the corresponding probability flow ordinary differential equations (ODEs). In comparison, SDE-based solvers can generate samples of higher quality and are suited for image translation tasks like stroke-based synthesis. During inference, however, existing SDE-based solvers are severely constrained by the efficiency-effectiveness dilemma. Our investigation suggests that this is because the Gaussian assumption in the reverse transition kernel is frequently violated (even in the case of simple mixture data) given a limited number of discretization steps. To overcome this limitation, we introduce a novel class of SDE-based solvers called \\emph{Gaussian Mixture Solvers (GMS)} for diffusion models. Our solver estimates the first three-order moments and optimizes the parameters of a Gaussian mixture transition kernel using generalized methods of moments in each step during sampling. Empirically, our solver outperforms numerous SDE-based solvers in terms of sample quality in image generation and stroke-based synthesis in various diffusion models, which validates the motivation and effectiveness of GMS. Our code is available at https://github.com/Guohanzhong/GMS.", "url": "https://arxiv.org/abs/2311.00941"}, {"metadata": {"arXiv": "2311.00971", "Date": "Thu, 02 Nov 2023 03:39:14 ", "Title": "An Integrated Framework Integrating Monte Carlo Tree Search and Supervised Learning for Train Timetabling Problem", "Authors": ["Feiyu Yang"], "Categories": "cs.LG cs.AI cs.CE"}, "abstract": "The single-track railway train timetabling problem (TTP) is an important and complex problem. This article proposes an integrated Monte Carlo Tree Search (MCTS) computing framework that combines heuristic methods, unsupervised learning methods, and supervised learning methods for solving TTP in discrete action spaces. This article first describes the mathematical model and simulation system dynamics of TTP, analyzes the characteristics of the solution from the perspective of MCTS, and proposes some heuristic methods to improve MCTS. This article considers these methods as planners in the proposed framework. Secondly, this article utilizes deep convolutional neural networks to approximate the value of nodes and further applies them to the MCTS search process, referred to as learners. The experiment shows that the proposed heuristic MCTS method is beneficial for solving TTP; The algorithm framework that integrates planners and learners can improve the data efficiency of solving TTP; The proposed method provides a new paradigm for solving TTP.", "url": "https://arxiv.org/abs/2311.00971"}, {"metadata": {"arXiv": "2311.00983", "Date": "Thu, 02 Nov 2023 04:05:28 ", "Title": "Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks", "Authors": ["MD Shafikul Islam and Azmine Toushik Wasi"], "Categories": "cs.LG cs.AI math.OC", "Comments": ["3 Pages", "2 figures", "New in ML Workshop at NeurIPS 2023. Openreview forum: https://openreview.net/forum?id=r0fzjB8f7f&"]}, "abstract": "Inventory Routing Problem (IRP) is a crucial challenge in supply chain management as it involves optimizing efficient route selection while considering the uncertainty of inventory demand planning. To solve IRPs, usually a two-stage approach is employed, where demand is predicted using machine learning techniques first, and then an optimization algorithm is used to minimize routing costs. Our experiment shows machine learning models fall short of achieving perfect accuracy because inventory levels are influenced by the dynamic business environment, which, in turn, affects the optimization problem in the next stage, resulting in sub-optimal decisions. In this paper, we formulate and propose a decision-focused learning-based approach to solving real-world IRPs. This approach directly integrates inventory prediction and routing optimization within an end-to-end system potentially ensuring a robust supply chain strategy.", "url": "https://arxiv.org/abs/2311.00983"}, {"metadata": {"arXiv": "2311.01002", "Date": "Thu, 02 Nov 2023 05:40:26 ", "Title": "Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy", "Authors": ["Dongmin Park", "Seola Choi", "Doyoung Kim", "Hwanjun Song", "Jae-Gil Lee"], "Categories": "cs.LG cs.AI"}, "abstract": "Data pruning, which aims to downsize a large training set into a small informative subset, is crucial for reducing the enormous computational costs of modern deep learning. Though large-scale data collections invariably contain annotation noise and numerous robust learning methods have been developed, data pruning for the noise-robust learning scenario has received little attention. With state-of-the-art Re-labeling methods that self-correct erroneous labels while training, it is challenging to identify which subset induces the most accurate re-labeling of erroneous labels in the entire training set. In this paper, we formalize the problem of data pruning with re-labeling. We first show that the likelihood of a training example being correctly re-labeled is proportional to the prediction confidence of its neighborhood in the subset. Therefore, we propose a novel data pruning algorithm, Prune4Rel, that finds a subset maximizing the total neighborhood confidence of all training examples, thereby maximizing the re-labeling accuracy and generalization performance. Extensive experiments on four real and one synthetic noisy datasets show that \\algname{} outperforms the baselines with Re-labeling models by up to 9.1% as well as those with a standard model by up to 21.6%.", "url": "https://arxiv.org/abs/2311.01002"}, {"metadata": {"arXiv": "2311.01007", "Date": "Thu, 02 Nov 2023 06:00:48 ", "Title": "Effective Human-AI Teams via Learned Natural Language Rules and Onboarding", "Authors": ["Hussein Mozannar", "Jimin J Lee", "Dennis Wei", "Prasanna Sattigeri", "Subhro Das", "David Sontag"], "Categories": "cs.LG cs.AI cs.HC", "Comments": ["NeurIPS 2023 Spotlight"]}, "abstract": "People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately.", "url": "https://arxiv.org/abs/2311.01007"}, {"metadata": {"arXiv": "2311.01024", "Date": "Thu, 02 Nov 2023 06:37:46 ", "Title": "Distance-Based Propagation for Efficient Knowledge Graph Reasoning", "Authors": ["Harry Shomer", "Yao Ma", "Juanhui Li", "Bo Wu", "Charu C. Aggarwal", "Jiliang Tang"], "Categories": "cs.LG cs.AI"}, "abstract": "Knowledge graph completion (KGC) aims to predict unseen edges in knowledge graphs (KGs), resulting in the discovery of new facts. A new class of methods have been proposed to tackle this problem by aggregating path information. These methods have shown tremendous ability in the task of KGC. However they are plagued by efficiency issues. Though there are a few recent attempts to address this through learnable path pruning, they often sacrifice the performance to gain efficiency. In this work, we identify two intrinsic limitations of these methods that affect the efficiency and representation quality. To address the limitations, we introduce a new method, TAGNet, which is able to efficiently propagate information. This is achieved by only aggregating paths in a fixed window for each source-target pair. We demonstrate that the complexity of TAGNet is independent of the number of layers. Extensive experiments demonstrate that TAGNet can cut down on the number of propagated messages by as much as 90% while achieving competitive performance on multiple KG datasets. The code is available at https://github.com/HarryShomer/TAGNet.", "url": "https://arxiv.org/abs/2311.01024"}, {"metadata": {"arXiv": "2311.01033", "Date": "Thu, 02 Nov 2023 06:52:44 ", "Title": "Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction", "Authors": ["Wang-Tao Zhou", "Zhao Kang", "Ling Tian"], "Categories": "cs.LG cs.AI cs.SI"}, "abstract": "Continuous-time long-term event prediction plays an important role in many application scenarios. Most existing works rely on autoregressive frameworks to predict event sequences, which suffer from error accumulation, thus compromising prediction quality. Inspired by the success of denoising diffusion probabilistic models, we propose a diffusion-based non-autoregressive temporal point process model for long-term event prediction in continuous time. Instead of generating events one at a time in an autoregressive way, our model predicts the future event sequence entirely as a whole. In order to perform diffusion processes on event sequences, we develop a bidirectional map between target event sequences and the Euclidean vector space. Furthermore, we design a novel denoising network to capture both sequential and contextual features for better sample quality. Extensive experiments are conducted to prove the superiority of our proposed model over state-of-the-art methods on long-term event prediction in continuous time. To the best of our knowledge, this is the first work to apply diffusion methods to long-term event prediction problems.", "url": "https://arxiv.org/abs/2311.01033"}, {"metadata": {"arXiv": "2311.01191", "Date": "Thu, 02 Nov 2023 12:36:19 ", "Title": "VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification", "Authors": ["Yulan Hu", "Sheng Ouyang", "Zhirui Yang", "Yong Liu"], "Categories": "cs.LG cs.AI"}, "abstract": "Class imbalance in graph data poses significant challenges for node classification. Existing methods, represented by SMOTE-based approaches, partially alleviate this issue but still exhibit limitations during imbalanced scenario construction. Self-supervised learning (SSL) offers a promising solution by synthesizing minority nodes from the data itself, yet its potential remains unexplored. In this paper, we analyze the limitations of SMOTE-based approaches and introduce VIGraph, a novel SSL model based on the self-supervised Variational Graph Auto-Encoder (VGAE) that leverages Variational Inference (VI) to generate minority nodes. Specifically, VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and utilizes the generative VGAE to generate minority nodes. Moreover, VIGraph introduces a novel Siamese contrastive strategy at the decoding phase to improve the overall quality of generated nodes. VIGraph can generate high-quality nodes without reintegrating them into the original graph, eliminating the \"Generating, Reintegrating, and Retraining\" process found in SMOTE-based methods. Experiments on multiple real-world datasets demonstrate that VIGraph achieves promising results for class-imbalanced node classification tasks.", "url": "https://arxiv.org/abs/2311.01191"}, {"metadata": {"arXiv": "2311.01195", "Date": "Thu, 02 Nov 2023 12:46:03 ", "Title": "Batch Bayesian Optimization for Replicable Experimental Design", "Authors": ["Zhongxiang Dai", "Quoc Phong Nguyen", "Sebastian Shenghong Tay", "Daisuke Urano", "Richalynn Leong", "Bryan Kian Hsiang Low", "Patrick Jaillet"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted to NeurIPS 2023"]}, "abstract": "Many real-world experimental design problems (a) evaluate multiple experimental conditions in parallel and (b) replicate each condition multiple times due to large and heteroscedastic observation noise. Given a fixed total budget, this naturally induces a trade-off between evaluating more unique conditions while replicating each of them fewer times vs. evaluating fewer unique conditions and replicating each more times. Moreover, in these problems, practitioners may be risk-averse and hence prefer an input with both good average performance and small variability. To tackle both challenges, we propose the Batch Thompson Sampling for Replicable Experimental Design (BTS-RED) framework, which encompasses three algorithms. Our BTS-RED-Known and BTS-RED-Unknown algorithms, for, respectively, known and unknown noise variance, choose the number of replications adaptively rather than deterministically such that an input with a larger noise variance is replicated more times. As a result, despite the noise heteroscedasticity, both algorithms enjoy a theoretical guarantee and are asymptotically no-regret. Our Mean-Var-BTS-RED algorithm aims at risk-averse optimization and is also asymptotically no-regret. We also show the effectiveness of our algorithms in two practical real-world applications: precision agriculture and AutoML.", "url": "https://arxiv.org/abs/2311.01195"}, {"metadata": {"arXiv": "2311.01201", "Date": "Thu, 02 Nov 2023 12:55:26 ", "Title": "Federated Learning on Edge Sensing Devices: A Review", "Authors": ["Berrenur Saylam", "\\\"Ozlem Durmaz \\.Incel"], "Categories": "cs.LG cs.AI cs.HC"}, "abstract": "The ability to monitor ambient characteristics, interact with them, and derive information about the surroundings has been made possible by the rapid proliferation of edge sensing devices like IoT, mobile, and wearable devices and their measuring capabilities with integrated sensors. Even though these devices are small and have less capacity for data storage and processing, they produce vast amounts of data. Some example application areas where sensor data is collected and processed include healthcare, environmental (including air quality and pollution levels), automotive, industrial, aerospace, and agricultural applications. These enormous volumes of sensing data collected from the edge devices are analyzed using a variety of Machine Learning (ML) and Deep Learning (DL) approaches. However, analyzing them on the cloud or a server presents challenges related to privacy, hardware, and connectivity limitations. Federated Learning (FL) is emerging as a solution to these problems while preserving privacy by jointly training a model without sharing raw data. In this paper, we review the FL strategies from the perspective of edge sensing devices to get over the limitations of conventional machine learning techniques. We focus on the key FL principles, software frameworks, and testbeds. We also explore the current sensor technologies, properties of the sensing devices and sensing applications where FL is utilized. We conclude with a discussion on open issues and future research directions on FL for further studies", "url": "https://arxiv.org/abs/2311.01201"}, {"metadata": {"arXiv": "2311.01205", "Date": "Thu, 02 Nov 2023 12:59:32 ", "Title": "Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent", "Authors": ["Lorenz Kummer", "Samir Moustafa", "Nils N. Kriege", "Wilfried N. Gansterer"], "Categories": "cs.LG cs.AI cs.CR cs.NE"}, "abstract": "Prior attacks on graph neural networks have mostly focused on graph poisoning and evasion, neglecting the network's weights and biases. Traditional weight-based fault injection attacks, such as bit flip attacks used for convolutional neural networks, do not consider the unique properties of graph neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest that exploiting mathematical properties specific to certain graph neural network architectures can significantly increase their vulnerability to bit flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive Graph Isomorphism Networks trained on various graph property prediction datasets to random output by flipping only a small fraction of the network's bits, demonstrating its higher destructive power compared to a bit flip attack transferred from convolutional neural networks. Our attack is transparent and motivated by theoretical insights which are confirmed by extensive empirical results.", "url": "https://arxiv.org/abs/2311.01205"}, {"metadata": {"arXiv": "2311.01223", "Date": "Thu, 02 Nov 2023 13:23:39 ", "Title": "Diffusion Models for Reinforcement Learning: A Survey", "Authors": ["Zhengbang Zhu", "Hanye Zhao", "Haoran He", "Yichao Zhong", "Shenyu Zhang", "Yong Yu", "Weinan Zhang"], "Categories": "cs.LG cs.AI", "Comments": ["16 pages", "2 figures", "1 table"]}, "abstract": "Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion models to broader tasks. We are actively maintaining a GitHub repository for papers and other related resources in applying diffusion models in RL: https://github.com/apexrl/Diff4RLSurvey .", "url": "https://arxiv.org/abs/2311.01223"}, {"metadata": {"arXiv": "2311.01230", "Date": "Thu, 02 Nov 2023 13:33:07 ", "Title": "Multi-Operational Mathematical Derivations in Latent Space", "Authors": ["Marco Valentino", "Jordan Meadows", "Lan Zhang", "Andr\\'e Freitas"], "Categories": "cs.LG cs.AI cs.SC"}, "abstract": "This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders. Specifically, we investigate how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusions for a single operation is achievable in the original expression encoder. Moreover, we show that architectural choices can heavily affect the training dynamics, structural organisation, and generalisation of the latent space, resulting in significant variations across paradigms and classes of encoders.", "url": "https://arxiv.org/abs/2311.01230"}, {"metadata": {"arXiv": "2311.01301", "Date": "Thu, 02 Nov 2023 15:15:47 ", "Title": "TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models", "Authors": ["Javier Gonz\\'alez", "Cliff Wong", "Zelalem Gero", "Jass Bagga", "Risa Ueno", "Isabel Chien", "Eduard Orakvin", "Emre Kiciman", "Aditya Nori", "Roshanthi Weerasinghe", "Rom S. Leidner", "Brian Piening", "Tristan Naumann", "Carlo Bifulco", "Hoifung Poon"], "Categories": "cs.LG cs.AI stat.ME", "Comments": ["6 Figures", "22 Pages", "3 Tables"]}, "abstract": "The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million cancer patients from a large US healthcare network, we show that TRIALSCOPE can produce high-quality structuring of real-world data and generates comparable results to marquee cancer trials. In addition to facilitating in-silicon clinical trial design and optimization, TRIALSCOPE may be used to empower synthetic controls, pragmatic trials, post-market surveillance, as well as support fine-grained patient-like-me reasoning in precision diagnosis and treatment.", "url": "https://arxiv.org/abs/2311.01301"}, {"metadata": {"arXiv": "2311.01305", "Date": "Thu, 02 Nov 2023 15:18:22 ", "Title": "AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models", "Authors": ["Baisong Li and Xingwang Wang and Haixiao Xu"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing post-training quantization methods for large models.", "url": "https://arxiv.org/abs/2311.01305"}, {"metadata": {"arXiv": "2311.01329", "Date": "Thu, 02 Nov 2023 15:41:09 ", "Title": "A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories", "Authors": ["Kai Yan", "Alexander G. Schwing", "Yu-Xiong Wang"], "Categories": "cs.LG cs.AI", "Comments": ["35 pages; Accepted as a poster for NeurIPS2023"]}, "abstract": "Offline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art \"DIstribution Correction Estimation\" (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectories or segments of expert behavior in the task-agnostic data, a common assumption in prior work. In experiments across multiple testbeds, we find TAILO to be more robust and effective, particularly with incomplete trajectories.", "url": "https://arxiv.org/abs/2311.01329"}, {"metadata": {"arXiv": "2311.01331", "Date": "Thu, 02 Nov 2023 15:41:57 ", "Title": "Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching", "Authors": ["Kai Yan", "Alexander G. Schwing", "Yu-xiong Wang"], "Categories": "cs.LG cs.AI", "Comments": ["23 pages. Accepted to the Optimal Transport and Machine Learning Workshop at NeurIPS 2023"]}, "abstract": "In real-world scenarios, arbitrary interactions with the environment can often be costly, and actions of expert demonstrations are not always available. To reduce the need for both, Offline Learning from Observations (LfO) is extensively studied, where the agent learns to solve a task with only expert states and \\textit{task-agnostic} non-expert state-action pairs. The state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the state occupancy divergence between the learner and expert policies. However, they are limited to either $f$-divergences (KL and $\\chi^2$) or Wasserstein distance with Rubinstein duality, the latter of which constrains the underlying distance metric crucial to the performance of Wasserstein-based solutions. To address this problem, we propose Primal Wasserstein DICE (PW-DICE), which minimizes the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer and leverages a contrastively learned distance as the underlying metric for the Wasserstein distance. Theoretically, we prove that our framework is a generalization of the state-of-the-art, SMODICE, and unifies $f$-divergence and Wasserstein minimization. Empirically, we find that PW-DICE improves upon several state-of-the-art methods on multiple testbeds.", "url": "https://arxiv.org/abs/2311.01331"}, {"metadata": {"arXiv": "2311.01406", "Date": "Thu, 02 Nov 2023 17:19:45 ", "Title": "Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability", "Authors": ["Stefan Kambiz Behfar and Jon Crowcroft"], "Categories": "cs.LG cs.AI"}, "abstract": "Blockchain technology has revolutionized the way information is propagated in decentralized networks. Ethereum plays a pivotal role in facilitating smart contracts and decentralized applications. Understanding information propagation dynamics in Ethereum is crucial for ensuring network efficiency, security, and scalability. In this study, we propose an innovative approach that utilizes Graph Convolutional Networks (GCNs) to analyze the information propagation patterns in the Ethereum network. The first phase of our research involves data collection from the Ethereum blockchain, consisting of blocks, transactions, and node degrees. We construct a transaction graph representation using adjacency matrices to capture the node embeddings; while our major contribution is to develop a combined Graph Attention Network (GAT) and Reinforcement Learning (RL) model to optimize the network efficiency and scalability. It learns the best actions to take in various network states, ultimately leading to improved network efficiency, throughput, and optimize gas limits for block processing. In the experimental evaluation, we analyze the performance of our model on a large-scale Ethereum dataset. We investigate effectively aggregating information from neighboring nodes capturing graph structure and updating node embeddings using GCN with the objective of transaction pattern prediction, accounting for varying network loads and number of blocks. Not only we design a gas limit optimization model and provide the algorithm, but also to address scalability, we demonstrate the use and implementation of sparse matrices in GraphConv, GraphSAGE, and GAT. The results indicate that our designed GAT-RL model achieves superior results compared to other GCN models in terms of performance. It effectively propagates information across the network, optimizing gas limits for block processing and improving network efficiency.", "url": "https://arxiv.org/abs/2311.01406"}, {"metadata": {"arXiv": "2311.01412", "Date": "Thu, 02 Nov 2023 17:26:49 ", "Title": "Castor: Causal Temporal Regime Structure Learning", "Authors": ["Abdellah Rahmani", "Pascal Frossard"], "Categories": "cs.LG cs.AI stat.ME"}, "abstract": "The task of uncovering causal relationships among multivariate time series data stands as an essential and challenging objective that cuts across a broad array of disciplines ranging from climate science to healthcare. Such data entails linear or non-linear relationships, and usually follow multiple a priori unknown regimes. Existing causal discovery methods can infer summary causal graphs from heterogeneous data with known regimes, but they fall short in comprehensively learning both regimes and the corresponding causal graph. In this paper, we introduce CASTOR, a novel framework designed to learn causal relationships in heterogeneous time series data composed of various regimes, each governed by a distinct causal graph. Through the maximization of a score function via the EM algorithm, CASTOR infers the number of regimes and learns linear or non-linear causal relationships in each regime. We demonstrate the robust convergence properties of CASTOR, specifically highlighting its proficiency in accurately identifying unique regimes. Empirical evidence, garnered from exhaustive synthetic experiments and two real-world benchmarks, confirm CASTOR's superior performance in causal discovery compared to baseline methods. By learning a full temporal causal graph for each regime, CASTOR establishes itself as a distinctly interpretable method for causal discovery in heterogeneous time series.", "url": "https://arxiv.org/abs/2311.01412"}, {"metadata": {"arXiv": "2311.01434", "Date": "Thu, 02 Nov 2023 17:48:28 ", "Title": "Tailoring Mixup to Data using Kernel Warping functions", "Authors": ["Quentin Bouniot", "Pavlo Mozharovskyi", "Florence d'Alch\\'e-Buc"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty", "url": "https://arxiv.org/abs/2311.01434"}, {"metadata": {"arXiv": "2311.01441", "Date": "Thu, 02 Nov 2023 17:55:13 ", "Title": "Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models", "Authors": ["Andy Zhou and Jindong Wang and Yu-Xiong Wang and Haohan Wang"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Published in NeurIPS 2023"]}, "abstract": "We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmentations for further improvements.", "url": "https://arxiv.org/abs/2311.01441"}, {"metadata": {"arXiv": "2311.01450", "Date": "Thu, 02 Nov 2023 17:57:38 ", "Title": "DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing", "Authors": ["Vint Lee", "Pieter Abbeel", "Youngwoon Lee"], "Categories": "cs.LG cs.AI cs.RO"}, "abstract": "Model-based reinforcement learning (MBRL) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: planning actions by generating imaginary trajectories with predicted rewards. Despite its success, we found that surprisingly, reward prediction is often a bottleneck of MBRL, especially for sparse rewards that are challenging (or even ambiguous) to predict. Motivated by the intuition that humans can learn from rough reward estimates, we propose a simple yet effective reward smoothing approach, DreamSmooth, which learns to predict a temporally-smoothed reward, instead of the exact reward at the given timestep. We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks, such as Deepmind Control Suite and Atari benchmarks.", "url": "https://arxiv.org/abs/2311.01450"}, {"metadata": {"arXiv": "2311.01452", "Date": "Thu, 02 Nov 2023 17:58:09 ", "Title": "Time Series Anomaly Detection using Diffusion-based Models", "Authors": ["Ioana Pintilie", "Andrei Manolache and Florin Brad"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at the AI4TS workshop of the 23rd IEEE International Conference on Data Mining (ICDM 2023)", "9 pages", "7 figures", "2 tables"]}, "abstract": "Diffusion models have been recently used for anomaly detection (AD) in images. In this paper we investigate whether they can also be leveraged for AD on multivariate time series (MTS). We test two diffusion-based models and compare them to several strong neural baselines. We also extend the PA%K protocol, by computing a ROCK-AUC metric, which is agnostic to both the detection threshold and the ratio K of correctly detected points. Our models outperform the baselines on synthetic datasets and are competitive on real-world datasets, illustrating the potential of diffusion-based methods for AD in multivariate time series.", "url": "https://arxiv.org/abs/2311.01452"}, {"metadata": {"arXiv": "2311.00754", "Date": "Wed, 01 Nov 2023 18:00:10 ", "Title": "Learning to Design and Use Tools for Robotic Manipulation", "Authors": ["Ziang Liu", "Stephen Tian", "Michelle Guo", "C. Karen Liu", "Jiajun Wu"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["First two authors contributed equally. Accepted at CoRL 2023"]}, "abstract": "When limited by their own morphologies, humans and some species of animals have the remarkable ability to use objects from the environment toward accomplishing otherwise impossible tasks. Robots might similarly unlock a range of additional capabilities through tool use. Recent techniques for jointly optimizing morphology and control via deep learning are effective at designing locomotion agents. But while outputting a single morphology makes sense for locomotion, manipulation involves a variety of strategies depending on the task goals at hand. A manipulation agent must be capable of rapidly prototyping specialized tools for different goals. Therefore, we propose learning a designer policy, rather than a single design. A designer policy is conditioned on task information and outputs a tool design that helps solve the task. A design-conditioned controller policy can then perform manipulation using these tools. In this work, we take a step towards this goal by introducing a reinforcement learning framework for jointly learning these policies. Through simulated manipulation tasks, we show that this framework is more sample efficient than prior methods in multi-goal or multi-variant settings, can perform zero-shot interpolation or fine-tuning to tackle previously unseen goals, and allows tradeoffs between the complexity of design and control policies under practical constraints. Finally, we deploy our learned policies onto a real robot. Please see our supplementary video and website at https://robotic-tool-design.github.io/ for visualizations.", "url": "https://arxiv.org/abs/2311.00754"}, {"metadata": {"arXiv": "2311.01248", "Date": "Thu, 02 Nov 2023 14:02:42 ", "Title": "Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching", "Authors": ["Trevor Ablett", "Oliver Limoyo", "Adam Sigal", "Affan Jilani", "Jonathan Kelly", "Kaleem Siddiqi", "Francois Hogan", "Gregory Dudek"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["8 pages", "11 figures"]}, "abstract": "Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a real robotic manipulator with door-opening and closing tasks, including over 3,000 real test episodes. Our results highlight the importance of tactile sensing for imitation learning, both for data collection to allow force matching, and for policy execution to allow accurate task feedback.", "url": "https://arxiv.org/abs/2311.01248"}, {"metadata": {"arXiv": "2311.01378", "Date": "Thu, 02 Nov 2023 16:34:33 ", "Title": "Vision-Language Foundation Models as Effective Robot Imitators", "Authors": ["Xinghang Li", "Minghuan Liu", "Hanbo Zhang", "Cunjun Yu", "Jie Xu", "Hongtao Wu", "Chilam Cheang", "Ya Jing", "Weinan Zhang", "Huaping Liu", "Hang Li", "Tao Kong"], "Categories": "cs.RO cs.AI cs.LG", "Comments": ["Project page: https://roboflamingo.github.io"]}, "abstract": "Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.", "url": "https://arxiv.org/abs/2311.01378"}, {"metadata": {"arXiv": "2311.01455", "Date": "Thu, 02 Nov 2023 17:59:21 ", "Title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation", "Authors": ["Yufei Wang", "Zhou Xian", "Feng Chen", "Tsun-Hsuan Wang", "Yian Wang", "Katerina Fragkiadaki", "Zackory Erickson", "David Held", "Chuang Gan"], "Categories": "cs.RO cs.AI cs.CV cs.LG"}, "abstract": "We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes inter- esting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial con- figurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.", "url": "https://arxiv.org/abs/2311.01455"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
