<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2311.14772", "Date": "Fri, 24 Nov 2023 13:37:19 ", "Title": "Trainwreck: A damaging adversarial attack on image classifiers", "Authors": ["Jan Zah\\'alka"], "Categories": "cs.CV cs.CR cs.LG"}, "abstract": "Adversarial attacks are an important security concern for computer vision (CV), as they enable malicious attackers to reliably manipulate CV models. Existing attacks aim to elicit an output desired by the attacker, but keep the model fully intact on clean data. With CV models becoming increasingly valuable assets in applied practice, a new attack vector is emerging: disrupting the models as a form of economic sabotage. This paper opens up the exploration of damaging adversarial attacks (DAAs) that seek to damage the target model and maximize the total cost incurred by the damage. As a pioneer DAA, this paper proposes Trainwreck, a train-time attack that poisons the training data of image classifiers to degrade their performance. Trainwreck conflates the data of similar classes using stealthy ($\\epsilon \\leq 8/255$) class-pair universal perturbations computed using a surrogate model. Trainwreck is a black-box, transferable attack: it requires no knowledge of the target model's architecture, and a single poisoned dataset degrades the performance of any model trained on it. The experimental evaluation on CIFAR-10 and CIFAR-100 demonstrates that Trainwreck is indeed an effective attack across various model architectures including EfficientNetV2, ResNeXt-101, and a finetuned ViT-L-16. The strength of the attack can be customized by the poison rate parameter. Finally, data redundancy with file hashing and/or pixel difference are identified as a reliable defense technique against Trainwreck or similar DAAs. The code is available at https://github.com/JanZahalka/trainwreck.", "url": "https://arxiv.org/abs/2311.14772"}, {"metadata": {"arXiv": "2311.14773", "Date": "Fri, 24 Nov 2023 13:56:06 ", "Title": "Set Features for Anomaly Detection", "Authors": ["Niv Cohen", "Issar Tzachor", "Yedid Hoshen"], "Categories": "cs.CV cs.LG", "Comments": ["arXiv admin note: substantial text overlap with arXiv:2302.12245"]}, "abstract": "This paper proposes set features for detecting anomalies in samples that consist of unusual combinations of normal elements. Many leading methods discover anomalies by detecting an unusual part of a sample. For example, state-of-the-art segmentation-based approaches, first classify each element of the sample (e.g., image patch) as normal or anomalous and then classify the entire sample as anomalous if it contains anomalous elements. However, such approaches do not extend well to scenarios where the anomalies are expressed by an unusual combination of normal elements. In this paper, we overcome this limitation by proposing set features that model each sample by the distribution of its elements. We compute the anomaly score of each sample using a simple density estimation method, using fixed features. Our approach outperforms the previous state-of-the-art in image-level logical anomaly detection and sequence-level time series anomaly detection.", "url": "https://arxiv.org/abs/2311.14773"}, {"metadata": {"arXiv": "2311.14939", "Date": "Sat, 25 Nov 2023 06:02:50 ", "Title": "OpenNet: Incremental Learning for Autonomous Driving Object Detection with Balanced Loss", "Authors": ["Zezhou Wang", "Guitao Cao", "Xidong Xi", "Jiangtao Wang"], "Categories": "cs.CV cs.LG"}, "abstract": "Automated driving object detection has always been a challenging task in computer vision due to environmental uncertainties. These uncertainties include significant differences in object sizes and encountering the class unseen. It may result in poor performance when traditional object detection models are directly applied to automated driving detection. Because they usually presume fixed categories of common traffic participants, such as pedestrians and cars. Worsely, the huge class imbalance between common and novel classes further exacerbates performance degradation. To address the issues stated, we propose OpenNet to moderate the class imbalance with the Balanced Loss, which is based on Cross Entropy Loss. Besides, we adopt an inductive layer based on gradient reshaping to fast learn new classes with limited samples during incremental learning. To against catastrophic forgetting, we employ normalized feature distillation. By the way, we improve multi-scale detection robustness and unknown class recognition through FPN and energy-based detection, respectively. The Experimental results upon the CODA dataset show that the proposed method can obtain better performance than that of the existing methods.", "url": "https://arxiv.org/abs/2311.14939"}, {"metadata": {"arXiv": "2311.14971", "Date": "Sat, 25 Nov 2023 09:08:30 ", "Title": "Segmentation of diagnostic tissue compartments on whole slide images with renal thrombotic microangiopathies (TMAs)", "Authors": ["Huy Q. Vo", "Pietro A. Cicalese", "Surya Seshan", "Syed A. Rizvi", "Aneesh Vathul", "Gloria Bueno", "Anibal Pedraza Dorado", "Niels Grabe", "Katharina Stolle", "Francesco Pesce", "Joris J.T.H. Roelofs", "Jesper Kers", "Vitoantonio Bevilacqua", "Nicola Altinini", "Bernd Schr\\\"oppel", "Dario Roccatello", "Antonella Barreca", "Savino Sciascia", "Chandra Mohan", "Hien V. Nguyen", "Jan U. Becker"], "Categories": "cs.CV cs.LG q-bio.TO", "Comments": ["12 pages", "3 figures"]}, "abstract": "The thrombotic microangiopathies (TMAs) manifest in renal biopsy histology with a broad spectrum of acute and chronic findings. Precise diagnostic criteria for a renal biopsy diagnosis of TMA are missing. As a first step towards a machine learning- and computer vision-based analysis of wholes slide images from renal biopsies, we trained a segmentation model for the decisive diagnostic kidney tissue compartments artery, arteriole, glomerulus on a set of whole slide images from renal biopsies with TMAs and Mimickers (distinct diseases with a similar nephropathological appearance as TMA like severe benign nephrosclerosis, various vasculitides, Bevacizumab-plug glomerulopathy, arteriolar light chain deposition disease). Our segmentation model combines a U-Net-based tissue detection with a Shifted windows-transformer architecture to reach excellent segmentation results for even the most severely altered glomeruli, arterioles and arteries, even on unseen staining domains from a different nephropathology lab. With accurate automatic segmentation of the decisive renal biopsy compartments in human renal vasculopathies, we have laid the foundation for large-scale compartment-specific machine learning and computer vision analysis of renal biopsy repositories with TMAs.", "url": "https://arxiv.org/abs/2311.14971"}, {"metadata": {"arXiv": "2311.15200", "Date": "Sun, 26 Nov 2023 05:45:27 ", "Title": "SpliceMix: A Cross-scale and Semantic Blending Augmentation Strategy for Multi-label Image Classification", "Authors": ["Lei Wang and Yibing Zhan and Leilei Ma and Dapeng Tao and Liang Ding and Chen Gong"], "Categories": "cs.CV cs.LG", "Comments": ["13 pages", "10 figures"]}, "abstract": "Recently, Mix-style data augmentation methods (e.g., Mixup and CutMix) have shown promising performance in various visual tasks. However, these methods are primarily designed for single-label images, ignoring the considerable discrepancies between single- and multi-label images, i.e., a multi-label image involves multiple co-occurred categories and fickle object scales. On the other hand, previous multi-label image classification (MLIC) methods tend to design elaborate models, bringing expensive computation. In this paper, we introduce a simple but effective augmentation strategy for multi-label image classification, namely SpliceMix. The \"splice\" in our method is two-fold: 1) Each mixed image is a splice of several downsampled images in the form of a grid, where the semantics of images attending to mixing are blended without object deficiencies for alleviating co-occurred bias; 2) We splice mixed images and the original mini-batch to form a new SpliceMixed mini-batch, which allows an image with different scales to contribute to training together. Furthermore, such splice in our SpliceMixed mini-batch enables interactions between mixed images and original regular images. We also offer a simple and non-parametric extension based on consistency learning (SpliceMix-CL) to show the flexible extensibility of our SpliceMix. Extensive experiments on various tasks demonstrate that only using SpliceMix with a baseline model (e.g., ResNet) achieves better performance than state-of-the-art methods. Moreover, the generalizability of our SpliceMix is further validated by the improvements in current MLIC methods when married with our SpliceMix. The code is available at https://github.com/zuiran/SpliceMix.", "url": "https://arxiv.org/abs/2311.15200"}, {"metadata": {"arXiv": "2311.15231", "Date": "Sun, 26 Nov 2023 08:09:43 ", "Title": "Double Reverse Regularization Network Based on Self-Knowledge Distillation for SAR Object Classification", "Authors": ["Bo Xu", "Hao Zheng", "Zhigang Hu", "Liu Yang", "Meiguang Zheng"], "Categories": "cs.CV cs.LG eess.IV", "Comments": ["6 pages", "8 figures"]}, "abstract": "In current synthetic aperture radar (SAR) object classification, one of the major challenges is the severe overfitting issue due to the limited dataset (few-shot) and noisy data. Considering the advantages of knowledge distillation as a learned label smoothing regularization, this paper proposes a novel Double Reverse Regularization Network based on Self-Knowledge Distillation (DRRNet-SKD). Specifically, through exploring the effect of distillation weight on the process of distillation, we are inspired to adopt the double reverse thought to implement an effective regularization network by combining offline and online distillation in a complementary way. Then, the Adaptive Weight Assignment (AWA) module is designed to adaptively assign two reverse-changing weights based on the network performance, allowing the student network to better benefit from both teachers. The experimental results on OpenSARShip and FUSAR-Ship demonstrate that DRRNet-SKD exhibits remarkable performance improvement on classical CNNs, outperforming state-of-the-art self-knowledge distillation methods.", "url": "https://arxiv.org/abs/2311.15231"}, {"metadata": {"arXiv": "2311.15262", "Date": "Sun, 26 Nov 2023 10:33:36 ", "Title": "Revealing Cortical Layers In Histological Brain Images With Self-Supervised Graph Convolutional Networks Applied To Cell-Graphs", "Authors": ["Valentina Vadori", "Antonella Peruffo", "Jean-Marie Gra\\\"ic", "Giulia Vadori", "Livio Finos", "Enrico Grisan"], "Categories": "cs.CV cs.LG q-bio.QM"}, "abstract": "Identifying cerebral cortex layers is crucial for comparative studies of the cytoarchitecture aiming at providing insights into the relations between brain structure and function across species. The absence of extensive annotated datasets typically limits the adoption of machine learning approaches, leading to the manual delineation of cortical layers by neuroanatomists. We introduce a self-supervised approach to detect layers in 2D Nissl-stained histological slices of the cerebral cortex. It starts with the segmentation of individual cells and the creation of an attributed cell-graph. A self-supervised graph convolutional network generates cell embeddings that encode morphological and structural traits of the cellular environment and are exploited by a community detection algorithm for the final layering. Our method, the first self-supervised of its kind with no spatial transcriptomics data involved, holds the potential to accelerate cytoarchitecture analyses, sidestepping annotation needs and advancing cross-species investigation.", "url": "https://arxiv.org/abs/2311.15262"}, {"metadata": {"arXiv": "2311.15264", "Date": "Sun, 26 Nov 2023 10:38:47 ", "Title": "ChAda-ViT : Channel Adaptive Attention for Joint Representation Learning of Heterogeneous Microscopy Images", "Authors": ["Nicolas Bourriez", "Ihab Bendidi", "Ethan Cohen", "Gabriel Watkinson", "Maxime Sanchez", "Guillaume Bollot", "Auguste Genovesio"], "Categories": "cs.CV cs.LG"}, "abstract": "Unlike color photography images, which are consistently encoded into RGB channels, biological images encompass various modalities, where the type of microscopy and the meaning of each channel varies with each experiment. Importantly, the number of channels can range from one to a dozen and their correlation is often comparatively much lower than RGB, as each of them brings specific information content. This aspect is largely overlooked by methods designed out of the bioimage field, and current solutions mostly focus on intra-channel spatial attention, often ignoring the relationship between channels, yet crucial in most biological applications. Importantly, the variable channel type and count prevent the projection of several experiments to a unified representation for large scale pre-training. In this study, we propose ChAda-ViT, a novel Channel Adaptive Vision Transformer architecture employing an Inter-Channel Attention mechanism on images with an arbitrary number, order and type of channels. We also introduce IDRCell100k, a bioimage dataset with a rich set of 79 experiments covering 7 microscope modalities, with a multitude of channel types, and channel counts varying from 1 to 10 per experiment. Our proposed architecture, trained in a self-supervised manner, outperforms existing approaches in several biologically relevant downstream tasks. Additionally, it can be used to bridge the gap for the first time between assays with different microscopes, channel numbers or types by embedding various image and experimental modalities into a unified biological image representation. The latter should facilitate interdisciplinary studies and pave the way for better adoption of deep learning in biological image-based analyses. Code and Data to be released soon.", "url": "https://arxiv.org/abs/2311.15264"}, {"metadata": {"arXiv": "2311.15276", "Date": "Sun, 26 Nov 2023 12:36:05 ", "Title": "Efficient Rehearsal Free Zero Forgetting Continual Learning using Adaptive Weight Modulation", "Authors": ["Yonatan Sverdlov", "Shimon Ullman"], "Categories": "cs.CV cs.LG"}, "abstract": "Artificial neural networks encounter a notable challenge known as continual learning, which involves acquiring knowledge of multiple tasks over an extended period. This challenge arises due to the tendency of previously learned weights to be adjusted to suit the objectives of new tasks, resulting in a phenomenon called catastrophic forgetting. Most approaches to this problem seek a balance between maximizing performance on the new tasks and minimizing the forgetting of previous tasks. In contrast, our approach attempts to maximize the performance of the new task, while ensuring zero forgetting. This is accomplished by creating a task-specific modulation parameters for each task. Only these would be learnable parameters during learning of consecutive tasks. Through comprehensive experimental evaluations, our model demonstrates superior performance in acquiring and retaining novel tasks that pose difficulties for other multi-task models. This emphasizes the efficacy of our approach in preventing catastrophic forgetting while accommodating the acquisition of new tasks", "url": "https://arxiv.org/abs/2311.15276"}, {"metadata": {"arXiv": "2311.15339", "Date": "Sun, 26 Nov 2023 15:50:19 ", "Title": "Adversarial Purification of Information Masking", "Authors": ["Sitong Liu", "Zhichao Lian", "Shuangquan Zhang", "Liang Xiao"], "Categories": "cs.CV cs.CR cs.LG eess.IV"}, "abstract": "Adversarial attacks meticulously generate minuscule, imperceptible perturbations to images to deceive neural networks. Counteracting these, adversarial purification methods seek to transform adversarial input samples into clean output images to defend against adversarial attacks. Nonetheless, extent generative models fail to effectively eliminate adversarial perturbations, yielding less-than-ideal purification results. We emphasize the potential threat of residual adversarial perturbations to target models, quantitatively establishing a relationship between perturbation scale and attack capability. Notably, the residual perturbations on the purified image primarily stem from the same-position patch and similar patches of the adversarial sample. We propose a novel adversarial purification approach named Information Mask Purification (IMPure), aims to extensively eliminate adversarial perturbations. To obtain an adversarial sample, we first mask part of the patches information, then reconstruct the patches to resist adversarial perturbations from the patches. We reconstruct all patches in parallel to obtain a cohesive image. Then, in order to protect the purified samples against potential similar regional perturbations, we simulate this risk by randomly mixing the purified samples with the input samples before inputting them into the feature extraction network. Finally, we establish a combined constraint of pixel loss and perceptual loss to augment the model's reconstruction adaptability. Extensive experiments on the ImageNet dataset with three classifier models demonstrate that our approach achieves state-of-the-art results against nine adversarial attack methods. Implementation code and pre-trained weights can be accessed at \\textcolor{blue}{https://github.com/NoWindButRain/IMPure}.", "url": "https://arxiv.org/abs/2311.15339"}, {"metadata": {"arXiv": "2311.15435", "Date": "Sun, 26 Nov 2023 21:35:34 ", "Title": "Functional Diffusion", "Authors": ["Biao Zhang and Peter Wonka"], "Categories": "cs.CV cs.GR cs.LG", "Comments": ["For the project site", "see https://1zb.github.io/functional-diffusion/"]}, "abstract": "We propose a new class of generative diffusion models, called functional diffusion. In contrast to previous work, functional diffusion works on samples that are represented by functions with a continuous domain. Functional diffusion can be seen as an extension of classical diffusion models to an infinite-dimensional domain. Functional diffusion is very versatile as images, videos, audio, 3D shapes, deformations, \\etc, can be handled by the same framework with minimal changes. In addition, functional diffusion is especially suited for irregular data or data defined in non-standard domains. In our work, we derive the necessary foundations for functional diffusion and propose a first implementation based on the transformer architecture. We show generative results on complicated signed distance functions and deformation functions defined on 3D surfaces.", "url": "https://arxiv.org/abs/2311.15435"}, {"metadata": {"arXiv": "2311.15475", "Date": "Mon, 27 Nov 2023 01:20:11 ", "Title": "MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers", "Authors": ["Yawar Siddiqui", "Antonio Alliegro", "Alexey Artemov", "Tatiana Tommasi", "Daniele Sirigatti", "Vladislav Rosov", "Angela Dai", "Matthias Nie{\\ss}ner"], "Categories": "cs.CV cs.LG", "Comments": ["Project Page: https://nihalsid.github.io/mesh-gpt/", "Video: https://youtu.be/UV90O1_69_o"]}, "abstract": "We introduce MeshGPT, a new approach for generating triangle meshes that reflects the compactness typical of artist-created meshes, in contrast to dense triangle meshes extracted by iso-surfacing methods from neural fields. Inspired by recent advances in powerful large language models, we adopt a sequence-based approach to autoregressively generate triangle meshes as sequences of triangles. We first learn a vocabulary of latent quantized embeddings, using graph convolutions, which inform these embeddings of the local mesh geometry and topology. These embeddings are sequenced and decoded into triangles by a decoder, ensuring that they can effectively reconstruct the mesh. A transformer is then trained on this learned vocabulary to predict the index of the next embedding given previous embeddings. Once trained, our model can be autoregressively sampled to generate new triangle meshes, directly generating compact meshes with sharp edges, more closely imitating the efficient triangulation patterns of human-crafted meshes. MeshGPT demonstrates a notable improvement over state of the art mesh generation methods, with a 9% increase in shape coverage and a 30-point enhancement in FID scores across various categories.", "url": "https://arxiv.org/abs/2311.15475"}, {"metadata": {"arXiv": "2311.15547", "Date": "Mon, 27 Nov 2023 05:23:01 ", "Title": "Dataset Distillation in Latent Space", "Authors": ["Yuxuan Duan", "Jianfu Zhang", "Liqing Zhang"], "Categories": "cs.CV cs.LG", "Comments": ["Under review"]}, "abstract": "Dataset distillation (DD) is a newly emerging research area aiming at alleviating the heavy computational load in training models on large datasets. It tries to distill a large dataset into a small and condensed one so that models trained on the distilled dataset can perform comparably with those trained on the full dataset when performing downstream tasks. Among the previous works in this area, there are three key problems that hinder the performance and availability of the existing DD methods: high time complexity, high space complexity, and low info-compactness. In this work, we simultaneously attempt to settle these three problems by moving the DD processes from conventionally used pixel space to latent space. Encoded by a pretrained generic autoencoder, latent codes in the latent space are naturally info-compact representations of the original images in much smaller sizes. After transferring three mainstream DD algorithms to latent space, we significantly reduce time and space consumption while achieving similar performance, allowing us to distill high-resolution datasets or target at greater data ratio that previous methods have failed. Besides, within the same storage budget, we can also quantitatively deliver more latent codes than pixel-level images, which further boosts the performance of our methods.", "url": "https://arxiv.org/abs/2311.15547"}, {"metadata": {"arXiv": "2311.15906", "Date": "Mon, 27 Nov 2023 15:13:02 ", "Title": "MetaDefa: Meta-learning based on Domain Enhancement and Feature Alignment for Single Domain Generalization", "Authors": ["Can Sun", "Hao Zheng", "Zhigang Hu", "Liu Yang", "Meiguang Zheng", "Bo Xu"], "Categories": "cs.CV cs.LG", "Comments": ["5 pages", "3 figures"]}, "abstract": "The single domain generalization(SDG) based on meta-learning has emerged as an effective technique for solving the domain-shift problem. However, the inadequate match of data distribution between source and augmented domains and difficult separation of domain-invariant features from domain-related features make SDG model hard to achieve great generalization. Therefore, a novel meta-learning method based on domain enhancement and feature alignment (MetaDefa) is proposed to improve the model generalization performance. First, the background substitution and visual corruptions techniques are used to generate diverse and effective augmented domains. Then, the multi-channel feature alignment module based on class activation maps and class agnostic activation maps is designed to effectively extract adequate transferability knowledge. In this module, domain-invariant features can be fully explored by focusing on similar target regions between source and augmented domains feature space and suppressing the feature representation of non-similar target regions. Extensive experiments on two publicly available datasets show that MetaDefa has significant generalization performance advantages in unknown multiple target domains.", "url": "https://arxiv.org/abs/2311.15906"}, {"metadata": {"arXiv": "2311.16101", "Date": "Mon, 27 Nov 2023 18:59:42 ", "Title": "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs", "Authors": ["Haoqin Tu", "Chenhang Cui", "Zijun Wang", "Yiyang Zhou", "Bingchen Zhao", "Junlin Han", "Wangchunshu Zhou", "Huaxiu Yao", "Cihang Xie"], "Categories": "cs.CV cs.CL cs.LG", "Comments": ["H.T.", "C.C.", "and Z.W. contribute equally. Work done during H.T. and Z.W.'s internship at UCSC", "and C.C. and Y.Z.'s internship at UNC"]}, "abstract": "This work focuses on the potential of Vision LLMs (VLLMs) in visual reasoning. Different from prior studies, we shift our focus from evaluating standard performance to introducing a comprehensive safety evaluation suite, covering both out-of-distribution (OOD) generalization and adversarial robustness. For the OOD evaluation, we present two novel VQA datasets, each with one variant, designed to test model performance under challenging conditions. In exploring adversarial robustness, we propose a straightforward attack strategy for misleading VLLMs to produce visual-unrelated responses. Moreover, we assess the efficacy of two jailbreaking strategies, targeting either the vision or language component of VLLMs. Our evaluation of 21 diverse models, ranging from open-source VLLMs to GPT-4V, yields interesting observations: 1) Current VLLMs struggle with OOD texts but not images, unless the visual information is limited; and 2) These VLLMs can be easily misled by deceiving vision encoders only, and their vision-language training often compromise safety protocols. We release this safety evaluation suite at https://github.com/UCSC-VLAA/vllm-safety-benchmark.", "url": "https://arxiv.org/abs/2311.16101"}, {"metadata": {"arXiv": "2311.14754", "Date": "Thu, 23 Nov 2023 14:16:03 ", "Title": "ExCeL : Combined Extreme and Collective Logit Information for Enhancing Out-of-Distribution Detection", "Authors": ["Naveen Karunanayake", "Suranga Seneviratne", "Sanjay Chawla"], "Categories": "cs.LG"}, "abstract": "Deep learning models often exhibit overconfidence in predicting out-of-distribution (OOD) data, underscoring the crucial role of OOD detection in ensuring reliability in predictions. Among various OOD detection approaches, post-hoc detectors have gained significant popularity, primarily due to their ease of use and implementation. However, the effectiveness of most post-hoc OOD detectors has been constrained as they rely solely either on extreme information, such as the maximum logit, or on the collective information (i.e., information spanned across classes or training samples) embedded within the output layer. In this paper, we propose ExCeL that combines both extreme and collective information within the output layer for enhanced accuracy in OOD detection. We leverage the logit of the top predicted class as the extreme information (i.e., the maximum logit), while the collective information is derived in a novel approach that involves assessing the likelihood of other classes appearing in subsequent ranks across various training samples. Our idea is motivated by the observation that, for in-distribution (ID) data, the ranking of classes beyond the predicted class is more deterministic compared to that in OOD data. Experiments conducted on CIFAR100 and ImageNet-200 datasets demonstrate that ExCeL consistently is among the five top-performing methods out of twenty-one existing post-hoc baselines when the joint performance on near-OOD and far-OOD is considered (i.e., in terms of AUROC and FPR95). Furthermore, ExCeL shows the best overall performance across both datasets, unlike other baselines that work best on one dataset but has a performance drop in the other.", "url": "https://arxiv.org/abs/2311.14754"}, {"metadata": {"arXiv": "2311.14766", "Date": "Fri, 24 Nov 2023 07:50:52 ", "Title": "Reinforcement Learning from Statistical Feedback: the Journey from AB Testing to ANT Testing", "Authors": ["Feiyang Han and Yimin Wei and Zhaofeng Liu and Yanxing Qi"], "Categories": "cs.LG math.ST stat.ME stat.TH"}, "abstract": "Reinforcement Learning from Human Feedback (RLHF) has played a crucial role in the success of large models such as ChatGPT. RLHF is a reinforcement learning framework which combines human feedback to improve learning effectiveness and performance. However, obtaining preferences feedback manually is quite expensive in commercial applications. Some statistical commercial indicators are usually more valuable and always ignored in RLHF. There exists a gap between commercial target and model training. In our research, we will attempt to fill this gap with statistical business feedback instead of human feedback, using AB testing which is a well-established statistical method. Reinforcement Learning from Statistical Feedback (RLSF) based on AB testing is proposed. Statistical inference methods are used to obtain preferences for training the reward network, which fine-tunes the pre-trained model in reinforcement learning framework, achieving greater business value. Furthermore, we extend AB testing with double selections at a single time-point to ANT testing with multiple selections at different feedback time points. Moreover, we design numerical experiences to validate the effectiveness of our algorithm framework.", "url": "https://arxiv.org/abs/2311.14766"}, {"metadata": {"arXiv": "2311.14782", "Date": "Fri, 24 Nov 2023 16:32:47 ", "Title": "One Fits All: Universal Time Series Analysis by Pretrained LM and Specially Designed Adaptors", "Authors": ["Tian Zhou", "Peisong Niu", "Xue Wang", "Liang Sun", "Rong Jin"], "Categories": "cs.LG", "Comments": ["this article draws heavily from arXiv:2302.11939"]}, "abstract": "Despite the impressive achievements of pre-trained models in the fields of natural language processing (NLP) and computer vision (CV), progress in the domain of time series analysis has been limited. In contrast to NLP and CV, where a single model can handle various tasks, time series analysis still relies heavily on task-specific methods for activities such as classification, anomaly detection, forecasting, and few-shot learning. The primary obstacle to developing a pre-trained model for time series analysis is the scarcity of sufficient training data. In our research, we overcome this obstacle by utilizing pre-trained models from language or CV, which have been trained on billions of data points, and apply them to time series analysis. We assess the effectiveness of the pre-trained transformer model in two ways. Initially, we maintain the original structure of the self-attention and feedforward layers in the residual blocks of the pre-trained language or image model, using the Frozen Pre-trained Transformer (FPT) for time series analysis with the addition of projection matrices for input and output. Additionally, we introduce four unique adapters, designed specifically for downstream tasks based on the pre-trained model, including forecasting and anomaly detection. These adapters are further enhanced with efficient parameter tuning, resulting in superior performance compared to all state-of-the-art methods.Our comprehensive experimental studies reveal that (a) the simple FPT achieves top-tier performance across various time series analysis tasks; and (b) fine-tuning the FPT with the custom-designed adapters can further elevate its performance, outshining specialized task-specific models.", "url": "https://arxiv.org/abs/2311.14782"}, {"metadata": {"arXiv": "2311.14859", "Date": "Fri, 24 Nov 2023 22:30:38 ", "Title": "An Empirical Investigation into Benchmarking Model Multiplicity for Trustworthy Machine Learning: A Case Study on Image Classification", "Authors": ["Prakhar Ganesh"], "Categories": "cs.LG", "Comments": ["Accepted at WACV 2024"]}, "abstract": "Deep learning models have proven to be highly successful. Yet, their over-parameterization gives rise to model multiplicity, a phenomenon in which multiple models achieve similar performance but exhibit distinct underlying behaviours. This multiplicity presents a significant challenge and necessitates additional specifications in model selection to prevent unexpected failures during deployment. While prior studies have examined these concerns, they focus on individual metrics in isolation, making it difficult to obtain a comprehensive view of multiplicity in trustworthy machine learning. Our work stands out by offering a one-stop empirical benchmark of multiplicity across various dimensions of model design and its impact on a diverse set of trustworthy metrics. In this work, we establish a consistent language for studying model multiplicity by translating several trustworthy metrics into accuracy under appropriate interventions. We also develop a framework, which we call multiplicity sheets, to benchmark multiplicity in various scenarios. We demonstrate the advantages of our setup through a case study in image classification and provide actionable insights into the impact and trends of different hyperparameters on model multiplicity. Finally, we show that multiplicity persists in deep learning models even after enforcing additional specifications during model selection, highlighting the severity of over-parameterization. The concerns of under-specification thus remain, and we seek to promote a more comprehensive discussion of multiplicity in trustworthy machine learning.", "url": "https://arxiv.org/abs/2311.14859"}, {"metadata": {"arXiv": "2311.14864", "Date": "Fri, 24 Nov 2023 22:42:37 ", "Title": "Effective Structural Encodings via Local Curvature Profiles", "Authors": ["Lukas Fesser", "Melanie Weber"], "Categories": "cs.LG stat.ML"}, "abstract": "Structural and Positional Encodings can significantly improve the performance of Graph Neural Networks in downstream tasks. Recent literature has begun to systematically investigate differences in the structural properties that these approaches encode, as well as performance trade-offs between them. However, the question of which structural properties yield the most effective encoding remains open. In this paper, we investigate this question from a geometric perspective. We propose a novel structural encoding based on discrete Ricci curvature (Local Curvature Profiles, short LCP) and show that it significantly outperforms existing encoding approaches. We further show that combining local structural encodings, such as LCP, with global positional encodings improves downstream performance, suggesting that they capture complementary geometric information. Finally, we compare different encoding types with (curvature-based) rewiring techniques. Rewiring has recently received a surge of interest due to its ability to improve the performance of Graph Neural Networks by mitigating over-smoothing and over-squashing effects. Our results suggest that utilizing curvature information for structural encodings delivers significantly larger performance increases than rewiring.", "url": "https://arxiv.org/abs/2311.14864"}, {"metadata": {"arXiv": "2311.14885", "Date": "Sat, 25 Nov 2023 00:30:58 ", "Title": "Projected Off-Policy Q-Learning (POP-QL) for Stabilizing Offline Reinforcement Learning", "Authors": ["Melrose Roderick", "Gaurav Manek", "Felix Berkenkamp", "J. Zico Kolter"], "Categories": "cs.LG", "Comments": ["10 pages"]}, "abstract": "A key problem in off-policy Reinforcement Learning (RL) is the mismatch, or distribution shift, between the dataset and the distribution over states and actions visited by the learned policy. This problem is exacerbated in the fully offline setting. The main approach to correct this shift has been through importance sampling, which leads to high-variance gradients. Other approaches, such as conservatism or behavior-regularization, regularize the policy at the cost of performance. In this paper, we propose a new approach for stable off-policy Q-Learning. Our method, Projected Off-Policy Q-Learning (POP-QL), is a novel actor-critic algorithm that simultaneously reweights off-policy samples and constrains the policy to prevent divergence and reduce value-approximation error. In our experiments, POP-QL not only shows competitive performance on standard benchmarks, but also out-performs competing methods in tasks where the data-collection policy is significantly sub-optimal.", "url": "https://arxiv.org/abs/2311.14885"}, {"metadata": {"arXiv": "2311.14886", "Date": "Sat, 25 Nov 2023 00:43:22 ", "Title": "A unified framework for learning with nonlinear model classes from arbitrary linear samples", "Authors": ["Ben Adcock", "Juan M. Cardenas", "Nick Dexter"], "Categories": "cs.LG cs.IT math.IT"}, "abstract": "This work considers the fundamental problem of learning an unknown object from training data using a given model class. We introduce a unified framework that allows for objects in arbitrary Hilbert spaces, general types of (random) linear measurements as training data and general types of nonlinear model classes. We establish a series of learning guarantees for this framework. These guarantees provide explicit relations between the amount of training data and properties of the model class to ensure near-best generalization bounds. In doing so, we also introduce and develop the key notion of the variation of a model class with respect to a distribution of sampling operators. To exhibit the versatility of this framework, we show that it can accommodate many different types of well-known problems of interest. We present examples such as matrix sketching by random sampling, compressed sensing with isotropic vectors, active learning in regression and compressed sensing with generative models. In all cases, we show how known results become straightforward corollaries of our general learning guarantees. For compressed sensing with generative models, we also present a number of generalizations and improvements of recent results. In summary, our work not only introduces a unified way to study learning unknown objects from general types of data, but also establishes a series of general theoretical guarantees which consolidate and improve various known results.", "url": "https://arxiv.org/abs/2311.14886"}, {"metadata": {"arXiv": "2311.14904", "Date": "Sat, 25 Nov 2023 02:45:50 ", "Title": "LLM-Assisted Code Cleaning For Training Accurate Code Generators", "Authors": ["Naman Jain", "Tianjun Zhang", "Wei-Lin Chiang", "Joseph E. Gonzalez", "Koushik Sen", "Ion Stoica"], "Categories": "cs.LG cs.SE"}, "abstract": "Natural language to code generation is an important application area of LLMs and has received wide attention from the community. The majority of relevant studies have exclusively concentrated on increasing the quantity and functional correctness of training sets while disregarding other stylistic elements of programs. More recently, data quality has garnered a lot of interest and multiple works have showcased its importance for improving performance. In this work, we investigate data quality for code and find that making the code more structured and readable leads to improved code generation performance of the system. We build a novel data-cleaning pipeline that uses these principles to transform existing programs by 1.) renaming variables, 2.) modularizing and decomposing complex code into smaller helper sub-functions, and 3.) inserting natural-language based plans via LLM based transformations. We evaluate our approach on two challenging algorithmic code generation benchmarks and find that fine-tuning CodeLLaMa-7B on our transformed modularized programs improves the performance by up to 30% compared to fine-tuning on the original dataset. Additionally, we demonstrate improved performance from using a smaller amount of higher-quality data, finding that a model fine-tuned on the entire original dataset is outperformed by a model trained on 15% of our cleaned dataset. Even in comparison to closed-source models, our models outperform the much larger AlphaCoder models.", "url": "https://arxiv.org/abs/2311.14904"}, {"metadata": {"arXiv": "2311.14931", "Date": "Sat, 25 Nov 2023 05:02:15 ", "Title": "One-Shot Transfer Learning for Nonlinear ODEs", "Authors": ["Wanzhou Lei", "Pavlos Protopapas", "Joy Parikh"], "Categories": "cs.LG", "Comments": ["7 pages", "3 figures", "accepted to 2023 NeurIPS Workshop of The Symbiosis of Deep Learning and Differential Equations"], "MSC-class": "68T07", "ACM-class": "I.2.1"}, "abstract": "We introduce a generalizable approach that combines perturbation method and one-shot transfer learning to solve nonlinear ODEs with a single polynomial term, using Physics-Informed Neural Networks (PINNs). Our method transforms non-linear ODEs into linear ODE systems, trains a PINN across varied conditions, and offers a closed-form solution for new instances within the same non-linear ODE class. We demonstrate the effectiveness of this approach on the Duffing equation and suggest its applicability to similarly structured PDEs and ODE systems.", "url": "https://arxiv.org/abs/2311.14931"}, {"metadata": {"arXiv": "2311.14934", "Date": "Sat, 25 Nov 2023 05:34:36 ", "Title": "Robust Graph Neural Networks via Unbiased Aggregation", "Authors": ["Ruiqi Feng", "Zhichao Hou", "Tyler Derr", "Xiaorui Liu"], "Categories": "cs.LG"}, "abstract": "The adversarial robustness of Graph Neural Networks (GNNs) has been questioned due to the false sense of security uncovered by strong adaptive attacks despite the existence of numerous defenses. In this work, we delve into the robustness analysis of representative robust GNNs and provide a unified robust estimation point of view to understand their robustness and limitations. Our novel analysis of estimation bias motivates the design of a robust and unbiased graph signal estimator. We then develop an efficient Quasi-Newton iterative reweighted least squares algorithm to solve the estimation problem, which unfolds as robust unbiased aggregation layers in GNNs with a theoretical convergence guarantee. Our comprehensive experiments confirm the strong robustness of our proposed model, and the ablation study provides a deep understanding of its advantages.", "url": "https://arxiv.org/abs/2311.14934"}, {"metadata": {"arXiv": "2311.14955", "Date": "Sat, 25 Nov 2023 07:43:17 ", "Title": "Identification of morphological fingerprint in perinatal brains using quasi-conformal mapping and contrastive learning", "Authors": ["Boyang Wang", "Weihao Zheng", "Ying Wang", "Zhe Zhang", "Yuchen Sheng and Minmin Wang"], "Categories": "cs.LG"}, "abstract": "The morphological fingerprint in the brain is capable of identifying the uniqueness of an individual. However, whether such individual patterns are present in perinatal brains, and which morphological attributes or cortical regions better characterize the individual differences of ne-onates remain unclear. In this study, we proposed a deep learning framework that projected three-dimensional spherical meshes of three morphological features (i.e., cortical thickness, mean curvature, and sulcal depth) onto two-dimensional planes through quasi-conformal mapping, and employed the ResNet18 and contrastive learning for individual identification. We used the cross-sectional structural MRI data of 682 infants, incorporating with data augmentation, to train the model and fine-tuned the parameters based on 60 infants who had longitudinal scans. The model was validated on 30 longitudinal scanned infant data, and remarkable Top1 and Top5 accuracies of 71.37% and 84.10% were achieved, respectively. The sensorimotor and visual cortices were recognized as the most contributive regions in individual identification. Moreover, the folding morphology demonstrated greater discriminative capability than the cortical thickness, which could serve as the morphological fingerprint in perinatal brains. These findings provided evidence for the emergence of morphological fingerprints in the brain at the beginning of the third trimester, which may hold promising implications for understanding the formation of in-dividual uniqueness in the brain during early development.", "url": "https://arxiv.org/abs/2311.14955"}, {"metadata": {"arXiv": "2311.14975", "Date": "Sat, 25 Nov 2023 09:22:34 ", "Title": "Eliminating Domain Bias for Federated Learning in Representation Space", "Authors": ["Jianqing Zhang", "Yang Hua", "Jian Cao", "Hao Wang", "Tao Song", "Zhengui Xue", "Ruhui Ma", "and Haibing Guan"], "Categories": "cs.LG cs.DC", "Comments": ["Accepted by NeurIPS 2023", "24 pages"]}, "abstract": "Recently, federated learning (FL) is popular for its privacy-preserving and collaborative learning abilities. However, under statistically heterogeneous scenarios, we observe that biased data domains on clients cause a representation bias phenomenon and further degenerate generic representations during local training, i.e., the representation degeneration phenomenon. To address these issues, we propose a general framework Domain Bias Eliminator (DBE) for FL. Our theoretical analysis reveals that DBE can promote bi-directional knowledge transfer between server and client, as it reduces the domain discrepancy between server and client in representation space. Besides, extensive experiments on four datasets show that DBE can greatly improve existing FL methods in both generalization and personalization abilities. The DBE-equipped FL method can outperform ten state-of-the-art personalized FL methods by a large margin. Our code is public at https://github.com/TsingZ0/DBE.", "url": "https://arxiv.org/abs/2311.14975"}, {"metadata": {"arXiv": "2311.15000", "Date": "Sat, 25 Nov 2023 11:32:20 ", "Title": "Satellite-based feature extraction and multivariate time-series prediction of biotoxin contamination in shellfish", "Authors": ["Sergio Tavares", "Pedro R. Costa", "Ludwig Krippahl", "Marta B. Lopes"], "Categories": "cs.LG", "Comments": ["19 pages"]}, "abstract": "Shellfish production constitutes an important sector for the economy of many Portuguese coastal regions, yet the challenge of shellfish biotoxin contamination poses both public health concerns and significant economic risks. Thus, predicting shellfish contamination levels holds great potential for enhancing production management and safeguarding public health. In our study, we utilize a dataset with years of Sentinel-3 satellite imagery for marine surveillance, along with shellfish biotoxin contamination data from various production areas along Portugal's western coastline, collected by Portuguese official control. Our goal is to evaluate the integration of satellite data in forecasting models for predicting toxin concentrations in shellfish given forecasting horizons up to four weeks, which implies extracting a small set of useful features and assessing their impact on the predictive models. We framed this challenge as a time-series forecasting problem, leveraging historical contamination levels and satellite images for designated areas. While contamination measurements occurred weekly, satellite images were accessible multiple times per week. Unsupervised feature extraction was performed using autoencoders able to handle non-valid pixels caused by factors like cloud cover, land, or anomalies. Finally, several Artificial Neural Networks models were applied to compare univariate (contamination only) and multivariate (contamination and satellite data) time-series forecasting. Our findings show that incorporating these features enhances predictions, especially beyond one week in lagoon production areas (RIAV) and for the 1-week and 2-week horizons in the L5B area (oceanic). The methodology shows the feasibility of integrating information from a high-dimensional data source like remote sensing without compromising the model's predictive ability.", "url": "https://arxiv.org/abs/2311.15000"}, {"metadata": {"arXiv": "2311.15047", "Date": "Sat, 25 Nov 2023 14:50:37 ", "Title": "Training a Hopfield Variational Autoencoder with Equilibrium Propagation", "Authors": ["Tom Van Der Meersch", "Johannes Deleu", "Thomas Demeester"], "Categories": "cs.LG cs.NE", "Comments": ["Associative Memory & Hopfield Networks in 2023 (NeurIPS 2023 workshop)"]}, "abstract": "On dedicated analog hardware, equilibrium propagation is an energy-efficient alternative to backpropagation. In spite of its theoretical guarantees, its application in the AI domain remains limited to the discriminative setting. Meanwhile, despite its high computational demands, generative AI is on the rise. In this paper, we demonstrate the application of Equilibrium Propagation in training a variational autoencoder (VAE) for generative modeling. Leveraging the symmetric nature of Hopfield networks, we propose using a single model to serve as both the encoder and decoder which could effectively halve the required chip size for VAE implementations, paving the way for more efficient analog hardware configurations.", "url": "https://arxiv.org/abs/2311.15047"}, {"metadata": {"arXiv": "2311.15051", "Date": "Sat, 25 Nov 2023 15:10:00 ", "Title": "Large Catapults in Momentum Gradient Descent with Warmup: An Empirical Study", "Authors": ["Prin Phunyaphibarn", "Junghyun Lee", "Bohan Wang", "Huishuai Zhang", "Chulhee Yun"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["19 pages", "14 figures. Accepted to the NeurIPS 2023 M3L Workshop (oral). The first two authors contributed equally"]}, "abstract": "Although gradient descent with momentum is widely used in modern deep learning, a concrete understanding of its effects on the training trajectory still remains elusive. In this work, we empirically show that momentum gradient descent with a large learning rate and learning rate warmup displays large catapults, driving the iterates towards flatter minima than those found by gradient descent. We then provide empirical evidence and theoretical intuition that the large catapult is caused by momentum \"amplifying\" the self-stabilization effect (Damian et al., 2023).", "url": "https://arxiv.org/abs/2311.15051"}, {"metadata": {"arXiv": "2311.15053", "Date": "Sat, 25 Nov 2023 15:21:03 ", "Title": "Task adaption by biologically inspired stochastic comodulation", "Authors": ["Gauthier Boeshertz", "Caroline Haimerl and Cristina Savin"], "Categories": "cs.LG cs.CV"}, "abstract": "Brain representations must strike a balance between generalizability and adaptability. Neural codes capture general statistical regularities in the world, while dynamically adjusting to reflect current goals. One aspect of this adaptation is stochastically co-modulating neurons' gains based on their task relevance. These fluctuations then propagate downstream to guide decision-making. Here, we test the computational viability of such a scheme in the context of multi-task learning. We show that fine-tuning convolutional networks by stochastic gain modulation improves on deterministic gain modulation, achieving state-of-the-art results on the CelebA dataset. To better understand the mechanisms supporting this improvement, we explore how fine-tuning performance is affected by architecture using Cifar-100. Overall, our results suggest that stochastic comodulation can enhance learning efficiency and performance in multi-task learning, without additional learnable parameters. This offers a promising new direction for developing more flexible and robust intelligent systems.", "url": "https://arxiv.org/abs/2311.15053"}, {"metadata": {"arXiv": "2311.15089", "Date": "Sat, 25 Nov 2023 18:00:26 ", "Title": "Where2Start: Leveraging initial States for Robust and Sample-Efficient Reinforcement Learning", "Authors": ["Pouya Parsa", "Raoof Zare Moayedi", "Mohammad Bornosi", "Mohammad Mahdi Bejani"], "Categories": "cs.LG", "Comments": ["9 pages", "3 figures"]}, "abstract": "The reinforcement learning algorithms that focus on how to compute the gradient and choose next actions, are effectively improved the performance of the agents. However, these algorithms are environment-agnostic. This means that the algorithms did not use the knowledge that has been captured by trajectory. This poses that the algorithms should sample many trajectories to train the model. By considering the essence of environment and how much the agent learn from each scenario in that environment, the strategy of the learning procedure can be changed. The strategy retrieves more informative trajectories, so the agent can learn with fewer trajectory sample. We propose Where2Start algorithm that selects the initial state so that the agent has more instability in vicinity of that state. We show that this kind of selection decreases number of trajectories that should be sampled that the agent reach to acceptable reward. Our experiments shows that Where2Start can improve sample efficiency up to 8 times. Also Where2Start can combined with most of state-of-the-art algorithms and improve that robustness and sample efficiency significantly.", "url": "https://arxiv.org/abs/2311.15089"}, {"metadata": {"arXiv": "2311.15097", "Date": "Sat, 25 Nov 2023 18:54:38 ", "Title": "AugmentTRAJ: A framework for point-based trajectory data augmentation", "Authors": ["Yaksh J Haranwala"], "Categories": "cs.LG"}, "abstract": "Data augmentation has emerged as a powerful technique in machine learning, strengthening model robustness while mitigating overfitting and under-fitting issues by generating diverse synthetic data. Nevertheless, despite its success in other domains, data augmentation's potential remains largely untapped in mobility data analysis, primarily due to the intricate nature and unique format of trajectory data. Additionally, there is a lack of frameworks capable of point-wise data augmentation, which can reliably generate synthetic trajectories while preserving the inherent characteristics of the original data. To address these challenges, this research introduces AugmenTRAJ, an open-source Python3 framework designed explicitly for trajectory data augmentation. AugmenTRAJ offers a reliable and well-controlled approach for generating synthetic trajectories, thereby enabling the harnessing of data augmentation benefits in mobility analysis. This thesis presents a comprehensive overview of the methodologies employed in developing AugmenTRAJ and showcases the various data augmentation techniques available within the framework. AugmenTRAJ opens new possibilities for enhancing mobility data analysis models' performance and generalization capabilities by providing researchers with a practical and versatile tool for augmenting trajectory data, Its user-friendly implementation in Python3 facilitates easy integration into existing workflows, offering the community an accessible resource to leverage the full potential of data augmentation in trajectory-based applications.", "url": "https://arxiv.org/abs/2311.15097"}, {"metadata": {"arXiv": "2311.15098", "Date": "Sat, 25 Nov 2023 18:55:26 ", "Title": "Speech-Based Blood Pressure Estimation with Enhanced Optimization and Incremental Clustering", "Authors": ["Vaishali Rajput", "Preeti Mulay", "Rajeev Raje"], "Categories": "cs.LG", "Comments": ["29 pages", "2 tables", "9 figures"]}, "abstract": "Blood Pressure (BP) estimation plays a pivotal role in diagnosing various health conditions, highlighting the need for innovative approaches to overcome conventional measurement challenges. Leveraging machine learning and speech signals, this study investigates accurate BP estimation with a focus on preprocessing, feature extraction, and real-time applications. An advanced clustering-based strategy, incorporating the k-means algorithm and the proposed Fact-Finding Instructor optimization algorithm, is introduced to enhance accuracy. The combined outcome of these clustering techniques enables robust BP estimation. Moreover, extending beyond these insights, this study delves into the dynamic realm of contemporary digital content consumption. Platforms like YouTube have emerged as influential spaces, presenting an array of videos that evoke diverse emotions. From heartwarming and amusing content to intense narratives, YouTube captures a spectrum of human experiences, influencing information access and emotional engagement. Within this context, this research investigates the interplay between YouTube videos and physiological responses, particularly Blood Pressure (BP) levels. By integrating advanced BP estimation techniques with the emotional dimensions of YouTube videos, this study enriches our understanding of how modern media environments intersect with health implications.", "url": "https://arxiv.org/abs/2311.15098"}, {"metadata": {"arXiv": "2311.15165", "Date": "Sun, 26 Nov 2023 02:25:30 ", "Title": "Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off", "Authors": ["Yatong Bai", "Brendon G. Anderson", "Somayeh Sojoudi"], "Categories": "cs.LG cs.CV", "Comments": ["2023 IEEE Conference on Control Technology and Applications. arXiv admin note: substantial text overlap with arXiv:2301.12554"], "MSC-class": "68T07"}, "abstract": "Machine learning models have recently found tremendous success in data-driven control systems. However, standard learning models often suffer from an accuracy-robustness trade-off, which is a limitation that must be overcome in the control of safety-critical systems that require both high performance and rigorous robustness guarantees. In this work, we build upon the recent \"locally biased smoothing\" method to develop classifiers that simultaneously inherit high accuracy from standard models and high robustness from robust models. Specifically, we extend locally biased smoothing to the multi-class setting, and then overcome its performance bottleneck by generalizing the formulation to \"mix\" the outputs of a standard neural network and a robust neural network. We prove that when the robustness of the robust base model is certifiable, within a closed-form $\\ell_p$ radius, no alteration or attack on an input can result in misclassification of the mixed classifier; the proposed model inherits the certified robustness. Moreover, we use numerical experiments on the CIFAR-10 benchmark dataset to verify that the mixed model noticeably improves the accuracy-robustness trade-off.", "url": "https://arxiv.org/abs/2311.15165"}, {"metadata": {"arXiv": "2311.15210", "Date": "Sun, 26 Nov 2023 06:53:56 ", "Title": "Topology combined machine learning for consonant recognition", "Authors": ["Pingyao Feng", "Siheng Yi", "Qingrui Qu", "Zhiwang Yu", "Yifei Zhu"], "Categories": "cs.LG math.ST stat.TH"}, "abstract": "In artificial-intelligence-aided signal processing, existing deep learning models often exhibit a black-box structure, and their validity and comprehensibility remain elusive. The integration of topological methods, despite its relatively nascent application, serves a dual purpose of making models more interpretable as well as extracting structural information from time-dependent data for smarter learning. Here, we provide a transparent and broadly applicable methodology, TopCap, to capture the most salient topological features inherent in time series for machine learning. Rooted in high-dimensional ambient spaces, TopCap is capable of capturing features rarely detected in datasets with low intrinsic dimensionality. Applying time-delay embedding and persistent homology, we obtain descriptors which encapsulate information such as the vibration of a time series, in terms of its variability of frequency, amplitude, and average line, demonstrated with simulated data. This information is then vectorised and fed into multiple machine learning algorithms such as k-nearest neighbours and support vector machine. Notably, in classifying voiced and voiceless consonants, TopCap achieves an accuracy exceeding 96% and is geared towards designing topological convolutional layers for deep learning of speech and audio signals.", "url": "https://arxiv.org/abs/2311.15210"}, {"metadata": {"arXiv": "2311.15214", "Date": "Sun, 26 Nov 2023 07:11:58 ", "Title": "A Novel Normalized-Cut Solver with Nearest Neighbor Hierarchical Initialization", "Authors": ["Feiping Nie", "Jitao Lu", "Danyang Wu", "Rong Wang", "Xuelong Li"], "Categories": "cs.LG math.OC", "DOI": "10.1109/TPAMI.2023.3279394"}, "abstract": "Normalized-Cut (N-Cut) is a famous model of spectral clustering. The traditional N-Cut solvers are two-stage: 1) calculating the continuous spectral embedding of normalized Laplacian matrix; 2) discretization via $K$-means or spectral rotation. However, this paradigm brings two vital problems: 1) two-stage methods solve a relaxed version of the original problem, so they cannot obtain good solutions for the original N-Cut problem; 2) solving the relaxed problem requires eigenvalue decomposition, which has $\\mathcal{O}(n^3)$ time complexity ($n$ is the number of nodes). To address the problems, we propose a novel N-Cut solver designed based on the famous coordinate descent method. Since the vanilla coordinate descent method also has $\\mathcal{O}(n^3)$ time complexity, we design various accelerating strategies to reduce the time complexity to $\\mathcal{O}(|E|)$ ($|E|$ is the number of edges). To avoid reliance on random initialization which brings uncertainties to clustering, we propose an efficient initialization method that gives deterministic outputs. Extensive experiments on several benchmark datasets demonstrate that the proposed solver can obtain larger objective values of N-Cut, meanwhile achieving better clustering performance compared to traditional solvers.", "url": "https://arxiv.org/abs/2311.15214"}, {"metadata": {"arXiv": "2311.15222", "Date": "Sun, 26 Nov 2023 07:23:37 ", "Title": "Decision Tree Psychological Risk Assessment in Currency Trading", "Authors": ["Jai Pal"], "Categories": "cs.LG cs.CE q-fin.GN", "Comments": ["8 pages", "3 figures", "7 listings"]}, "abstract": "This research paper focuses on the integration of Artificial Intelligence (AI) into the currency trading landscape, positing the development of personalized AI models, essentially functioning as intelligent personal assistants tailored to the idiosyncrasies of individual traders. The paper posits that AI models are capable of identifying nuanced patterns within the trader's historical data, facilitating a more accurate and insightful assessment of psychological risk dynamics in currency trading. The PRI is a dynamic metric that experiences fluctuations in response to market conditions that foster psychological fragility among traders. By employing sophisticated techniques, a classifying decision tree is crafted, enabling clearer decision-making boundaries within the tree structure. By incorporating the user's chronological trade entries, the model becomes adept at identifying critical junctures when psychological risks are heightened. The real-time nature of the calculations enhances the model's utility as a proactive tool, offering timely alerts to traders about impending moments of psychological risks. The implications of this research extend beyond the confines of currency trading, reaching into the realms of other industries where the judicious application of personalized modeling emerges as an efficient and strategic approach. This paper positions itself at the intersection of cutting-edge technology and the intricate nuances of human psychology, offering a transformative paradigm for decision making support in dynamic and high-pressure environments.", "url": "https://arxiv.org/abs/2311.15222"}, {"metadata": {"arXiv": "2311.15238", "Date": "Sun, 26 Nov 2023 08:31:57 ", "Title": "A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation", "Authors": ["Heyang Zhao and Jiafan He and Quanquan Gu"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["52 pages", "1 table"]}, "abstract": "The exploration-exploitation dilemma has been a central challenge in reinforcement learning (RL) with complex model classes. In this paper, we propose a new algorithm, Monotonic Q-Learning with Upper Confidence Bound (MQL-UCB) for RL with general function approximation. Our key algorithmic design includes (1) a general deterministic policy-switching strategy that achieves low switching cost, (2) a monotonic value function structure with carefully controlled function class complexity, and (3) a variance-weighted regression scheme that exploits historical trajectories with high data efficiency. MQL-UCB achieves minimax optimal regret of $\\tilde{O}(d\\sqrt{HK})$ when $K$ is sufficiently large and near-optimal policy switching cost of $\\tilde{O}(dH)$, with $d$ being the eluder dimension of the function class, $H$ being the planning horizon, and $K$ being the number of episodes. Our work sheds light on designing provably sample-efficient and deployment-efficient Q-learning with nonlinear function approximation.", "url": "https://arxiv.org/abs/2311.15238"}, {"metadata": {"arXiv": "2311.15297", "Date": "Sun, 26 Nov 2023 13:45:21 ", "Title": "Controllable Expensive Multi-objective Optimization with Warm-starting Gaussian Processes", "Authors": ["Quang-Huy Nguyen", "Long P. Hoang", "Hoang V. Viet", "Dung D. Le"], "Categories": "cs.LG math.OC"}, "abstract": "Pareto Set Learning (PSL) is a promising approach for approximating the entire Pareto front in multi-objective optimization (MOO) problems. However, existing derivative-free PSL methods are often unstable and inefficient, especially for expensive black-box MOO problems where objective function evaluations are costly. In this work, we propose to address the instability and inefficiency of existing PSL methods with a novel controllable PSL method, called Co-PSL. Particularly, Co-PSL consists of two stages: (1) warm-starting Bayesian optimization to obtain quality Gaussian Processes priors and (2) controllable Pareto set learning to accurately acquire a parametric mapping from preferences to the corresponding Pareto solutions. The former is to help stabilize the PSL process and reduce the number of expensive function evaluations. The latter is to support real-time trade-off control between conflicting objectives. Performances across synthesis and real-world MOO problems showcase the effectiveness of our Co-PSL for expensive multi-objective optimization tasks.", "url": "https://arxiv.org/abs/2311.15297"}, {"metadata": {"arXiv": "2311.15317", "Date": "Sun, 26 Nov 2023 14:35:28 ", "Title": "Generalized Graph Prompt: Toward a Unification of Pre-Training and Downstream Tasks on Graphs", "Authors": ["Xingtong Yu", "Zhenghao Liu", "Yuan Fang", "Zemin Liu", "Sihong Chen and Xinming Zhang"], "Categories": "cs.LG", "Comments": ["28 pages. Under review. arXiv admin note: substantial text overlap with arXiv:2302.08043"]}, "abstract": "Graph neural networks have emerged as a powerful tool for graph representation learning, but their performance heavily relies on abundant task-specific supervision. To reduce labeling requirement, the \"pre-train, prompt\" paradigms have become increasingly common. However, existing study of prompting on graphs is limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. To further enhance GraphPrompt in these two stages, we extend it into GraphPrompt+ with two major enhancements. First, we generalize several popular graph pre-training tasks beyond simple link prediction to broaden the compatibility with our task template. Second, we propose a more generalized prompt design that incorporates a series of prompt vectors within every layer of the pre-trained graph encoder, in order to capitalize on the hierarchical information across different layers beyond just the readout layer. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt and GraphPrompt+.", "url": "https://arxiv.org/abs/2311.15317"}, {"metadata": {"arXiv": "2311.15331", "Date": "Sun, 26 Nov 2023 15:31:51 ", "Title": "How much data do I need? A case study on medical data", "Authors": ["Ayse Betul Cengiz and A. Stephen McGough"], "Categories": "cs.LG cs.CV", "Comments": ["10 pages", "7 figures"]}, "abstract": "The collection of data to train a Deep Learning network is costly in terms of effort and resources. In many cases, especially in a medical context, it may have detrimental impacts. Such as requiring invasive medical procedures or processes which could in themselves cause medical harm. However, Deep Learning is seen as a data hungry method. Here, we look at two commonly held adages i) more data gives better results and ii) transfer learning will aid you when you don't have enough data. These are widely assumed to be true and used as evidence for choosing how to solve a problem when Deep Learning is involved. We evaluate six medical datasets and six general datasets. Training a ResNet18 network on varying subsets of these datasets to evaluate `more data gives better results'. We take eleven of these datasets as the sources for Transfer Learning on subsets of the twelfth dataset -- Chest -- in order to determine whether Transfer Learning is universally beneficial. We go further to see whether multi-stage Transfer Learning provides a consistent benefit. Our analysis shows that the real situation is more complex than these simple adages -- more data could lead to a case of diminishing returns and an incorrect choice of dataset for transfer learning can lead to worse performance, with datasets which we would consider highly similar to the Chest dataset giving worse results than datasets which are more dissimilar. Multi-stage transfer learning likewise reveals complex relationships between datasets.", "url": "https://arxiv.org/abs/2311.15331"}, {"metadata": {"arXiv": "2311.15341", "Date": "Sun, 26 Nov 2023 15:57:20 ", "Title": "Generative Modelling of Stochastic Actions with Arbitrary Constraints in Reinforcement Learning", "Authors": ["Changyu Chen", "Ramesha Karunasena", "Thanh Hong Nguyen", "Arunesh Sinha", "Pradeep Varakantham"], "Categories": "cs.LG", "Comments": ["Accepted in NeurIPS 2023. Website: https://cameron-chen.github.io/flow-iar/"]}, "abstract": "Many problems in Reinforcement Learning (RL) seek an optimal policy with large discrete multidimensional yet unordered action spaces; these include problems in randomized allocation of resources such as placements of multiple security resources and emergency response units, etc. A challenge in this setting is that the underlying action space is categorical (discrete and unordered) and large, for which existing RL methods do not perform well. Moreover, these problems require validity of the realized action (allocation); this validity constraint is often difficult to express compactly in a closed mathematical form. The allocation nature of the problem also prefers stochastic optimal policies, if one exists. In this work, we address these challenges by (1) applying a (state) conditional normalizing flow to compactly represent the stochastic policy -- the compactness arises due to the network only producing one sampled action and the corresponding log probability of the action, which is then used by an actor-critic method; and (2) employing an invalid action rejection method (via a valid action oracle) to update the base policy. The action rejection is enabled by a modified policy gradient that we derive. Finally, we conduct extensive experiments to show the scalability of our approach compared to prior methods and the ability to enforce arbitrary state-conditional constraints on the support of the distribution of actions in any state.", "url": "https://arxiv.org/abs/2311.15341"}, {"metadata": {"arXiv": "2311.15365", "Date": "Sun, 26 Nov 2023 17:44:29 ", "Title": "A Convergence result of a continuous model of deep learning via \\L{}ojasiewicz--Simon inequality", "Authors": ["Noboru Isobe"], "Categories": "cs.LG math.AP math.FA math.PR", "Comments": ["31 pages"], "MSC-class": "35B40, 49J20, 49Q22, 68T07"}, "abstract": "This study focuses on a Wasserstein-type gradient flow, which represents an optimization process of a continuous model of a Deep Neural Network (DNN). First, we establish the existence of a minimizer for an average loss of the model under $L^2$-regularization. Subsequently, we show the existence of a curve of maximal slope of the loss. Our main result is the convergence of flow to a critical point of the loss as time goes to infinity. An essential aspect of proving this result involves the establishment of the \\L{}ojasiewicz--Simon gradient inequality for the loss. We derive this inequality by assuming the analyticity of NNs and loss functions. Our proofs offer a new approach for analyzing the asymptotic behavior of Wasserstein-type gradient flows for nonconvex functionals.", "url": "https://arxiv.org/abs/2311.15365"}, {"metadata": {"arXiv": "2311.15382", "Date": "Sun, 26 Nov 2023 18:55:46 ", "Title": "Evaluating Multi-Global Server Architecture for Federated Learning", "Authors": ["Asfia Kawnine", "Hung Cao", "Atah Nuh Mih", "Monica Wachowicz"], "Categories": "cs.LG cs.DC", "Comments": ["Key words: Federated Learning", "Edge AI", "Multiple global servers", "EV energy consumption"]}, "abstract": "Federated learning (FL) with a single global server framework is currently a popular approach for training machine learning models on decentralized environment, such as mobile devices and edge devices. However, the centralized server architecture poses a risk as any challenge on the central/global server would result in the failure of the entire system. To minimize this risk, we propose a novel federated learning framework that leverages the deployment of multiple global servers. We posit that implementing multiple global servers in federated learning can enhance efficiency by capitalizing on local collaborations and aggregating knowledge, and the error tolerance in regard to communication failure in the single server framework would be handled. We therefore propose a novel framework that leverages the deployment of multiple global servers. We conducted a series of experiments using a dataset containing the event history of electric vehicle (EV) charging at numerous stations. We deployed a federated learning setup with multiple global servers and client servers, where each client-server strategically represented a different region and a global server was responsible for aggregating local updates from those devices. Our preliminary results of the global models demonstrate that the difference in performance attributed to multiple servers is less than 1%. While the hypothesis of enhanced model efficiency was not as expected, the rule for handling communication challenges added to the algorithm could resolve the error tolerance issue. Future research can focus on identifying specific uses for the deployment of multiple global servers.", "url": "https://arxiv.org/abs/2311.15382"}, {"metadata": {"arXiv": "2311.15390", "Date": "Sun, 26 Nov 2023 19:19:02 ", "Title": "Local Convergence of Approximate Newton Method for Two Layer Nonlinear Regression", "Authors": ["Zhihang Li", "Zhao Song", "Zifan Wang", "Junze Yin"], "Categories": "cs.LG"}, "abstract": "There have been significant advancements made by large language models (LLMs) in various aspects of our daily lives. LLMs serve as a transformative force in natural language processing, finding applications in text generation, translation, sentiment analysis, and question-answering. The accomplishments of LLMs have led to a substantial increase in research efforts in this domain. One specific two-layer regression problem has been well-studied in prior works, where the first layer is activated by a ReLU unit, and the second layer is activated by a softmax unit. While previous works provide a solid analysis of building a two-layer regression, there is still a gap in the analysis of constructing regression problems with more than two layers. In this paper, we take a crucial step toward addressing this problem: we provide an analysis of a two-layer regression problem. In contrast to previous works, our first layer is activated by a softmax unit. This sets the stage for future analyses of creating more activation functions based on the softmax function. Rearranging the softmax function leads to significantly different analyses. Our main results involve analyzing the convergence properties of an approximate Newton method used to minimize the regularized training loss. We prove that the loss function for the Hessian matrix is positive definite and Lipschitz continuous under certain assumptions. This enables us to establish local convergence guarantees for the proposed training algorithm. Specifically, with an appropriate initialization and after $O(\\log(1/\\epsilon))$ iterations, our algorithm can find an $\\epsilon$-approximate minimizer of the training loss with high probability. Each iteration requires approximately $O(\\mathrm{nnz}(C) + d^\\omega)$ time, where $d$ is the model size, $C$ is the input matrix, and $\\omega < 2.374$ is the matrix multiplication exponent.", "url": "https://arxiv.org/abs/2311.15390"}, {"metadata": {"arXiv": "2311.15395", "Date": "Sun, 26 Nov 2023 19:31:52 ", "Title": "ConstraintMatch for Semi-constrained Clustering", "Authors": ["Jann Goschenhofer", "Bernd Bischl", "Zsolt Kira"], "Categories": "cs.LG cs.CV stat.ML", "Journal-ref": "2023 International Joint Conference on Neural Networks (IJCNN)"}, "abstract": "Constrained clustering allows the training of classification models using pairwise constraints only, which are weak and relatively easy to mine, while still yielding full-supervision-level model performance. While they perform well even in the absence of the true underlying class labels, constrained clustering models still require large amounts of binary constraint annotations for training. In this paper, we propose a semi-supervised context whereby a large amount of \\textit{unconstrained} data is available alongside a smaller set of constraints, and propose \\textit{ConstraintMatch} to leverage such unconstrained data. While a great deal of progress has been made in semi-supervised learning using full labels, there are a number of challenges that prevent a naive application of the resulting methods in the constraint-based label setting. Therefore, we reason about and analyze these challenges, specifically 1) proposing a \\textit{pseudo-constraining} mechanism to overcome the confirmation bias, a major weakness of pseudo-labeling, 2) developing new methods for pseudo-labeling towards the selection of \\textit{informative} unconstrained samples, 3) showing that this also allows the use of pairwise loss functions for the initial and auxiliary losses which facilitates semi-constrained model training. In extensive experiments, we demonstrate the effectiveness of ConstraintMatch over relevant baselines in both the regular clustering and overclustering scenarios on five challenging benchmarks and provide analyses of its several components.", "url": "https://arxiv.org/abs/2311.15395"}, {"metadata": {"arXiv": "2311.15404", "Date": "Sun, 26 Nov 2023 20:00:53 ", "Title": "Applying statistical learning theory to deep learning", "Authors": ["C\\'edric Gerbelot", "Avetik Karagulyan", "Stefani Karp", "Kavya Ravichandran", "Menachem Stern", "Nathan Srebro"], "Categories": "cs.LG cond-mat.dis-nn stat.ML", "Comments": ["51 pages", "20 figures"]}, "abstract": "Although statistical learning theory provides a robust framework to understand supervised learning, many theoretical aspects of deep learning remain unclear, in particular how different architectures may lead to inductive bias when trained using gradient based methods. The goal of these lectures is to provide an overview of some of the main questions that arise when attempting to understand deep learning from a learning theory perspective. After a brief reminder on statistical learning theory and stochastic optimization, we discuss implicit bias in the context of benign overfitting. We then move to a general description of the mirror descent algorithm, showing how we may go back and forth between a parameter space and the corresponding function space for a given learning problem, as well as how the geometry of the learning problem may be represented by a metric tensor. Building on this framework, we provide a detailed study of the implicit bias of gradient descent on linear diagonal networks for various regression tasks, showing how the loss function, scale of parameters at initialization and depth of the network may lead to various forms of implicit bias, in particular transitioning between kernel or feature learning.", "url": "https://arxiv.org/abs/2311.15404"}, {"metadata": {"arXiv": "2311.15414", "Date": "Sun, 26 Nov 2023 20:35:19 ", "Title": "KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All", "Authors": ["Quyen Tran", "Lam Tran", "Khoat Than", "Toan Tran", "Dinh Phung", "Trung Le"], "Categories": "cs.LG cs.CV"}, "abstract": "Drawing inspiration from prompt tuning techniques applied to Large Language Models, recent methods based on pre-trained ViT networks have achieved remarkable results in the field of Continual Learning. Specifically, these approaches propose to maintain a set of prompts and allocate a subset of them to learn each task using a key-query matching strategy. However, they may encounter limitations when lacking control over the correlations between old task queries and keys of future tasks, the shift of features in the latent space, and the relative separation of latent vectors learned in independent tasks. In this work, we introduce a novel key-query learning strategy based on orthogonal projection, inspired by model-agnostic meta-learning, to enhance prompt matching efficiency and address the challenge of shifting features. Furthermore, we introduce a One-Versus-All (OVA) prototype-based component that enhances the classification head distinction. Experimental results on benchmark datasets demonstrate that our method empowers the model to achieve results surpassing those of current state-of-the-art approaches by a large margin of up to 20%.", "url": "https://arxiv.org/abs/2311.15414"}, {"metadata": {"arXiv": "2311.15419", "Date": "Sun, 26 Nov 2023 21:03:25 ", "Title": "Frobenius-Type Norms and Inner Products of Matrices and Linear Maps with Applications to Neural Network Training", "Authors": ["Roland Herzog and Frederik K\\\"ohne and Leonie Kreis and Anton Schiela"], "Categories": "cs.LG cs.NA math.NA"}, "abstract": "The Frobenius norm is a frequent choice of norm for matrices. In particular, the underlying Frobenius inner product is typically used to evaluate the gradient of an objective with respect to matrix variable, such as those occuring in the training of neural networks. We provide a broader view on the Frobenius norm and inner product for linear maps or matrices, and establish their dependence on inner products in the domain and co-domain spaces. This shows that the classical Frobenius norm is merely one special element of a family of more general Frobenius-type norms. The significant extra freedom furnished by this realization can be used, among other things, to precondition neural network training.", "url": "https://arxiv.org/abs/2311.15419"}, {"metadata": {"arXiv": "2311.15500", "Date": "Mon, 27 Nov 2023 02:55:34 ", "Title": "Function-constrained Program Synthesis", "Authors": ["Patrick Hajali and Ignas Budvytis"], "Categories": "cs.LG cs.CL cs.PL", "Comments": ["17 pages", "6 figures", "2023 NeurIPS R0-Fomo Workshop"]}, "abstract": "This work introduces (1) a technique that allows large language models (LLMs) to leverage user-provided code when solving programming tasks and (2) a method to iteratively generate modular sub-functions that can aid future code generation attempts when the initial code generated by the LLM is inadequate. Generating computer programs in general-purpose programming languages like Python poses a challenge for LLMs when instructed to use code provided in the prompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code completions in real-time by drawing on all code available in a development environment. However, restricting code-specific LLMs to use only in-context code is not straightforward, as the model is not explicitly instructed to use the user-provided code and users cannot highlight precisely which snippets of code the model should incorporate into its context. Moreover, current systems lack effective recovery methods, forcing users to iteratively re-prompt the model with modified prompts until a sufficient solution is reached. Our method differs from traditional LLM-powered code-generation by constraining code-generation to an explicit function set and enabling recovery from failed attempts through automatically generated sub-functions. When the LLM cannot produce working code, we generate modular sub-functions to aid subsequent attempts at generating functional code. A by-product of our method is a library of reusable sub-functions that can solve related tasks, imitating a software team where efficiency scales with experience. We also introduce a new \"half-shot\" evaluation paradigm that provides tighter estimates of LLMs' coding abilities compared to traditional zero-shot evaluation. Our proposed evaluation method encourages models to output solutions in a structured format, decreasing syntax errors that can be mistaken for poor coding ability.", "url": "https://arxiv.org/abs/2311.15500"}, {"metadata": {"arXiv": "2311.15502", "Date": "Mon, 27 Nov 2023 02:59:17 ", "Title": "Learning with Complementary Labels Revisited: A Consistent Approach via Negative-Unlabeled Learning", "Authors": ["Wei Wang", "Takashi Ishida", "Yu-Jie Zhang", "Gang Niu", "Masashi Sugiyama"], "Categories": "cs.LG"}, "abstract": "Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix. However, both conditions may not be satisfied in real-world scenarios. In this paper, we propose a novel complementary-label learning approach that does not rely on these conditions. We find that complementary-label learning can be expressed as a set of negative-unlabeled binary classification problems when using the one-versus-rest strategy. This observation allows us to propose a risk-consistent approach with theoretical guarantees. Furthermore, we introduce a risk correction approach to address overfitting problems when using complex models. We also prove the statistical consistency and convergence rate of the corrected risk estimator. Extensive experimental results on both synthetic and real-world benchmark datasets validate the superiority of our proposed approach over state-of-the-art methods.", "url": "https://arxiv.org/abs/2311.15502"}, {"metadata": {"arXiv": "2311.15570", "Date": "Mon, 27 Nov 2023 06:38:07 ", "Title": "UFDA: Universal Federated Domain Adaptation with Practical Assumptions", "Authors": ["Xinhui Liu", "Zhenghao Chen", "Luping Zhou", "Dong Xu", "Wei Xi", "Gairui Bai", "Yihan Zhao", "and Jizhong Zhao"], "Categories": "cs.LG cs.CV", "Comments": ["Submitted to AAAI2024"]}, "abstract": "Conventional Federated Domain Adaptation (FDA) approaches usually demand an abundance of assumptions, such as label set consistency, which makes them significantly less feasible for real-world situations and introduces security hazards. In this work, we propose a more practical scenario named Universal Federated Domain Adaptation (UFDA). It only requires the black-box model and the label set information of each source domain, while the label sets of different source domains could be inconsistent and the target-domain label set is totally blind. This relaxes the assumptions made by FDA, which are often challenging to meet in real-world cases and diminish model security. To address the UFDA scenario, we propose a corresponding framework called Hot-Learning with Contrastive Label Disambiguation (HCLD), which tackles UFDA's domain shifts and category gaps problem by using one-hot outputs from the black-box models of various source domains. Moreover, to better distinguish the shared and unknown classes, we further present a cluster-level strategy named Mutual-Voting Decision (MVD) to extract robust consensus knowledge across peer classes from both source and target domains. The extensive experiments on three benchmarks demonstrate that our HCLD achieves comparable performance for our UFDA scenario with much fewer assumptions, compared to the previous methodologies with many additional assumptions.", "url": "https://arxiv.org/abs/2311.15570"}, {"metadata": {"arXiv": "2311.15578", "Date": "Mon, 27 Nov 2023 07:11:47 ", "Title": "Experimental Analysis of Large-scale Learnable Vector Storage Compression", "Authors": ["Hailin Zhang", "Penghao Zhao", "Xupeng Miao", "Yingxia Shao", "Zirui Liu", "Tong Yang", "Bin Cui"], "Categories": "cs.LG cs.DB cs.IR"}, "abstract": "Learnable embedding vector is one of the most important applications in machine learning, and is widely used in various database-related domains. However, the high dimensionality of sparse data in recommendation tasks and the huge volume of corpus in retrieval-related tasks lead to a large memory consumption of the embedding table, which poses a great challenge to the training and deployment of models. Recent research has proposed various methods to compress the embeddings at the cost of a slight decrease in model quality or the introduction of other overheads. Nevertheless, the relative performance of these methods remains unclear. Existing experimental comparisons only cover a subset of these methods and focus on limited metrics. In this paper, we perform a comprehensive comparative analysis and experimental evaluation of embedding compression. We introduce a new taxonomy that categorizes these techniques based on their characteristics and methodologies, and further develop a modular benchmarking framework that integrates 14 representative methods. Under a uniform test environment, our benchmark fairly evaluates each approach, presents their strengths and weaknesses under different memory budgets, and recommends the best method based on the use case. In addition to providing useful guidelines, our study also uncovers the limitations of current methods and suggests potential directions for future research.", "url": "https://arxiv.org/abs/2311.15578"}, {"metadata": {"arXiv": "2311.15583", "Date": "Mon, 27 Nov 2023 07:19:23 ", "Title": "A Simple Geometric-Aware Indoor Positioning Interpolation Algorithm Based on Manifold Learning", "Authors": ["Suorong Yang", "Geng Zhang", "Jian Zhao and Furao Shen"], "Categories": "cs.LG eess.SP"}, "abstract": "Interpolation methodologies have been widely used within the domain of indoor positioning systems. However, existing indoor positioning interpolation algorithms exhibit several inherent limitations, including reliance on complex mathematical models, limited flexibility, and relatively low precision. To enhance the accuracy and efficiency of indoor positioning interpolation techniques, this paper proposes a simple yet powerful geometric-aware interpolation algorithm for indoor positioning tasks. The key to our algorithm is to exploit the geometric attributes of the local topological manifold using manifold learning principles. Therefore, instead of constructing complicated mathematical models, the proposed algorithm facilitates the more precise and efficient estimation of points grounded in the local topological manifold. Moreover, our proposed method can be effortlessly integrated into any indoor positioning system, thereby bolstering its adaptability. Through a systematic array of experiments and comprehensive performance analyses conducted on both simulated and real-world datasets, we demonstrate that the proposed algorithm consistently outperforms the most commonly used and representative interpolation approaches regarding interpolation accuracy and efficiency. Furthermore, the experimental results also underscore the substantial practical utility of our method and its potential applicability in real-time indoor positioning scenarios.", "url": "https://arxiv.org/abs/2311.15583"}, {"metadata": {"arXiv": "2311.15609", "Date": "Mon, 27 Nov 2023 08:06:56 ", "Title": "A manometric feature descriptor with linear-SVM to distinguish esophageal contraction vigor", "Authors": ["Jialin Liu", "Lu Yan", "Xiaowei Liu", "Yuzhuo Dai", "Fanggen Lu", "Yuanting Ma", "Muzhou Hou", "Zheng Wang"], "Categories": "cs.LG cs.CV"}, "abstract": "n clinical, if a patient presents with nonmechanical obstructive dysphagia, esophageal chest pain, and gastro esophageal reflux symptoms, the physician will usually assess the esophageal dynamic function. High-resolution manometry (HRM) is a clinically commonly used technique for detection of esophageal dynamic function comprehensively and objectively. However, after the results of HRM are obtained, doctors still need to evaluate by a variety of parameters. This work is burdensome, and the process is complex. We conducted image processing of HRM to predict the esophageal contraction vigor for assisting the evaluation of esophageal dynamic function. Firstly, we used Feature-Extraction and Histogram of Gradients (FE-HOG) to analyses feature of proposal of swallow (PoS) to further extract higher-order features. Then we determine the classification of esophageal contraction vigor normal, weak and failed by using linear-SVM according to these features. Our data set includes 3000 training sets, 500 validation sets and 411 test sets. After verification our accuracy reaches 86.83%, which is higher than other common machine learning methods.", "url": "https://arxiv.org/abs/2311.15609"}, {"metadata": {"arXiv": "2311.15617", "Date": "Mon, 27 Nov 2023 08:28:08 ", "Title": "VeryFL: A Verify Federated Learning Framework Embedded with Blockchain", "Authors": ["Yihao Li", "Yanyi Lai", "Chuan Chen", "Zibin Zheng"], "Categories": "cs.LG cs.CR"}, "abstract": "Blockchain-empowered federated learning (FL) has provoked extensive research recently. Various blockchain-based federated learning algorithm, architecture and mechanism have been designed to solve issues like single point failure and data falsification brought by centralized FL paradigm. Moreover, it is easier to allocate incentives to nodes with the help of the blockchain. Various centralized federated learning frameworks like FedML, have emerged in the community to help boost the research on FL. However, decentralized blockchain-based federated learning framework is still missing, which cause inconvenience for researcher to reproduce or verify the algorithm performance based on blockchain. Inspired by the above issues, we have designed and developed a blockchain-based federated learning framework by embedding Ethereum network. This report will present the overall structure of this framework, which proposes a code practice paradigm for the combination of FL with blockchain and, at the same time, compatible with normal FL training task. In addition to implement some blockchain federated learning algorithms on smart contract to help execute a FL training, we also propose a model ownership authentication architecture based on blockchain and model watermarking to protect the intellectual property rights of models. These mechanism on blockchain shows an underlying support of blockchain for federated learning to provide a verifiable training, aggregation and incentive distribution procedure and thus we named this framework VeryFL (A Verify Federated Learninig Framework Embedded with Blockchain). The source code is avaliable on https://github.com/GTMLLab/VeryFL.", "url": "https://arxiv.org/abs/2311.15617"}, {"metadata": {"arXiv": "2311.15647", "Date": "Mon, 27 Nov 2023 09:19:01 ", "Title": "Bandits Meet Mechanism Design to Combat Clickbait in Online Recommendation", "Authors": ["Thomas Kleine Buening and Aadirupa Saha and Christos Dimitrakakis and Haifeng Xu"], "Categories": "cs.LG cs.GT"}, "abstract": "We study a strategic variant of the multi-armed bandit problem, which we coin the strategic click-bandit. This model is motivated by applications in online recommendation where the choice of recommended items depends on both the click-through rates and the post-click rewards. Like in classical bandits, rewards follow a fixed unknown distribution. However, we assume that the click-rate of each arm is chosen strategically by the arm (e.g., a host on Airbnb) in order to maximize the number of times it gets clicked. The algorithm designer does not know the post-click rewards nor the arms' actions (i.e., strategically chosen click-rates) in advance, and must learn both values over time. To solve this problem, we design an incentive-aware learning algorithm, UCB-S, which achieves two goals simultaneously: (a) incentivizing desirable arm behavior under uncertainty; (b) minimizing regret by learning unknown parameters. We characterize all approximate Nash equilibria among arms under UCB-S and show a $\\tilde{\\mathcal{O}} (\\sqrt{KT})$ regret bound uniformly in every equilibrium. We also show that incentive-unaware algorithms generally fail to achieve low regret in the strategic click-bandit. Finally, we support our theoretical results by simulations of strategic arm behavior which confirm the effectiveness and robustness of our proposed incentive design.", "url": "https://arxiv.org/abs/2311.15647"}, {"metadata": {"arXiv": "2311.15673", "Date": "Mon, 27 Nov 2023 10:02:12 ", "Title": "Accelerating Hierarchical Associative Memory: A Deep Equilibrium Approach", "Authors": ["C\\'edric Goemaere", "Johannes Deleu", "Thomas Demeester"], "Categories": "cs.LG cs.NE", "Comments": ["Accepted at the \"Associative Memory & Hopfield Networks'' workshop at NeurIPS", "2023"]}, "abstract": "Hierarchical Associative Memory models have recently been proposed as a versatile extension of continuous Hopfield networks. In order to facilitate future research on such models, especially at scale, we focus on increasing their simulation efficiency on digital hardware. In particular, we propose two strategies to speed up memory retrieval in these models, which corresponds to their use at inference, but is equally important during training. First, we show how they can be cast as Deep Equilibrium Models, which allows using faster and more stable solvers. Second, inspired by earlier work, we show that alternating optimization of the even and odd layers accelerates memory retrieval by a factor close to two. Combined, these two techniques allow for a much faster energy minimization, as shown in our proof-of-concept experimental results. The code is available at https://github.com/cgoemaere/hamdeq", "url": "https://arxiv.org/abs/2311.15673"}, {"metadata": {"arXiv": "2311.15682", "Date": "Mon, 27 Nov 2023 10:16:22 ", "Title": "Information theoretic study of the neural geometry induced by category learning", "Authors": ["Laurent Bonnasse-Gahot and Jean-Pierre Nadal"], "Categories": "cs.LG cs.IT math.IT q-bio.NC", "Comments": ["7 pages", "2 figures", "Accepted (Oral) to InfoCog@NeurIPS 2023"]}, "abstract": "Categorization is an important topic both for biological and artificial neural networks. Here, we take an information theoretic approach to assess the efficiency of the representations induced by category learning. We show that one can decompose the relevant Bayesian cost into two components, one for the coding part and one for the decoding part. Minimizing the coding cost implies maximizing the mutual information between the set of categories and the neural activities. We analytically show that this mutual information can be written as the sum of two terms that can be interpreted as (i) finding an appropriate representation space, and, (ii) building a representation with the appropriate metrics, based on the neural Fisher information on this space. One main consequence is that category learning induces an expansion of neural space near decision boundaries. Finally, we provide numerical illustrations that show how Fisher information of the coding neural population aligns with the boundaries between categories.", "url": "https://arxiv.org/abs/2311.15682"}, {"metadata": {"arXiv": "2311.15691", "Date": "Mon, 27 Nov 2023 10:28:44 ", "Title": "Automated discovery of trade-off between utility, privacy and fairness in machine learning models", "Authors": ["Bogdan Ficiu", "Neil D. Lawrence", "Andrei Paleyes"], "Categories": "cs.LG cs.CR cs.CY", "Comments": ["3rd Workshop on Bias and Fairness in AI (BIAS)", "ECML 2023"]}, "abstract": "Machine learning models are deployed as a central component in decision making and policy operations with direct impact on individuals' lives. In order to act ethically and comply with government regulations, these models need to make fair decisions and protect the users' privacy. However, such requirements can come with decrease in models' performance compared to their potentially biased, privacy-leaking counterparts. Thus the trade-off between fairness, privacy and performance of ML models emerges, and practitioners need a way of quantifying this trade-off to enable deployment decisions. In this work we interpret this trade-off as a multi-objective optimization problem, and propose PFairDP, a pipeline that uses Bayesian optimization for discovery of Pareto-optimal points between fairness, privacy and utility of ML models. We show how PFairDP can be used to replicate known results that were achieved through manual constraint setting process. We further demonstrate effectiveness of PFairDP with experiments on multiple models and datasets.", "url": "https://arxiv.org/abs/2311.15691"}, {"metadata": {"arXiv": "2311.15756", "Date": "Mon, 27 Nov 2023 12:22:44 ", "Title": "Learning Multi-Frequency Partial Correlation Graphs", "Authors": ["Gabriele D'Acunto", "Paolo Di Lorenzo", "Francesco Bonchi", "Stefania Sardellitti and Sergio Barbarossa"], "Categories": "cs.LG eess.SP stat.ML"}, "abstract": "Despite the large research effort devoted to learning dependencies between time series, the state of the art still faces a major limitation: existing methods learn partial correlations but fail to discriminate across distinct frequency bands. Motivated by many applications in which this differentiation is pivotal, we overcome this limitation by learning a block-sparse, frequency-dependent, partial correlation graph, in which layers correspond to different frequency bands, and partial correlations can occur over just a few layers. To this aim, we formulate and solve two nonconvex learning problems: the first has a closed-form solution and is suitable when there is prior knowledge about the number of partial correlations; the second hinges on an iterative solution based on successive convex approximation, and is effective for the general case where no prior knowledge is available. Numerical results on synthetic data show that the proposed methods outperform the current state of the art. Finally, the analysis of financial time series confirms that partial correlations exist only within a few frequency bands, underscoring how our methods enable the gaining of valuable insights that would be undetected without discriminating along the frequency domain.", "url": "https://arxiv.org/abs/2311.15756"}, {"metadata": {"arXiv": "2311.15772", "Date": "Mon, 27 Nov 2023 12:44:42 ", "Title": "Attend Who is Weak: Enhancing Graph Condensation via Cross-Free Adversarial Training", "Authors": ["Xinglin Li", "Kun Wang", "Hanhui Deng", "Yuxuan Liang", "Di Wu"], "Categories": "cs.LG"}, "abstract": "In this paper, we study the \\textit{graph condensation} problem by compressing the large, complex graph into a concise, synthetic representation that preserves the most essential and discriminative information of structure and features. We seminally propose the concept of Shock Absorber (a type of perturbation) that enhances the robustness and stability of the original graphs against changes in an adversarial training fashion. Concretely, (I) we forcibly match the gradients between pre-selected graph neural networks (GNNs) trained on a synthetic, simplified graph and the original training graph at regularly spaced intervals. (II) Before each update synthetic graph point, a Shock Absorber serves as a gradient attacker to maximize the distance between the synthetic dataset and the original graph by selectively perturbing the parts that are underrepresented or insufficiently informative. We iteratively repeat the above two processes (I and II) in an adversarial training fashion to maintain the highly-informative context without losing correlation with the original dataset. More importantly, our shock absorber and the synthesized graph parallelly share the backward process in a free training manner. Compared to the original adversarial training, it introduces almost no additional time overhead. We validate our framework across 8 datasets (3 graph and 5 node classification datasets) and achieve prominent results: for example, on Cora, Citeseer and Ogbn-Arxiv, we can gain nearly 1.13% to 5.03% improvements compare with SOTA models. Moreover, our algorithm adds only about 0.2% to 2.2% additional time overhead over Flicker, Citeseer and Ogbn-Arxiv. Compared to the general adversarial training, our approach improves time efficiency by nearly 4-fold.", "url": "https://arxiv.org/abs/2311.15772"}, {"metadata": {"arXiv": "2311.15782", "Date": "Mon, 27 Nov 2023 12:55:39 ", "Title": "Relationship between Model Compression and Adversarial Robustness: A Review of Current Evidence", "Authors": ["Svetlana Pavlitska", "Hannes Grolig and J. Marius Z\\\"ollner"], "Categories": "cs.LG cs.CV", "Comments": ["Accepted for publication at SSCI 2023"]}, "abstract": "Increasing the model capacity is a known approach to enhance the adversarial robustness of deep learning networks. On the other hand, various model compression techniques, including pruning and quantization, can reduce the size of the network while preserving its accuracy. Several recent studies have addressed the relationship between model compression and adversarial robustness, while some experiments have reported contradictory results. This work summarizes available evidence and discusses possible explanations for the observed effects.", "url": "https://arxiv.org/abs/2311.15782"}, {"metadata": {"arXiv": "2311.15792", "Date": "Mon, 27 Nov 2023 13:14:39 ", "Title": "Rethinking Privacy in Machine Learning Pipelines from an Information Flow Control Perspective", "Authors": ["Lukas Wutschitz", "Boris K\\\"opf", "Andrew Paverd", "Saravan Rajmohan", "Ahmed Salem", "Shruti Tople", "Santiago Zanella-B\\'eguelin", "Menglin Xia", "Victor R\\\"uhle"], "Categories": "cs.LG cs.CR"}, "abstract": "Modern machine learning systems use models trained on ever-growing corpora. Typically, metadata such as ownership, access control, or licensing information is ignored during training. Instead, to mitigate privacy risks, we rely on generic techniques such as dataset sanitization and differentially private model training, with inherent privacy/utility trade-offs that hurt model performance. Moreover, these techniques have limitations in scenarios where sensitive information is shared across multiple participants and fine-grained access control is required. By ignoring metadata, we therefore miss an opportunity to better address security, privacy, and confidentiality challenges. In this paper, we take an information flow control perspective to describe machine learning systems, which allows us to leverage metadata such as access control policies and define clear-cut privacy and confidentiality guarantees with interpretable information flows. Under this perspective, we contrast two different approaches to achieve user-level non-interference: 1) fine-tuning per-user models, and 2) retrieval augmented models that access user-specific datasets at inference time. We compare these two approaches to a trivially non-interfering zero-shot baseline using a public model and to a baseline that fine-tunes this model on the whole corpus. We evaluate trained models on two datasets of scientific articles and demonstrate that retrieval augmented architectures deliver the best utility, scalability, and flexibility while satisfying strict non-interference guarantees.", "url": "https://arxiv.org/abs/2311.15792"}, {"metadata": {"arXiv": "2311.15807", "Date": "Mon, 27 Nov 2023 13:30:20 ", "Title": "Exploring Artificial Intelligence Methods for Energy Prediction in Healthcare Facilities: An In-Depth Extended Systematic Review", "Authors": ["Marjan FatehiJananloo", "Helen Stopps", "J.J. McArthur"], "Categories": "cs.LG cs.SY eess.SY", "Comments": ["38 pages", "1 figure", "3 tables", "systematic literature review"], "ACM-class": "A.1; I.2; J.2"}, "abstract": "Hospitals, due to their complexity and unique requirements, play a pivotal role in global energy consumption patterns. This study conducted a comprehensive literature review, utilizing the PRISMA framework, of articles that employed machine learning and artificial intelligence techniques for predicting energy consumption in hospital buildings. Of the 1884 publications identified, 17 were found to address this specific domain and have been thoroughly reviewed to establish the state-of-the-art and identify gaps where future research is needed. This review revealed a diverse range of data inputs influencing energy prediction, with occupancy and meteorological data emerging as significant predictors. However, many studies failed to delve deep into the implications of their data choices, and gaps were evident regarding the understanding of time dynamics, operational status, and preprocessing methods. Machine learning, especially deep learning models like ANNs, have shown potential in this domain, yet they come with challenges, including interpretability and computational demands. The findings underscore the immense potential of AI in optimizing hospital energy consumption but also highlight the need for more comprehensive and granular research. Key areas for future research include the optimization of ANN approaches, new optimization and data integration techniques, the integration of real-time data into Intelligent Energy Management Systems, and increasing focus on long-term energy forecasting.", "url": "https://arxiv.org/abs/2311.15807"}, {"metadata": {"arXiv": "2311.15831", "Date": "Mon, 27 Nov 2023 13:55:21 ", "Title": "Temporal Action Localization for Inertial-based Human Activity Recognition", "Authors": ["Marius Bock", "Michael Moeller", "Kristof Van Laerhoven"], "Categories": "cs.LG cs.HC eess.SP", "Comments": ["20 pages", "7 figures", "2 tables"]}, "abstract": "A persistent trend in Deep Learning has been the applicability of machine learning concepts to other areas than originally introduced for. As of today, state-of-the-art activity recognition from wearable sensors relies on classifiers being trained on fixed windows of data. Contrarily, video-based Human Activity Recognition has followed a segment-based prediction approach, localizing activity occurrences from start to end. This paper is the first to systematically demonstrate the applicability of state-of-the-art TAL models for wearable Human Activity Recongition (HAR) using raw inertial data as input. Our results show that state-of-the-art TAL models are able to outperform popular inertial models on 4 out of 6 wearable activity recognition benchmark datasets, with improvements ranging as much as 25% in F1-score. Introducing the TAL community's most popular metric to inertial-based HAR, namely mean Average Precision, our analysis shows that TAL models are able to produce more coherent segments along with an overall higher NULL-class accuracy across all datasets. Being the first to provide such an analysis, the TAL community offers an interesting new perspective to inertial-based HAR with yet to be explored design choices and training concepts, which could be of significant value for the inertial-based HAR community.", "url": "https://arxiv.org/abs/2311.15831"}, {"metadata": {"arXiv": "2311.15854", "Date": "Mon, 27 Nov 2023 14:21:47 ", "Title": "A systematic study comparing hyperparameter optimization engines on tabular data", "Authors": ["Balazs Kegl"], "Categories": "cs.LG"}, "abstract": "We run an independent comparison of all hyperparameter optimization (hyperopt) engines available in the Ray Tune library. We introduce two ways to normalize and aggregate statistics across data sets and models, one rank-based, and another one sandwiching the score between the random search score and the full grid search score. This affords us i) to rank the hyperopt engines, ii) to make generalized and statistically significant statements on how much they improve over random search, and iii) to make recommendations on which engine should be used to hyperopt a given learning algorithm. We find that most engines beat random search, but that only three of them (HEBO, AX, and BlendSearch) clearly stand out. We also found that some engines seem to specialize in hyperopting certain learning algorithms, which makes it tricky to use hyperopt in comparison studies, since the choice of the hyperopt technique may favor some of the models in the comparison.", "url": "https://arxiv.org/abs/2311.15854"}, {"metadata": {"arXiv": "2311.15887", "Date": "Mon, 27 Nov 2023 14:55:16 ", "Title": "FLASC: A Flare-Sensitive Clustering Algorithm: Extending HDBSCAN* for Detecting Branches in Clusters", "Authors": ["D. M. Bot", "J. Peeters", "J. Liesenborgs", "J. Aerts"], "Categories": "cs.LG cs.DB", "Comments": ["20 pages", "11 figures", "submitted to ACM TKDD"], "ACM-class": "I.5.3; H.3.3"}, "abstract": "We present FLASC, an algorithm for flare-sensitive clustering. Our algorithm builds upon HDBSCAN* -- which provides high-quality density-based clustering performance -- through a post-processing step that differentiates branches within the detected clusters' manifold, adding a type of pattern that can be discovered. Two variants of the algorithm are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* in terms of computational cost and provide stable outputs using synthetic data sets, resulting in an efficient flare-sensitive clustering algorithm. In addition, we demonstrate the algorithm's benefit in data exploration over HDBSCAN* clustering on two real-world data sets.", "url": "https://arxiv.org/abs/2311.15887"}, {"metadata": {"arXiv": "2311.15890", "Date": "Mon, 27 Nov 2023 14:56:47 ", "Title": "Stability-Informed Initialization of Neural Ordinary Differential Equations", "Authors": ["Theodor Westny and Arman Mohammadi and Daniel Jung and Erik Frisk"], "Categories": "cs.LG cs.CV"}, "abstract": "This paper addresses the training of Neural Ordinary Differential Equations (neural ODEs), and in particular explores the interplay between numerical integration techniques, stability regions, step size, and initialization techniques. It is shown how the choice of integration technique implicitly regularizes the learned model, and how the solver's corresponding stability region affects training and prediction performance. From this analysis, a stability-informed parameter initialization technique is introduced. The effectiveness of the initialization method is displayed across several learning benchmarks and industrial applications.", "url": "https://arxiv.org/abs/2311.15890"}, {"metadata": {"arXiv": "2311.15940", "Date": "Mon, 27 Nov 2023 15:47:33 ", "Title": "Physics-informed neural networks for transformed geometries and manifolds", "Authors": ["Samuel Burbulla"], "Categories": "cs.LG cs.CE"}, "abstract": "Physics-informed neural networks (PINNs) effectively embed physical principles into machine learning, but often struggle with complex or alternating geometries. We propose a novel method for integrating geometric transformations within PINNs to robustly accommodate geometric variations. Our method incorporates a diffeomorphism as a mapping of a reference domain and adapts the derivative computation of the physics-informed loss function. This generalizes the applicability of PINNs not only to smoothly deformed domains, but also to lower-dimensional manifolds and allows for direct shape optimization while training the network. We demonstrate the effectivity of our approach on several problems: (i) Eikonal equation on Archimedean spiral, (ii) Poisson problem on surface manifold, (iii) Incompressible Stokes flow in deformed tube, and (iv) Shape optimization with Laplace operator. Through these examples, we demonstrate the enhanced flexibility over traditional PINNs, especially under geometric variations. The proposed framework presents an outlook for training deep neural operators over parametrized geometries, paving the way for advanced modeling with PDEs on complex geometries in science and engineering.", "url": "https://arxiv.org/abs/2311.15940"}, {"metadata": {"arXiv": "2311.15945", "Date": "Mon, 27 Nov 2023 15:51:07 ", "Title": "Over-Squashing in Riemannian Graph Neural Networks", "Authors": ["Julia Balla"], "Categories": "cs.LG"}, "abstract": "Most graph neural networks (GNNs) are prone to the phenomenon of over-squashing in which node features become insensitive to information from distant nodes in the graph. Recent works have shown that the topology of the graph has the greatest impact on over-squashing, suggesting graph rewiring approaches as a suitable solution. In this work, we explore whether over-squashing can be mitigated through the embedding space of the GNN. In particular, we consider the generalization of Hyperbolic GNNs (HGNNs) to Riemannian manifolds of variable curvature in which the geometry of the embedding space is faithful to the graph's topology. We derive bounds on the sensitivity of the node features in these Riemannian GNNs as the number of layers increases, which yield promising theoretical and empirical results for alleviating over-squashing in graphs with negative curvature.", "url": "https://arxiv.org/abs/2311.15945"}, {"metadata": {"arXiv": "2311.15947", "Date": "Mon, 27 Nov 2023 15:54:20 ", "Title": "GloNets: Globally Connected Neural Networks", "Authors": ["Antonio Di Cecco", "Carlo Metta", "Marco Fantozzi", "Francesco Morandin", "Maurizio Parton"], "Categories": "cs.LG cs.NE"}, "abstract": "Deep learning architectures suffer from depth-related performance degradation, limiting the effective depth of neural networks. Approaches like ResNet are able to mitigate this, but they do not completely eliminate the problem. We introduce Globally Connected Neural Networks (GloNet), a novel architecture overcoming depth-related issues, designed to be superimposed on any model, enhancing its depth without increasing complexity or reducing performance. With GloNet, the network's head uniformly receives information from all parts of the network, regardless of their level of abstraction. This enables GloNet to self-regulate information flow during training, reducing the influence of less effective deeper layers, and allowing for stable training irrespective of network depth. This paper details GloNet's design, its theoretical basis, and a comparison with existing similar architectures. Experiments show GloNet's self-regulation ability and resilience to depth-related learning challenges, like performance degradation. Our findings suggest GloNet as a strong alternative to traditional architectures like ResNets.", "url": "https://arxiv.org/abs/2311.15947"}, {"metadata": {"arXiv": "2311.15990", "Date": "Mon, 27 Nov 2023 16:39:55 ", "Title": "Should We Learn Most Likely Functions or Parameters?", "Authors": ["Shikai Qiu", "Tim G. J. Rudner", "Sanyam Kapoor", "Andrew Gordon Wilson"], "Categories": "cs.LG stat.ML", "Comments": ["NeurIPS 2023. Code available at https://github.com/activatedgeek/function-space-map"]}, "abstract": "Standard regularized training procedures correspond to maximizing a posterior distribution over parameters, known as maximum a posteriori (MAP) estimation. However, model parameters are of interest only insomuch as they combine with the functional form of a model to provide a function that can make good predictions. Moreover, the most likely parameters under the parameter posterior do not generally correspond to the most likely function induced by the parameter posterior. In fact, we can re-parametrize a model such that any setting of parameters can maximize the parameter posterior. As an alternative, we investigate the benefits and drawbacks of directly estimating the most likely function implied by the model and the data. We show that this procedure leads to pathological solutions when using neural networks and prove conditions under which the procedure is well-behaved, as well as a scalable approximation. Under these conditions, we find that function-space MAP estimation can lead to flatter minima, better generalization, and improved robustness to overfitting.", "url": "https://arxiv.org/abs/2311.15990"}, {"metadata": {"arXiv": "2311.15995", "Date": "Mon, 27 Nov 2023 16:44:13 ", "Title": "Sensitivity-Based Layer Insertion for Residual and Feedforward Neural Networks", "Authors": ["Evelyn Herberg and Roland Herzog and Frederik K\\\"ohne and Leonie Kreis and Anton Schiela"], "Categories": "cs.LG math.OC"}, "abstract": "The training of neural networks requires tedious and often manual tuning of the network architecture. We propose a systematic method to insert new layers during the training process, which eliminates the need to choose a fixed network size before training. Our technique borrows techniques from constrained optimization and is based on first-order sensitivity information of the objective with respect to the virtual parameters that additional layers, if inserted, would offer. We consider fully connected feedforward networks with selected activation functions as well as residual neural networks. In numerical experiments, the proposed sensitivity-based layer insertion technique exhibits improved training decay, compared to not inserting the layer. Furthermore, the computational effort is reduced in comparison to inserting the layer from the beginning. The code is available at \\url{https://github.com/LeonieKreis/layer_insertion_sensitivity_based}.", "url": "https://arxiv.org/abs/2311.15995"}, {"metadata": {"arXiv": "2311.15996", "Date": "Mon, 27 Nov 2023 16:44:50 ", "Title": "Closing the ODE-SDE gap in score-based diffusion models through the Fokker-Planck equation", "Authors": ["Teo Deveney", "Jan Stanczuk", "Lisa Maria Kreusser", "Chris Budd", "Carola-Bibiane Sch\\\"onlieb"], "Categories": "cs.LG cs.NA math.NA stat.ML"}, "abstract": "Score-based diffusion models have emerged as one of the most promising frameworks for deep generative modelling, due to their state-of-the art performance in many generation tasks while relying on mathematical foundations such as stochastic differential equations (SDEs) and ordinary differential equations (ODEs). Empirically, it has been reported that ODE based samples are inferior to SDE based samples. In this paper we rigorously describe the range of dynamics and approximations that arise when training score-based diffusion models, including the true SDE dynamics, the neural approximations, the various approximate particle dynamics that result, as well as their associated Fokker--Planck equations and the neural network approximations of these Fokker--Planck equations. We systematically analyse the difference between the ODE and SDE dynamics of score-based diffusion models, and link it to an associated Fokker--Planck equation. We derive a theoretical upper bound on the Wasserstein 2-distance between the ODE- and SDE-induced distributions in terms of a Fokker--Planck residual. We also show numerically that conventional score-based diffusion models can exhibit significant differences between ODE- and SDE-induced distributions which we demonstrate using explicit comparisons. Moreover, we show numerically that reducing the Fokker--Planck residual by adding it as an additional regularisation term leads to closing the gap between ODE- and SDE-induced distributions. Our experiments suggest that this regularisation can improve the distribution generated by the ODE, however that this can come at the cost of degraded SDE sample quality.", "url": "https://arxiv.org/abs/2311.15996"}, {"metadata": {"arXiv": "2311.16008", "Date": "Mon, 27 Nov 2023 17:02:56 ", "Title": "Using Decentralized Aggregation for Federated Learning with Differential Privacy", "Authors": ["Hadeel Abd El-Kareem and Abd El-Moaty Saleh and Ana Fern\\'andez-Vilas and Manuel Fern\\'andez-Veiga and asser El-Sonbaty"], "Categories": "cs.LG cs.CR", "Journal-ref": "PE-WASUN '22: Proceedings of the 19th ACM International Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, & Ubiquitous NetworksOctober 2022", "DOI": "10.1145/3551663.3558682"}, "abstract": "Nowadays, the ubiquitous usage of mobile devices and networks have raised concerns about the loss of control over personal data and research advance towards the trade-off between privacy and utility in scenarios that combine exchange communications, big databases and distributed and collaborative (P2P) Machine Learning techniques. On the other hand, although Federated Learning (FL) provides some level of privacy by retaining the data at the local node, which executes a local training to enrich a global model, this scenario is still susceptible to privacy breaches as membership inference attacks. To provide a stronger level of privacy, this research deploys an experimental environment for FL with Differential Privacy (DP) using benchmark datasets. The obtained results show that the election of parameters and techniques of DP is central in the aforementioned trade-off between privacy and utility by means of a classification example.", "url": "https://arxiv.org/abs/2311.16008"}, {"metadata": {"arXiv": "2311.16021", "Date": "Mon, 27 Nov 2023 17:35:28 ", "Title": "Scheduling and Communication Schemes for Decentralized Federated Learning", "Authors": ["Bahaa-Eldin Ali Abdelghany and Ana Fern\\'andez-Vilas and Manuel Fern\\'andez-Veiga and Nashwa El-Bendary and Ammar M. Hassan and Walid M. Abdelmoez"], "Categories": "cs.LG cs.DC", "Comments": ["32nd International Conference on Computer Theory and Applications (ICCTA)", "Alexandria", "Egypt", "2022"], "DOI": "10.1109/ICCTA58027.2022.10206255."}, "abstract": "Federated learning (FL) is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. One central server is not enough, due to problems of connectivity with clients. In this paper, a decentralized federated learning (DFL) model with the stochastic gradient descent (SGD) algorithm has been introduced, as a more scalable approach to improve the learning performance in a network of agents with arbitrary topology. Three scheduling policies for DFL have been proposed for communications between the clients and the parallel servers, and the convergence, accuracy, and loss have been tested in a totally decentralized mplementation of SGD. The experimental results show that the proposed scheduling polices have an impact both on the speed of convergence and in the final global model.", "url": "https://arxiv.org/abs/2311.16021"}, {"metadata": {"arXiv": "2311.16026", "Date": "Mon, 27 Nov 2023 17:40:02 ", "Title": "A Neural Framework for Generalized Causal Sensitivity Analysis", "Authors": ["Dennis Frauen", "Fergus Imrie", "Alicia Curth", "Valentyn Melnychuk", "Stefan Feuerriegel", "Mihaela van der Schaar"], "Categories": "cs.LG stat.ML"}, "abstract": "Unobserved confounding is common in many applications, making causal inference from observational data challenging. As a remedy, causal sensitivity analysis is an important tool to draw causal conclusions under unobserved confounding with mathematical guarantees. In this paper, we propose NeuralCSA, a neural framework for generalized causal sensitivity analysis. Unlike previous work, our framework is compatible with (i) a large class of sensitivity models, including the marginal sensitivity model, f-sensitivity models, and Rosenbaum's sensitivity model; (ii) different treatment types (i.e., binary and continuous); and (iii) different causal queries, including (conditional) average treatment effects and simultaneous effects on multiple outcomes. The generality of \\frameworkname is achieved by learning a latent distribution shift that corresponds to a treatment intervention using two conditional normalizing flows. We provide theoretical guarantees that NeuralCSA is able to infer valid bounds on the causal query of interest and also demonstrate this empirically using both simulated and real-world data.", "url": "https://arxiv.org/abs/2311.16026"}, {"metadata": {"arXiv": "2311.16054", "Date": "Mon, 27 Nov 2023 18:19:07 ", "Title": "Metric Space Magnitude for Evaluating Unsupervised Representation Learning", "Authors": ["Katharina Limbeck", "Rayna Andreeva", "Rik Sarkar", "Bastian Rieck"], "Categories": "cs.LG math.GT stat.ML"}, "abstract": "The magnitude of a metric space was recently established as a novel invariant, providing a measure of the `effective size' of a space across multiple scales. By capturing both geometrical and topological properties of data, magnitude is poised to address challenges in unsupervised representation learning tasks. We formalise a novel notion of dissimilarity between magnitude functions of finite metric spaces and use them to derive a quality measure for dimensionality reduction tasks. Our measure is provably stable under perturbations of the data, can be efficiently calculated, and enables a rigorous multi-scale comparison of embeddings. We show the utility of our measure in an experimental suite that comprises different domains and tasks, including the comparison of data visualisations.", "url": "https://arxiv.org/abs/2311.16054"}, {"metadata": {"arXiv": "2311.16093", "Date": "Mon, 27 Nov 2023 18:58:34 ", "Title": "Have we built machines that think like people?", "Authors": ["Luca M. Schulze Buschoff", "Elif Akata", "Matthias Bethge", "Eric Schulz"], "Categories": "cs.LG"}, "abstract": "A chief goal of artificial intelligence is to build machines that think like people. Yet it has been argued that deep neural network architectures fail to accomplish this. Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology. Yet recent advancements, namely the rise of large language models, particularly those designed for visual processing, have rekindled interest in the potential to emulate human-like cognitive abilities. This paper evaluates the current state of vision-based large language models in the domains of intuitive physics, causal reasoning, and intuitive psychology. Through a series of controlled experiments, we investigate the extent to which these modern models grasp complex physical interactions, causal relationships, and intuitive understanding of others' preferences. Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas. The models exhibit a rudimentary understanding of physical laws and causal relationships, but their performance is hindered by a lack of deeper insights-a key aspect of human cognition. Furthermore, in tasks requiring an intuitive theory of mind, the models fail altogether. Our results emphasize the need for integrating more robust mechanisms for understanding causality, physical dynamics, and social cognition into modern-day, vision-based language models, and point out the importance of cognitively-inspired benchmarks.", "url": "https://arxiv.org/abs/2311.16093"}, {"metadata": {"arXiv": "2311.15327", "Date": "Sun, 26 Nov 2023 15:11:17 ", "Title": "FRAC-Q-Learning: A Reinforcement Learning with Boredom Avoidance Processes for Social Robots", "Authors": ["Akinari Onishi"], "Categories": "cs.RO cs.HC cs.LG"}, "abstract": "The reinforcement learning algorithms have often been applied to social robots. However, most reinforcement learning algorithms were not optimized for the use of social robots, and consequently they may bore users. We proposed a new reinforcement learning method specialized for the social robot, the FRAC-Q-learning, that can avoid user boredom. The proposed algorithm consists of a forgetting process in addition to randomizing and categorizing processes. This study evaluated interest and boredom hardness scores of the FRAC-Q-learning by a comparison with the traditional Q-learning. The FRAC-Q-learning showed significantly higher trend of interest score, and indicated significantly harder to bore users compared to the traditional Q-learning. Therefore, the FRAC-Q-learning can contribute to develop a social robot that will not bore users. The proposed algorithm can also find applications in Web-based communication and educational systems. This paper presents the entire process, detailed implementation and a detailed evaluation method of the of the FRAC-Q-learning for the first time.", "url": "https://arxiv.org/abs/2311.15327"}, {"metadata": {"arXiv": "2311.15168", "Date": "Sun, 26 Nov 2023 02:52:37 ", "Title": "A Data-Driven Approach for High-Impedance Fault Localization in Distribution Systems", "Authors": ["Yuqi Zhou", "Yuqing Dong and Rui Yang"], "Categories": "eess.SY cs.LG cs.SY"}, "abstract": "Accurate and quick identification of high-impedance faults is critical for the reliable operation of distribution systems. Unlike other faults in power grids, HIFs are very difficult to detect by conventional overcurrent relays due to the low fault current. Although HIFs can be affected by various factors, the voltage current characteristics can substantially imply how the system responds to the disturbance and thus provides opportunities to effectively localize HIFs. In this work, we propose a data-driven approach for the identification of HIF events. To tackle the nonlinearity of the voltage current trajectory, first, we formulate optimization problems to approximate the trajectory with piecewise functions. Then we collect the function features of all segments as inputs and use the support vector machine approach to efficiently identify HIFs at different locations. Numerical studies on the IEEE 123-node test feeder demonstrate the validity and accuracy of the proposed approach for real-time HIF identification.", "url": "https://arxiv.org/abs/2311.15168"}, {"metadata": {"arXiv": "2311.15216", "Date": "Sun, 26 Nov 2023 07:17:45 ", "Title": "Solve Large-scale Unit Commitment Problems by Physics-informed Graph Learning", "Authors": ["Jingtao Qin", "Nanpeng Yu"], "Categories": "eess.SY cs.LG cs.SY"}, "abstract": "Unit commitment (UC) problems are typically formulated as mixed-integer programs (MIP) and solved by the branch-and-bound (B&B) scheme. The recent advances in graph neural networks (GNN) enable it to enhance the B&B algorithm in modern MIP solvers by learning to dive and branch. Existing GNN models that tackle MIP problems are mostly constructed from mathematical formulation, which is computationally expensive when dealing with large-scale UC problems. In this paper, we propose a physics-informed hierarchical graph convolutional network (PI-GCN) for neural diving that leverages the underlying features of various components of power systems to find high-quality variable assignments. Furthermore, we adopt the MIP model-based graph convolutional network (MB-GCN) for neural branching to select the optimal variables for branching at each node of the B&B tree. Finally, we integrate neural diving and neural branching into a modern MIP solver to establish a novel neural MIP solver designed for large-scale UC problems. Numeral studies show that PI-GCN has better performance and scalability than the baseline MB-GCN on neural diving. Moreover, the neural MIP solver yields the lowest operational cost and outperforms a modern MIP solver for all testing days after combining it with our proposed neural diving model and the baseline neural branching model.", "url": "https://arxiv.org/abs/2311.15216"}, {"metadata": {"arXiv": "2311.15875", "Date": "Mon, 27 Nov 2023 14:48:37 ", "Title": "Nodal Hydraulic Head Estimation through Unscented Kalman Filter for Data-driven Leak Localization in Water Networks", "Authors": ["Luis Romero-Ben", "Paul Irofti", "Florin Stoican and Vicen\\c{c} Puig"], "Categories": "eess.SY cs.LG cs.NA cs.SY math.NA", "Comments": ["This work has been submitted to IFAC for possible publication. It has 6 pages and 3 figures"]}, "abstract": "In this paper, we present a nodal hydraulic head estimation methodology for water distribution networks (WDN) based on an Unscented Kalman Filter (UKF) scheme with application to leak localization. The UKF refines an initial estimation of the hydraulic state by considering the prediction model, as well as available pressure and demand measurements. To this end, it provides customized prediction and data assimilation steps. Additionally, the method is enhanced by dynamically updating the prediction function weight matrices. Performance testing on the Modena benchmark under realistic conditions demonstrates the method's effectiveness in enhancing state estimation and data-driven leak localization.", "url": "https://arxiv.org/abs/2311.15875"}, {"metadata": {"arXiv": "2311.15162", "Date": "Sun, 26 Nov 2023 01:55:55 ", "Title": "Domain Knowledge Injection in Bayesian Search for New Materials", "Authors": ["Zikai Xie", "Xenophon Evangelopoulos", "Joseph Thacker", "Andrew Cooper"], "Categories": "cs.AI", "Comments": ["8 pages", "5 figures", "published in ECAI23"], "MSC-class": "68W99", "ACM-class": "I.2.8", "Journal-ref": "Twenty-sixth European Conference on Artificial Intelligence (ECAI 2023)", "DOI": "10.3233/FAIA230587"}, "abstract": "In this paper we propose DKIBO, a Bayesian optimization (BO) algorithm that accommodates domain knowledge to tune exploration in the search space. Bayesian optimization has recently emerged as a sample-efficient optimizer for many intractable scientific problems. While various existing BO frameworks allow the input of prior beliefs to accelerate the search by narrowing down the space, incorporating such knowledge is not always straightforward and can often introduce bias and lead to poor performance. Here we propose a simple approach to incorporate structural knowledge in the acquisition function by utilizing an additional deterministic surrogate model to enrich the approximation power of the Gaussian process. This is suitably chosen according to structural information of the problem at hand and acts a corrective term towards a better-informed sampling. We empirically demonstrate the practical utility of the proposed method by successfully injecting domain knowledge in a materials design task. We further validate our method's performance on different experimental settings and ablation analyses.", "url": "https://arxiv.org/abs/2311.15162"}, {"metadata": {"arXiv": "2311.15209", "Date": "Sun, 26 Nov 2023 06:38:16 ", "Title": "See and Think: Embodied Agent in Virtual Environment", "Authors": ["Zhonghan Zhao", "Wenhao Chai", "Xuan Wang", "Li Boyi", "Shengyu Hao", "Shidong Cao", "Tian Ye", "Jenq-Neng Hwang", "Gaoang Wang"], "Categories": "cs.AI", "Comments": ["Preprint. First three authors contribute equally to this work"]}, "abstract": "Large language models (LLMs) have achieved impressive progress on several open-world tasks. Recently, using LLMs to build embodied agents has been a hotspot. In this paper, we propose STEVE, a comprehensive and visionary embodied agent in the Minecraft virtual environment. STEVE consists of three key components: vision perception, language instruction, and code action. Vision perception involves the interpretation of visual information in the environment, which is then integrated into the LLMs component with agent state and task instruction. Language instruction is responsible for iterative reasoning and decomposing complex tasks into manageable guidelines. Code action generates executable skill actions based on retrieval in skill database, enabling the agent to interact effectively within the Minecraft environment. We also collect STEVE-21K dataset, which includes 600$+$ vision-environment pairs, 20K knowledge question-answering pairs, and 200$+$ skill-code pairs. We conduct continuous block search, knowledge question and answering, and tech tree mastery to evaluate the performance. Extensive experiments show that STEVE achieves at most $1.5 \\times$ faster unlocking key tech trees and $2.5 \\times$ quicker in block search tasks compared to previous state-of-the-art methods.", "url": "https://arxiv.org/abs/2311.15209"}, {"metadata": {"arXiv": "2311.15920", "Date": "Mon, 27 Nov 2023 15:29:21 ", "Title": "A Fully Data-Driven Approach for Realistic Traffic Signal Control Using Offline Reinforcement Learning", "Authors": ["Jianxiong Li", "Shichao Lin", "Tianyu Shi", "Chujie Tian", "Yu Mei", "Jian Song", "Xianyuan Zhan", "Ruimin Li"], "Categories": "cs.AI", "Comments": ["15 pages", "6 figures"]}, "abstract": "The optimization of traffic signal control (TSC) is critical for an efficient transportation system. In recent years, reinforcement learning (RL) techniques have emerged as a popular approach for TSC and show promising results for highly adaptive control. However, existing RL-based methods suffer from notably poor real-world applicability and hardly have any successful deployments. The reasons for such failures are mostly due to the reliance on over-idealized traffic simulators for policy optimization, as well as using unrealistic fine-grained state observations and reward signals that are not directly obtainable from real-world sensors. In this paper, we propose a fully Data-Driven and simulator-free framework for realistic Traffic Signal Control (D2TSC). Specifically, we combine well-established traffic flow theory with machine learning to construct a reward inference model to infer the reward signals from coarse-grained traffic data. With the inferred rewards, we further propose a sample-efficient offline RL method to enable direct signal control policy learning from historical offline datasets of real-world intersections. To evaluate our approach, we collect historical traffic data from a real-world intersection, and develop a highly customized simulation environment that strictly follows real data characteristics. We demonstrate through extensive experiments that our approach achieves superior performance over conventional and offline RL baselines, and also enjoys much better real-world applicability.", "url": "https://arxiv.org/abs/2311.15920"}, {"metadata": {"arXiv": "2311.15933", "Date": "Mon, 27 Nov 2023 15:41:30 ", "Title": "A new fuzzy multi-attribute group decision-making method based on TOPSIS and optimization models", "Authors": ["Qixiao Hu", "Shiquan Zhang", "Chaolang Hu", "Yuetong Liu"], "Categories": "cs.AI"}, "abstract": "In this paper, a new method based on TOPSIS and optimization models is proposed for multi-attribute group decision-making in the environment of interval-valued intuitionistic fuzzy sets.Firstly, by minimizing the sum of differences between individual evaluations and the overallconsistent evaluations of all experts, a new optimization model is established for determining expert weights. Secondly, based on TOPSIS method, the improved closeness index for evaluating each alternative is obtained. Finally, the attribute weight is determined by establishing an optimization model with the goal of maximizing the closeness of each alternative, and it is brought into the closeness index so that the alternatives can be ranked. Combining all these together, the complete fuzzy multi-attribute group decision-making algorithm is formulated, which can give full play to the advantages of subjective and objective weighting methods. In the end, the feasibility and effectiveness of the provided method are verified by a real case study.", "url": "https://arxiv.org/abs/2311.15933"}, {"metadata": {"arXiv": "2311.14757", "Date": "Thu, 23 Nov 2023 15:51:50 ", "Title": "PointOBB: Learning Oriented Object Detection via Single Point Supervision", "Authors": ["Junwei Luo", "Xue Yang", "Yi Yu", "Qingyun Li", "Junchi Yan", "Yansheng Li"], "Categories": "cs.CV cs.AI", "Comments": ["11 pages,5 figures", "6 tables. Code: https://github.com/Luo-Z13/pointobb"]}, "abstract": "Single point-supervised object detection is gaining attention due to its cost-effectiveness. However, existing approaches focus on generating horizontal bounding boxes (HBBs) while ignoring oriented bounding boxes (OBBs) commonly used for objects in aerial images. This paper proposes PointOBB, the first single Point-based OBB generation method, for oriented object detection. PointOBB operates through the collaborative utilization of three distinctive views: an original view, a resized view, and a rotated/flipped (rot/flp) view. Upon the original view, we leverage the resized and rot/flp views to build a scale augmentation module and an angle acquisition module, respectively. In the former module, a Scale-Sensitive Consistency (SSC) loss is designed to enhance the deep network's ability to perceive the object scale. For accurate object angle predictions, the latter module incorporates self-supervised learning to predict angles, which is associated with a scale-guided Dense-to-Sparse (DS) matching strategy for aggregating dense angles corresponding to sparse objects. The resized and rot/flp views are switched using a progressive multi-view switching strategy during training to achieve coupled optimization of scale and angle. Experimental results on the DIOR-R and DOTA-v1.0 datasets demonstrate that PointOBB achieves promising performance, and significantly outperforms potential point-supervised baselines.", "url": "https://arxiv.org/abs/2311.14757"}, {"metadata": {"arXiv": "2311.14758", "Date": "Thu, 23 Nov 2023 15:57:41 ", "Title": "Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision", "Authors": ["Yu Yi", "Xue Yang", "Qingyun Li", "Feipeng Da", "Junchi Yan", "Jifeng Dai", "Yu Qiao"], "Categories": "cs.CV cs.AI", "Comments": ["11 pages", "3 figures", "5 tables", "code: https://github.com/open-mmlab/mmrotate"]}, "abstract": "With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning rotated box (RBox) from the horizontal box (HBox) has attracted more and more attention. In this paper, we explore a more challenging yet label-efficient setting, namely single point-supervised OOD, and present our approach called Point2RBox. Specifically, we propose to leverage two principles: 1) Synthetic pattern knowledge combination: By sampling around each labelled point on the image, we transfer the object feature to synthetic visual patterns with the known bounding box to provide the knowledge for box regression. 2) Transform self-supervision: With a transformed input image (e.g. scaled/rotated), the output RBoxes are trained to follow the same transformation so that the network can perceive the relative size/rotation between objects. The detector is further enhanced by a few devised techniques to cope with peripheral issues, e.g. the anchor/layer assignment as the size of the object is not available in our point supervision setting. To our best knowledge, Point2RBox is the first end-to-end solution for point-supervised OOD. In particular, our method uses a lightweight paradigm, yet it achieves a competitive performance among point-supervised alternatives, 41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.", "url": "https://arxiv.org/abs/2311.14758"}, {"metadata": {"arXiv": "2311.14762", "Date": "Thu, 23 Nov 2023 21:01:14 ", "Title": "The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024", "Authors": ["Benjamin Kiefer", "Lojze \\v{Z}ust", "Matej Kristan", "Janez Per\\v{s}", "Matija Ter\\v{s}ek", "Arnold Wiliem", "Martin Messmer", "Cheng-Yen Yang", "Hsiang-Wei Huang", "Zhongyu Jiang", "Heng-Cheng Kuo", "Jie Mei", "Jenq-Neng Hwang", "Daniel Stadler", "Lars Sommer", "Kaer Huang", "Aiguo Zheng", "Weitu Chong", "Kanokphan Lertniphonphan", "Jun Xie", "Feng Chen", "Jian Li", "Zhepeng Wang", "Luca Zedda", "Andrea Loddo", "Cecilia Di Ruberto", "Tuan-Anh Vu", "Hai Nguyen-Truong", "Tan-Sang Ha", "Quan-Dung Pham", "Sai-Kit Yeung", "Yuan Feng", "Nguyen Thanh Thien", "Lixin Tian", "Sheng-Yao Kuan", "Yuan-Hao Ho", "Angel Bueno Rodriguez", "Borja Carrillo-Perez", "Alexander Klein", "Antje Alex", "Yannik Steiniger", "Felix Sattler", "Edgardo Solano-Carrillo", "Matej Fabijani\\'c", "Magdalena \\v{S}umunec", "Nadir Kapetanovi\\'c", "Andreas Michel", "Wolfgang Gross", "Martin Weinmann"], "Categories": "cs.CV cs.AI", "Comments": ["Part of 2nd Workshop on Maritime Computer Vision (MaCVi) 2024 IEEE Xplore submission as part of WACV 2024"]}, "abstract": "The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024 addresses maritime computer vision for Unmanned Aerial Vehicles (UAV) and Unmanned Surface Vehicles (USV). Three challenges categories are considered: (i) UAV-based Maritime Object Tracking with Re-identification, (ii) USV-based Maritime Obstacle Segmentation and Detection, (iii) USV-based Maritime Boat Tracking. The USV-based Maritime Obstacle Segmentation and Detection features three sub-challenges, including a new embedded challenge addressing efficicent inference on real-world embedded devices. This report offers a comprehensive overview of the findings from the challenges. We provide both statistical and qualitative analyses, evaluating trends from over 195 submissions. All datasets, evaluation code, and the leaderboard are available to the public at https://macvi.org/workshop/macvi24.", "url": "https://arxiv.org/abs/2311.14762"}, {"metadata": {"arXiv": "2311.14768", "Date": "Fri, 24 Nov 2023 11:20:38 ", "Title": "AdaDiff: Adaptive Step Selection for Fast Diffusion", "Authors": ["Hui Zhang and Zuxuan Wu and Zhen Xing and Jie Shao and Yu-Gang Jiang"], "Categories": "cs.CV cs.AI", "Comments": ["10 pages", "5 figures"]}, "abstract": "Diffusion models, as a type of generative models, have achieved impressive results in generating images and videos conditioned on textual conditions. However, the generation process of diffusion models involves denoising for dozens of steps to produce photorealistic images/videos, which is computationally expensive. Unlike previous methods that design ``one-size-fits-all'' approaches for speed up, we argue denoising steps should be sample-specific conditioned on the richness of input texts. To this end, we introduce AdaDiff, a lightweight framework designed to learn instance-specific step usage policies, which are then used by the diffusion model for generation. AdaDiff is optimized using a policy gradient method to maximize a carefully designed reward function, balancing inference time and generation quality. We conduct experiments on three image generation and two video generation benchmarks and demonstrate that our approach achieves similar results in terms of visual quality compared to the baseline using a fixed 50 denoising steps while reducing inference time by at least 33%, going as high as 40%. Furthermore, our qualitative analysis shows that our method allocates more steps to more informative text conditions and fewer steps to simpler text conditions.", "url": "https://arxiv.org/abs/2311.14768"}, {"metadata": {"arXiv": "2311.14786", "Date": "Fri, 24 Nov 2023 18:02:49 ", "Title": "GPT-4V Takes the Wheel: Evaluating Promise and Challenges for Pedestrian Behavior Prediction", "Authors": ["Jia Huang", "Peng Jiang", "Alvika Gautam", "and Srikanth Saripalli"], "Categories": "cs.CV cs.AI cs.RO"}, "abstract": "Existing pedestrian behavior prediction methods rely primarily on deep neural networks that utilize features extracted from video frame sequences. Although these vision-based models have shown promising results, they face limitations in effectively capturing and utilizing the dynamic spatio-temporal interactions between the target pedestrian and its surrounding traffic elements, crucial for accurate reasoning. Additionally, training these models requires manually annotating domain-specific datasets, a process that is expensive, time-consuming, and difficult to generalize to new environments and scenarios. The recent emergence of Large Multimodal Models (LMMs) offers potential solutions to these limitations due to their superior visual understanding and causal reasoning capabilities, which can be harnessed through semi-supervised training. GPT-4V(ision), the latest iteration of the state-of-the-art Large-Language Model GPTs, now incorporates vision input capabilities. This report provides a comprehensive evaluation of the potential of GPT-4V for pedestrian behavior prediction in autonomous driving using publicly available datasets: JAAD, PIE, and WiDEVIEW. Quantitative and qualitative evaluations demonstrate GPT-4V(ision)'s promise in zero-shot pedestrian behavior prediction and driving scene understanding ability for autonomous driving. However, it still falls short of the state-of-the-art traditional domain-specific models. Challenges include difficulties in handling small pedestrians and vehicles in motion. These limitations highlight the need for further research and development in this area.", "url": "https://arxiv.org/abs/2311.14786"}, {"metadata": {"arXiv": "2311.14900", "Date": "Sat, 25 Nov 2023 02:09:38 ", "Title": "Resfusion: Prior Residual Noise embedded Denoising Diffusion Probabilistic Models", "Authors": ["Shi Zhenning", "Dong Changsheng", "Pan Bin", "Xie Xueshuo", "He Along", "Qu Qiaoying", "Li Tao"], "Categories": "cs.CV cs.AI"}, "abstract": "Recently, Denoising Diffusion Probabilistic Models have been widely used in image segmentation, by generating segmentation masks conditioned on the input image. However, previous works can not seamlessly integrate existing end-to-end models with denoising diffusion models. Existing research can only select acceleration steps based on experience rather than calculating them specifically. Moreover, most methods are limited to small models and small-scale datasets, unable to generalize to general datasets and a wider range of tasks. Therefore, we propose Resfusion with a novel resnoise-diffusion process, which gradually generates segmentation masks or any type of target image, seamlessly integrating state-of-the-art end-to-end models and denoising diffusion models. Resfusion bridges the discrepancy between the likelihood output and the ground truth output through a Markov process. Through the novel smooth equivalence transformation in resnoise-diffusion process, we determine the optimal acceleration step. Experimental results demonstrate that Resfusion combines the capabilities of existing end-to-end models and denoising diffusion models, further enhancing performance and achieving outstanding results. Moreover, Resfusion is not limited to segmentation tasks, it can easily generalize to any general tasks of image generation and exhibit strong competitiveness.", "url": "https://arxiv.org/abs/2311.14900"}, {"metadata": {"arXiv": "2311.14926", "Date": "Sat, 25 Nov 2023 04:23:49 ", "Title": "FreePIH: Training-Free Painterly Image Harmonization with Diffusion Model", "Authors": ["Ruibin Li", "Jingcai Guo", "Song Guo", "Qihua Zhou", "Jie Zhang"], "Categories": "cs.CV cs.AI"}, "abstract": "This paper provides an efficient training-free painterly image harmonization (PIH) method, dubbed FreePIH, that leverages only a pre-trained diffusion model to achieve state-of-the-art harmonization results. Unlike existing methods that require either training auxiliary networks or fine-tuning a large pre-trained backbone, or both, to harmonize a foreground object with a painterly-style background image, our FreePIH tames the denoising process as a plug-in module for foreground image style transfer. Specifically, we find that the very last few steps of the denoising (i.e., generation) process strongly correspond to the stylistic information of images, and based on this, we propose to augment the latent features of both the foreground and background images with Gaussians for a direct denoising-based harmonization. To guarantee the fidelity of the harmonized image, we make use of multi-scale features to enforce the consistency of the content and stability of the foreground objects in the latent space, and meanwhile, aligning both fore-/back-grounds with the same style. Moreover, to accommodate the generation with more structural and textural details, we further integrate text prompts to attend to the latent features, hence improving the generation quality. Quantitative and qualitative evaluations on COCO and LAION 5B datasets demonstrate that our method can surpass representative baselines by large margins.", "url": "https://arxiv.org/abs/2311.14926"}, {"metadata": {"arXiv": "2311.15072", "Date": "Sat, 25 Nov 2023 16:57:24 ", "Title": "Introducing SSBD+ Dataset with a Convolutional Pipeline for detecting Self-Stimulatory Behaviours in Children using raw videos", "Authors": ["Vaibhavi Lokegaonkar", "Vijay Jaisankar", "Pon Deepika", "Madhav Rao", "T K Srikanth", "Sarbani Mallick", "Manjit Sodhi"], "Categories": "cs.CV cs.AI", "Comments": ["Copyright 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "in any current or future media", "including reprinting/republishing this material for advertising or promotional purposes", "creating new collective works", "for resale or redistribution to servers or lists", "or reuse of any copyrighted component of this work in other works"]}, "abstract": "Conventionally, evaluation for the diagnosis of Autism spectrum disorder is done by a trained specialist through questionnaire-based formal assessments and by observation of behavioral cues under various settings to capture the early warning signs of autism. These evaluation techniques are highly subjective and their accuracy relies on the experience of the specialist. In this regard, machine learning-based methods for automated capturing of early signs of autism from the recorded videos of the children is a promising alternative. In this paper, the authors propose a novel pipelined deep learning architecture to detect certain self-stimulatory behaviors that help in the diagnosis of autism spectrum disorder (ASD). The authors also supplement their tool with an augmented version of the Self Stimulatory Behavior Dataset (SSBD) and also propose a new label in SSBD Action detection: no-class. The deep learning model with the new dataset is made freely available for easy adoption to the researchers and developers community. An overall accuracy of around 81% was achieved from the proposed pipeline model that is targeted for real-time and hands-free automated diagnosis. All of the source code, data, licenses of use, and other relevant material is made freely available in https://github.com/sarl-iiitb/", "url": "https://arxiv.org/abs/2311.15072"}, {"metadata": {"arXiv": "2311.15108", "Date": "Sat, 25 Nov 2023 19:40:13 ", "Title": "Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision", "Authors": ["Nicholas Lui", "Bryan Chia", "William Berrios", "Candace Ross", "Douwe Kiela"], "Categories": "cs.CV cs.AI", "Comments": ["The Appendix can be found at https://bit.ly/dp-appendix"]}, "abstract": "Computer vision models have been known to encode harmful biases, leading to the potentially unfair treatment of historically marginalized groups, such as people of color. However, there remains a lack of datasets balanced along demographic traits that can be used to evaluate the downstream fairness of these models. In this work, we demonstrate that diffusion models can be leveraged to create such a dataset. We first use a diffusion model to generate a large set of images depicting various occupations. Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race. Using this dataset, we benchmark several vision-language models on a multi-class occupation classification task. We find that images generated with non-Caucasian labels have a significantly higher occupation misclassification rate than images generated with Caucasian labels, and that several misclassifications are suggestive of racial biases. We measure a model's downstream fairness by computing the standard deviation in the probability of predicting the true occupation label across the different perceived identity groups. Using this fairness metric, we find significant disparities between the evaluated vision-and-language models. We hope that our work demonstrates the potential value of diffusion methods for fairness evaluations.", "url": "https://arxiv.org/abs/2311.15108"}, {"metadata": {"arXiv": "2311.15193", "Date": "Sun, 26 Nov 2023 05:17:11 ", "Title": "IA-LSTM: Interaction-Aware LSTM for Pedestrian Trajectory Prediction", "Authors": ["Yuehai Chen"], "Categories": "cs.CV cs.AI"}, "abstract": "Predicting the trajectory of pedestrians in crowd scenarios is indispensable in self-driving or autonomous mobile robot field because estimating the future locations of pedestrians around is beneficial for policy decision to avoid collision. It is a challenging issue because humans have different walking motions and the interactions between humans and objects in the current environment, especially between human themselves, are complex. Previous researches have focused on how to model the human-human interactions, however, neglecting the relative importance of interactions. In order to address this issue, we introduce a novel mechanism based on the correntropy, which not only can measure the relative importance of human-human interactions, but also can build personal space for each pedestrian. We further propose an Interaction Module including this data-driven mechanism that can effectively extract feature representations of dynamic human-human interactions in the scene and calculate corresponding weights to represent the importance of different interactions. To share such social messages among pedestrians, we design an interaction-aware architecture based on the Long Short-Term Memory (LSTM) network for trajectory prediction. We demonstrate the performance of our model on two public datasets and the experimental results demonstrate that our model can achieve better performance than several latest methods with good performance.", "url": "https://arxiv.org/abs/2311.15193"}, {"metadata": {"arXiv": "2311.15326", "Date": "Sun, 26 Nov 2023 15:01:00 ", "Title": "Lightweight Face Recognition: An Improved MobileFaceNet Model", "Authors": ["Ahmad Hassanpour", "Yasamin Kowsari"], "Categories": "cs.CV cs.AI"}, "abstract": "This paper presents an extensive exploration and comparative analysis of lightweight face recognition (FR) models, specifically focusing on MobileFaceNet and its modified variant, MMobileFaceNet. The need for efficient FR models on devices with limited computational resources has led to the development of models with reduced memory footprints and computational demands without sacrificing accuracy. Our research delves into the impact of dataset selection, model architecture, and optimization algorithms on the performance of FR models. We highlight our participation in the EFaR-2023 competition, where our models showcased exceptional performance, particularly in categories restricted by the number of parameters. By employing a subset of the Webface42M dataset and integrating sharpness-aware minimization (SAM) optimization, we achieved significant improvements in accuracy across various benchmarks, including those that test for cross-pose, cross-age, and cross-ethnicity performance. The results underscore the efficacy of our approach in crafting models that are not only computationally efficient but also maintain high accuracy in diverse conditions.", "url": "https://arxiv.org/abs/2311.15326"}, {"metadata": {"arXiv": "2311.15356", "Date": "Sun, 26 Nov 2023 17:17:28 ", "Title": "Having Second Thoughts? Let's hear it", "Authors": ["Jung H. Lee and Sujith Vijayan"], "Categories": "cs.CV cs.AI", "Comments": ["13 pages", "11 figures", "1 table", "2 supplementary tables and 1 supplementary figure"]}, "abstract": "Deep learning models loosely mimic bottom-up signal pathways from low-order sensory areas to high-order cognitive areas. After training, DL models can outperform humans on some domain-specific tasks, but their decision-making process has been known to be easily disrupted. Since the human brain consists of multiple functional areas highly connected to one another and relies on intricate interplays between bottom-up and top-down (from high-order to low-order areas) processing, we hypothesize that incorporating top-down signal processing may make DL models more robust. To address this hypothesis, we propose a certification process mimicking selective attention and test if it could make DL models more robust. Our empirical evaluations suggest that this newly proposed certification can improve DL models' accuracy and help us build safety measures to alleviate their vulnerabilities with both artificial and natural adversarial examples.", "url": "https://arxiv.org/abs/2311.15356"}, {"metadata": {"arXiv": "2311.15421", "Date": "Sun, 26 Nov 2023 21:09:00 ", "Title": "Wired Perspectives: Multi-View Wire Art Embraces Generative AI", "Authors": ["Zhiyu Qu and Lan Yang and Honggang Zhang and Tao Xiang and Kaiyue Pang and Yi-Zhe Song"], "Categories": "cs.CV cs.AI", "Comments": ["Project page: https://dreamwireart.github.io"]}, "abstract": "Creating multi-view wire art (MVWA), a static 3D sculpture with diverse interpretations from different viewpoints, is a complex task even for skilled artists. In response, we present DreamWire, an AI system enabling everyone to craft MVWA easily. Users express their vision through text prompts or scribbles, freeing them from intricate 3D wire organisation. Our approach synergises 3D B\\'ezier curves, Prim's algorithm, and knowledge distillation from diffusion models or their variants (e.g., ControlNet). This blend enables the system to represent 3D wire art, ensuring spatial continuity and overcoming data scarcity. Extensive evaluation and analysis are conducted to shed insight on the inner workings of the proposed system, including the trade-off between connectivity and visual aesthetics.", "url": "https://arxiv.org/abs/2311.15421"}, {"metadata": {"arXiv": "2311.15438", "Date": "Sun, 26 Nov 2023 21:52:47 ", "Title": "ProtoArgNet: Interpretable Image Classification with Super-Prototypes and Argumentation [Technical Report]", "Authors": ["Hamed Ayoobi", "Nico Potyka", "Francesca Toni"], "Categories": "cs.CV cs.AI"}, "abstract": "We propose ProtoArgNet, a novel interpretable deep neural architecture for image classification in the spirit of prototypical-part-learning as found, e.g. in ProtoPNet. While earlier approaches associate every class with multiple prototypical-parts, ProtoArgNet uses super-prototypes that combine prototypical-parts into single prototypical class representations. Furthermore, while earlier approaches use interpretable classification layers, e.g. logistic regression in ProtoPNet, ProtoArgNet improves accuracy with multi-layer perceptrons while relying upon an interpretable reading thereof based on a form of argumentation. ProtoArgNet is customisable to user cognitive requirements by a process of sparsification of the multi-layer perceptron/argumentation component. Also, as opposed to other prototypical-part-learning approaches, ProtoArgNet can recognise spatial relations between different prototypical-parts that are from different regions in images, similar to how CNNs capture relations between patterns recognized in earlier layers.", "url": "https://arxiv.org/abs/2311.15438"}, {"metadata": {"arXiv": "2311.15569", "Date": "Mon, 27 Nov 2023 06:37:05 ", "Title": "Improving Adaptability and Generalizability of Efficient Transfer Learning for Vision-Language Models", "Authors": ["Yongjin Yang", "Jongwoo Ko", "Se-Young Yun"], "Categories": "cs.CV cs.AI", "Comments": ["11 pages (19 pages including supplementary)", "10 figures (12 figures including supplementary)", "6 tables (17 tables including supplementary)"]}, "abstract": "Vision-Language Models (VLMs) like CLIP have demonstrated remarkable applicability across a variety of downstream tasks, including zero-shot image classification. Recently, the use of prompts or adapters for efficient transfer learning has gained significant attention for effectively adapting to downstream tasks. However, the roles of vision and text prompts, as well as adapters in terms of generalization and transfer difficulty, have been overlooked, limiting performance on unseen tasks. In this paper, we empirically analyze how VLMs behave when using vision and text prompts, adapters, and a combination of these components, marking a novel exploration by our study. Our observations find that utilizing vision prompts for class separability and text adapters for task adaptation is crucial for adaptability and generalizability. Moreover, to improve generalization across every domain, we propose an adaptive ensemble method that effectively combines the general knowledge of VLMs with task-specific knowledge according to transfer difficulty. Upon experimenting with extensive benchmarks, our method consistently outperforms all baselines, particularly on unseen tasks, demonstrating the effectiveness of our proposed approach.", "url": "https://arxiv.org/abs/2311.15569"}, {"metadata": {"arXiv": "2311.15619", "Date": "Mon, 27 Nov 2023 08:32:28 ", "Title": "Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition", "Authors": ["Yifei Chen", "Dapeng Chen", "Ruijin Liu", "Sai Zhou", "Wenyuan Xue", "Wei Peng"], "Categories": "cs.CV cs.AI"}, "abstract": "Large-scale visual-language pre-trained models have achieved significant success in various video tasks. However, most existing methods follow an \"adapt then align\" paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static images to complicated activity concepts. In this paper, we propose a novel \"Align before Adapt\" (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This paradigm reuses the visual-language alignment of VLP during adaptation and tries to explain an action by the underlying entities. This helps understand actions by bridging the gap with complex activity semantics, particularly when facing unfamiliar or unseen categories. ALT achieves competitive performance and superior generalizability while requiring significantly low computational costs. In fully supervised scenarios, it achieves 88.1% top-1 accuracy on Kinetics-400 with only 4947 GFLOPs. In 2-shot experiments, ALT outperforms the previous state-of-the-art by 7.1% and 9.2% on HMDB-51 and UCF-101, respectively.", "url": "https://arxiv.org/abs/2311.15619"}, {"metadata": {"arXiv": "2311.15813", "Date": "Mon, 27 Nov 2023 13:39:44 ", "Title": "FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax", "Authors": ["Yu Lu", "Linchao Zhu", "Hehe Fan", "Yi Yang"], "Categories": "cs.CV cs.AI", "Comments": ["Project page: https://flowzero-video.github.io"]}, "abstract": "Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.", "url": "https://arxiv.org/abs/2311.15813"}, {"metadata": {"arXiv": "2311.15826", "Date": "Fri, 24 Nov 2023 18:59:10 ", "Title": "GeoChat: Grounded Large Vision-Language Model for Remote Sensing", "Authors": ["Kartik Kuckreja", "Muhammad Sohail Danish", "Muzammal Naseer", "Abhijit Das", "Salman Khan", "Fahad Shahbaz Khan"], "Categories": "cs.CV cs.AI", "Comments": ["10 pages", "4 figures"]}, "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have shown great promise in natural image domains, allowing users to hold a dialogue about given visual content. However, such general-domain VLMs perform poorly for Remote Sensing (RS) scenarios, leading to inaccurate or fabricated information when presented with RS domain-specific queries. Such a behavior emerges due to the unique challenges introduced by RS imagery. For example, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-level reasoning is necessary alongside holistic scene interpretation. Furthermore, the lack of domain-specific multimodal instruction following data as well as strong backbone models for RS make it hard for the models to align their behavior with user queries. To address these limitations, we propose GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabilities with high-resolution RS images. Specifically, GeoChat can not only answer image-level queries but also accepts region inputs to hold region-specific dialogue. Furthermore, it can visually ground objects in its responses by referring to their spatial coordinates. To address the lack of domain-specific datasets, we generate a novel RS multimodal instruction-following dataset by extending image-text pairs from existing diverse RS datasets. We establish a comprehensive benchmark for RS multitask conversations and compare with a number of baseline methods. GeoChat demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conversations and referring detection. Our code is available at https://github.com/mbzuai-oryx/geochat.", "url": "https://arxiv.org/abs/2311.15826"}, {"metadata": {"arXiv": "2311.15993", "Date": "Mon, 27 Nov 2023 16:41:31 ", "Title": "Unified Batch Normalization: Identifying and Alleviating the Feature Condensation in Batch Normalization and a Unified Framework", "Authors": ["Shaobo Wang", "Xiangdong Zhang", "Junchi Yan"], "Categories": "cs.CV cs.AI"}, "abstract": "Batch Normalization (BN) has become an essential technique in contemporary neural network design, enhancing training stability. Specifically, BN employs centering and scaling operations to standardize features along the batch dimension and uses an affine transformation to recover features. Although standard BN has shown its capability to improve deep neural network training and convergence, it still exhibits inherent limitations in certain cases. Most existing techniques that enhance BN consider a single or a few aspects of BN. In this paper, we first identify problems with BN from a feature perspective and explore that feature condensation exists in the learning when employing BN, which negatively affects testing performance. To tackle this problem, we propose a two-stage unified framework called Unified Batch Normalization (UBN). In the first stage, we utilize a simple feature condensation threshold to alleviate the feature condensation, which hinders inappropriate statistic updates in normalization. In the second stage, we unify various normalization variants to boost each component of BN. Our experimental results reveal that UBN significantly enhances performance across different visual backbones and notably expedites network training convergence, particularly in early training stages. Notably, our method improved about 3% in top-1 accuracy on ImageNet classification with large batch sizes, showing the effectiveness of our approach in real-world scenarios.", "url": "https://arxiv.org/abs/2311.15993"}, {"metadata": {"arXiv": "2311.16081", "Date": "Mon, 27 Nov 2023 18:52:09 ", "Title": "ViT-Lens-2: Gateway to Omni-modal Intelligence", "Authors": ["Weixian Lei", "Yixiao Ge", "Kun Yi", "Jianfeng Zhang", "Difei Gao", "Dylan Sun", "Yuying Ge", "Ying Shan", "Mike Zheng Shou"], "Categories": "cs.CV cs.AI", "Comments": ["This work is a follow-up of \"ViT-Lens: Towards Omni-modal Representations\". arXiv admin note: text overlap with arXiv:2308.10185"]}, "abstract": "Aiming to advance AI agents, large foundation models significantly improve reasoning and instruction execution, yet the current focus on vision and language neglects the potential of perceiving diverse modalities in open-world environments. However, the success of data-driven vision and language models is costly or even infeasible to be reproduced for rare modalities. In this paper, we present ViT-Lens-2 that facilitates efficient omni-modal representation learning by perceiving novel modalities with a pretrained ViT and aligning them to a pre-defined space. Specifically, the modality-specific lens is tuned to project any-modal signals to an intermediate embedding space, which are then processed by a strong ViT with pre-trained visual knowledge. The encoded representations are optimized toward aligning with the modal-independent space, pre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified solution for representation learning of increasing modalities with two appealing advantages: (i) Unlocking the great potential of pretrained ViTs to novel modalities effectively with efficient data regime; (ii) Enabling emergent downstream capabilities through modality alignment and shared ViT parameters. We tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio, tactile and EEG, and set new state-of-the-art results across various understanding tasks, such as zero-shot classification. By seamlessly integrating ViT-Lens-2 into Multimodal Foundation Models, we enable Any-modality to Text and Image Generation in a zero-shot manner. Code and models are available at https://github.com/TencentARC/ViT-Lens.", "url": "https://arxiv.org/abs/2311.16081"}, {"metadata": {"arXiv": "2311.16103", "Date": "Mon, 27 Nov 2023 18:59:58 ", "Title": "Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models", "Authors": ["Munan Ning and Bin Zhu and Yujia Xie and Bin Lin and Jiaxi Cui and Lu Yuan and Dongdong Chen and Li Yuan"], "Categories": "cs.CV cs.AI", "Comments": ["Benchmark is available at https://github.com/PKU-YuanGroup/Video-Bench"]}, "abstract": "Video-based large language models (Video-LLMs) have been recently introduced, targeting both fundamental improvements in perception and comprehension, and a diverse range of user inquiries. In pursuit of the ultimate goal of achieving artificial general intelligence, a truly intelligent Video-LLM model should not only see and understand the surroundings, but also possess human-level commonsense, and make well-informed decisions for the users. To guide the development of such a model, the establishment of a robust and comprehensive evaluation system becomes crucial. To this end, this paper proposes \\textit{Video-Bench}, a new comprehensive benchmark along with a toolkit specifically designed for evaluating Video-LLMs. The benchmark comprises 10 meticulously crafted tasks, evaluating the capabilities of Video-LLMs across three distinct levels: Video-exclusive Understanding, Prior Knowledge-based Question-Answering, and Comprehension and Decision-making. In addition, we introduce an automatic toolkit tailored to process model outputs for various tasks, facilitating the calculation of metrics and generating convenient final scores. We evaluate 8 representative Video-LLMs using \\textit{Video-Bench}. The findings reveal that current Video-LLMs still fall considerably short of achieving human-like comprehension and analysis of real-world videos, offering valuable insights for future research directions. The benchmark and toolkit are available at: \\url{https://github.com/PKU-YuanGroup/Video-Bench}.", "url": "https://arxiv.org/abs/2311.16103"}, {"metadata": {"arXiv": "2311.15033", "Date": "Sat, 25 Nov 2023 14:14:01 ", "Title": "Agent as Cerebrum, Controller as Cerebellum: Implementing an Embodied LMM-based Agent on Drones", "Authors": ["Haoran Zhao", "Fengxing Pan", "Huqiuyue Ping and Yaoming Zhou"], "Categories": "cs.RO cs.AI", "Comments": ["17 pages", "12 figures"]}, "abstract": "In this study, we present a novel paradigm for industrial robotic embodied agents, encapsulating an 'agent as cerebrum, controller as cerebellum' architecture. Our approach harnesses the power of Large Multimodal Models (LMMs) within an agent framework known as AeroAgent, tailored for drone technology in industrial settings. To facilitate seamless integration with robotic systems, we introduce ROSchain, a bespoke linkage framework connecting LMM-based agents to the Robot Operating System (ROS). We report findings from extensive empirical research, including simulated experiments on the Airgen and real-world case study, particularly in individual search and rescue operations. The results demonstrate AeroAgent's superior performance in comparison to existing Deep Reinforcement Learning (DRL)-based agents, highlighting the advantages of the embodied LMM in complex, real-world scenarios.", "url": "https://arxiv.org/abs/2311.15033"}, {"metadata": {"arXiv": "2311.15400", "Date": "Sun, 26 Nov 2023 19:50:23 ", "Title": "A Framework for Realistic Simulation of Daily Human Activity", "Authors": ["Ifrah Idrees", "Siddharth Singh", "Kerui Xu", "Dylan F. Glas"], "Categories": "cs.RO cs.AI", "Comments": ["Accepted and Presented at IEEE International Conference on Robot and Human Communication (ROMAN) 2023"]}, "abstract": "For social robots like Astro which interact with and adapt to the daily movements of users within the home, realistic simulation of human activity is needed for feature development and testing. This paper presents a framework for simulating daily human activity patterns in home environments at scale, supporting manual configurability of different personas or activity patterns, variation of activity timings, and testing on multiple home layouts. We introduce a method for specifying day-to-day variation in schedules and present a bidirectional constraint propagation algorithm for generating schedules from templates. We validate the expressive power of our framework through a use case scenario analysis and demonstrate that our method can be used to generate data closely resembling human behavior from three public datasets and a self-collected dataset. Our contribution supports systematic testing of social robot behaviors at scale, enables procedural generation of synthetic datasets of human movement in different households, and can help minimize bias in training data, leading to more robust and effective robots for home environments.", "url": "https://arxiv.org/abs/2311.15400"}, {"metadata": {"arXiv": "2311.15736", "Date": "Mon, 27 Nov 2023 11:39:27 ", "Title": "SceneDM: Scene-level Multi-agent Trajectory Generation with Consistent Diffusion Models", "Authors": ["Zhiming Guo", "Xing Gao", "Jianlan Zhou", "Xinyu Cai", "Botian Shi"], "Categories": "cs.RO cs.AI"}, "abstract": "Realistic scene-level multi-agent motion simulations are crucial for developing and evaluating self-driving algorithms. However, most existing works focus on generating trajectories for a certain single agent type, and typically ignore the consistency of generated trajectories. In this paper, we propose a novel framework based on diffusion models, called SceneDM, to generate joint and consistent future motions of all the agents, including vehicles, bicycles, pedestrians, etc., in a scene. To enhance the consistency of the generated trajectories, we resort to a new Transformer-based network to effectively handle agent-agent interactions in the inverse process of motion diffusion. In consideration of the smoothness of agent trajectories, we further design a simple yet effective consistent diffusion approach, to improve the model in exploiting short-term temporal dependencies. Furthermore, a scene-level scoring function is attached to evaluate the safety and road-adherence of the generated agent's motions and help filter out unrealistic simulations. Finally, SceneDM achieves state-of-the-art results on the Waymo Sim Agents Benchmark. Project webpage is available at https://alperen-hub.github.io/SceneDM.", "url": "https://arxiv.org/abs/2311.15736"}, {"metadata": {"arXiv": "2311.15594", "Date": "Mon, 27 Nov 2023 07:41:28 ", "Title": "Networked Multiagent Safe Reinforcement Learning for Low-carbon Demand Management in Distribution Network", "Authors": ["Jichen Zhang", "Linwei Sang", "Yinliang Xu", "Hongbin Sun"], "Categories": "eess.SY cs.AI cs.SY", "Comments": ["Submitted to IEEE Transactions on Sustainable Energy"]}, "abstract": "This paper proposes a multiagent based bi-level operation framework for the low-carbon demand management in distribution networks considering the carbon emission allowance on the demand side. In the upper level, the aggregate load agents optimize the control signals for various types of loads to maximize the profits; in the lower level, the distribution network operator makes optimal dispatching decisions to minimize the operational costs and calculates the distribution locational marginal price and carbon intensity. The distributed flexible load agent has only incomplete information of the distribution network and cooperates with other agents using networked communication. Finally, the problem is formulated into a networked multi-agent constrained Markov decision process, which is solved using a safe reinforcement learning algorithm called consensus multi-agent constrained policy optimization considering the carbon emission allowance for each agent. Case studies with the IEEE 33-bus and 123-bus distribution network systems demonstrate the effectiveness of the proposed approach, in terms of satisfying the carbon emission constraint on demand side, ensuring the safe operation of the distribution network and preserving privacy of both sides.", "url": "https://arxiv.org/abs/2311.15594"}, {"metadata": {"arXiv": "2311.15298", "Date": "Sun, 26 Nov 2023 13:46:20 ", "Title": "A Data-driven and multi-agent decision support system for time slot management at container terminals: A case study for the Port of Rotterdam", "Authors": ["Ali Nadi", "Maaike Snelder", "J.W.C. van Lint", "L\\'or\\'ant Tavasszy"], "Categories": "cs.AI cs.LG cs.MA"}, "abstract": "Controlling the departure time of the trucks from a container hub is important to both the traffic and the logistics systems. This, however, requires an intelligent decision support system that can control and manage truck arrival times at terminal gates. This paper introduces an integrated model that can be used to understand, predict, and control logistics and traffic interactions in the port-hinterland ecosystem. This approach is context-aware and makes use of big historical data to predict system states and apply control policies accordingly, on truck inflow and outflow. The control policies ensure multiple stakeholders satisfaction including those of trucking companies, terminal operators, and road traffic agencies. The proposed method consists of five integrated modules orchestrated to systematically steer truckers toward choosing those time slots that are expected to result in lower gate waiting times and more cost-effective schedules. The simulation is supported by real-world data and shows that significant gains can be obtained in the system.", "url": "https://arxiv.org/abs/2311.15298"}, {"metadata": {"arXiv": "2311.15781", "Date": "Mon, 27 Nov 2023 12:54:47 ", "Title": "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs", "Authors": ["Simone Conia and Min Li and Daniel Lee and Umar Farooq Minhas and Ihab Ilyas and Yunyao Li"], "Categories": "cs.AI cs.CL cs.LG", "Comments": ["Camera ready for EMNLP 2023"]}, "abstract": "Recent work in Natural Language Processing and Computer Vision has been using textual information -- e.g., entity names and descriptions -- available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and non-English languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families.", "url": "https://arxiv.org/abs/2311.15781"}, {"metadata": {"arXiv": "2311.16030", "Date": "Mon, 27 Nov 2023 17:50:14 ", "Title": "Machine Learning-Enhanced Aircraft Landing Scheduling under Uncertainties", "Authors": ["Yutian Pang", "Peng Zhao", "Jueming Hu", "Yongming Liu"], "Categories": "cs.AI cs.LG math.OC"}, "abstract": "This paper addresses aircraft delays, emphasizing their impact on safety and financial losses. To mitigate these issues, an innovative machine learning (ML)-enhanced landing scheduling methodology is proposed, aiming to improve automation and safety. Analyzing flight arrival delay scenarios reveals strong multimodal distributions and clusters in arrival flight time durations. A multi-stage conditional ML predictor enhances separation time prediction based on flight events. ML predictions are then integrated as safety constraints in a time-constrained traveling salesman problem formulation, solved using mixed-integer linear programming (MILP). Historical flight recordings and model predictions address uncertainties between successive flights, ensuring reliability. The proposed method is validated using real-world data from the Atlanta Air Route Traffic Control Center (ARTCC ZTL). Case studies demonstrate an average 17.2% reduction in total landing time compared to the First-Come-First-Served (FCFS) rule. Unlike FCFS, the proposed methodology considers uncertainties, instilling confidence in scheduling. The study concludes with remarks and outlines future research directions.", "url": "https://arxiv.org/abs/2311.16030"}, {"metadata": {"arXiv": "2311.14824", "Date": "Fri, 24 Nov 2023 19:45:55 ", "Title": "A Reusable AI-Enabled Defect Detection System for Railway Using Ensembled CNN", "Authors": ["Rahatara Ferdousi", "Fedwa Laamarti", "Chunsheng Yang", "Abdulmotaleb El Saddik"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["28 pages", "13 Figures", "Applied Intelligence Journal", "Springer Nature"], "MSC-class": "68T45, 68T05", "ACM-class": "I.2.10; I.5.2"}, "abstract": "Accurate Defect detection is crucial for ensuring the trustworthiness of intelligent railway systems. Current approaches rely on single deep-learning models, like CNNs, which employ a large amount of data to capture underlying patterns. Training a new defect classifier with limited samples often leads to overfitting and poor performance on unseen images. To address this, researchers have advocated transfer learning and fine-tuning the pre-trained models. However, using a single backbone network in transfer learning still may cause bottleneck issues and inconsistent performance if it is not suitable for a specific problem domain. To overcome these challenges, we propose a reusable AI-enabled defect detection approach. By combining ensemble learning with transfer learning models (VGG-19, MobileNetV3, and ResNet-50), we improved the classification accuracy and achieved consistent performance at a certain phase of training. Our empirical analysis demonstrates better and more consistent performance compared to other state-of-the-art approaches. The consistency substantiates the reusability of the defect detection system for newly evolved defected rail parts. Therefore we anticipate these findings to benefit further research and development of reusable AI-enabled solutions for railway systems.", "url": "https://arxiv.org/abs/2311.14824"}, {"metadata": {"arXiv": "2311.15080", "Date": "Sat, 25 Nov 2023 17:18:35 ", "Title": "Weakly-Supervised Audio-Visual Segmentation", "Authors": ["Shentong Mo", "Bhiksha Raj"], "Categories": "cs.CV cs.AI cs.LG cs.MM cs.SD eess.AS"}, "abstract": "Audio-visual segmentation is a challenging task that aims to predict pixel-level masks for sound sources in a video. Previous work applied a comprehensive manually designed architecture with countless pixel-wise accurate masks as supervision. However, these pixel-level masks are expensive and not available in all cases. In this work, we aim to simplify the supervision as the instance-level annotation, i.e., weakly-supervised audio-visual segmentation. We present a novel Weakly-Supervised Audio-Visual Segmentation framework, namely WS-AVS, that can learn multi-scale audio-visual alignment with multi-scale multiple-instance contrastive learning for audio-visual segmentation. Extensive experiments on AVSBench demonstrate the effectiveness of our WS-AVS in the weakly-supervised audio-visual segmentation of single-source and multi-source scenarios.", "url": "https://arxiv.org/abs/2311.15080"}, {"metadata": {"arXiv": "2311.15100", "Date": "Sat, 25 Nov 2023 18:58:15 ", "Title": "Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation", "Authors": ["Luca Eyring", "Dominik Klein", "Th\\'eo Uscidda", "Giovanni Palla", "Niki Kilbertus", "Zeynep Akata", "Fabian Theis"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "In optimal transport (OT), a Monge map is known as a mapping that transports a source distribution to a target distribution in the most cost-efficient way. Recently, multiple neural estimators for Monge maps have been developed and applied in diverse unpaired domain translation tasks, e.g. in single-cell biology and computer vision. However, the classic OT framework enforces mass conservation, which makes it prone to outliers and limits its applicability in real-world scenarios. The latter can be particularly harmful in OT domain translation tasks, where the relative position of a sample within a distribution is explicitly taken into account. While unbalanced OT tackles this challenge in the discrete setting, its integration into neural Monge map estimators has received limited attention. We propose a theoretically grounded method to incorporate unbalancedness into any Monge map estimator. We improve existing estimators to model cell trajectories over time and to predict cellular responses to perturbations. Moreover, our approach seamlessly integrates with the OT flow matching (OT-FM) framework. While we show that OT-FM performs competitively in image translation, we further improve performance by incorporating unbalancedness (UOT-FM), which better preserves relevant features. We hence establish UOT-FM as a principled method for unpaired image translation.", "url": "https://arxiv.org/abs/2311.15100"}, {"metadata": {"arXiv": "2311.15113", "Date": "Sat, 25 Nov 2023 20:29:35 ", "Title": "NCL-SM: A Fully Annotated Dataset of Images from Human Skeletal Muscle Biopsies", "Authors": ["Atif Khan", "Conor Lawless", "Amy Vincent", "Charlotte Warren", "Valeria Di Leo", "Tiago Gomes", "A. Stephen McGough"], "Categories": "cs.CV cs.AI cs.LG q-bio.TO", "Comments": ["Paper accepted at the Big Data Analytics for Health and Medicine (BDA4HM) workshop", "IEEE BigData 2023", "December 15th-18th", "2023", "Sorrento", "Italy", "07 pages. arXiv admin note: substantial text overlap with arXiv:2311.11099"]}, "abstract": "Single cell analysis of human skeletal muscle (SM) tissue cross-sections is a fundamental tool for understanding many neuromuscular disorders. For this analysis to be reliable and reproducible, identification of individual fibres within microscopy images (segmentation) of SM tissue should be automatic and precise. Biomedical scientists in this field currently rely on custom tools and general machine learning (ML) models, both followed by labour intensive and subjective manual interventions to fine-tune segmentation. We believe that fully automated, precise, reproducible segmentation is possible by training ML models. However, in this important biomedical domain, there are currently no good quality, publicly available annotated imaging datasets available for ML model training. In this paper we release NCL-SM: a high quality bioimaging dataset of 46 human SM tissue cross-sections from both healthy control subjects and from patients with genetically diagnosed muscle pathology. These images include $>$ 50k manually segmented muscle fibres (myofibres). In addition we also curated high quality myofibre segmentations, annotating reasons for rejecting low quality myofibres and low quality regions in SM tissue images, making these annotations completely ready for downstream analysis. This, we believe, will pave the way for development of a fully automatic pipeline that identifies individual myofibres within images of tissue sections and, in particular, also classifies individual myofibres that are fit for further analysis.", "url": "https://arxiv.org/abs/2311.15113"}, {"metadata": {"arXiv": "2311.15243", "Date": "Sun, 26 Nov 2023 09:06:40 ", "Title": "ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection", "Authors": ["Yichen Bai", "Zongbo Han", "Changqing Zhang", "Bing Cao", "Xiaoheng Jiang", "Qinghua Hu"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Under review"]}, "abstract": "Out-of-distribution (OOD) detection methods often exploit auxiliary outliers to train model identifying OOD samples, especially discovering challenging outliers from auxiliary outliers dataset to improve OOD detection. However, they may still face limitations in effectively distinguishing between the most challenging OOD samples that are much like in-distribution (ID) data, i.e., ID-like samples. To this end, we propose a novel OOD detection framework that discovers ID-like outliers using CLIP from the vicinity space of the ID samples, thus helping to identify these most challenging OOD samples. Then a prompt learning framework is proposed that utilizes the identified ID-like outliers to further leverage the capabilities of CLIP for OOD detection. Benefiting from the powerful CLIP, we only need a small number of ID samples to learn the prompts of the model without exposing other auxiliary outlier datasets. By focusing on the most challenging ID-like OOD samples and elegantly exploiting the capabilities of CLIP, our method achieves superior few-shot learning performance on various real-world image datasets (e.g., in 4-shot OOD detection on the ImageNet-1k dataset, our method reduces the average FPR95 by 12.16% and improves the average AUROC by 2.76%, compared to state-of-the-art methods).", "url": "https://arxiv.org/abs/2311.15243"}, {"metadata": {"arXiv": "2311.15497", "Date": "Mon, 27 Nov 2023 02:48:06 ", "Title": "Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision", "Authors": ["Gabriel De Araujo", "Shanlin Sun", "Xiaohui Xie"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Image registration has traditionally been done using two distinct approaches: learning based methods, relying on robust deep neural networks, and optimization-based methods, applying complex mathematical transformations to warp images accordingly. Of course, both paradigms offer advantages and disadvantages, and, in this work, we seek to combine their respective strengths into a single streamlined framework, using the outputs of the learning based method as initial parameters for optimization while prioritizing computational power for the image pairs that offer the greatest loss. Our investigations showed that an improvement of 0.3\\% in testing when utilizing the best performing state-of-the-art model as the backbone of the framework, while maintaining the same inference time and with only a 0.8\\% loss in deformation field smoothness.", "url": "https://arxiv.org/abs/2311.15497"}, {"metadata": {"arXiv": "2311.15551", "Date": "Mon, 27 Nov 2023 05:35:49 ", "Title": "Instruct2Attack: Language-Guided Semantic Adversarial Attacks", "Authors": ["Jiang Liu", "Chen Wei", "Yuxiang Guo", "Heng Yu", "Alan Yuille", "Soheil Feizi", "Chun Pong Lau", "Rama Chellappa"], "Categories": "cs.CV cs.AI cs.CR cs.LG eess.IV", "Comments": ["under submission", "code coming soon"]}, "abstract": "We propose Instruct2Attack (I2A), a language-guided semantic attack that generates semantically meaningful perturbations according to free-form language instructions. We make use of state-of-the-art latent diffusion models, where we adversarially guide the reverse diffusion process to search for an adversarial latent code conditioned on the input image and text instruction. Compared to existing noise-based and semantic attacks, I2A generates more natural and diverse adversarial examples while providing better controllability and interpretability. We further automate the attack process with GPT-4 to generate diverse image-specific text instructions. We show that I2A can successfully break state-of-the-art deep neural networks even under strong adversarial defenses, and demonstrate great transferability among a variety of network architectures.", "url": "https://arxiv.org/abs/2311.15551"}, {"metadata": {"arXiv": "2311.15599", "Date": "Mon, 27 Nov 2023 07:48:50 ", "Title": "UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition", "Authors": ["Xiaohan Ding", "Yiyuan Zhang", "Yixiao Ge", "Sijie Zhao", "Lin Song", "Xiangyu Yue", "Ying Shan"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Code", "all the models and reproducible training scripts at https://github.com/AILab-CVC/UniRepLKNet"]}, "abstract": "Large-kernel convolutional neural networks (ConvNets) have recently received extensive research attention, but there are two unresolved and critical issues that demand further investigation. 1) The architectures of existing large-kernel ConvNets largely follow the design principles of conventional ConvNets or transformers, while the architectural design for large-kernel ConvNets remains under-addressed. 2) As transformers have dominated multiple modalities, it remains to be investigated whether ConvNets also have a strong universal perception ability in domains beyond vision. In this paper, we contribute from two aspects. 1) We propose four architectural guidelines for designing large-kernel ConvNets, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep. Following such guidelines, our proposed large-kernel ConvNet shows leading performance in image recognition. For example, our models achieve an ImageNet accuracy of 88.0%, ADE20K mIoU of 55.6%, and COCO box AP of 56.4%, demonstrating better performance and higher speed than a number of recently proposed powerful competitors. 2) We discover that large kernels are the key to unlocking the exceptional performance of ConvNets in domains where they were originally not proficient. With certain modality-related preprocessing approaches, the proposed model achieves state-of-the-art performance on time-series forecasting and audio recognition tasks even without modality-specific customization to the architecture. Code and all the models at https://github.com/AILab-CVC/UniRepLKNet.", "url": "https://arxiv.org/abs/2311.15599"}, {"metadata": {"arXiv": "2311.15648", "Date": "Mon, 27 Nov 2023 09:20:12 ", "Title": "Reinforcement Learning from Diffusion Feedback: Q* for Image Search", "Authors": ["Aboli Marathe"], "Categories": "cs.CV cs.AI cs.CL cs.LG cs.RO"}, "abstract": "Large vision-language models are steadily gaining personalization capabilities at the cost of fine-tuning or data augmentation. We present two models for image generation using model-agnostic learning that align semantic priors with generative capabilities. RLDF, or Reinforcement Learning from Diffusion Feedback, is a singular approach for visual imitation through prior-preserving reward function guidance. This employs Q-learning (with standard Q*) for generation and follows a semantic-rewarded trajectory for image search through finite encoding-tailored actions. The second proposed method, noisy diffusion gradient, is optimization driven. At the root of both methods is a special CFG encoding that we propose for continual semantic guidance. Using only a single input image and no text input, RLDF generates high-quality images over varied domains including retail, sports and agriculture showcasing class-consistency and strong visual diversity. Project website is available at https://infernolia.github.io/RLDF.", "url": "https://arxiv.org/abs/2311.15648"}, {"metadata": {"arXiv": "2311.15658", "Date": "Mon, 27 Nov 2023 09:40:14 ", "Title": "Regularization by Texts for Latent Diffusion Inverse Solvers", "Authors": ["Jeongsol Kim", "Geon Yeong Park", "Hyungjin Chung", "Jong Chul Ye"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "The recent advent of diffusion models has led to significant progress in solving inverse problems, leveraging these models as effective generative priors. Nonetheless, challenges related to the ill-posed nature of such problems remain, often due to inherent ambiguities in measurements. Drawing inspiration from the human ability to resolve visual ambiguities through perceptual biases, here we introduce a novel latent diffusion inverse solver by incorporating regularization by texts (TReg). Specifically, TReg applies the textual description of the preconception of the solution during the reverse sampling phase, of which description isndynamically reinforced through null-text optimization for adaptive negation. Our comprehensive experimental results demonstrate that TReg successfully mitigates ambiguity in latent diffusion inverse solvers, enhancing their effectiveness and accuracy.", "url": "https://arxiv.org/abs/2311.15658"}, {"metadata": {"arXiv": "2311.15719", "Date": "Mon, 27 Nov 2023 11:12:33 ", "Title": "Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions", "Authors": ["Benjamin Keel", "Aaron Quyn", "David Jayne", "Samuel D. Relton"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["10 pages (main paper)", "5 pages (references)", "5 figures", "2 tables", "work accepted for BMVC 2023"]}, "abstract": "Lung cancer is responsible for 21% of cancer deaths in the UK and five-year survival rates are heavily influenced by the stage the cancer was identified at. Recent studies have demonstrated the capability of AI methods for accurate and early diagnosis of lung cancer from routine scans. However, this evidence has not translated into clinical practice with one barrier being a lack of interpretable models. This study investigates the application Variational Autoencoders (VAEs), a type of generative AI model, to lung cancer lesions. Proposed models were trained on lesions extracted from 3D CT scans in the LIDC-IDRI public dataset. Latent vector representations of 2D slices produced by the VAEs were explored through clustering to justify their quality and used in an MLP classifier model for lung cancer diagnosis, the best model achieved state-of-the-art metrics of AUC 0.98 and 93.1% accuracy. Cluster analysis shows the VAE latent space separates the dataset of malignant and benign lesions based on meaningful feature components including tumour size, shape, patient and malignancy class. We also include a comparative analysis of the standard Gaussian VAE (GVAE) and the more recent Dirichlet VAE (DirVAE), which replaces the prior with a Dirichlet distribution to encourage a more explainable latent space with disentangled feature representation. Finally, we demonstrate the potential for latent space traversals corresponding to clinically meaningful feature changes.", "url": "https://arxiv.org/abs/2311.15719"}, {"metadata": {"arXiv": "2311.15728", "Date": "Mon, 27 Nov 2023 11:26:41 ", "Title": "Adinkra Symbol Recognition using Classical Machine Learning and Deep Learning", "Authors": ["Michael Adjeisah", "Kwame Omono Asamoah", "Martha Asamoah Yeboah", "Raji Rafiu King", "Godwin Ferguson Achaab and Kingsley Adjei"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["15 pages", "6 figures", "5 tables"]}, "abstract": "Artificial intelligence (AI) has emerged as a transformative influence, engendering paradigm shifts in global societies, spanning academia and industry. However, in light of these rapid advances, addressing the underrepresentation of black communities and African countries in AI is crucial. Boosting enthusiasm for AI can be effectively accomplished by showcasing straightforward applications around tasks like identifying and categorizing traditional symbols, such as Adinkra symbols, or familiar objects within the community. In this research endeavor, we dived into classical machine learning and harnessed the power of deep learning models to tackle the intricate task of classifying and recognizing Adinkra symbols. The idea led to a newly constructed ADINKRA dataset comprising 174,338 images meticulously organized into 62 distinct classes, each representing a singular and emblematic symbol. We constructed a CNN model for classification and recognition using six convolutional layers, three fully connected (FC) layers, and optional dropout regularization. The model is a simpler and smaller version of VGG, with fewer layers, smaller channel sizes, and a fixed kernel size. Additionally, we tap into the transfer learning capabilities provided by pre-trained models like VGG and ResNet. These models assist us in both classifying images and extracting features that can be used with classical machine learning models. We assess the model's performance by measuring its accuracy and convergence rate and visualizing the areas that significantly influence its predictions. These evaluations serve as a foundational benchmark for future assessments of the ADINKRA dataset. We hope this application exemplar inspires ideas on the various uses of AI in organizing our traditional and modern lives.", "url": "https://arxiv.org/abs/2311.15728"}, {"metadata": {"arXiv": "2311.15876", "Date": "Mon, 27 Nov 2023 14:49:06 ", "Title": "RO-LLaMA: Generalist LLM for Radiation Oncology via Noise Augmentation and Consistency Regularization", "Authors": ["Kwanyoung Kim", "Yujin Oh", "Sangjoon Park", "Hwa Kyung Byun", "Jin Sung Kim", "Yong Bae Kim", "Jong Chul Ye"], "Categories": "cs.CV cs.AI cs.LG"}, "abstract": "Recent advancements in Artificial Intelligence (AI) have profoundly influenced medical fields, by providing tools to reduce clinical workloads. However, most AI models are constrained to execute uni-modal tasks, in stark contrast to the comprehensive approaches utilized by medical professionals. To address this, here we present RO-LLaMA, a versatile generalist large language model (LLM) tailored for the field of radiation oncology. This model seamlessly covers a wide range of the workflow of radiation oncologists, adept at various tasks such as clinical report summarization, radiation therapy plan suggestion, and plan-guided therapy target volume segmentation. In particular, to maximize the end-to-end performance, we further present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LLM's robustness to additional errors at the intermediates while preserving the capability of handling clean inputs, and creatively transform this concept into LLM-driven segmentation framework as Consistency Embedding Segmentation (CESEG). Experimental results on multi-centre cohort sets demonstrate our proposed RO-LLaMA's promising performance for diverse tasks with generalization capabilities.", "url": "https://arxiv.org/abs/2311.15876"}, {"metadata": {"arXiv": "2311.15964", "Date": "Mon, 27 Nov 2023 16:07:37 ", "Title": "Efficient Pre-training for Localized Instruction Generation of Videos", "Authors": ["Anil Batra", "Davide Moltisanti", "Laura Sevilla-Lara", "Marcus Rohrbach", "Frank Keller"], "Categories": "cs.CV cs.AI cs.CL cs.LG"}, "abstract": "Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. The curated dataset, three orders of magnitude smaller than current web-scale datasets, enables efficient training of large-scale models with competitive performance. We complement our Sieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step localization and instruction generation for procedural videos. When this model is pre-trained on our curated dataset, it achieves state-of-the-art performance in zero-shot and finetuning settings on YouCook2 and Tasty, while using a fraction of the computational resources.", "url": "https://arxiv.org/abs/2311.15964"}, {"metadata": {"arXiv": "2311.16038", "Date": "Mon, 27 Nov 2023 17:59:41 ", "Title": "OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving", "Authors": ["Wenzhao Zheng", "Weiliang Chen", "Yuanhui Huang", "Borui Zhang", "Yueqi Duan", "Jiwen Lu"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Code is available at: https://github.com/wzzheng/OccWorld"]}, "abstract": "Understanding how the 3D scene evolves is vital for making decisions in autonomous driving. Most existing methods achieve this by predicting the movements of object boxes, which cannot capture more fine-grained scene information. In this paper, we explore a new framework of learning a world model, OccWorld, in the 3D Occupancy space to simultaneously predict the movement of the ego car and the evolution of the surrounding scenes. We propose to learn a world model based on 3D occupancy rather than 3D bounding boxes and segmentation maps for three reasons: 1) expressiveness. 3D occupancy can describe the more fine-grained 3D structure of the scene; 2) efficiency. 3D occupancy is more economical to obtain (e.g., from sparse LiDAR points). 3) versatility. 3D occupancy can adapt to both vision and LiDAR. To facilitate the modeling of the world evolution, we learn a reconstruction-based scene tokenizer on the 3D occupancy to obtain discrete scene tokens to describe the surrounding scenes. We then adopt a GPT-like spatial-temporal generative transformer to generate subsequent scene and ego tokens to decode the future occupancy and ego trajectory. Extensive experiments on the widely used nuScenes benchmark demonstrate the ability of OccWorld to effectively model the evolution of the driving scenes. OccWorld also produces competitive planning results without using instance and map supervision. Code: https://github.com/wzzheng/OccWorld.", "url": "https://arxiv.org/abs/2311.16038"}, {"metadata": {"arXiv": "2311.16102", "Date": "Mon, 27 Nov 2023 18:59:53 ", "Title": "Test-time Adaptation of Discriminative Models via Diffusion Generative Feedback", "Authors": ["Mihir Prabhudesai and Tsung-Wei Ke and Alexander C. Li and Deepak Pathak and Katerina Fragkiadaki"], "Categories": "cs.CV cs.AI cs.LG cs.RO", "Comments": ["Accepted at NeurIPS 2023 Webpage with Code: https://diffusion-tta.github.io/"]}, "abstract": "The advancements in generative modeling, particularly the advent of diffusion models, have sparked a fundamental question: how can these models be effectively used for discriminative tasks? In this work, we find that generative models can be great test-time adapters for discriminative models. Our method, Diffusion-TTA, adapts pre-trained discriminative models such as image classifiers, segmenters and depth predictors, to each unlabelled example in the test set using generative feedback from a diffusion model. We achieve this by modulating the conditioning of the diffusion model using the output of the discriminative model. We then maximize the image likelihood objective by backpropagating the gradients to discriminative model's parameters. We show Diffusion-TTA significantly enhances the accuracy of various large-scale pre-trained discriminative models, such as, ImageNet classifiers, CLIP models, image pixel labellers and image depth predictors. Diffusion-TTA outperforms existing test-time adaptation methods, including TTT-MAE and TENT, and particularly shines in online adaptation setups, where the discriminative model is continually adapted to each example in the test set. We provide access to code, results, and visualizations on our website: https://diffusion-tta.github.io/.", "url": "https://arxiv.org/abs/2311.16102"}, {"metadata": {"arXiv": "2311.14756", "Date": "Thu, 23 Nov 2023 15:46:54 ", "Title": "Task-Distributionally Robust Data-Free Meta-Learning", "Authors": ["Zixuan Hu", "Li Shen", "Zhenyi Wang", "Yongxian Wei", "Baoyuan Wu", "Chun Yuan", "Dacheng Tao"], "Categories": "cs.LG cs.AI"}, "abstract": "Data-Free Meta-Learning (DFML) aims to efficiently learn new tasks by leveraging multiple pre-trained models without requiring their original training data. Existing inversion-based DFML methods construct pseudo tasks from a learnable dataset, which is inversely generated from the pre-trained model pool. For the first time, we reveal two major challenges hindering their practical deployments: Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC). TDS leads to a biased meta-learner because of the skewed task distribution towards newly generated tasks. TDC occurs when untrusted models characterized by misleading labels or poor quality pollute the task distribution. To tackle these issues, we introduce a robust DFML framework that ensures task distributional robustness. We propose to meta-learn from a pseudo task distribution, diversified through task interpolation within a compact task-memory buffer. This approach reduces the meta-learner's overreliance on newly generated tasks by maintaining consistent performance across a broader range of interpolated memory tasks, thus ensuring its generalization for unseen tasks. Additionally, our framework seamlessly incorporates an automated model selection mechanism into the meta-training phase, parameterizing each model's reliability as a learnable weight. This is optimized with a policy gradient algorithm inspired by reinforcement learning, effectively addressing the non-differentiable challenge posed by model selection. Comprehensive experiments across various datasets demonstrate the framework's effectiveness in mitigating TDS and TDC, underscoring its potential to improve DFML in real-world scenarios.", "url": "https://arxiv.org/abs/2311.14756"}, {"metadata": {"arXiv": "2311.14948", "Date": "Sat, 25 Nov 2023 06:55:13 ", "Title": "Effective Backdoor Mitigation Depends on the Pre-training Objective", "Authors": ["Sahil Verma and Gantavya Bhatt and Avi Schwarzschild and Soumye Singhal and Arnav Mohanty Das and Chirag Shah and John P Dickerson and Jeff Bilmes"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["Accepted for oral presentation at BUGS workshop @ NeurIPS 2023 (https://neurips2023-bugs.github.io/)"]}, "abstract": "Despite the advanced capabilities of contemporary machine learning (ML) models, they remain vulnerable to adversarial and backdoor attacks. This vulnerability is particularly concerning in real-world deployments, where compromised models may exhibit unpredictable behavior in critical scenarios. Such risks are heightened by the prevalent practice of collecting massive, internet-sourced datasets for pre-training multimodal models, as these datasets may harbor backdoors. Various techniques have been proposed to mitigate the effects of backdooring in these models such as CleanCLIP which is the current state-of-the-art approach. In this work, we demonstrate that the efficacy of CleanCLIP in mitigating backdoors is highly dependent on the particular objective used during model pre-training. We observe that stronger pre-training objectives correlate with harder to remove backdoors behaviors. We show this by training multimodal models on two large datasets consisting of 3 million (CC3M) and 6 million (CC6M) datapoints, under various pre-training objectives, followed by poison removal using CleanCLIP. We find that CleanCLIP is ineffective when stronger pre-training objectives are used, even with extensive hyperparameter tuning. Our findings underscore critical considerations for ML practitioners who pre-train models using large-scale web-curated data and are concerned about potential backdoor threats. Notably, our results suggest that simpler pre-training objectives are more amenable to effective backdoor removal. This insight is pivotal for practitioners seeking to balance the trade-offs between using stronger pre-training objectives and security against backdoor attacks.", "url": "https://arxiv.org/abs/2311.14948"}, {"metadata": {"arXiv": "2311.14994", "Date": "Sat, 25 Nov 2023 10:46:06 ", "Title": "Exploring Causal Learning through Graph Neural Networks: An In-depth Review", "Authors": ["Simi Job", "Xiaohui Tao", "Taotao Cai", "Haoran Xie", "Lin Li", "Jianming Yong and Qing Li"], "Categories": "cs.LG cs.AI"}, "abstract": "In machine learning, exploring data correlations to predict outcomes is a fundamental task. Recognizing causal relationships embedded within data is pivotal for a comprehensive understanding of system dynamics, the significance of which is paramount in data-driven decision-making processes. Beyond traditional methods, there has been a surge in the use of graph neural networks (GNNs) for causal learning, given their capabilities as universal data approximators. Thus, a thorough review of the advancements in causal learning using GNNs is both relevant and timely. To structure this review, we introduce a novel taxonomy that encompasses various state-of-the-art GNN methods employed in studying causality. GNNs are further categorized based on their applications in the causality domain. We further provide an exhaustive compilation of datasets integral to causal learning with GNNs to serve as a resource for practical study. This review also touches upon the application of causal learning across diverse sectors. We conclude the review with insights into potential challenges and promising avenues for future exploration in this rapidly evolving field of machine learning.", "url": "https://arxiv.org/abs/2311.14994"}, {"metadata": {"arXiv": "2311.15036", "Date": "Sat, 25 Nov 2023 14:18:29 ", "Title": "On-Device Soft Sensors: Real-Time Fluid Flow Estimation from Level Sensor Data", "Authors": ["Tianheng Ling", "Chao Qian", "Gregor Schiele"], "Categories": "cs.LG cs.AI", "Comments": ["8 pages", "6 figures", "1 Table", "Accepted by the 1st AUTONOMOUS UBIQUITOUS SYSTEMS (AUTOQUITOUS) WORKSHOP of EAI MobiQuitous 2023 - 20th EAI International Conference on Mobile and Ubiquitous Systems: Computing", "Networking and Services"]}, "abstract": "Soft sensors are crucial in bridging autonomous systems' physical and digital realms, enhancing sensor fusion and perception. Instead of deploying soft sensors on the Cloud, this study shift towards employing on-device soft sensors, promising heightened efficiency and bolstering data security. Our approach substantially improves energy efficiency by deploying Artificial Intelligence (AI) directly on devices within a wireless sensor network. Furthermore, the synergistic integration of the Microcontroller Unit and Field-Programmable Gate Array (FPGA) leverages the rapid AI inference capabilities of the latter. Empirical evidence from our real-world use case demonstrates that FPGA-based soft sensors achieve inference times ranging remarkably from 1.04 to 12.04 microseconds. These compelling results highlight the considerable potential of our innovative approach for executing real-time inference tasks efficiently, thereby presenting a feasible alternative that effectively addresses the latency challenges intrinsic to Cloud-based deployments.", "url": "https://arxiv.org/abs/2311.15036"}, {"metadata": {"arXiv": "2311.15041", "Date": "Sat, 25 Nov 2023 14:39:12 ", "Title": "MPCNN: A Novel Matrix Profile Approach for CNN-based Sleep Apnea Classification", "Authors": ["Hieu X. Nguyen", "Duong V. Nguyen", "Hieu H. Pham", "and Cuong D. Do"], "Categories": "cs.LG cs.AI eess.SP"}, "abstract": "Sleep apnea (SA) is a significant respiratory condition that poses a major global health challenge. Previous studies have investigated several machine and deep learning models for electrocardiogram (ECG)-based SA diagnoses. Despite these advancements, conventional feature extractions derived from ECG signals, such as R-peaks and RR intervals, may fail to capture crucial information encompassed within the complete PQRST segments. In this study, we propose an innovative approach to address this diagnostic gap by delving deeper into the comprehensive segments of the ECG signal. The proposed methodology draws inspiration from Matrix Profile algorithms, which generate an Euclidean distance profile from fixed-length signal subsequences. From this, we derived the Min Distance Profile (MinDP), Max Distance Profile (MaxDP), and Mean Distance Profile (MeanDP) based on the minimum, maximum, and mean of the profile distances, respectively. To validate the effectiveness of our approach, we use the modified LeNet-5 architecture as the primary CNN model, along with two existing lightweight models, BAFNet and SE-MSCNN, for ECG classification tasks. Our extensive experimental results on the PhysioNet Apnea-ECG dataset revealed that with the new feature extraction method, we achieved a per-segment accuracy up to 92.11 \\% and a per-recording accuracy of 100\\%. Moreover, it yielded the highest correlation compared to state-of-the-art methods, with a correlation coefficient of 0.989. By introducing a new feature extraction method based on distance relationships, we enhanced the performance of certain lightweight models, showing potential for home sleep apnea test (HSAT) and SA detection in IoT devices. The source code for this work is made publicly available in GitHub: https://github.com/vinuni-vishc/MPCNN-Sleep-Apnea.", "url": "https://arxiv.org/abs/2311.15041"}, {"metadata": {"arXiv": "2311.15056", "Date": "Sat, 25 Nov 2023 15:44:28 ", "Title": "Accurate and interpretable drug-drug interaction prediction enabled by knowledge subgraph learning", "Authors": ["Yaqing Wang and Zaifei Yang and Quanming Yao"], "Categories": "cs.LG cs.AI"}, "abstract": "Background: Discovering potential drug-drug interactions (DDIs) is a long-standing challenge in clinical treatments and drug developments. Recently, deep learning techniques have been developed for DDI prediction. However, they generally require a huge number of samples, while known DDIs are rare. Methods: In this work, we present KnowDDI, a graph neural network-based method that addresses the above challenge. KnowDDI enhances drug representations by adaptively leveraging rich neighborhood information from large biomedical knowledge graphs. Then, it learns a knowledge subgraph for each drug-pair to interpret the predicted DDI, where each of the edges is associated with a connection strength indicating the importance of a known DDI or resembling strength between a drug-pair whose connection is unknown. Thus, the lack of DDIs is implicitly compensated by the enriched drug representations and propagated drug similarities. Results: We evaluate KnowDDI on two benchmark DDI datasets. Results show that KnowDDI obtains the state-of-the-art prediction performance with better interpretability. We also find that KnowDDI suffers less than existing works given a sparser knowledge graph. This indicates that the propagated drug similarities play a more important role in compensating for the lack of DDIs when the drug representations are less enriched. Conclusions: KnowDDI nicely combines the efficiency of deep learning techniques and the rich prior knowledge in biomedical knowledge graphs. As an original open-source tool, KnowDDI can help detect possible interactions in a broad range of relevant interaction prediction tasks, such as protein-protein interactions, drug-target interactions and disease-gene interactions, eventually promoting the development of biomedicine and healthcare.", "url": "https://arxiv.org/abs/2311.15056"}, {"metadata": {"arXiv": "2311.15112", "Date": "Sat, 25 Nov 2023 20:06:46 ", "Title": "Everybody Needs a Little HELP: Explaining Graphs via Hierarchical Concepts", "Authors": ["Jonas J\\\"ur{\\ss}", "Lucie Charlotte Magister", "Pietro Barbiero", "Pietro Li\\`o", "Nikola Simidjievski"], "Categories": "cs.LG cs.AI", "Comments": ["33 pages", "16 figures", "accepted at the NeurIPS 2023 GLFrontiers Workshop"]}, "abstract": "Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.", "url": "https://arxiv.org/abs/2311.15112"}, {"metadata": {"arXiv": "2311.15131", "Date": "Sat, 25 Nov 2023 22:41:23 ", "Title": "Localizing Lying in Llama: Understanding Instructed Dishonesty on True-False Questions Through Prompting, Probing, and Patching", "Authors": ["James Campbell", "Richard Ren", "Phillip Guo"], "Categories": "cs.LG cs.AI cs.CL", "Comments": ["14 pages", "12 figures"]}, "abstract": "Large language models (LLMs) demonstrate significant knowledge through their outputs, though it is often unclear whether false outputs are due to a lack of knowledge or dishonesty. In this paper, we investigate instructed dishonesty, wherein we explicitly prompt LLaMA-2-70b-chat to lie. We perform prompt engineering to find which prompts best induce lying behavior, and then use mechanistic interpretability approaches to localize where in the network this behavior occurs. Using linear probing and activation patching, we localize five layers that appear especially important for lying. We then find just 46 attention heads within these layers that enable us to causally intervene such that the lying model instead answers honestly. We show that these interventions work robustly across many prompts and dataset splits. Overall, our work contributes a greater understanding of dishonesty in LLMs so that we may hope to prevent it.", "url": "https://arxiv.org/abs/2311.15131"}, {"metadata": {"arXiv": "2311.15134", "Date": "Sat, 25 Nov 2023 22:51:01 ", "Title": "SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling", "Authors": ["Habib Hajimolahoseini", "Omar Mohamed Awad", "Walid Ahmed", "Austin Wen", "Saina Asani", "Mohammad Hassanpour", "Farnoosh Javadi", "Mehdi Ahmadi", "Foozhan Ataiefard", "Kangling Liu", "Yang Liu"], "Categories": "cs.LG cs.AI"}, "abstract": "In this paper, we present SwiftLearn, a data-efficient approach to accelerate training of deep learning models using a subset of data samples selected during the warm-up stages of training. This subset is selected based on an importance criteria measured over the entire dataset during warm-up stages, aiming to preserve the model performance with fewer examples during the rest of training. The importance measure we propose could be updated during training every once in a while, to make sure that all of the data samples have a chance to return to the training loop if they show a higher importance. The model architecture is unchanged but since the number of data samples controls the number of forward and backward passes during training, we can reduce the training time by reducing the number of training samples used in each epoch of training. Experimental results on a variety of CV and NLP models during both pretraining and finetuning show that the model performance could be preserved while achieving a significant speed-up during training. More specifically, BERT finetuning on GLUE benchmark shows that almost 90% of the data can be dropped achieving an end-to-end average speedup of 3.36x while keeping the average accuracy drop less than 0.92%.", "url": "https://arxiv.org/abs/2311.15134"}, {"metadata": {"arXiv": "2311.15156", "Date": "Sun, 26 Nov 2023 01:23:01 ", "Title": "xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data", "Authors": ["Jing Gong", "Minsheng Hao", "Xingyi Cheng", "Xin Zeng", "Chiming Liu", "Jianzhu Ma", "Xuegong Zhang", "Taifeng Wang", "Le Song"], "Categories": "cs.LG cs.AI q-bio.GN", "Comments": ["Accepted by NeurIPS 2023"]}, "abstract": "Advances in high-throughput sequencing technology have led to significant progress in measuring gene expressions at the single-cell level. The amount of publicly available single-cell RNA-seq (scRNA-seq) data is already surpassing 50M records for humans with each record measuring 20,000 genes. This highlights the need for unsupervised representation learning to fully ingest these data, yet classical transformer architectures are prohibitive to train on such data in terms of both computation and memory. To address this challenge, we propose a novel asymmetric encoder-decoder transformer for scRNA-seq data, called xTrimoGene$^\\alpha$ (or xTrimoGene for short), which leverages the sparse characteristic of the data to scale up the pre-training. This scalable design of xTrimoGene reduces FLOPs by one to two orders of magnitude compared to classical transformers while maintaining high accuracy, enabling us to train the largest transformer models over the largest scRNA-seq dataset today. Our experiments also show that the performance of xTrimoGene improves as we scale up the model sizes, and it also leads to SOTA performance over various downstream tasks, such as cell type annotation, perturb-seq effect prediction, and drug combination prediction. xTrimoGene model is now available for use as a service via the following link: https://api.biomap.com/xTrimoGene/apply.", "url": "https://arxiv.org/abs/2311.15156"}, {"metadata": {"arXiv": "2311.15161", "Date": "Sun, 26 Nov 2023 01:44:01 ", "Title": "Hessian Aware Low-Rank Weight Perturbation for Continual Learning", "Authors": ["Jiaqi Li", "Rui Wang", "Yuanhao Lai", "Changjian Shui", "Sabyasachi Sahoo", "Charles X. Ling", "Shichun Yang", "Boyu Wang", "Christian Gagn\\'e", "Fan Zhou"], "Categories": "cs.LG cs.AI"}, "abstract": "Continual learning aims to learn a series of tasks sequentially without forgetting the knowledge acquired from the previous ones. In this work, we propose the Hessian Aware Low-Rank Perturbation algorithm for continual learning. By modeling the parameter transitions along the sequential tasks with the weight matrix transformation, we propose to apply the low-rank approximation on the task-adaptive parameters in each layer of the neural networks. Specifically, we theoretically demonstrate the quantitative relationship between the Hessian and the proposed low-rank approximation. The approximation ranks are then globally determined according to the marginal increment of the empirical loss estimated by the layer-specific gradient and low-rank approximation error. Furthermore, we control the model capacity by pruning less important parameters to diminish the parameter growth. We conduct extensive experiments on various benchmarks, including a dataset with large-scale tasks, and compare our method against some recent state-of-the-art methods to demonstrate the effectiveness and scalability of our proposed method. Empirical results show that our method performs better on different benchmarks, especially in achieving task order robustness and handling the forgetting issue. A demo code can be found at https://github.com/lijiaqi/HALRP.", "url": "https://arxiv.org/abs/2311.15161"}, {"metadata": {"arXiv": "2311.15194", "Date": "Sun, 26 Nov 2023 05:17:45 ", "Title": "Neural Network Models of Becoming a Cardinal Principle Knower", "Authors": ["Vima Gupta", "Sashank Varma"], "Categories": "cs.LG cs.AI", "Comments": ["6 pages", "11 figures"]}, "abstract": "As children enter elementary school, their understanding of the ordinal structure of numbers transitions from a memorized count list of the first 50-100 numbers to knowing the successor function and understanding the countably infinite. We investigate this developmental change in two neural network models that learn the successor function on the pairs (N, N+1) for N in (0, 98). The first uses a one-hot encoding of the input and output values and corresponds to children memorizing a count list, while the second model uses a place-value encoding and corresponds to children learning the language rules for naming numbers. The place-value model showed a predicted drop in representational similarity across tens boundaries. Counting across a tens boundary can be understood as a vector operation in 2D space, where the numbers with the same tens place are organized in a linearly separable manner, whereas those with the same ones place are grouped together. A curriculum learning simulation shows that, in the expanding numerical environment of the developing child, representations of smaller numbers continue to be sharpened even as larger numbers begin to be learned. These models set the stage for future work using recurrent architectures to move beyond learning the successor function to simulating the counting process more generally, and point towards a deeper understanding of what it means to understand the countably infinite.", "url": "https://arxiv.org/abs/2311.15194"}, {"metadata": {"arXiv": "2311.15268", "Date": "Sun, 26 Nov 2023 11:12:30 ", "Title": "Unlearning via Sparse Representations", "Authors": ["Vedant Shah", "Frederik Tr\\\"auble", "Ashish Malik", "Hugo Larochelle", "Michael Mozer", "Sanjeev Arora", "Yoshua Bengio", "Anirudh Goyal"], "Categories": "cs.LG cs.AI"}, "abstract": "Machine \\emph{unlearning}, which involves erasing knowledge about a \\emph{forget set} from a trained model, can prove to be costly and infeasible by existing techniques. We propose a nearly compute-free zero-shot unlearning technique based on a discrete representational bottleneck. We show that the proposed technique efficiently unlearns the forget set and incurs negligible damage to the model's performance on the rest of the data set. We evaluate the proposed technique on the problem of \\textit{class unlearning} using three datasets: CIFAR-10, CIFAR-100, and LACUNA-100. We compare the proposed technique to SCRUB, a state-of-the-art approach which uses knowledge distillation for unlearning. Across all three datasets, the proposed technique performs as well as, if not better than SCRUB while incurring almost no computational cost.", "url": "https://arxiv.org/abs/2311.15268"}, {"metadata": {"arXiv": "2311.15283", "Date": "Sun, 26 Nov 2023 12:50:28 ", "Title": "Bias-Variance Trade-off in Physics-Informed Neural Networks with Randomized Smoothing for High-Dimensional PDEs", "Authors": ["Zheyuan Hu", "Zhouhao Yang", "Yezhen Wang", "George Em Karniadakis", "Kenji Kawaguchi"], "Categories": "cs.LG cs.AI cs.NA math.DS math.NA stat.ML", "Comments": ["21 pages", "5 figures"], "MSC-class": "14J60"}, "abstract": "While physics-informed neural networks (PINNs) have been proven effective for low-dimensional partial differential equations (PDEs), the computational cost remains a hurdle in high-dimensional scenarios. This is particularly pronounced when computing high-order and high-dimensional derivatives in the physics-informed loss. Randomized Smoothing PINN (RS-PINN) introduces Gaussian noise for stochastic smoothing of the original neural net model, enabling Monte Carlo methods for derivative approximation, eliminating the need for costly auto-differentiation. Despite its computational efficiency in high dimensions, RS-PINN introduces biases in both loss and gradients, negatively impacting convergence, especially when coupled with stochastic gradient descent (SGD). We present a comprehensive analysis of biases in RS-PINN, attributing them to the nonlinearity of the Mean Squared Error (MSE) loss and the PDE nonlinearity. We propose tailored bias correction techniques based on the order of PDE nonlinearity. The unbiased RS-PINN allows for a detailed examination of its pros and cons compared to the biased version. Specifically, the biased version has a lower variance and runs faster than the unbiased version, but it is less accurate due to the bias. To optimize the bias-variance trade-off, we combine the two approaches in a hybrid method that balances the rapid convergence of the biased version with the high accuracy of the unbiased version. In addition, we present an enhanced implementation of RS-PINN. Extensive experiments on diverse high-dimensional PDEs, including Fokker-Planck, HJB, viscous Burgers', Allen-Cahn, and Sine-Gordon equations, illustrate the bias-variance trade-off and highlight the effectiveness of the hybrid RS-PINN. Empirical guidelines are provided for selecting biased, unbiased, or hybrid versions, depending on the dimensionality and nonlinearity of the specific PDE problem.", "url": "https://arxiv.org/abs/2311.15283"}, {"metadata": {"arXiv": "2311.15287", "Date": "Sun, 26 Nov 2023 13:02:24 ", "Title": "Spatial and Temporal Characteristics of Freight Tours: A Data-Driven Exploratory Analysis", "Authors": ["Ali Nadi", "L\\'or\\'ant Tavasszy", "J.W.C. van Lint", "Maaike Snelder"], "Categories": "cs.LG cs.AI physics.soc-ph"}, "abstract": "This paper presents a modeling approach to infer scheduling and routing patterns from digital freight transport activity data for different freight markets. We provide a complete modeling framework including a new discrete-continuous decision tree approach for extracting rules from the freight transport data. We apply these models to collected tour data for the Netherlands to understand departure time patterns and tour strategies, also allowing us to evaluate the effectiveness of the proposed algorithm. We find that spatial and temporal characteristics are important to capture the types of tours and time-of-day patterns of freight activities. Also, the empirical evidence indicates that carriers in most of the transport markets are sensitive to the level of congestion. Many of them adjust the type of tour, departure time, and the number of stops per tour when facing a congested zone. The results can be used by practitioners to get more grip on transport markets and develop freight and traffic management measures.", "url": "https://arxiv.org/abs/2311.15287"}, {"metadata": {"arXiv": "2311.15303", "Date": "Sun, 26 Nov 2023 14:00:14 ", "Title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement", "Authors": ["Avani Gupta", "Saurabh Saini", "P J Narayanan"], "Categories": "cs.LG cs.AI", "Comments": ["Neurips 2023"]}, "abstract": "Humans use abstract concepts for understanding instead of hard features. Recent interpretability research has focused on human-centered concept explanations of neural networks. Concept Activation Vectors (CAVs) estimate a model's sensitivity and possible biases to a given concept. In this paper, we extend CAVs from post-hoc analysis to ante-hoc training in order to reduce model bias through fine-tuning using an additional Concept Loss. Concepts were defined on the final layer of the network in the past. We generalize it to intermediate layers using class prototypes. This facilitates class learning in the last convolution layer, which is known to be most informative. We also introduce Concept Distillation to create richer concepts using a pre-trained knowledgeable model as the teacher. Our method can sensitize or desensitize a model towards concepts. We show applications of concept-sensitive training to debias several classification problems. We also use concepts to induce prior knowledge into IID, a reconstruction problem. Concept-sensitive training can improve model interpretability, reduce biases, and induce prior knowledge. Please visit https://avani17101.github.io/Concept-Distilllation/ for code and more details.", "url": "https://arxiv.org/abs/2311.15303"}, {"metadata": {"arXiv": "2311.15332", "Date": "Sun, 26 Nov 2023 15:34:36 ", "Title": "ASI: Accuracy-Stability Index for Evaluating Deep Learning Models", "Authors": ["Wei Dai", "Daniel Berleant"], "Categories": "cs.LG cs.AI cs.CV cs.IT cs.PF math.IT", "Comments": ["6 pages", "3 figures"]}, "abstract": "In the context of deep learning research, where model introductions continually occur, the need for effective and efficient evaluation remains paramount. Existing methods often emphasize accuracy metrics, overlooking stability. To address this, the paper introduces the Accuracy-Stability Index (ASI), a quantitative measure incorporating both accuracy and stability for assessing deep learning models. Experimental results demonstrate the application of ASI, and a 3D surface model is presented for visualizing ASI, mean accuracy, and coefficient of variation. This paper addresses the important issue of quantitative benchmarking metrics for deep learning models, providing a new approach for accurately evaluating accuracy and stability of deep learning models. The paper concludes with discussions on potential weaknesses and outlines future research directions.", "url": "https://arxiv.org/abs/2311.15332"}, {"metadata": {"arXiv": "2311.15335", "Date": "Sun, 26 Nov 2023 15:39:57 ", "Title": "Token Recycling for Efficient Sequential Inference with Vision Transformers", "Authors": ["Jan Olszewski and Dawid Rymarczyk and Piotr W\\'ojcik and Mateusz Pach and Bartosz Zieli\\'nski"], "Categories": "cs.LG cs.AI cs.CV", "Comments": ["The code will be released upon acceptance"]}, "abstract": "Vision Transformers (ViTs) overpass Convolutional Neural Networks in processing incomplete inputs because they do not require the imputation of missing values. Therefore, ViTs are well suited for sequential decision-making, e.g. in the Active Visual Exploration problem. However, they are computationally inefficient because they perform a full forward pass each time a piece of new sequential information arrives. To reduce this computational inefficiency, we introduce the TOken REcycling (TORE) modification for the ViT inference, which can be used with any architecture. TORE divides ViT into two parts, iterator and aggregator. An iterator processes sequential information separately into midway tokens, which are cached. The aggregator processes midway tokens jointly to obtain the prediction. This way, we can reuse the results of computations made by iterator. Except for efficient sequential inference, we propose a complementary training policy, which significantly reduces the computational burden associated with sequential decision-making while achieving state-of-the-art accuracy.", "url": "https://arxiv.org/abs/2311.15335"}, {"metadata": {"arXiv": "2311.15373", "Date": "Sun, 26 Nov 2023 18:09:24 ", "Title": "Confidence Is All You Need for MI Attacks", "Authors": ["Abhishek Sinha", "Himanshi Tibrewal", "Mansi Gupta", "Nikhar Waghela", "Shivank Garg"], "Categories": "cs.LG cs.AI cs.CR", "Comments": ["2 pages", "1 figure"]}, "abstract": "In this evolving era of machine learning security, membership inference attacks have emerged as a potent threat to the confidentiality of sensitive data. In this attack, adversaries aim to determine whether a particular point was used during the training of a target model. This paper proposes a new method to gauge a data point's membership in a model's training set. Instead of correlating loss with membership, as is traditionally done, we have leveraged the fact that training examples generally exhibit higher confidence values when classified into their actual class. During training, the model is essentially being 'fit' to the training data and might face particular difficulties in generalization to unseen data. This asymmetry leads to the model achieving higher confidence on the training data as it exploits the specific patterns and noise present in the training data. Our proposed approach leverages the confidence values generated by the machine learning model. These confidence values provide a probabilistic measure of the model's certainty in its predictions and can further be used to infer the membership of a given data point. Additionally, we also introduce another variant of our method that allows us to carry out this attack without knowing the ground truth(true class) of a given data point, thus offering an edge over existing label-dependent attack methods.", "url": "https://arxiv.org/abs/2311.15373"}, {"metadata": {"arXiv": "2311.15399", "Date": "Sun, 26 Nov 2023 19:47:39 ", "Title": "Optimally Teaching a Linear Behavior Cloning Agent", "Authors": ["Shubham Kumar Bharti", "Stephen Wright", "Adish Singla", "Xiaojin Zhu"], "Categories": "cs.LG cs.AI"}, "abstract": "We study optimal teaching of Linear Behavior Cloning (LBC) learners. In this setup, the teacher can select which states to demonstrate to an LBC learner. The learner maintains a version space of infinite linear hypotheses consistent with the demonstration. The goal of the teacher is to teach a realizable target policy to the learner using minimum number of state demonstrations. This number is known as the Teaching Dimension(TD). We present a teaching algorithm called ``Teach using Iterative Elimination(TIE)\" that achieves instance optimal TD. However, we also show that finding optimal teaching set computationally is NP-hard. We further provide an approximation algorithm that guarantees an approximation ratio of $\\log(|A|-1)$ on the teaching dimension. Finally, we provide experimental results to validate the efficiency and effectiveness of our algorithm.", "url": "https://arxiv.org/abs/2311.15399"}, {"metadata": {"arXiv": "2311.15448", "Date": "Sun, 26 Nov 2023 22:22:38 ", "Title": "GGNNs : Generalizing GNNs using Residual Connections and Weighted Message Passing", "Authors": ["Abhinav Raghuvanshi and Kushal Sokke Malleshappa"], "Categories": "cs.LG cs.AI"}, "abstract": "Many real-world phenomena can be modeled as a graph, making them extremely valuable due to their ubiquitous presence. GNNs excel at capturing those relationships and patterns within these graphs, enabling effective learning and prediction tasks. GNNs are constructed using Multi-Layer Perceptrons (MLPs) and incorporate additional layers for message passing to facilitate the flow of features among nodes. It is commonly believed that the generalizing power of GNNs is attributed to the message-passing mechanism between layers, where nodes exchange information with their neighbors, enabling them to effectively capture and propagate information across the nodes of a graph. Our technique builds on these results, modifying the message-passing mechanism further: one by weighing the messages before accumulating at each node and another by adding Residual connections. These two mechanisms show significant improvements in learning and faster convergence", "url": "https://arxiv.org/abs/2311.15448"}, {"metadata": {"arXiv": "2311.15480", "Date": "Mon, 27 Nov 2023 01:44:02 ", "Title": "Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure", "Authors": ["Callie C. Liao", "Duoduo Liao", "Jesse Guessford"], "Categories": "cs.LG cs.AI cs.CL cs.MM cs.SD", "Comments": ["Submitted to IEEE Big Data 2023 Conference"]}, "abstract": "There has recently been a sharp increase in interest in Artificial Intelligence-Generated Content (AIGC). Despite this, musical components such as time signatures have not been studied sufficiently to form an algorithmic determination approach for new compositions, especially lyrical songs. This is likely because of the neglect of musical details, which is critical for constructing a robust framework. Specifically, time signatures establish the fundamental rhythmic structure for almost all aspects of a song, including the phrases and notes. In this paper, we propose a novel approach that only uses lyrics as input to automatically generate a fitting time signature for lyrical songs and uncover the latent rhythmic structure utilizing explainable machine learning models. In particular, we devise multiple methods that are associated with discovering lyrical patterns and creating new features that simultaneously contain lyrical, rhythmic, and statistical information. In this approach, the best of our experimental results reveal a 97.6% F1 score and a 0.996 Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) score. In conclusion, our research directly generates time signatures from lyrics automatically for new scores utilizing machine learning, which is an innovative idea that approaches an understudied component of musicology and therefore contributes significantly to the future of Artificial Intelligence (AI) music generation.", "url": "https://arxiv.org/abs/2311.15480"}, {"metadata": {"arXiv": "2311.15487", "Date": "Mon, 27 Nov 2023 02:12:02 ", "Title": "Global $\\mathcal{L}^2$ minimization with certainty via geometrically adapted gradient descent in Deep Learning", "Authors": ["Thomas Chen"], "Categories": "cs.LG cs.AI math-ph math.MP math.OC stat.ML", "Comments": ["AMS Latex", "12 pages"], "MSC-class": "57R70, 62M45"}, "abstract": "We consider the gradient descent flow widely used for the minimization of the $\\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate. We point out relations of the latter to sub-Riemannian geometry.", "url": "https://arxiv.org/abs/2311.15487"}, {"metadata": {"arXiv": "2311.15530", "Date": "Mon, 27 Nov 2023 04:23:47 ", "Title": "SSIN: Self-Supervised Learning for Rainfall Spatial Interpolation", "Authors": ["Jia Li", "Yanyan Shen", "Lei Chen", "Charles Wang Wai NG"], "Categories": "cs.LG cs.AI physics.ao-ph", "Comments": ["SIGMOD 2023 Data-intensive Applications (DIA) Track; Code is available at https://github.com/jlidw/SSIN"], "DOI": "10.1145/3589321"}, "abstract": "The acquisition of accurate rainfall distribution in space is an important task in hydrological analysis and natural disaster pre-warning. However, it is impossible to install rain gauges on every corner. Spatial interpolation is a common way to infer rainfall distribution based on available raingauge data. However, the existing works rely on some unrealistic pre-settings to capture spatial correlations, which limits their performance in real scenarios. To tackle this issue, we propose the SSIN, which is a novel data-driven self-supervised learning framework for rainfall spatial interpolation by mining latent spatial patterns from historical observation data. Inspired by the Cloze task and BERT, we fully consider the characteristics of spatial interpolation and design the SpaFormer model based on the Transformer architecture as the core of SSIN. Our main idea is: by constructing rich self-supervision signals via random masking, SpaFormer can learn informative embeddings for raw data and then adaptively model spatial correlations based on rainfall spatial context. Extensive experiments on two real-world raingauge datasets show that our method outperforms the state-of-the-art solutions. In addition, we take traffic spatial interpolation as another use case to further explore the performance of our method, and SpaFormer achieves the best performance on one large real-world traffic dataset, which further confirms the effectiveness and generality of our method.", "url": "https://arxiv.org/abs/2311.15530"}, {"metadata": {"arXiv": "2311.15545", "Date": "Mon, 27 Nov 2023 05:21:08 ", "Title": "Out-of-Distribution Generalized Dynamic Graph Neural Network for Human Albumin Prediction", "Authors": ["Zeyang Zhang and Xingwang Li and Fei Teng and Ning Lin and Xueling Zhu and Xin Wang and Wenwu Zhu"], "Categories": "cs.LG cs.AI cs.CE", "Comments": ["MedAI'23"]}, "abstract": "Human albumin is essential for indicating the body's overall health. Accurately predicting plasma albumin levels and determining appropriate doses are urgent clinical challenges, particularly in critically ill patients, to maintain optimal blood levels. However, human albumin prediction is non-trivial that has to leverage the dynamics of biochemical markers as well as the experience of treating patients. Moreover, the problem of distribution shift is often encountered in real clinical data, which may lead to a decline in the model prediction performance and reduce the reliability of the model's application. In this paper, we propose a framework named Out-of-Distribution Generalized Dynamic Graph Neural Network for Human Albumin Prediction (DyG-HAP), which is able to provide accurate albumin predictions for Intensity Care Unit (ICU) patients during hospitalization. We first model human albumin prediction as a dynamic graph regression problem to model the dynamics and patient relationship. Then, we propose a disentangled dynamic graph attention mechanism to capture and disentangle the patterns whose relationship to labels under distribution shifts is invariant and variant respectively. Last, we propose an invariant dynamic graph regression method to encourage the model to rely on invariant patterns to make predictions. Moreover, we propose a dataset named Albumin level testing and nutritional dosing data for Intensive Care (ANIC) for evaluation. Extensive experiments demonstrate the superiority of our method compared to several baseline methods in human albumin prediction.", "url": "https://arxiv.org/abs/2311.15545"}, {"metadata": {"arXiv": "2311.15603", "Date": "Mon, 27 Nov 2023 07:53:44 ", "Title": "QuickDrop: Efficient Federated Unlearning by Integrated Dataset Distillation", "Authors": ["Akash Dhasade", "Yaohong Ding", "Song Guo", "Anne-marie Kermarrec", "Martijn De Vos", "Leijie Wu"], "Categories": "cs.LG cs.AI"}, "abstract": "Federated Unlearning (FU) aims to delete specific training data from an ML model trained using Federated Learning (FL). We introduce QuickDrop, an efficient and original FU method that utilizes dataset distillation (DD) to accelerate unlearning and drastically reduces computational overhead compared to existing approaches. In QuickDrop, each client uses DD to generate a compact dataset representative of the original training dataset, called a distilled dataset, and uses this compact dataset during unlearning. To unlearn specific knowledge from the global model, QuickDrop has clients execute Stochastic Gradient Ascent with samples from the distilled datasets, thus significantly reducing computational overhead compared to conventional FU methods. We further increase the efficiency of QuickDrop by ingeniously integrating DD into the FL training process. By reusing the gradient updates produced during FL training for DD, the overhead of creating distilled datasets becomes close to negligible. Evaluations on three standard datasets show that, with comparable accuracy guarantees, QuickDrop reduces the duration of unlearning by 463.8x compared to model retraining from scratch and 65.1x compared to existing FU approaches. We also demonstrate the scalability of QuickDrop with 100 clients and show its effectiveness while handling multiple unlearning operations.", "url": "https://arxiv.org/abs/2311.15603"}, {"metadata": {"arXiv": "2311.15722", "Date": "Mon, 27 Nov 2023 11:17:20 ", "Title": "GLIME: General, Stable and Local LIME Explanation", "Authors": ["Zeren Tan", "Yang Tian", "Jian Li"], "Categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "Comments": ["Accepted by NeurIPS 2023 as a Spotlight paper"]}, "abstract": "As black-box machine learning models grow in complexity and find applications in high-stakes scenarios, it is imperative to provide explanations for their predictions. Although Local Interpretable Model-agnostic Explanations (LIME) [22] is a widely adpoted method for understanding model behaviors, it is unstable with respect to random seeds [35,24,3] and exhibits low local fidelity (i.e., how well the explanation approximates the model's local behaviors) [21,16]. Our study shows that this instability problem stems from small sample weights, leading to the dominance of regularization and slow convergence. Additionally, LIME's sampling neighborhood is non-local and biased towards the reference, resulting in poor local fidelity and sensitivity to reference choice. To tackle these challenges, we introduce GLIME, an enhanced framework extending LIME and unifying several prior methods. Within the GLIME framework, we derive an equivalent formulation of LIME that achieves significantly faster convergence and improved stability. By employing a local and unbiased sampling distribution, GLIME generates explanations with higher local fidelity compared to LIME. GLIME explanations are independent of reference choice. Moreover, GLIME offers users the flexibility to choose a sampling distribution based on their specific scenarios.", "url": "https://arxiv.org/abs/2311.15722"}, {"metadata": {"arXiv": "2311.15816", "Date": "Mon, 27 Nov 2023 13:41:20 ", "Title": "Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale", "Authors": ["Soyed Tuhin Ahmed", "Kamal Danouchi", "Michael Hefenbrock", "Guillaume Prenat", "Lorena Anghel", "Mehdi B. Tahoori"], "Categories": "cs.LG cs.AI cs.ET"}, "abstract": "Uncertainty estimation in Neural Networks (NNs) is vital in improving reliability and confidence in predictions, particularly in safety-critical applications. Bayesian Neural Networks (BayNNs) with Dropout as an approximation offer a systematic approach to quantifying uncertainty, but they inherently suffer from high hardware overhead in terms of power, memory, and computation. Thus, the applicability of BayNNs to edge devices with limited resources or to high-performance applications is challenging. Some of the inherent costs of BayNNs can be reduced by accelerating them in hardware on a Computation-In-Memory (CIM) architecture with spintronic memories and binarizing their parameters. However, numerous stochastic units are required to implement conventional dropout-based BayNN. In this paper, we propose the Scale Dropout, a novel regularization technique for Binary Neural Networks (BNNs), and Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient uncertainty estimation. Our approach requires only one stochastic unit for the entire model, irrespective of the model size, leading to a highly scalable Bayesian NN. Furthermore, we introduce a novel Spintronic memory-based CIM architecture for the proposed BayNN that achieves more than $100\\times$ energy savings compared to the state-of-the-art. We validated our method to show up to a $1\\%$ improvement in predictive performance and superior uncertainty estimates compared to related works.", "url": "https://arxiv.org/abs/2311.15816"}, {"metadata": {"arXiv": "2311.15838", "Date": "Mon, 27 Nov 2023 14:02:47 ", "Title": "Utilizing Explainability Techniques for Reinforcement Learning Model Assurance", "Authors": ["Alexander Tapley and Kyle Gatesman and Luis Robaina and Brett Bissey and Joseph Weissman"], "Categories": "cs.LG cs.AI", "Comments": ["9 pages", "8 figures including appendices (A", "B", "C). Accepted as a poster presentation in the demo track at the \"XAI in Action: Past", "Present", "and Future Applications\" workshop at NeurIPS 2023. MITRE Public Release Case Number 23-3095"]}, "abstract": "Explainable Reinforcement Learning (XRL) can provide transparency into the decision-making process of a Deep Reinforcement Learning (DRL) model and increase user trust and adoption in real-world use cases. By utilizing XRL techniques, researchers can identify potential vulnerabilities within a trained DRL model prior to deployment, therefore limiting the potential for mission failure or mistakes by the system. This paper introduces the ARLIN (Assured RL Model Interrogation) Toolkit, an open-source Python library that identifies potential vulnerabilities and critical points within trained DRL models through detailed, human-interpretable explainability outputs. To illustrate ARLIN's effectiveness, we provide explainability visualizations and vulnerability analysis for a publicly available DRL model. The open-source code repository is available for download at https://github.com/mitre/arlin.", "url": "https://arxiv.org/abs/2311.15838"}, {"metadata": {"arXiv": "2311.15924", "Date": "Mon, 27 Nov 2023 15:34:40 ", "Title": "Diagnosis driven Anomaly Detection for CPS", "Authors": ["Henrik S. Steude and Lukas Moddemann and Alexander Diedrich and Jonas Ehrhardt and Oliver Niggemann"], "Categories": "cs.LG cs.AI"}, "abstract": "In Cyber-Physical Systems (CPS) research, anomaly detection (detecting abnormal behavior) and diagnosis (identifying the underlying root cause) are often treated as distinct, isolated tasks. However, diagnosis algorithms require symptoms, i.e. temporally and spatially isolated anomalies, as input. Thus, anomaly detection and diagnosis must be developed together to provide a holistic solution for diagnosis in CPS. We therefore propose a method for utilizing deep learning-based anomaly detection to generate inputs for Consistency-Based Diagnosis (CBD). We evaluate our approach on a simulated and a real-world CPS dataset, where our model demonstrates strong performance relative to other state-of-the-art models.", "url": "https://arxiv.org/abs/2311.15924"}, {"metadata": {"arXiv": "2311.15925", "Date": "Mon, 27 Nov 2023 15:37:05 ", "Title": "Reinforcement Learning for Wildfire Mitigation in Simulated Disaster Environments", "Authors": ["Alexander Tapley and Marissa Dotter and Michael Doyle and Aidan Fennelly and Dhanuj Gandikota and Savanna Smith and Michael Threet and Tim Welsh"], "Categories": "cs.LG cs.AI cs.MA cs.SE", "Comments": ["12 pages", "4 figures including Appendices (A", "B). Accepted as a paper in the Proposals track at the \"Tackling Climate Change with Machine Learning\" workshop at NeurIPS 2023. MITRE Public Release Case Number 23-3920"]}, "abstract": "Climate change has resulted in a year over year increase in adverse weather and weather conditions which contribute to increasingly severe fire seasons. Without effective mitigation, these fires pose a threat to life, property, ecology, cultural heritage, and critical infrastructure. To better prepare for and react to the increasing threat of wildfires, more accurate fire modelers and mitigation responses are necessary. In this paper, we introduce SimFire, a versatile wildland fire projection simulator designed to generate realistic wildfire scenarios, and SimHarness, a modular agent-based machine learning wrapper capable of automatically generating land management strategies within SimFire to reduce the overall damage to the area. Together, this publicly available system allows researchers and practitioners the ability to emulate and assess the effectiveness of firefighter interventions and formulate strategic plans that prioritize value preservation and resource allocation optimization. The repositories are available for download at https://github.com/mitrefireline.", "url": "https://arxiv.org/abs/2311.15925"}, {"metadata": {"arXiv": "2311.15951", "Date": "Mon, 27 Nov 2023 15:57:11 ", "Title": "Replay across Experiments: A Natural Extension of Off-Policy RL", "Authors": ["Dhruva Tirumala", "Thomas Lampe", "Jose Enrique Chen", "Tuomas Haarnoja", "Sandy Huang", "Guy Lever", "Ben Moran", "Tim Hertweck", "Leonard Hasenclever", "Martin Riedmiller", "Nicolas Heess and Markus Wulfmeier"], "Categories": "cs.LG cs.AI cs.RO"}, "abstract": "Replaying data is a principal mechanism underlying the stability and data efficiency of off-policy reinforcement learning (RL). We present an effective yet simple framework to extend the use of replays across multiple experiments, minimally adapting the RL workflow for sizeable improvements in controller performance and research iteration times. At its core, Replay Across Experiments (RaE) involves reusing experience from previous experiments to improve exploration and bootstrap learning while reducing required changes to a minimum in comparison to prior work. We empirically show benefits across a number of RL algorithms and challenging control domains spanning both locomotion and manipulation, including hard exploration tasks from egocentric vision. Through comprehensive ablations, we demonstrate robustness to the quality and amount of data available and various hyperparameter choices. Finally, we discuss how our approach can be applied more broadly across research life cycles and can increase resilience by reloading data across random seeds or hyperparameter variations.", "url": "https://arxiv.org/abs/2311.15951"}, {"metadata": {"arXiv": "2311.15960", "Date": "Mon, 27 Nov 2023 16:06:39 ", "Title": "Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines", "Authors": ["Yu-An Lin", "Chen-Tao Lee", "Guan-Ting Liu", "Pu-Jen Cheng", "Shao-Hua Sun"], "Categories": "cs.LG cs.AI cs.PL cs.RO"}, "abstract": "Deep reinforcement learning excels in various domains but lacks generalizability and interoperability. Programmatic RL methods (Trivedi et al., 2021; Liu et al., 2023) reformulate solving RL tasks as synthesizing interpretable programs that can be executed in the environments. Despite encouraging results, these methods are limited to short-horizon tasks. On the other hand, representing RL policies using state machines (Inala et al., 2020) can inductively generalize to long-horizon tasks; however, it struggles to scale up to acquire diverse and complex behaviors. This work proposes Program Machine Policies (POMPs), which bridge the advantages of programmatic RL and state machine policies, allowing for the representation of complex behaviors and the address of long-term tasks. Specifically, we introduce a method that can retrieve a set of effective, diverse, compatible programs. Then, we use these programs as modes of a state machine and learn a transition function to transition among mode programs, allowing for capturing long-horizon repetitive behaviors. Our proposed framework outperforms programmatic RL and deep RL baselines on various tasks and demonstrates the ability to generalize to even longer horizons without any fine-tuning inductively. Ablation studies justify the effectiveness of our proposed search algorithm for retrieving a set of programs as modes.", "url": "https://arxiv.org/abs/2311.15960"}, {"metadata": {"arXiv": "2311.15979", "Date": "Mon, 27 Nov 2023 16:25:12 ", "Title": "Soil Organic Carbon Estimation from Climate-related Features with Graph Neural Network", "Authors": ["Weiying Zhao and Natalia Efremova"], "Categories": "cs.LG cs.AI", "Journal-ref": "Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023"}, "abstract": "Soil organic carbon (SOC) plays a pivotal role in the global carbon cycle, impacting climate dynamics and necessitating accurate estimation for sustainable land and agricultural management. While traditional methods of SOC estimation face resolution and accuracy challenges, recent technological solutions harness remote sensing, machine learning, and high-resolution satellite mapping. Graph Neural Networks (GNNs), especially when integrated with positional encoders, can capture complex relationships between soil and climate. Using the LUCAS database, this study compared four GNN operators in the positional encoder framework. Results revealed that the PESAGE and PETransformer models outperformed others in SOC estimation, indicating their potential in capturing the complex relationship between SOC and climate features. Our findings confirm the feasibility of applications of GNN architectures in SOC prediction, establishing a framework for future explorations of this topic with more advanced GNN models.", "url": "https://arxiv.org/abs/2311.15979"}, {"metadata": {"arXiv": "2311.15983", "Date": "Mon, 27 Nov 2023 16:28:20 ", "Title": "Sparsify-then-Classify: From Internal Neurons of Large Language Models To Efficient Text Classifiers", "Authors": ["Yilun Liu", "Difan Jiao", "Ashton Anderson"], "Categories": "cs.LG cs.AI cs.CL", "Comments": ["23 pages", "5 figures", "8 tables Code available at https://github.com/difanj0713/Sparsify-then-Classify"]}, "abstract": "Among the many tasks that Large Language Models (LLMs) have revolutionized is text classification. However, existing approaches for applying pretrained LLMs to text classification predominantly rely on using single token outputs from only the last layer of hidden states. As a result, they suffer from limitations in efficiency, task-specificity, and interpretability. In our work, we contribute an approach that uses all internal representations by employing multiple pooling strategies on all activation and hidden states. Our novel lightweight strategy, Sparsify-then-Classify (STC) first sparsifies task-specific features layer-by-layer, then aggregates across layers for text classification. STC can be applied as a seamless plug-and-play module on top of existing LLMs. Our experiments on a comprehensive set of models and datasets demonstrate that STC not only consistently improves the classification performance of pretrained and fine-tuned models, but is also more efficient for both training and inference, and is more intrinsically interpretable.", "url": "https://arxiv.org/abs/2311.15983"}, {"metadata": {"arXiv": "2311.16003", "Date": "Mon, 27 Nov 2023 16:52:25 ", "Title": "Forecasting Auxiliary Energy Consumption for Electric Heavy-Duty Vehicles", "Authors": ["Yuantao Fan", "Zhenkan Wang", "Sepideh Pashami", "Slawomir Nowaczyk", "Henrik Ydreskog"], "Categories": "cs.LG cs.AI"}, "abstract": "Accurate energy consumption prediction is crucial for optimizing the operation of electric commercial heavy-duty vehicles, e.g., route planning for charging. Moreover, understanding why certain predictions are cast is paramount for such a predictive model to gain user trust and be deployed in practice. Since commercial vehicles operate differently as transportation tasks, ambient, and drivers vary, a heterogeneous population is expected when building an AI system for forecasting energy consumption. The dependencies between the input features and the target values are expected to also differ across sub-populations. One well-known example of such a statistical phenomenon is the Simpson paradox. In this paper, we illustrate that such a setting poses a challenge for existing XAI methods that produce global feature statistics, e.g. LIME or SHAP, causing them to yield misleading results. We demonstrate a potential solution by training multiple regression models on subsets of data. It not only leads to superior regression performance but also more relevant and consistent LIME explanations. Given that the employed groupings correspond to relevant sub-populations, the associations between the input features and the target values are consistent within each cluster but different across clusters. Experiments on both synthetic and real-world datasets show that such splitting of a complex problem into simpler ones yields better regression performance and interpretability.", "url": "https://arxiv.org/abs/2311.16003"}, {"metadata": {"arXiv": "2311.16065", "Date": "Mon, 27 Nov 2023 18:32:08 ", "Title": "A Survey on Vulnerability of Federated Learning: A Learning Algorithm Perspective", "Authors": ["Xianghua Xie", "Chen Hu", "Hanchi Ren", "Jingjing Deng"], "Categories": "cs.LG cs.AI", "Comments": ["https://github.com/Rand2AI/Awesome-Vulnerability-of-Federated-Learning"]}, "abstract": "This review paper takes a comprehensive look at malicious attacks against FL, categorizing them from new perspectives on attack origins and targets, and providing insights into their methodology and impact. In this survey, we focus on threat models targeting the learning process of FL systems. Based on the source and target of the attack, we categorize existing threat models into four types, Data to Model (D2M), Model to Data (M2D), Model to Model (M2M) and composite attacks. For each attack type, we discuss the defense strategies proposed, highlighting their effectiveness, assumptions and potential areas for improvement. Defense strategies have evolved from using a singular metric to excluding malicious clients, to employing a multifaceted approach examining client models at various phases. In this survey paper, our research indicates that the to-learn data, the learning gradients, and the learned model at different stages all can be manipulated to initiate malicious attacks that range from undermining model performance, reconstructing private local data, and to inserting backdoors. We have also seen these threat are becoming more insidious. While earlier studies typically amplified malicious gradients, recent endeavors subtly alter the least significant weights in local models to bypass defense measures. This literature review provides a holistic understanding of the current FL threat landscape and highlights the importance of developing robust, efficient, and privacy-preserving defenses to ensure the safe and trusted adoption of FL in real-world applications.", "url": "https://arxiv.org/abs/2311.16065"}, {"metadata": {"arXiv": "2311.16086", "Date": "Mon, 27 Nov 2023 18:56:03 ", "Title": "MAST: Model-Agnostic Sparsified Training", "Authors": ["Yury Demidovich", "Grigory Malinovsky", "Egor Shulgin", "Peter Richt\\'arik"], "Categories": "cs.LG cs.AI cs.DC math.OC", "Comments": ["58 pages", "5 figures"]}, "abstract": "We introduce a novel optimization problem formulation that departs from the conventional way of minimizing machine learning model loss as a black-box function. Unlike traditional formulations, the proposed approach explicitly incorporates an initially pre-trained model and random sketch operators, allowing for sparsification of both the model and gradient during training. We establish insightful properties of the proposed objective function and highlight its connections to the standard formulation. Furthermore, we present several variants of the Stochastic Gradient Descent (SGD) method adapted to the new problem formulation, including SGD with general sampling, a distributed version, and SGD with variance reduction techniques. We achieve tighter convergence rates and relax assumptions, bridging the gap between theoretical principles and practical applications, covering several important techniques such as Dropout and Sparse training. This work presents promising opportunities to enhance the theoretical understanding of model training through a sparsification-aware optimization approach.", "url": "https://arxiv.org/abs/2311.16086"}, {"metadata": {"arXiv": "2311.14770", "Date": "Fri, 24 Nov 2023 12:15:48 ", "Title": "Learning to Cooperate and Communicate Over Imperfect Channels", "Authors": ["Jannis Weil", "Gizem Ekinci", "Heinz Koeppl", "Tobias Meuser"], "Categories": "cs.MA cs.AI cs.LG"}, "abstract": "Information exchange in multi-agent systems improves the cooperation among agents, especially in partially observable settings. In the real world, communication is often carried out over imperfect channels. This requires agents to handle uncertainty due to potential information loss. In this paper, we consider a cooperative multi-agent system where the agents act and exchange information in a decentralized manner using a limited and unreliable channel. To cope with such channel constraints, we propose a novel communication approach based on independent Q-learning. Our method allows agents to dynamically adapt how much information to share by sending messages of different sizes, depending on their local observations and the channel's properties. In addition to this message size selection, agents learn to encode and decode messages to improve their jointly trained policies. We show that our approach outperforms approaches without adaptive capabilities in a novel cooperative digit-prediction environment and discuss its limitations in the traffic junction environment.", "url": "https://arxiv.org/abs/2311.14770"}, {"metadata": {"arXiv": "2311.15649", "Date": "Mon, 27 Nov 2023 09:20:23 ", "Title": "RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks", "Authors": ["Yaran Chen", "Wenbo Cui", "Yuanwen Chen", "Mining Tan", "Xinyao Zhang", "Dongbin Zhao", "He Wang"], "Categories": "cs.RO cs.AI cs.LG"}, "abstract": "Robotic agents must master common sense and long-term sequential decisions to solve daily tasks through natural language instruction. The developments in Large Language Models (LLMs) in natural language processing have inspired efforts to use LLMs in complex robot planning. Despite LLMs' great generalization and comprehension of instruction tasks, LLMs-generated task plans sometimes lack feasibility and correctness. To address the problem, we propose a RoboGPT agent\\footnote{our code and dataset will be released soon} for making embodied long-term decisions for daily tasks, with two modules: 1) LLMs-based planning with re-plan to break the task into multiple sub-goals; 2) RoboSkill individually designed for sub-goals to learn better navigation and manipulation skills. The LLMs-based planning is enhanced with a new robotic dataset and re-plan, called RoboGPT. The new robotic dataset of 67k daily instruction tasks is gathered for fine-tuning the Llama model and obtaining RoboGPT. RoboGPT planner with strong generalization can plan hundreds of daily instruction tasks. Additionally, a low-computational Re-Plan module is designed to allow plans to flexibly adapt to the environment, thereby addressing the nomenclature diversity challenge. The proposed RoboGPT agent outperforms SOTA methods on the ALFRED daily tasks. Moreover, RoboGPT planner exceeds SOTA LLM-based planners like ChatGPT in task-planning rationality for hundreds of unseen daily tasks, and even other domain tasks, while keeping the large model's original broad application and generality.", "url": "https://arxiv.org/abs/2311.15649"}, {"metadata": {"arXiv": "2311.16091", "Date": "Mon, 27 Nov 2023 18:57:42 ", "Title": "Interactive Autonomous Navigation with Internal State Inference and Interactivity Estimation", "Authors": ["Jiachen Li and David Isele and Kanghoon Lee and Jinkyoo Park and Kikuo Fujimura and Mykel J. Kochenderfer"], "Categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "Comments": ["18 pages", "14 figures"]}, "abstract": "Deep reinforcement learning (DRL) provides a promising way for intelligent agents (e.g., autonomous vehicles) to learn to navigate complex scenarios. However, DRL with neural networks as function approximators is typically considered a black box with little explainability and often suffers from suboptimal performance, especially for autonomous navigation in highly interactive multi-agent environments. To address these issues, we propose three auxiliary tasks with spatio-temporal relational reasoning and integrate them into the standard DRL framework, which improves the decision making performance and provides explainable intermediate indicators. We propose to explicitly infer the internal states (i.e., traits and intentions) of surrounding agents (e.g., human drivers) as well as to predict their future trajectories in the situations with and without the ego agent through counterfactual reasoning. These auxiliary tasks provide additional supervision signals to infer the behavior patterns of other interactive agents. Multiple variants of framework integration strategies are compared. We also employ a spatio-temporal graph neural network to encode relations between dynamic entities, which enhances both internal state inference and decision making of the ego agent. Moreover, we propose an interactivity estimation mechanism based on the difference between predicted trajectories in these two situations, which indicates the degree of influence of the ego agent on other agents. To validate the proposed method, we design an intersection driving simulator based on the Intelligent Intersection Driver Model (IIDM) that simulates vehicles and pedestrians. Our approach achieves robust and state-of-the-art performance in terms of standard evaluation metrics and provides explainable intermediate indicators (i.e., internal states, and interactivity scores) for decision making.", "url": "https://arxiv.org/abs/2311.16091"}, {"metadata": {"arXiv": "2311.16098", "Date": "Mon, 27 Nov 2023 18:59:25 ", "Title": "On Bringing Robots Home", "Authors": ["Nur Muhammad Mahi Shafiullah and Anant Rai and Haritheja Etukuru and Yiqian Liu and Ishan Misra and Soumith Chintala and Lerrel Pinto"], "Categories": "cs.RO cs.AI cs.CV cs.LG", "Comments": ["Project website and videos are available at https://dobb-e.com", "technical documentation for getting started is available at https://docs.dobb-e.com", "and code is released at https://github.com/notmahi/dobb-e"]}, "abstract": "Throughout history, we have successfully integrated various machines into our homes. Dishwashers, laundry machines, stand mixers, and robot vacuums are a few recent examples. However, these machines excel at performing only a single task effectively. The concept of a \"generalist machine\" in homes - a domestic assistant that can adapt and learn from our needs, all while remaining cost-effective - has long been a goal in robotics that has been steadily pursued for decades. In this work, we initiate a large-scale effort towards this goal by introducing Dobb-E, an affordable yet versatile general-purpose system for learning robotic manipulation within household settings. Dobb-E can learn a new task with only five minutes of a user showing it how to do it, thanks to a demonstration collection tool (\"The Stick\") we built out of cheap parts and iPhones. We use the Stick to collect 13 hours of data in 22 homes of New York City, and train Home Pretrained Representations (HPR). Then, in a novel home environment, with five minutes of demonstrations and fifteen minutes of adapting the HPR model, we show that Dobb-E can reliably solve the task on the Stretch, a mobile robot readily available on the market. Across roughly 30 days of experimentation in homes of New York City and surrounding areas, we test our system in 10 homes, with a total of 109 tasks in different environments, and finally achieve a success rate of 81%. Beyond success percentages, our experiments reveal a plethora of unique challenges absent or ignored in lab robotics. These range from effects of strong shadows, to variable demonstration quality by non-expert users. With the hope of accelerating research on home robots, and eventually seeing robot butlers in every home, we open-source Dobb-E software stack and models, our data, and our hardware designs at https://dobb-e.com", "url": "https://arxiv.org/abs/2311.16098"}, {"metadata": {"arXiv": "2311.14767", "Date": "Fri, 24 Nov 2023 10:19:21 ", "Title": "Low-Cost HEM with Arduino and Zigbee Technologies in the Energy Sector in Colombia", "Authors": ["Zurisaddai de la Cruz Severiche Maury and Ana Fernandez Vilas and Rebeca Diaz Redondo"], "Categories": "eess.SY cs.AI cs.LG cs.SY", "Journal-ref": "Energies 2022, 15(10), 3819", "DOI": "10.3390/en15103819"}, "abstract": "Since no solutions have been proposed in Colombia that seek to reduce the consumption of electricity at the residential level, this paper describes the design and implementation of a simple prototype of a low-cost home energy management system (HEMS). The objective of this plat-form is to monitor the energy consumption of typical household devices so that users can access the consumption of each device separately and then establish the strategy that allows them to reduce energy consumption at home. In order to demonstrate that our system is viable, the system has been evaluated by measuring weekly energy consumption with the on-line and off-line HEMS using a test bench with typical household devices in a Sincelejo typical household. The evaluation has shown that with the installation of this HEMS, consumption is reduced by 27%. This shows that it is possible to achieve a good reduction percentage with a low-cost system.", "url": "https://arxiv.org/abs/2311.14767"}, {"metadata": {"arXiv": "2311.14874", "Date": "Fri, 24 Nov 2023 23:51:53 ", "Title": "Advancing Fluid-Based Thermal Management Systems Design: Leveraging Graph Neural Networks for Graph Regression and Efficient Enumeration Reduction", "Authors": ["Saeid Bayat", "Nastaran Shahmansouri", "Satya RT Peddada", "Alex Tessier", "Adrian Butscher", "James T Allison"], "Categories": "eess.SY cs.AI cs.LG cs.SY", "Comments": ["13 pages", "17 figures"]}, "abstract": "In this research, we developed a graph-based framework to represent various aspects of optimal thermal management system design, with the aim of rapidly and efficiently identifying optimal design candidates. Initially, the graph-based framework is utilized to generate diverse thermal management system architectures. The dynamics of these system architectures are modeled under various loading conditions, and an open-loop optimal controller is employed to determine each system's optimal performance. These modeled cases constitute the dataset, with the corresponding optimal performance values serving as the labels for the data. In the subsequent step, a Graph Neural Network (GNN) model is trained on 30% of the labeled data to predict the systems' performance, effectively addressing a regression problem. Utilizing this trained model, we estimate the performance values for the remaining 70% of the data, which serves as the test set. In the third step, the predicted performance values are employed to rank the test data, facilitating prioritized evaluation of the design scenarios. Specifically, a small subset of the test data with the highest estimated ranks undergoes evaluation via the open-loop optimal control solver. This targeted approach concentrates on evaluating higher-ranked designs identified by the GNN, replacing the exhaustive search (enumeration-based) of all design cases. The results demonstrate a significant average reduction of over 92% in the number of system dynamic modeling and optimal control analyses required to identify optimal design scenarios.", "url": "https://arxiv.org/abs/2311.14874"}, {"metadata": {"arXiv": "2311.15516", "Date": "Mon, 27 Nov 2023 03:25:12 ", "Title": "Active Foundational Models for Fault Diagnosis of Electrical Motors", "Authors": ["Sriram Anbalagan", "Sai Shashank GP", "Deepesh Agarwal", "Balasubramaniam Natarajan", "Babji Srinivasan"], "Categories": "eess.SY cs.AI cs.LG cs.SY", "Comments": ["30 pages", "2 figures", "7 tables"]}, "abstract": "Fault detection and diagnosis of electrical motors are of utmost importance in ensuring the safe and reliable operation of several industrial systems. Detection and diagnosis of faults at the incipient stage allows corrective actions to be taken in order to reduce the severity of faults. The existing data-driven deep learning approaches for machine fault diagnosis rely extensively on huge amounts of labeled samples, where annotations are expensive and time-consuming. However, a major portion of unlabeled condition monitoring data is not exploited in the training process. To overcome this limitation, we propose a foundational model-based Active Learning framework that utilizes less amount of labeled samples, which are most informative and harnesses a large amount of available unlabeled data by effectively combining Active Learning and Contrastive Self-Supervised Learning techniques. It consists of a transformer network-based backbone model trained using an advanced nearest-neighbor contrastive self-supervised learning method. This approach empowers the backbone to learn improved representations of samples derived from raw, unlabeled vibration data. Subsequently, the backbone can undergo fine-tuning to address a range of downstream tasks, both within the same machines and across different machines. The effectiveness of the proposed methodology has been assessed through the fine-tuning of the backbone for multiple target tasks using three distinct machine-bearing fault datasets. The experimental evaluation demonstrates a superior performance as compared to existing state-of-the-art fault diagnosis methods with less amount of labeled data.", "url": "https://arxiv.org/abs/2311.15516"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
