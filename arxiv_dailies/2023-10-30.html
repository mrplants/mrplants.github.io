<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2310.17887", "Date": "Fri, 27 Oct 2023 04:30:18 ", "Title": "Impressions: Understanding Visual Semiotics and Aesthetic Impact", "Authors": ["Julia Kruk", "Caleb Ziems", "Diyi Yang"], "Categories": "cs.CV cs.LG", "Comments": ["To be published in EMNLP 2023"]}, "abstract": "Is aesthetic impact different from beauty? Is visual salience a reflection of its capacity for effective communication? We present Impressions, a novel dataset through which to investigate the semiotics of images, and how specific visual features and design choices can elicit specific emotions, thoughts and beliefs. We posit that the impactfulness of an image extends beyond formal definitions of aesthetics, to its success as a communicative act, where style contributes as much to meaning formation as the subject matter. However, prior image captioning datasets are not designed to empower state-of-the-art architectures to model potential human impressions or interpretations of images. To fill this gap, we design an annotation task heavily inspired by image analysis techniques in the Visual Arts to collect 1,440 image-caption pairs and 4,320 unique annotations exploring impact, pragmatic image description, impressions, and aesthetic design choices. We show that existing multimodal image captioning and conditional generation models struggle to simulate plausible human responses to images. However, this dataset significantly improves their ability to model impressions and aesthetic evaluations of images through fine-tuning and few-shot adaptation.", "url": "https://arxiv.org/abs/2310.17887"}, {"metadata": {"arXiv": "2310.18141", "Date": "Fri, 27 Oct 2023 13:45:30 ", "Title": "Unsupervised Representation Learning for Diverse Deformable Shape Collections", "Authors": ["Sara Hahner", "Souhaib Attaiki", "Jochen Garcke", "Maks Ovsjanikov"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted at International Conference on 3D Vision 2024"]}, "abstract": "We introduce a novel learning-based method for encoding and manipulating 3D surface meshes. Our method is specifically designed to create an interpretable embedding space for deformable shape collections. Unlike previous 3D mesh autoencoders that require meshes to be in a 1-to-1 correspondence, our approach is trained on diverse meshes in an unsupervised manner. Central to our method is a spectral pooling technique that establishes a universal latent space, breaking free from traditional constraints of mesh connectivity and shape categories. The entire process consists of two stages. In the first stage, we employ the functional map paradigm to extract point-to-point (p2p) maps between a collection of shapes in an unsupervised manner. These p2p maps are then utilized to construct a common latent space, which ensures straightforward interpretation and independence from mesh connectivity and shape category. Through extensive experiments, we demonstrate that our method achieves excellent reconstructions and produces more realistic and smoother interpolations than baseline approaches.", "url": "https://arxiv.org/abs/2310.18141"}, {"metadata": {"arXiv": "2310.18236", "Date": "Fri, 27 Oct 2023 16:20:34 ", "Title": "How Re-sampling Helps for Long-Tail Learning?", "Authors": ["Jiang-Xin Shi", "Tong Wei", "Yuke Xiang", "Yu-Feng Li"], "Categories": "cs.CV cs.LG", "Comments": ["Accepted by NeurIPS 2023"]}, "abstract": "Long-tail learning has received significant attention in recent years due to the challenge it poses with extremely imbalanced datasets. In these datasets, only a few classes (known as the head classes) have an adequate number of training samples, while the rest of the classes (known as the tail classes) are infrequent in the training data. Re-sampling is a classical and widely used approach for addressing class imbalance issues. Unfortunately, recent studies claim that re-sampling brings negligible performance improvements in modern long-tail learning tasks. This paper aims to investigate this phenomenon systematically. Our research shows that re-sampling can considerably improve generalization when the training images do not contain semantically irrelevant contexts. In other scenarios, however, it can learn unexpected spurious correlations between irrelevant contexts and target labels. We design experiments on two homogeneous datasets, one containing irrelevant context and the other not, to confirm our findings. To prevent the learning of spurious correlations, we propose a new context shift augmentation module that generates diverse training images for the tail class by maintaining a context bank extracted from the head-class images. Experiments demonstrate that our proposed module can boost the generalization and outperform other approaches, including class-balanced re-sampling, decoupled classifier re-training, and data augmentation methods. The source code is available at https://www.lamda.nju.edu.cn/code_CSA.ashx.", "url": "https://arxiv.org/abs/2310.18236"}, {"metadata": {"arXiv": "2310.18268", "Date": "Fri, 27 Oct 2023 16:56:28 ", "Title": "PlantPlotGAN: A Physics-Informed Generative Adversarial Network for Plant Disease Prediction", "Authors": ["Felipe A. Lopes", "Vasit Sagan", "Flavio Esposito"], "Categories": "cs.CV cs.LG eess.IV", "Comments": ["Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "2024"]}, "abstract": "Monitoring plantations is crucial for crop management and producing healthy harvests. Unmanned Aerial Vehicles (UAVs) have been used to collect multispectral images that aid in this monitoring. However, given the number of hectares to be monitored and the limitations of flight, plant disease signals become visually clear only in the later stages of plant growth and only if the disease has spread throughout a significant portion of the plantation. This limited amount of relevant data hampers the prediction models, as the algorithms struggle to generalize patterns with unbalanced or unrealistic augmented datasets effectively. To address this issue, we propose PlantPlotGAN, a physics-informed generative model capable of creating synthetic multispectral plot images with realistic vegetation indices. These indices served as a proxy for disease detection and were used to evaluate if our model could help increase the accuracy of prediction models. The results demonstrate that the synthetic imagery generated from PlantPlotGAN outperforms state-of-the-art methods regarding the Fr\\'echet inception distance. Moreover, prediction models achieve higher accuracy metrics when trained with synthetic and original imagery for earlier plant disease detection compared to the training processes based solely on real imagery.", "url": "https://arxiv.org/abs/2310.18268"}, {"metadata": {"arXiv": "2310.18274", "Date": "Fri, 27 Oct 2023 16:59:51 ", "Title": "LipSim: A Provably Robust Perceptual Similarity Metric", "Authors": ["Sara Ghazanfari", "Alexandre Araujo", "Prashanth Krishnamurthy", "Farshad Khorrami", "Siddharth Garg"], "Categories": "cs.CV cs.LG"}, "abstract": "Recent years have seen growing interest in developing and applying perceptual similarity metrics. Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system. On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks. It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks. In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks. We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas around each data point and certificates for all perturbations within an $\\ell_2$ ball. Finally, a comprehensive set of experiments shows the performance of LipSim in terms of natural and certified scores and on the image retrieval application. The code is available at https://github.com/SaraGhazanfari/LipSim.", "url": "https://arxiv.org/abs/2310.18274"}, {"metadata": {"arXiv": "2310.17664", "Date": "Mon, 23 Oct 2023 06:43:50 ", "Title": "Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search", "Authors": ["Yingying Gao", "Shilei Zhang", "Zihao Cui", "Chao Deng", "Junlan Feng"], "Categories": "cs.LG eess.AS eess.SP"}, "abstract": "Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance.", "url": "https://arxiv.org/abs/2310.17664"}, {"metadata": {"arXiv": "2310.17668", "Date": "Tue, 24 Oct 2023 20:28:59 ", "Title": "Fine tuning Pre trained Models for Robustness Under Noisy Labels", "Authors": ["Sumyeong Ahn", "Sihyeon Kim", "Jongwoo Ko", "Se-Young Yun"], "Categories": "cs.LG", "Comments": ["10 pages (17 pages including supplementary)"], "MSC-class": "Computer Science, Artificial Intelligence"}, "abstract": "The presence of noisy labels in a training dataset can significantly impact the performance of machine learning models. To tackle this issue, researchers have explored methods for Learning with Noisy Labels to identify clean samples and reduce the influence of noisy labels. However, constraining the influence of a certain portion of the training dataset can result in a reduction in overall generalization performance. To alleviate this, recent studies have considered the careful utilization of noisy labels by leveraging huge computational resources. Therefore, the increasing training cost necessitates a reevaluation of efficiency. In other areas of research, there has been a focus on developing fine-tuning techniques for large pre-trained models that aim to achieve both high generalization performance and efficiency. However, these methods have mainly concentrated on clean datasets, and there has been limited exploration of the noisy label scenario. In this research, our aim is to find an appropriate way to fine-tune pre-trained models for noisy labeled datasets. To achieve this goal, we investigate the characteristics of pre-trained models when they encounter noisy datasets. Through empirical analysis, we introduce a novel algorithm called TURN, which robustly and efficiently transfers the prior knowledge of pre-trained models. The algorithm consists of two main steps: (1) independently tuning the linear classifier to protect the feature extractor from being distorted by noisy labels, and (2) reducing the noisy label ratio and fine-tuning the entire model based on the noise-reduced dataset to adapt it to the target dataset. The proposed algorithm has been extensively tested and demonstrates efficient yet improved denoising performance on various benchmarks compared to previous methods.", "url": "https://arxiv.org/abs/2310.17668"}, {"metadata": {"arXiv": "2310.17669", "Date": "Wed, 25 Oct 2023 08:07:29 ", "Title": "An Approach for Efficient Neural Architecture Search Space Definition", "Authors": ["L\\'eo Pouy (ESTACA'Lab)", "Fouad Khenfri (ESTACA'Lab)", "Patrick Leserf (ESTACA'Lab)", "Chokri Mraidha (LIST (CEA))", "Cherif Larouci (ESTACA'Lab)"], "Categories": "cs.LG cs.NE", "Comments": ["AAAI-22 Workshop: Learning Network Architecture During Training", "Feb 2022", "Online", "United States"]}, "abstract": "As we advance in the fast-growing era of Machine Learning, various new and more complex neural architectures are arising to tackle problem more efficiently. On the one hand their efficient usage requires advanced knowledge and expertise, which is most of the time difficult to find on the labor market. On the other hand, searching for an optimized neural architecture is a time-consuming task when it is performed manually using a trial and error approach. Hence, a method and a tool support is needed to assist users of neural architectures, leading to an eagerness in the field of Automatic Machine Learning (AutoML). When it comes to Deep Learning, an important part of AutoML is the Neural Architecture Search (NAS). In this paper, we propose a novel cell-based hierarchical search space, easy to comprehend and manipulate. The objectives of the proposed approach are to optimize the search-time and to be general enough to handle most of state of the art Convolutional Neural Networks (CNN) architectures.", "url": "https://arxiv.org/abs/2310.17669"}, {"metadata": {"arXiv": "2310.17670", "Date": "Wed, 25 Oct 2023 08:24:48 ", "Title": "Unknown Health States Recognition With Collective Decision Based Deep Learning Networks In Predictive Maintenance Applications", "Authors": ["Chuyue Lou and M. Amine Atoui"], "Categories": "cs.LG"}, "abstract": "At present, decision making solutions developed based on deep learning (DL) models have received extensive attention in predictive maintenance (PM) applications along with the rapid improvement of computing power. Relying on the superior properties of shared weights and spatial pooling, Convolutional Neural Network (CNN) can learn effective representations of health states from industrial data. Many developed CNN-based schemes, such as advanced CNNs that introduce residual learning and multi-scale learning, have shown good performance in health state recognition tasks under the assumption that all the classes are known. However, these schemes have no ability to deal with new abnormal samples that belong to state classes not part of the training set. In this paper, a collective decision framework for different CNNs is proposed. It is based on a One-vs-Rest network (OVRN) to simultaneously achieve classification of known and unknown health states. OVRN learn state-specific discriminative features and enhance the ability to reject new abnormal samples incorporated to different CNNs. According to the validation results on the public dataset of Tennessee Eastman Process (TEP), the proposed CNN-based decision schemes incorporating OVRN have outstanding recognition ability for samples of unknown heath states, while maintaining satisfactory accuracy on known states. The results show that the new DL framework outperforms conventional CNNs, and the one based on residual and multi-scale learning has the best overall performance.", "url": "https://arxiv.org/abs/2310.17670"}, {"metadata": {"arXiv": "2310.17678", "Date": "Thu, 26 Oct 2023 04:56:31 ", "Title": "Spatio-Temporal Meta Contrastive Learning", "Authors": ["Jiabin Tang and Lianghao Xia and Jie Hu and Chao Huang"], "Categories": "cs.LG", "Comments": ["32nd ACM International Conference on Information and Knowledge Management (CIKM' 23)"]}, "abstract": "Spatio-temporal prediction is crucial in numerous real-world applications, including traffic forecasting and crime prediction, which aim to improve public transportation and safety management. Many state-of-the-art models demonstrate the strong capability of spatio-temporal graph neural networks (STGNN) to capture complex spatio-temporal correlations. However, despite their effectiveness, existing approaches do not adequately address several key challenges. Data quality issues, such as data scarcity and sparsity, lead to data noise and a lack of supervised signals, which significantly limit the performance of STGNN. Although recent STGNN models with contrastive learning aim to address these challenges, most of them use pre-defined augmentation strategies that heavily depend on manual design and cannot be customized for different Spatio-Temporal Graph (STG) scenarios. To tackle these challenges, we propose a new spatio-temporal contrastive learning (CL4ST) framework to encode robust and generalizable STG representations via the STG augmentation paradigm. Specifically, we design the meta view generator to automatically construct node and edge augmentation views for each disentangled spatial and temporal graph in a data-driven manner. The meta view generator employs meta networks with parameterized generative model to customize the augmentations for each input. This personalizes the augmentation strategies for every STG and endows the learning framework with spatio-temporal-aware information. Additionally, we integrate a unified spatio-temporal graph attention network with the proposed meta view generator and two-branch graph contrastive learning paradigms. Extensive experiments demonstrate that our CL4ST significantly improves performance over various state-of-the-art baselines in traffic and crime prediction.", "url": "https://arxiv.org/abs/2310.17678"}, {"metadata": {"arXiv": "2310.17683", "Date": "Thu, 26 Oct 2023 14:43:07 ", "Title": "Sliceformer: Make Multi-head Attention as Simple as Sorting in Discriminative Tasks", "Authors": ["Shen Yuan and Hongteng Xu"], "Categories": "cs.LG"}, "abstract": "As one of the most popular neural network modules, Transformer plays a central role in many fundamental deep learning models, e.g., the ViT in computer vision and the BERT and GPT in natural language processing. The effectiveness of the Transformer is often attributed to its multi-head attention (MHA) mechanism. In this study, we discuss the limitations of MHA, including the high computational complexity due to its ``query-key-value'' architecture and the numerical issue caused by its softmax operation. Considering the above problems and the recent development tendency of the attention layer, we propose an effective and efficient surrogate of the Transformer, called Sliceformer. Our Sliceformer replaces the classic MHA mechanism with an extremely simple ``slicing-sorting'' operation, i.e., projecting inputs linearly to a latent space and sorting them along different feature dimensions (or equivalently, called channels). For each feature dimension, the sorting operation implicitly generates an implicit attention map with sparse, full-rank, and doubly-stochastic structures. We consider different implementations of the slicing-sorting operation and analyze their impacts on the Sliceformer. We test the Sliceformer in the Long-Range Arena benchmark, image classification, text classification, and molecular property prediction, demonstrating its advantage in computational complexity and universal effectiveness in discriminative tasks. Our Sliceformer achieves comparable or better performance with lower memory cost and faster speed than the Transformer and its variants. Moreover, the experimental results reveal that applying our Sliceformer can empirically suppress the risk of mode collapse when representing data. The code is available at \\url{https://github.com/SDS-Lab/sliceformer}.", "url": "https://arxiv.org/abs/2310.17683"}, {"metadata": {"arXiv": "2310.17687", "Date": "Thu, 26 Oct 2023 17:58:39 ", "Title": "Counterfactual Fairness for Predictions using Generative Adversarial Networks", "Authors": ["Yuchen Ma", "Dennis Frauen", "Valentyn Melnychuk", "Stefan Feuerriegel"], "Categories": "cs.LG cs.CY"}, "abstract": "Fairness in predictions is of direct importance in practice due to legal, ethical, and societal reasons. It is often achieved through counterfactual fairness, which ensures that the prediction for an individual is the same as that in a counterfactual world under a different sensitive attribute. However, achieving counterfactual fairness is challenging as counterfactuals are unobservable. In this paper, we develop a novel deep neural network called Generative Counterfactual Fairness Network (GCFN) for making predictions under counterfactual fairness. Specifically, we leverage a tailored generative adversarial network to directly learn the counterfactual distribution of the descendants of the sensitive attribute, which we then use to enforce fair predictions through a novel counterfactual mediator regularization. If the counterfactual distribution is learned sufficiently well, our method is mathematically guaranteed to ensure the notion of counterfactual fairness. Thereby, our GCFN addresses key shortcomings of existing baselines that are based on inferring latent variables, yet which (a) are potentially correlated with the sensitive attributes and thus lead to bias, and (b) have weak capability in constructing latent representations and thus low prediction performance. Across various experiments, our method achieves state-of-the-art performance. Using a real-world case study from recidivism prediction, we further demonstrate that our method makes meaningful predictions in practice.", "url": "https://arxiv.org/abs/2310.17687"}, {"metadata": {"arXiv": "2310.17723", "Date": "Thu, 26 Oct 2023 18:34:41 ", "Title": "ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers", "Authors": ["Zhewei Yao", "Reza Yazdani Aminabadi", "Stephen Youn", "Xiaoxia Wu", "Elton Zheng", "Yuxiong He"], "Categories": "cs.LG cs.CL", "Comments": ["8 pages", "2 figures"]}, "abstract": "Quantization techniques are pivotal in reducing the memory and computational demands of deep neural network inference. Existing solutions, such as ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook crucial memory-bounded operators and the complexities of per-token quantization. Addressing these gaps, we present a novel, fully hardware-enhanced robust optimized post-training W8A8 quantization framework, ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and compute-intensive operators, aiming for optimal hardware performance. Additionally, it offers flexibility by allowing specific INT8 modules to switch to FP16/BF16 mode, enhancing accuracy.", "url": "https://arxiv.org/abs/2310.17723"}, {"metadata": {"arXiv": "2310.17748", "Date": "Thu, 26 Oct 2023 19:43:16 ", "Title": "Making the End-User a Priority in Benchmarking: OrionBench for Unsupervised Time Series Anomaly Detection", "Authors": ["Sarah Alnegheimish", "Laure Berti-Equille", "Kalyan Veeramachaneni"], "Categories": "cs.LG"}, "abstract": "Time series anomaly detection is a prevalent problem in many application domains such as patient monitoring in healthcare, forecasting in finance, or predictive maintenance in energy. This has led to the emergence of a plethora of anomaly detection methods, including more recently, deep learning based methods. Although several benchmarks have been proposed to compare newly developed models, they usually rely on one-time execution over a limited set of datasets and the comparison is restricted to a few models. We propose OrionBench -- a user centric continuously maintained benchmark for unsupervised time series anomaly detection. The framework provides universal abstractions to represent models, extensibility to add new pipelines and datasets, hyperparameter standardization, pipeline verification, and frequent releases with published benchmarks. We demonstrate the usage of OrionBench, and the progression of pipelines across 15 releases published over the course of three years. Moreover, we walk through two real scenarios we experienced with OrionBench that highlight the importance of continuous benchmarks in unsupervised time series anomaly detection.", "url": "https://arxiv.org/abs/2310.17748"}, {"metadata": {"arXiv": "2310.17752", "Date": "Thu, 26 Oct 2023 19:46:11 ", "Title": "PockEngine: Sparse and Efficient Fine-tuning in a Pocket", "Authors": ["Ligeng Zhu", "Lanxiang Hu", "Ji Lin", "Wei-Chen Wang", "Wei-Ming Chen", "Chuang Gan", "Song Han"], "Categories": "cs.LG", "Journal-ref": "56th IEEE/ACM International Symposium on Microarchitecture (MICRO 2023)", "DOI": "10.1145/3613424.3614307"}, "abstract": "On-device learning and efficient fine-tuning enable continuous and privacy-preserving customization (e.g., locally fine-tuning large language models on personalized data). However, existing training frameworks are designed for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and lack the optimizations for learning on the edge, which faces challenges of resource limitations and edge hardware diversity. We introduce PockEngine: a tiny, sparse and efficient engine to enable fine-tuning on various edge devices. PockEngine supports sparse backpropagation: it prunes the backward graph and sparsely updates the model with measured memory saving and latency reduction while maintaining the model quality. Secondly, PockEngine is compilation first: the entire training graph (including forward, backward and optimization steps) is derived at compile-time, which reduces the runtime overhead and brings opportunities for graph transformations. PockEngine also integrates a rich set of training graph optimizations, thus can further accelerate the training cost, including operator reordering and backend switching. PockEngine supports diverse applications, frontends and hardware backends: it flexibly compiles and tunes models defined in PyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. We evaluated PockEngine on both vision models and large language models. PockEngine achieves up to 15 $\\times$ speedup over off-the-shelf TensorFlow (Raspberry Pi), 5.6 $\\times$ memory saving back-propagation (Jetson AGX Orin). Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orin at 550 tokens/s, 7.9$\\times$ faster than the PyTorch.", "url": "https://arxiv.org/abs/2310.17752"}, {"metadata": {"arXiv": "2310.17759", "Date": "Thu, 26 Oct 2023 19:56:52 ", "Title": "Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization", "Authors": ["Liang Zhang", "Junchi Yang", "Amin Karbasi", "Niao He"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["NeurIPS 2023 Spotlight"]}, "abstract": "Algorithmic reproducibility measures the deviation in outputs of machine learning algorithms upon minor changes in the training process. Previous work suggests that first-order methods would need to trade-off convergence rate (gradient complexity) for better reproducibility. In this work, we challenge this perception and demonstrate that both optimal reproducibility and near-optimal convergence guarantees can be achieved for smooth convex minimization and smooth convex-concave minimax problems under various error-prone oracle settings. Particularly, given the inexact initialization oracle, our regularization-based algorithms achieve the best of both worlds - optimal reproducibility and near-optimal gradient complexity - for minimization and minimax optimization. With the inexact gradient oracle, the near-optimal guarantees also hold for minimax optimization. Additionally, with the stochastic gradient oracle, we show that stochastic gradient descent ascent is optimal in terms of both reproducibility and gradient complexity. We believe our results contribute to an enhanced understanding of the reproducibility-convergence trade-off in the context of convex optimization.", "url": "https://arxiv.org/abs/2310.17759"}, {"metadata": {"arXiv": "2310.17761", "Date": "Thu, 26 Oct 2023 20:07:33 ", "Title": "Distributed Personalized Empirical Risk Minimization", "Authors": ["Yuyang Deng", "Mohammad Mahdi Kamani", "Pouria Mahdavinia", "Mehrdad Mahdavi"], "Categories": "cs.LG"}, "abstract": "This paper advocates a new paradigm Personalized Empirical Risk Minimization (PERM) to facilitate learning from heterogeneous data sources without imposing stringent constraints on computational resources shared by participating devices. In PERM, we aim to learn a distinct model for each client by learning who to learn with and personalizing the aggregation of local empirical losses by effectively estimating the statistical discrepancy among data distributions, which entails optimal statistical accuracy for all local distributions and overcomes the data heterogeneity issue. To learn personalized models at scale, we propose a distributed algorithm that replaces the standard model averaging with model shuffling to simultaneously optimize PERM objectives for all devices. This also allows us to learn distinct model architectures (e.g., neural networks with different numbers of parameters) for different clients, thus confining underlying memory and compute resources of individual clients. We rigorously analyze the convergence of the proposed algorithm and conduct experiments that corroborate the effectiveness of the proposed paradigm.", "url": "https://arxiv.org/abs/2310.17761"}, {"metadata": {"arXiv": "2310.17772", "Date": "Thu, 26 Oct 2023 20:37:29 ", "Title": "Learning Optimal Classification Trees Robust to Distribution Shifts", "Authors": ["Nathan Justin", "Sina Aghaei", "Andr\\'es G\\'omez", "Phebe Vayanos"], "Categories": "cs.LG math.OC stat.ML", "Comments": ["47 pages", "11 figures"]}, "abstract": "We consider the problem of learning classification trees that are robust to distribution shifts between training and testing/deployment data. This problem arises frequently in high stakes settings such as public health and social work where data is often collected using self-reported surveys which are highly sensitive to e.g., the framing of the questions, the time when and place where the survey is conducted, and the level of comfort the interviewee has in sharing information with the interviewer. We propose a method for learning optimal robust classification trees based on mixed-integer robust optimization technology. In particular, we demonstrate that the problem of learning an optimal robust tree can be cast as a single-stage mixed-integer robust optimization problem with a highly nonlinear and discontinuous objective. We reformulate this problem equivalently as a two-stage linear robust optimization problem for which we devise a tailored solution procedure based on constraint generation. We evaluate the performance of our approach on numerous publicly available datasets, and compare the performance to a regularized, non-robust optimal tree. We show an increase of up to 12.48% in worst-case accuracy and of up to 4.85% in average-case accuracy across several datasets and distribution shifts from using our robust solution in comparison to the non-robust one.", "url": "https://arxiv.org/abs/2310.17772"}, {"metadata": {"arXiv": "2310.17786", "Date": "Thu, 26 Oct 2023 21:28:50 ", "Title": "Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates", "Authors": ["Nicholas E. Corrado", "Josiah P. Hanna"], "Categories": "cs.LG"}, "abstract": "Recently, data augmentation (DA) has emerged as a method for leveraging domain knowledge to inexpensively generate additional data in reinforcement learning (RL) tasks, often yielding substantial improvements in data efficiency. While prior work has demonstrated the utility of incorporating augmented data directly into model-free RL updates, it is not well-understood when a particular DA strategy will improve data efficiency. In this paper, we seek to identify general aspects of DA responsible for observed learning improvements. Our study focuses on sparse-reward tasks with dynamics-invariant data augmentation functions, serving as an initial step towards a more general understanding of DA and its integration into RL training. Experimentally, we isolate three relevant aspects of DA: state-action coverage, reward density, and the number of augmented transitions generated per update (the augmented replay ratio). From our experiments, we draw two conclusions: (1) increasing state-action coverage often has a much greater impact on data efficiency than increasing reward density, and (2) decreasing the augmented replay ratio substantially improves data efficiency. In fact, certain tasks in our empirical study are solvable only when the replay ratio is sufficiently low.", "url": "https://arxiv.org/abs/2310.17786"}, {"metadata": {"arXiv": "2310.17800", "Date": "Thu, 26 Oct 2023 22:17:25 ", "Title": "Interacting Diffusion Processes for Event Sequence Forecasting", "Authors": ["Mai Zeng", "Florence Regol", "Mark Coates"], "Categories": "cs.LG"}, "abstract": "Neural Temporal Point Processes (TPPs) have emerged as the primary framework for predicting sequences of events that occur at irregular time intervals, but their sequential nature can hamper performance for long-horizon forecasts. To address this, we introduce a novel approach that incorporates a diffusion generative model. The model facilitates sequence-to-sequence prediction, allowing multi-step predictions based on historical event sequences. In contrast to previous approaches, our model directly learns the joint probability distribution of types and inter-arrival times for multiple events. This allows us to fully leverage the high dimensional modeling capability of modern generative models. Our model is composed of two diffusion processes, one for the time intervals and one for the event types. These processes interact through their respective denoising functions, which can take as input intermediate representations from both processes, allowing the model to learn complex interactions. We demonstrate that our proposal outperforms state-of-the-art baselines for long-horizon forecasting of TPP.", "url": "https://arxiv.org/abs/2310.17800"}, {"metadata": {"arXiv": "2310.17813", "Date": "Thu, 26 Oct 2023 23:17:39 ", "Title": "A Spectral Condition for Feature Learning", "Authors": ["Greg Yang", "James B. Simon", "Jeremy Bernstein"], "Categories": "cs.LG"}, "abstract": "The push to train ever larger neural networks has motivated the study of initialization and training at large network width. A key challenge is to scale training so that a network's internal representations evolve nontrivially at all widths, a process known as feature learning. Here, we show that feature learning is achieved by scaling the spectral norm of weight matrices and their updates like $\\sqrt{\\texttt{fan-out}/\\texttt{fan-in}}$, in contrast to widely used but heuristic scalings based on Frobenius norm and entry size. Our spectral scaling analysis also leads to an elementary derivation of \\emph{maximal update parametrization}. All in all, we aim to provide the reader with a solid conceptual understanding of feature learning in neural networks.", "url": "https://arxiv.org/abs/2310.17813"}, {"metadata": {"arXiv": "2310.17836", "Date": "Fri, 27 Oct 2023 01:29:41 ", "Title": "Positional Encoding-based Resident Identification in Multi-resident Smart Homes", "Authors": ["Zhiyi Song", "Dipankar Chaki", "Abdallah Lakhdari", "Athman Bouguettaya"], "Categories": "cs.LG cs.CR", "Comments": ["27 pages", "11 figures", "2 tables"]}, "abstract": "We propose a novel resident identification framework to identify residents in a multi-occupant smart environment. The proposed framework employs a feature extraction model based on the concepts of positional encoding. The feature extraction model considers the locations of homes as a graph. We design a novel algorithm to build such graphs from layout maps of smart environments. The Node2Vec algorithm is used to transform the graph into high-dimensional node embeddings. A Long Short-Term Memory (LSTM) model is introduced to predict the identities of residents using temporal sequences of sensor events with the node embeddings. Extensive experiments show that our proposed scheme effectively identifies residents in a multi-occupant environment. Evaluation results on two real-world datasets demonstrate that our proposed approach achieves 94.5% and 87.9% accuracy, respectively.", "url": "https://arxiv.org/abs/2310.17836"}, {"metadata": {"arXiv": "2310.17843", "Date": "Fri, 27 Oct 2023 01:49:13 ", "Title": "A Data-Centric Online Market for Machine Learning: From Discovery to Pricing", "Authors": ["Minbiao Han", "Jonathan Light", "Steven Xia", "Sainyam Galhotra", "Raul Castro Fernandez", "Haifeng Xu"], "Categories": "cs.LG cs.GT"}, "abstract": "Data fuels machine learning (ML) - rich and high-quality training data is essential to the success of ML. However, to transform ML from the race among a few large corporations to an accessible technology that serves numerous normal users' data analysis requests, there still exist important challenges. One gap we observed is that many ML users can benefit from new data that other data owners possess, whereas these data owners sit on piles of data without knowing who can benefit from it. This gap creates the opportunity for building an online market that can automatically connect supply with demand. While online matching markets are prevalent (e.g., ride-hailing systems), designing a data-centric market for ML exhibits many unprecedented challenges. This paper develops new techniques to tackle two core challenges in designing such a market: (a) to efficiently match demand with supply, we design an algorithm to automatically discover useful data for any ML task from a pool of thousands of datasets, achieving high-quality matching between ML models and data; (b) to encourage market participation of ML users without much ML expertise, we design a new pricing mechanism for selling data-augmented ML models. Furthermore, our market is designed to be API-compatible with existing online ML markets like Vertex AI and Sagemaker, making it easy to use while providing better results due to joint data and model search. We envision that the synergy of our data and model discovery algorithm and pricing mechanism will be an important step towards building a new data-centric online market that serves ML users effectively.", "url": "https://arxiv.org/abs/2310.17843"}, {"metadata": {"arXiv": "2310.17882", "Date": "Fri, 27 Oct 2023 04:11:13 ", "Title": "Machine Learning Infused Distributed Optimization for Coordinating Virtual Power Plant Assets", "Authors": ["Meiyi Li", "Javad Mohammadi"], "Categories": "cs.LG cs.SY eess.SY"}, "abstract": "Amid the increasing interest in the deployment of Distributed Energy Resources (DERs), the Virtual Power Plant (VPP) has emerged as a pivotal tool for aggregating diverse DERs and facilitating their participation in wholesale energy markets. These VPP deployments have been fueled by the Federal Energy Regulatory Commission's Order 2222, which makes DERs and VPPs competitive across market segments. However, the diversity and decentralized nature of DERs present significant challenges to the scalable coordination of VPP assets. To address efficiency and speed bottlenecks, this paper presents a novel machine learning-assisted distributed optimization to coordinate VPP assets. Our method, named LOOP-MAC(Learning to Optimize the Optimization Process for Multi-agent Coordination), adopts a multi-agent coordination perspective where each VPP agent manages multiple DERs and utilizes neural network approximators to expedite the solution search. The LOOP-MAC method employs a gauge map to guarantee strict compliance with local constraints, effectively reducing the need for additional post-processing steps. Our results highlight the advantages of LOOP-MAC, showcasing accelerated solution times per iteration and significantly reduced convergence times. The LOOP-MAC method outperforms conventional centralized and distributed optimization methods in optimization tasks that require repetitive and sequential execution.", "url": "https://arxiv.org/abs/2310.17882"}, {"metadata": {"arXiv": "2310.17890", "Date": "Fri, 27 Oct 2023 04:42:59 ", "Title": "Submodel Partitioning in Hierarchical Federated Learning: Algorithm Design and Convergence Analysis", "Authors": ["Wenzhi Fang", "Dong-Jun Han", "and Christopher G. Brinton"], "Categories": "cs.LG cs.IT eess.SP math.IT", "Comments": ["14 pages", "4 figures"]}, "abstract": "Hierarchical federated learning (HFL) has demonstrated promising scalability advantages over the traditional \"star-topology\" architecture-based federated learning (FL). However, HFL still imposes significant computation, communication, and storage burdens on the edge, especially when training a large-scale model over resource-constrained Internet of Things (IoT) devices. In this paper, we propose hierarchical independent submodel training (HIST), a new FL methodology that aims to address these issues in hierarchical settings. The key idea behind HIST is a hierarchical version of model partitioning, where we partition the global model into disjoint submodels in each round, and distribute them across different cells, so that each cell is responsible for training only one partition of the full model. This enables each client to save computation/storage costs while alleviating the communication loads throughout the hierarchy. We characterize the convergence behavior of HIST for non-convex loss functions under mild assumptions, showing the impact of several attributes (e.g., number of cells, local and global aggregation frequency) on the performance-efficiency tradeoff. Finally, through numerical experiments, we verify that HIST is able to save communication costs by a wide margin while achieving the same target testing accuracy.", "url": "https://arxiv.org/abs/2310.17890"}, {"metadata": {"arXiv": "2310.17901", "Date": "Fri, 27 Oct 2023 05:25:02 ", "Title": "Improving the Knowledge Gradient Algorithm", "Authors": ["Yang Le and Gao Siyang and Ho Chin Pang"], "Categories": "cs.LG stat.ML", "Comments": ["32 pages", "42 figures"]}, "abstract": "The knowledge gradient (KG) algorithm is a popular policy for the best arm identification (BAI) problem. It is built on the simple idea of always choosing the measurement that yields the greatest expected one-step improvement in the estimate of the best mean of the arms. In this research, we show that this policy has limitations, causing the algorithm not asymptotically optimal. We next provide a remedy for it, by following the manner of one-step look ahead of KG, but instead choosing the measurement that yields the greatest one-step improvement in the probability of selecting the best arm. The new policy is called improved knowledge gradient (iKG). iKG can be shown to be asymptotically optimal. In addition, we show that compared to KG, it is easier to extend iKG to variant problems of BAI, with the $\\epsilon$-good arm identification and feasible arm identification as two examples. The superior performances of iKG on these problems are further demonstrated using numerical examples.", "url": "https://arxiv.org/abs/2310.17901"}, {"metadata": {"arXiv": "2310.17915", "Date": "Fri, 27 Oct 2023 06:15:33 ", "Title": "Lifting the Veil: Unlocking the Power of Depth in Q-learning", "Authors": ["Shao-Bo Lin", "Tao Li", "Shaojie Tang", "Yao Wang", "Ding-Xuan Zhou"], "Categories": "cs.LG"}, "abstract": "With the help of massive data and rich computational resources, deep Q-learning has been widely used in operations research and management science and has contributed to great success in numerous applications, including recommender systems, supply chains, games, and robotic manipulation. However, the success of deep Q-learning lacks solid theoretical verification and interpretability. The aim of this paper is to theoretically verify the power of depth in deep Q-learning. Within the framework of statistical learning theory, we rigorously prove that deep Q-learning outperforms its traditional version by demonstrating its good generalization error bound. Our results reveal that the main reason for the success of deep Q-learning is the excellent performance of deep neural networks (deep nets) in capturing the special properties of rewards namely, spatial sparseness and piecewise constancy, rather than their large capacities. In this paper, we make fundamental contributions to the field of reinforcement learning by answering to the following three questions: Why does deep Q-learning perform so well? When does deep Q-learning perform better than traditional Q-learning? How many samples are required to achieve a specific prediction accuracy for deep Q-learning? Our theoretical assertions are verified by applying deep Q-learning in the well-known beer game in supply chain management and a simulated recommender system.", "url": "https://arxiv.org/abs/2310.17915"}, {"metadata": {"arXiv": "2310.17944", "Date": "Fri, 27 Oct 2023 07:39:54 ", "Title": "Trustworthy Edge Machine Learning: A Survey", "Authors": ["Xiaojie Wang", "Beibei Wang", "Yu Wu", "Zhaolong Ning", "Song Guo", "and Fei Richard Yu"], "Categories": "cs.LG", "Comments": ["27 pages", "7 figures", "10 tables"]}, "abstract": "The convergence of Edge Computing (EC) and Machine Learning (ML), known as Edge Machine Learning (EML), has become a highly regarded research area by utilizing distributed network resources to perform joint training and inference in a cooperative manner. However, EML faces various challenges due to resource constraints, heterogeneous network environments, and diverse service requirements of different applications, which together affect the trustworthiness of EML in the eyes of its stakeholders. This survey provides a comprehensive summary of definitions, attributes, frameworks, techniques, and solutions for trustworthy EML. Specifically, we first emphasize the importance of trustworthy EML within the context of Sixth-Generation (6G) networks. We then discuss the necessity of trustworthiness from the perspective of challenges encountered during deployment and real-world application scenarios. Subsequently, we provide a preliminary definition of trustworthy EML and explore its key attributes. Following this, we introduce fundamental frameworks and enabling technologies for trustworthy EML systems, and provide an in-depth literature review of the latest solutions to enhance trustworthiness of EML. Finally, we discuss corresponding research challenges and open issues.", "url": "https://arxiv.org/abs/2310.17944"}, {"metadata": {"arXiv": "2310.17972", "Date": "Fri, 27 Oct 2023 08:37:10 ", "Title": "CEFL: Carbon-Efficient Federated Learning", "Authors": ["Talha Mehboob", "Noman Bashir", "Jesus Omana Iglesias", "Michael Zink", "David Irwin"], "Categories": "cs.LG cs.DC"}, "abstract": "Federated Learning (FL) distributes machine learning (ML) training across many edge devices to reduce data transfer overhead and protect data privacy. Since FL model training may span millions of devices and is thus resource-intensive, prior work has focused on improving its resource efficiency to optimize time-to-accuracy. However, prior work generally treats all resources the same, while, in practice, they may incur widely different costs, which instead motivates optimizing cost-to-accuracy. To address the problem, we design CEFL, which uses adaptive cost-aware client selection policies to optimize an arbitrary cost metric when training FL models. Our policies extend and combine prior work on utility-based client selection and critical learning periods by making them cost-aware. We demonstrate CEFL by designing carbon-efficient FL, where energy's carbon-intensity is the cost, and show that it i) reduces carbon emissions by 93\\% and reduces training time by 50% compared to random client selection and ii) reduces carbon emissions by 80%, while only increasing training time by 38%, compared to a state-of-the-art approach that optimizes training time.", "url": "https://arxiv.org/abs/2310.17972"}, {"metadata": {"arXiv": "2310.17998", "Date": "Fri, 27 Oct 2023 09:16:58 ", "Title": "Closing the Gap Between the Upper Bound and the Lower Bound of Adam's Iteration Complexity", "Authors": ["Bohan Wang", "Jingwen Fu", "Huishuai Zhang", "Nanning Zheng", "Wei Chen"], "Categories": "cs.LG math.OC", "Comments": ["NeurIPS 2023 Accept"]}, "abstract": "Recently, Arjevani et al. [1] established a lower bound of iteration complexity for the first-order optimization under an $L$-smooth condition and a bounded noise variance assumption. However, a thorough review of existing literature on Adam's convergence reveals a noticeable gap: none of them meet the above lower bound. In this paper, we close the gap by deriving a new convergence guarantee of Adam, with only an $L$-smooth condition and a bounded noise variance assumption. Our results remain valid across a broad spectrum of hyperparameters. Especially with properly chosen hyperparameters, we derive an upper bound of the iteration complexity of Adam and show that it meets the lower bound for first-order optimizers. To the best of our knowledge, this is the first to establish such a tight upper bound for Adam's convergence. Our proof utilizes novel techniques to handle the entanglement between momentum and adaptive learning rate and to convert the first-order term in the Descent Lemma to the gradient norm, which may be of independent interest.", "url": "https://arxiv.org/abs/2310.17998"}, {"metadata": {"arXiv": "2310.18001", "Date": "Fri, 27 Oct 2023 09:17:15 ", "Title": "DP-SGD with weight clipping", "Authors": ["Antoine Barczewski and Jan Ramon"], "Categories": "cs.LG cs.CR"}, "abstract": "Recently, due to the popularity of deep neural networks and other methods whose training typically relies on the optimization of an objective function, and due to concerns for data privacy, there is a lot of interest in differentially private gradient descent methods. To achieve differential privacy guarantees with a minimum amount of noise, it is important to be able to bound precisely the sensitivity of the information which the participants will observe. In this study, we present a novel approach that mitigates the bias arising from traditional gradient clipping. By leveraging public information concerning the current global model and its location within the search domain, we can achieve improved gradient bounds, leading to enhanced sensitivity determinations and refined noise level adjustments. We extend the state of the art algorithms, present improved differential privacy guarantees requiring less noise and present an empirical evaluation.", "url": "https://arxiv.org/abs/2310.18001"}, {"metadata": {"arXiv": "2310.18074", "Date": "Fri, 27 Oct 2023 11:42:56 ", "Title": "On kernel-based statistical learning in the mean field limit", "Authors": ["Christian Fiedler", "Michael Herty", "Sebastian Trimpe"], "Categories": "cs.LG math.ST stat.ML stat.TH", "Comments": ["NeurIPS 2023"]}, "abstract": "In many applications of machine learning, a large number of variables are considered. Motivated by machine learning of interacting particle systems, we consider the situation when the number of input variables goes to infinity. First, we continue the recent investigation of the mean field limit of kernels and their reproducing kernel Hilbert spaces, completing the existing theory. Next, we provide results relevant for approximation with such kernels in the mean field limit, including a representer theorem. Finally, we use these kernels in the context of statistical learning in the mean field limit, focusing on Support Vector Machines. In particular, we show mean field convergence of empirical and infinite-sample solutions as well as the convergence of the corresponding risks. On the one hand, our results establish rigorous mean field limits in the context of kernel methods, providing new theoretical tools and insights for large-scale problems. On the other hand, our setting corresponds to a new form of limit of learning problems, which seems to have not been investigated yet in the statistical learning theory literature.", "url": "https://arxiv.org/abs/2310.18074"}, {"metadata": {"arXiv": "2310.18080", "Date": "Fri, 27 Oct 2023 12:01:16 ", "Title": "Unveiling the Potential of Probabilistic Embeddings in Self-Supervised Learning", "Authors": ["Denis Janiak", "Jakub Binkowski", "Piotr Bielak", "Tomasz Kajdanowicz"], "Categories": "cs.LG", "Comments": ["Under review by AISTATS 2024"]}, "abstract": "In recent years, self-supervised learning has played a pivotal role in advancing machine learning by allowing models to acquire meaningful representations from unlabeled data. An intriguing research avenue involves developing self-supervised models within an information-theoretic framework, but many studies often deviate from the stochasticity assumptions made when deriving their objectives. To gain deeper insights into this issue, we propose to explicitly model the representation with stochastic embeddings and assess their effects on performance, information compression and potential for out-of-distribution detection. From an information-theoretic perspective, we seek to investigate the impact of probabilistic modeling on the information bottleneck, shedding light on a trade-off between compression and preservation of information in both representation and loss space. Emphasizing the importance of distinguishing between these two spaces, we demonstrate how constraining one can affect the other, potentially leading to performance degradation. Moreover, our findings suggest that introducing an additional bottleneck in the loss space can significantly enhance the ability to detect out-of-distribution examples, only leveraging either representation features or the variance of their underlying distribution.", "url": "https://arxiv.org/abs/2310.18080"}, {"metadata": {"arXiv": "2310.18091", "Date": "Fri, 27 Oct 2023 12:24:08 ", "Title": "Adversarial Anomaly Detection using Gaussian Priors and Nonlinear Anomaly Scores", "Authors": ["Fiete L\\\"uer", "Tobias Weber", "Maxim Dolgich", "Christian B\\\"ohm"], "Categories": "cs.LG stat.ML", "Comments": ["accepted at AI4TS @ ICDMW 2023"]}, "abstract": "Anomaly detection in imbalanced datasets is a frequent and crucial problem, especially in the medical domain where retrieving and labeling irregularities is often expensive. By combining the generative stability of a $\\beta$-variational autoencoder (VAE) with the discriminative strengths of generative adversarial networks (GANs), we propose a novel model, $\\beta$-VAEGAN. We investigate methods for composing anomaly scores based on the discriminative and reconstructive capabilities of our model. Existing work focuses on linear combinations of these components to determine if data is anomalous. We advance existing work by training a kernelized support vector machine (SVM) on the respective error components to also consider nonlinear relationships. This improves anomaly detection performance, while allowing faster optimization. Lastly, we use the deviations from the Gaussian prior of $\\beta$-VAEGAN to form a novel anomaly score component. In comparison to state-of-the-art work, we improve the $F_1$ score during anomaly detection from 0.85 to 0.92 on the widely used MITBIH Arrhythmia Database.", "url": "https://arxiv.org/abs/2310.18091"}, {"metadata": {"arXiv": "2310.18118", "Date": "Fri, 27 Oct 2023 13:04:53 ", "Title": "A Global Multi-Unit Calibration as a Method for Large Scale IoT Particulate Matter Monitoring Systems Deployments", "Authors": ["Saverio De Vito", "Gerardo D Elia", "Sergio Ferlito", "Girolamo Di Francia", "Milos Davidovic", "Duska Kleut", "Danka Stojanovic", "Milena Jovasevic Stojanovic"], "Categories": "cs.LG"}, "abstract": "Scalable and effective calibration is a fundamental requirement for Low Cost Air Quality Monitoring Systems and will enable accurate and pervasive monitoring in cities. Suffering from environmental interferences and fabrication variance, these devices need to encompass sensors specific and complex calibration processes for reaching a sufficient accuracy to be deployed as indicative measurement devices in Air Quality (AQ) monitoring networks. Concept and sensor drift often force calibration process to be frequently repeated. These issues lead to unbearable calibration costs which denies their massive deployment when accuracy is a concern. In this work, We propose a zero transfer samples, global calibration methodology as a technological enabler for IoT AQ multisensory devices which relies on low cost Particulate Matter (PM) sensors. This methodology is based on field recorded responses from a limited number of IoT AQ multisensors units and machine learning concepts and can be universally applied to all units of the same type. A multi season test campaign shown that, when applied to different sensors, this methodology performances match those of state of the art methodology which requires to derive different calibration parameters for each different unit. If confirmed, these results show that, when properly derived, a global calibration law can be exploited for a large number of networked devices with dramatic cost reduction eventually allowing massive deployment of accurate IoT AQ monitoring devices. Furthermore, this calibration model could be easily embedded on board of the device or implemented on the edge allowing immediate access to accurate readings for personal exposure monitor applications as well as reducing long range data transfer needs.", "url": "https://arxiv.org/abs/2310.18118"}, {"metadata": {"arXiv": "2310.18123", "Date": "Fri, 27 Oct 2023 13:09:56 ", "Title": "Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling", "Authors": ["Zhenyu Zhu", "Francesco Locatello", "Volkan Cevher"], "Categories": "cs.LG stat.ML", "Comments": ["Accepted in NeurIPS 2023"]}, "abstract": "This paper provides statistical sample complexity bounds for score-matching and its applications in causal discovery. We demonstrate that accurate estimation of the score function is achievable by training a standard deep ReLU neural network using stochastic gradient descent. We establish bounds on the error rate of recovering causal relationships using the score-matching-based causal discovery method of Rolland et al. [2022], assuming a sufficiently good estimation of the score function. Finally, we analyze the upper bound of score-matching estimation within the score-based generative modeling, which has been applied for causal discovery but is also of independent interest within the domain of generative models.", "url": "https://arxiv.org/abs/2310.18123"}, {"metadata": {"arXiv": "2310.18162", "Date": "Fri, 27 Oct 2023 14:12:56 ", "Title": "Proportional Fairness in Clustering: A Social Choice Perspective", "Authors": ["Leon Kellerhals and Jannik Peters"], "Categories": "cs.LG cs.CY cs.GT"}, "abstract": "We study the proportional clustering problem of Chen et al. [ICML'19] and relate it to the area of multiwinner voting in computational social choice. We show that any clustering satisfying a weak proportionality notion of Brill and Peters [EC'23] simultaneously obtains the best known approximations to the proportional fairness notion of Chen et al. [ICML'19], but also to individual fairness [Jung et al., FORC'20] and the \"core\" [Li et al. ICML'21]. In fact, we show that any approximation to proportional fairness is also an approximation to individual fairness and vice versa. Finally, we also study stronger notions of proportional representation, in which deviations do not only happen to single, but multiple candidate centers, and show that stronger proportionality notions of Brill and Peters [EC'23] imply approximations to these stronger guarantees.", "url": "https://arxiv.org/abs/2310.18162"}, {"metadata": {"arXiv": "2310.18212", "Date": "Fri, 27 Oct 2023 15:34:08 ", "Title": "Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice", "Authors": ["Damian Machlanski", "Spyridon Samothrakis", "Paul Clarke"], "Categories": "cs.LG stat.ME", "Comments": ["26 pages", "16 figures"]}, "abstract": "Hyperparameters play a critical role in machine learning. Hyperparameter tuning can make the difference between state-of-the-art and poor prediction performance for any algorithm, but it is particularly challenging for structure learning due to its unsupervised nature. As a result, hyperparameter tuning is often neglected in favour of using the default values provided by a particular implementation of an algorithm. While there have been numerous studies on performance evaluation of causal discovery algorithms, how hyperparameters affect individual algorithms, as well as the choice of the best algorithm for a specific problem, has not been studied in depth before. This work addresses this gap by investigating the influence of hyperparameters on causal structure learning tasks. Specifically, we perform an empirical evaluation of hyperparameter selection for some seminal learning algorithms on datasets of varying levels of complexity. We find that, while the choice of algorithm remains crucial to obtaining state-of-the-art performance, hyperparameter selection in ensemble settings strongly influences the choice of algorithm, in that a poor choice of hyperparameters can lead to analysts using algorithms which do not give state-of-the-art performance for their data.", "url": "https://arxiv.org/abs/2310.18212"}, {"metadata": {"arXiv": "2310.18215", "Date": "Fri, 27 Oct 2023 15:42:04 ", "Title": "One Model Fits All: Cross-Region Taxi-Demand Forecasting", "Authors": ["Ren Ozeki", "Haruki Yonekura", "Aidana Baimbetova", "Hamada Rizk", "Hirozumi Yamaguchi"], "Categories": "cs.LG", "Comments": ["Accepted to The 31st ACM International Conference on Advances in Geographic Information Systems(SIGSPATIAL '23) as a short paper in the Research", "Systems and Industrial Experience Papers track"]}, "abstract": "The growing demand for ride-hailing services has led to an increasing need for accurate taxi demand prediction. Existing systems are limited to specific regions, lacking generalizability to unseen areas. This paper presents a novel taxi demand forecasting system that leverages a graph neural network to capture spatial dependencies and patterns in urban environments. Additionally, the proposed system employs a region-neutral approach, enabling it to train a model that can be applied to any region, including unseen regions. To achieve this, the framework incorporates the power of Variational Autoencoder to disentangle the input features into region-specific and region-neutral components. The region-neutral features facilitate cross-region taxi demand predictions, allowing the model to generalize well across different urban areas. Experimental results demonstrate the effectiveness of the proposed system in accurately forecasting taxi demand, even in previously unobserved regions, thus showcasing its potential for optimizing taxi services and improving transportation efficiency on a broader scale.", "url": "https://arxiv.org/abs/2310.18215"}, {"metadata": {"arXiv": "2310.18230", "Date": "Fri, 27 Oct 2023 16:09:39 ", "Title": "Deep Transformed Gaussian Processes", "Authors": ["S\\'aez-Maldonado Francisco Javier", "Maro\\~nas Juan", "Hern\\'andez-Lobato Daniel"], "Categories": "cs.LG stat.ML"}, "abstract": "Transformed Gaussian Processes (TGPs) are stochastic processes specified by transforming samples from the joint distribution from a prior process (typically a GP) using an invertible transformation; increasing the flexibility of the base process. Furthermore, they achieve competitive results compared with Deep Gaussian Processes (DGPs), which are another generalization constructed by a hierarchical concatenation of GPs. In this work, we propose a generalization of TGPs named Deep Transformed Gaussian Processes (DTGPs), which follows the trend of concatenating layers of stochastic processes. More precisely, we obtain a multi-layer model in which each layer is a TGP. This generalization implies an increment of flexibility with respect to both TGPs and DGPs. Exact inference in such a model is intractable. However, we show that one can use variational inference to approximate the required computations yielding a straightforward extension of the popular DSVI inference algorithm Salimbeni et al (2017). The experiments conducted evaluate the proposed novel DTGPs in multiple regression datasets, achieving good scalability and performance.", "url": "https://arxiv.org/abs/2310.18230"}, {"metadata": {"arXiv": "2310.18241", "Date": "Fri, 27 Oct 2023 16:26:14 ", "Title": "$\\alpha$-Mutual Information: A Tunable Privacy Measure for Privacy Protection in Data Sharing", "Authors": ["MirHamed Jafarzadeh Asl", "Mohammadhadi Shateri", "Fabrice Labeau"], "Categories": "cs.LG cs.CR cs.IT eess.SP math.IT", "Comments": ["2023 22nd IEEE International Conference on Machine Learning and Applications (ICMLA)"]}, "abstract": "This paper adopts Arimoto's $\\alpha$-Mutual Information as a tunable privacy measure, in a privacy-preserving data release setting that aims to prevent disclosing private data to adversaries. By fine-tuning the privacy metric, we demonstrate that our approach yields superior models that effectively thwart attackers across various performance dimensions. We formulate a general distortion-based mechanism that manipulates the original data to offer privacy protection. The distortion metrics are determined according to the data structure of a specific experiment. We confront the problem expressed in the formulation by employing a general adversarial deep learning framework that consists of a releaser and an adversary, trained with opposite goals. This study conducts empirical experiments on images and time-series data to verify the functionality of $\\alpha$-Mutual Information. We evaluate the privacy-utility trade-off of customized models and compare them to mutual information as the baseline measure. Finally, we analyze the consequence of an attacker's access to side information about private data and witness that adapting the privacy measure results in a more refined model than the state-of-the-art in terms of resiliency against side information.", "url": "https://arxiv.org/abs/2310.18241"}, {"metadata": {"arXiv": "2310.18247", "Date": "Fri, 27 Oct 2023 16:34:00 ", "Title": "Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning", "Authors": ["Nicholas E. Corrado", "Yuxiao Qu", "John U. Balis", "Adam Labiosa", "Josiah P. Hanna"], "Categories": "cs.LG cs.RO"}, "abstract": "Learning from demonstration (LfD) is a popular technique that uses expert demonstrations to learn robot control policies. However, the difficulty in acquiring expert-quality demonstrations limits the applicability of LfD methods: real-world data collection is often costly, and the quality of the demonstrations depends greatly on the demonstrator's abilities and safety concerns. A number of works have leveraged data augmentation (DA) to inexpensively generate additional demonstration data, but most DA works generate augmented data in a random fashion and ultimately produce highly suboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a human-guided DA framework that generates expert-quality augmented data. The key insight of GuDA is that while it may be difficult to demonstrate the sequence of actions required to produce expert data, a user can often easily identify when an augmented trajectory segment represents task progress. Thus, the user can impose a series of simple rules on the DA process to automatically generate augmented samples that approximate expert behavior. To extract a policy from GuDA, we use off-the-shelf offline reinforcement learning and behavior cloning algorithms. We evaluate GuDA on a physical robot soccer task as well as simulated D4RL navigation tasks, a simulated autonomous driving task, and a simulated soccer task. Empirically, we find that GuDA enables learning from a small set of potentially suboptimal demonstrations and substantially outperforms a DA strategy that samples augmented data randomly.", "url": "https://arxiv.org/abs/2310.18247"}, {"metadata": {"arXiv": "2310.18257", "Date": "Thu, 26 Oct 2023 02:09:39 ", "Title": "MIM-GAN-based Anomaly Detection for Multivariate Time Series Data", "Authors": ["Shan Lu", "Zhicheng Dong", "Donghong Cai", "Fang Fang and Dongcai Zhao"], "Categories": "cs.LG", "Comments": ["7 pages,6 figures"]}, "abstract": "The loss function of Generative adversarial network(GAN) is an important factor that affects the quality and diversity of the generated samples for anomaly detection. In this paper, we propose an unsupervised multiple time series anomaly detection algorithm based on the GAN with message importance measure(MIM-GAN). In particular, the time series data is divided into subsequences using a sliding window. Then a generator and a discriminator designed based on the Long Short-Term Memory (LSTM) are employed to capture the temporal correlations of the time series data. To avoid the local optimal solution of loss function and the model collapse, we introduce an exponential information measure into the loss function of GAN. Additionally, a discriminant reconstruction score consisting on discrimination and reconstruction loss is taken into account. The global optimal solution for the loss function is derived and the model collapse is proved to be avoided in our proposed MIM-GAN-based anomaly detection algorithm. Experimental results show that the proposed MIM-GAN-based anomaly detection algorithm has superior performance in terms of precision, recall, and F1 score.", "url": "https://arxiv.org/abs/2310.18257"}, {"metadata": {"arXiv": "2310.18285", "Date": "Fri, 27 Oct 2023 17:22:09 ", "Title": "Heterogeneous Federated Learning with Group-Aware Prompt Tuning", "Authors": ["Wenlong Deng", "Christos Thrampoulidis", "Xiaoxiao Li"], "Categories": "cs.LG cs.CV"}, "abstract": "Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridges the gap between global and personalized local models in Federated Learning and surpasses alternative approaches that lack the capability to adapt to previously unseen clients. The effectiveness of our approach is rigorously validated through extensive experimentation and ablation studies.", "url": "https://arxiv.org/abs/2310.18285"}, {"metadata": {"arXiv": "2310.18286", "Date": "Fri, 27 Oct 2023 17:22:45 ", "Title": "Optimal Transport for Treatment Effect Estimation", "Authors": ["Hao Wang", "Zhichao Chen", "Jiajun Fan", "Haoxuan Li", "Tianqiao Liu", "Weiming Liu", "Quanyu Dai", "Yichao Wang", "Zhenhua Dong", "Ruiming Tang"], "Categories": "cs.LG stat.AP stat.ML", "Comments": ["Accepted as NeurIPS 2023 Poster"]}, "abstract": "Estimating conditional average treatment effect from observational data is highly challenging due to the existence of treatment selection bias. Prevalent methods mitigate this issue by aligning distributions of different treatment groups in the latent space. However, there are two critical problems that these methods fail to address: (1) mini-batch sampling effects (MSE), which causes misalignment in non-ideal mini-batches with outcome imbalance and outliers; (2) unobserved confounder effects (UCE), which results in inaccurate discrepancy calculation due to the neglect of unobserved confounders. To tackle these problems, we propose a principled approach named Entire Space CounterFactual Regression (ESCFR), which is a new take on optimal transport in the context of causality. Specifically, based on the framework of stochastic optimal transport, we propose a relaxed mass-preserving regularizer to address the MSE issue and design a proximal factual outcome regularizer to handle the UCE issue. Extensive experiments demonstrate that our proposed ESCFR can successfully tackle the treatment selection bias and achieve significantly better performance than state-of-the-art methods.", "url": "https://arxiv.org/abs/2310.18286"}, {"metadata": {"arXiv": "2310.18288", "Date": "Fri, 27 Oct 2023 17:25:12 ", "Title": "Sustainable Concrete via Bayesian Optimization", "Authors": ["Sebastian Ament", "Andrew Witte", "Nishant Garg", "Julius Kusuma"], "Categories": "cs.LG physics.soc-ph", "Comments": ["NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World"]}, "abstract": "Eight percent of global carbon dioxide emissions can be attributed to the production of cement, the main component of concrete, which is also the dominant source of CO2 emissions in the construction of data centers. The discovery of lower-carbon concrete formulae is therefore of high significance for sustainability. However, experimenting with new concrete formulae is time consuming and labor intensive, as one usually has to wait to record the concrete's 28-day compressive strength, a quantity whose measurement can by its definition not be accelerated. This provides an opportunity for experimental design methodology like Bayesian Optimization (BO) to accelerate the search for strong and sustainable concrete formulae. Herein, we 1) propose modeling steps that make concrete strength amenable to be predicted accurately by a Gaussian process model with relatively few measurements, 2) formulate the search for sustainable concrete as a multi-objective optimization problem, and 3) leverage the proposed model to carry out multi-objective BO with real-world strength measurements of the algorithmically proposed mixes. Our experimental results show improved trade-offs between the mixtures' global warming potential (GWP) and their associated compressive strengths, compared to mixes based on current industry practices.", "url": "https://arxiv.org/abs/2310.18288"}, {"metadata": {"arXiv": "2310.18291", "Date": "Fri, 27 Oct 2023 17:29:07 ", "Title": "Addressing GAN Training Instabilities via Tunable Classification Losses", "Authors": ["Monica Welfert", "Gowtham R. Kurri", "Kyle Otstot", "Lalitha Sankar"], "Categories": "cs.LG cs.IT math.IT stat.ML", "Comments": ["arXiv admin note: text overlap with arXiv:2302.14320"]}, "abstract": "Generative adversarial networks (GANs), modeled as a zero-sum game between a generator (G) and a discriminator (D), allow generating synthetic data with formal guarantees. Noting that D is a classifier, we begin by reformulating the GAN value function using class probability estimation (CPE) losses. We prove a two-way correspondence between CPE loss GANs and $f$-GANs which minimize $f$-divergences. We also show that all symmetric $f$-divergences are equivalent in convergence. In the finite sample and model capacity setting, we define and obtain bounds on estimation and generalization errors. We specialize these results to $\\alpha$-GANs, defined using $\\alpha$-loss, a tunable CPE loss family parametrized by $\\alpha\\in(0,\\infty]$. We next introduce a class of dual-objective GANs to address training instabilities of GANs by modeling each player's objective using $\\alpha$-loss to obtain $(\\alpha_D,\\alpha_G)$-GANs. We show that the resulting non-zero sum game simplifies to minimizing an $f$-divergence under appropriate conditions on $(\\alpha_D,\\alpha_G)$. Generalizing this dual-objective formulation using CPE losses, we define and obtain upper bounds on an appropriately defined estimation error. Finally, we highlight the value of tuning $(\\alpha_D,\\alpha_G)$ in alleviating training instabilities for the synthetic 2D Gaussian mixture ring as well as the large publicly available Celeb-A and LSUN Classroom image datasets.", "url": "https://arxiv.org/abs/2310.18291"}, {"metadata": {"arXiv": "2310.18313", "Date": "Fri, 27 Oct 2023 17:59:51 ", "Title": "FP8-LM: Training FP8 Large Language Models", "Authors": ["Houwen Peng and Kan Wu and Yixuan Wei and Guoshuai Zhao and Yuxiang Yang and Ze Liu and Yifan Xiong and Ziyue Yang and Bolin Ni and Jingcheng Hu and Ruihang Li and Miaosen Zhang and Chen Li and Jia Ning and Ruizhe Wang and Zheng Zhang and Shuguang Liu and Joe Chau and Han Hu and Peng Cheng"], "Categories": "cs.LG cs.CL"}, "abstract": "In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at {https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.", "url": "https://arxiv.org/abs/2310.18313"}, {"metadata": {"arXiv": "2310.17785", "Date": "Thu, 26 Oct 2023 21:28:23 ", "Title": "Learning Extrinsic Dexterity with Parameterized Manipulation Primitives", "Authors": ["Shih-Min Yang", "Martin Magnusson", "Johannes A. Stork", "Todor Stoyano"], "Categories": "cs.RO cs.LG"}, "abstract": "Many practically relevant robot grasping problems feature a target object for which all grasps are occluded, e.g., by the environment. Single-shot grasp planning invariably fails in such scenarios. Instead, it is necessary to first manipulate the object into a configuration that affords a grasp. We solve this problem by learning a sequence of actions that utilize the environment to change the object's pose. Concretely, we employ hierarchical reinforcement learning to combine a sequence of learned parameterized manipulation primitives. By learning the low-level manipulation policies, our approach can control the object's state through exploiting interactions between the object, the gripper, and the environment. Designing such a complex behavior analytically would be infeasible under uncontrolled conditions, as an analytic approach requires accurate physical modeling of the interaction and contact dynamics. In contrast, we learn a hierarchical policy model that operates directly on depth perception data, without the need for object detection, pose estimation, or manual design of controllers. We evaluate our approach on picking box-shaped objects of various weight, shape, and friction properties from a constrained table-top workspace. Our method transfers to a real robot and is able to successfully complete the object picking task in 98\\% of experimental trials.", "url": "https://arxiv.org/abs/2310.17785"}, {"metadata": {"arXiv": "2310.17788", "Date": "Thu, 26 Oct 2023 21:36:06 ", "Title": "Utilizing Language Models for Energy Load Forecasting", "Authors": ["Hao Xue and Flora D. Salim"], "Categories": "cs.AI cs.CL", "Comments": ["BuildSys 2023 Accepted"]}, "abstract": "Energy load forecasting plays a crucial role in optimizing resource allocation and managing energy consumption in buildings and cities. In this paper, we propose a novel approach that leverages language models for energy load forecasting. We employ prompting techniques to convert energy consumption data into descriptive sentences, enabling fine-tuning of language models. By adopting an autoregressive generating approach, our proposed method enables predictions of various horizons of future energy load consumption. Through extensive experiments on real-world datasets, we demonstrate the effectiveness and accuracy of our proposed method. Our results indicate that utilizing language models for energy load forecasting holds promise for enhancing energy efficiency and facilitating intelligent decision-making in energy systems.", "url": "https://arxiv.org/abs/2310.17788"}, {"metadata": {"arXiv": "2310.17811", "Date": "Thu, 26 Oct 2023 23:06:38 ", "Title": "Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting", "Authors": ["Benjamin Yan", "Ruochen Liu", "David E. Kuo", "Subathra Adithan", "Eduardo Pontes Reis", "Stephen Kwak", "Vasantha Kumar Venugopal", "Chloe P. O'Connell", "Agustina Saenz", "Pranav Rajpurkar", "Michael Moor"], "Categories": "cs.AI cs.CL", "Comments": ["Accepted to Findings of EMNLP 2023"]}, "abstract": "Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist despite leveraging only a few examples as context.", "url": "https://arxiv.org/abs/2310.17811"}, {"metadata": {"arXiv": "2310.17884", "Date": "Fri, 27 Oct 2023 04:15:30 ", "Title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory", "Authors": ["Niloofar Mireshghallah", "Hyunwoo Kim", "Xuhui Zhou", "Yulia Tsvetkov", "Maarten Sap", "Reza Shokri", "Yejin Choi"], "Categories": "cs.AI cs.CL cs.CR", "Comments": ["confaide.github.io"]}, "abstract": "The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.", "url": "https://arxiv.org/abs/2310.17884"}, {"metadata": {"arXiv": "2310.17909", "Date": "Fri, 27 Oct 2023 05:57:41 ", "Title": "The Innovation-to-Occupations Ontology: Linking Business Transformation Initiatives to Occupations and Skills", "Authors": ["Daniela Elia", "Fang Chen", "Didar Zowghi and Marian-Andrei Rizoiu"], "Categories": "cs.AI", "Comments": ["14 pages", "3 figures", "Camera-ready version in ACIS 2023"]}, "abstract": "The fast adoption of new technologies forces companies to continuously adapt their operations making it harder to predict workforce requirements. Several recent studies have attempted to predict the emergence of new roles and skills in the labour market from online job ads. This paper aims to present a novel ontology linking business transformation initiatives to occupations and an approach to automatically populating it by leveraging embeddings extracted from job ads and Wikipedia pages on business transformation and emerging technologies topics. To our knowledge, no previous research explicitly links business transformation initiatives, like the adoption of new technologies or the entry into new markets, to the roles needed. Our approach successfully matches occupations to transformation initiatives under ten different scenarios, five linked to technology adoption and five related to business. This framework presents an innovative approach to guide enterprises and educational institutions on the workforce requirements for specific business transformation initiatives.", "url": "https://arxiv.org/abs/2310.17909"}, {"metadata": {"arXiv": "2310.18021", "Date": "Fri, 27 Oct 2023 09:55:12 ", "Title": "FormalGeo: The First Step Toward Human-like IMO-level Geometric Automated Reasoning", "Authors": ["Xiaokai Zhang", "Na Zhu", "Yiming He", "Jia Zou", "Qike Huang", "Xiaoxiao Jin", "Yanjun Guo", "Chenyang Mao", "Zhe Zhu", "Dengfeng Yue", "Fangzhen Zhu", "Yang Li", "Yifan Wang", "Yiwen Huang", "Runan Wang", "Cheng Qin", "Zhenbing Zeng", "Shaorong Xie", "Xiangfeng Luo", "Tuo Leng"], "Categories": "cs.AI", "Comments": ["43 pages"]}, "abstract": "This is the first article of our work over the past decade. In this series of papers, we have constructed a complete and compatible formal plane geometry system. This will serve as a crucial bridge between IMO-level plane geometry challenges and readable AI automated reasoning. With this formal system in place, we have been able to seamlessly integrate modern AI models with our formal system. Within this formal framework, AI is now capable of providing deductive reasoning solutions to IMO-level plane geometry problems, just like handling other natural languages, and these proofs are readable, traceable, and verifiable. We propose the geometry formalization theory (GFT) to guide the development of the geometry formal system. Based on the GFT, we have established the FormalGeo, which consists of 88 geometric predicates and 196 theorems. It can represent, validate, and solve IMO-level geometry problems. we also have crafted the FGPS (formal geometry problem solver) in Python. It serves as both an interactive assistant for verifying problem-solving processes and an automated problem solver, utilizing various methods such as forward search, backward search and AI-assisted search. We've annotated the FormalGeo7k dataset, containing 6,981 (expand to 186,832 through data augmentation) geometry problems with complete formal language annotations. Implementation of the formal system and experiments on the FormalGeo7k validate the correctness and utility of the GFT. The backward depth-first search method only yields a 2.42% problem-solving failure rate, and we can incorporate deep learning techniques to achieve lower one. The source code of FGPS and FormalGeo7k dataset are available at https://github.com/BitSecret/FormalGeo.", "url": "https://arxiv.org/abs/2310.18021"}, {"metadata": {"arXiv": "2310.18040", "Date": "Fri, 27 Oct 2023 10:37:47 ", "Title": "Moral Responsibility for AI Systems", "Authors": ["Sander Beckers"], "Categories": "cs.AI cs.CY cs.LO", "Comments": ["Accepted at NeurIPS 2023"]}, "abstract": "As more and more decisions that have a significant ethical dimension are being outsourced to AI systems, it is important to have a definition of moral responsibility that can be applied to AI systems. Moral responsibility for an outcome of an agent who performs some action is commonly taken to involve both a causal condition and an epistemic condition: the action should cause the outcome, and the agent should have been aware -- in some form or other -- of the possible moral consequences of their action. This paper presents a formal definition of both conditions within the framework of causal models. I compare my approach to the existing approaches of Braham and van Hees (BvH) and of Halpern and Kleiman-Weiner (HK). I then generalize my definition into a degree of responsibility.", "url": "https://arxiv.org/abs/2310.18040"}, {"metadata": {"arXiv": "2310.18233", "Date": "Wed, 25 Oct 2023 13:43:16 ", "Title": "Will releasing the weights of large language models grant widespread access to pandemic agents?", "Authors": ["Anjali Gopal", "Nathan Helm-Burger", "Lenni Justen", "Emily H. Soice", "Tiffany Tzeng", "Geetha Jeyapragasan", "Simon Grimm", "Benjamin Mueller", "Kevin M. Esvelt"], "Categories": "cs.AI"}, "abstract": "Large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide \"dual-use\" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. Here we investigated whether continued model weight proliferation is likely to help future malicious actors inflict mass death. We organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the \"Base\" Llama-2-70B model and a \"Spicy\" version that we tuned to remove safeguards. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Future models will be more capable. Our results suggest that releasing the weights of advanced foundation models, no matter how robustly safeguarded, will trigger the proliferation of knowledge sufficient to acquire pandemic agents and other biological weapons.", "url": "https://arxiv.org/abs/2310.18233"}, {"metadata": {"arXiv": "2310.18239", "Date": "Fri, 27 Oct 2023 16:24:24 ", "Title": "Fine-Tuning Language Models Using Formal Methods Feedback", "Authors": ["Yunhao Yang", "Neel P. Bhatt", "Tyler Ingebrand", "William Ward", "Steven Carr", "Zhangyang Wang", "Ufuk Topcu"], "Categories": "cs.AI cs.CL cs.FL"}, "abstract": "Although pre-trained language models encode generic knowledge beneficial for planning and control, they may fail to generate appropriate control policies for domain-specific tasks. Existing fine-tuning methods use human feedback to address this limitation, however, sourcing human feedback is labor intensive and costly. We present a fully automated approach to fine-tune pre-trained language models for applications in autonomous systems, bridging the gap between generic knowledge and domain-specific requirements while reducing cost. The method synthesizes automaton-based controllers from pre-trained models guided by natural language task descriptions. These controllers are verifiable against independently provided specifications within a world model, which can be abstract or obtained from a high-fidelity simulator. Controllers with high compliance with the desired specifications receive higher ranks, guiding the iterative fine-tuning process. We provide quantitative evidences, primarily in autonomous driving, to demonstrate the method's effectiveness across multiple tasks. The results indicate an improvement in percentage of specifications satisfied by the controller from 60% to 90%.", "url": "https://arxiv.org/abs/2310.18239"}, {"metadata": {"arXiv": "2310.18273", "Date": "Fri, 27 Oct 2023 16:59:13 ", "Title": "Moments for Perceptive Narration Analysis Through the Emotional Attachment of Audience to Discourse and Story", "Authors": ["Gary Bruins and Ergun Akleman"], "Categories": "cs.AI cs.CY cs.DL", "Comments": ["20 pages"]}, "abstract": "In this work, our goal is to develop a theoretical framework that can eventually be used for analyzing the effectiveness of visual stories such as feature films to comic books. To develop this theoretical framework, we introduce a new story element called moments. Our conjecture is that any linear story such as the story of a feature film can be decomposed into a set of moments that follow each other. Moments are defined as the perception of the actions, interactions, and expressions of all characters or a single character during a given time period. We categorize the moments into two major types: story moments and discourse moments. Each type of moment can further be classified into three types, which we call universal storytelling moments. We believe these universal moments foster or deteriorate the emotional attachment of the audience to a particular character or the story. We present a methodology to catalog the occurrences of these universal moments as they are found in the story. The cataloged moments can be represented using curves or color strips. Therefore, we can visualize a character's journey through the story as either a 3D curve or a color strip. We also demonstrated that both story and discourse moments can be transformed into one lump-sum attraction parameter. The attraction parameter in time provides a function that can be plotted graphically onto a timeline illustrating changes in the emotional attachment of audience to a character or the story. By inspecting these functions the story analyst can analytically decipher the moments in the story where the attachment is being established, maintained, strengthened, or conversely where it is languishing.", "url": "https://arxiv.org/abs/2310.18273"}, {"metadata": {"arXiv": "2310.17835", "Date": "Fri, 27 Oct 2023 01:17:48 ", "Title": "One Style is All you Need to Generate a Video", "Authors": ["Sandeep Manandhar and Auguste Genovesio"], "Categories": "cs.CV cs.AI"}, "abstract": "In this paper, we propose a style-based conditional video generative model. We introduce a novel temporal generator based on a set of learned sinusoidal bases. Our method learns dynamic representations of various actions that are independent of image content and can be transferred between different actors. Beyond the significant enhancement of video quality compared to prevalent methods, we demonstrate that the disentangled dynamic and content permit their independent manipulation, as well as temporal GAN-inversion to retrieve and transfer a video motion from one content or identity to another without further preprocessing such as landmark points.", "url": "https://arxiv.org/abs/2310.17835"}, {"metadata": {"arXiv": "2310.17951", "Date": "Fri, 27 Oct 2023 07:48:36 ", "Title": "Understanding Parameter Saliency via Extreme Value Theory", "Authors": ["Shuo Wang and Issei Sato"], "Categories": "cs.CV cs.AI"}, "abstract": "Deep neural networks are being increasingly implemented throughout society in recent years. It is useful to identify which parameters trigger misclassification in diagnosing undesirable model behaviors. The concept of parameter saliency is proposed and used to diagnose convolutional neural networks (CNNs) by ranking convolution filters that may have caused misclassification on the basis of parameter saliency. It is also shown that fine-tuning the top ranking salient filters has efficiently corrected misidentification on ImageNet. However, there is still a knowledge gap in terms of understanding why parameter saliency ranking can find the filters inducing misidentification. In this work, we attempt to bridge the gap by analyzing parameter saliency ranking from a statistical viewpoint, namely, extreme value theory. We first show that the existing work implicitly assumes that the gradient norm computed for each filter follows a normal distribution. Then, we clarify the relationship between parameter saliency and the score based on the peaks-over-threshold (POT) method, which is often used to model extreme values. Finally, we reformulate parameter saliency in terms of the POT method, where this reformulation is regarded as statistical anomaly detection and does not require the implicit assumptions of the existing parameter-saliency formulation. Our experimental results demonstrate that our reformulation can detect malicious filters as well. Furthermore, we show that the existing parameter saliency method exhibits a bias against the depth of layers in deep neural networks. In particular, this bias has the potential to inhibit the discovery of filters that cause misidentification in situations where domain shift occurs. In contrast, parameter saliency based on POT shows less of this bias.", "url": "https://arxiv.org/abs/2310.17951"}, {"metadata": {"arXiv": "2310.17956", "Date": "Fri, 27 Oct 2023 08:05:21 ", "Title": "Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare", "Authors": ["Junling Liu", "Ziming Wang", "Qichen Ye", "Dading Chong", "Peilin Zhou", "Yining Hua"], "Categories": "cs.CV cs.AI cs.CL"}, "abstract": "Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.", "url": "https://arxiv.org/abs/2310.17956"}, {"metadata": {"arXiv": "2310.18297", "Date": "Fri, 27 Oct 2023 17:35:01 ", "Title": "Image Clustering Conditioned on Text Criteria", "Authors": ["Sehyun Kwon", "Jaeseung Park", "Minkyu Kim", "Jaewoong Cho", "Ernest K. Ryu", "Kangwook Lee"], "Categories": "cs.CV cs.AI"}, "abstract": "Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.", "url": "https://arxiv.org/abs/2310.18297"}, {"metadata": {"arXiv": "2310.17977", "Date": "Fri, 27 Oct 2023 08:45:30 ", "Title": "Autonomous 3D Exploration in Large-Scale Environments with Dynamic Obstacles", "Authors": ["Emil Wiman", "Ludvig Wid\\'en", "Mattias Tiger", "Fredrik Heintz"], "Categories": "cs.RO cs.AI", "Comments": ["6 pages", "6 figures"]}, "abstract": "Exploration in dynamic and uncertain real-world environments is an open problem in robotics and constitutes a foundational capability of autonomous systems operating in most of the real world. While 3D exploration planning has been extensively studied, the environments are assumed static or only reactive collision avoidance is carried out. We propose a novel approach to not only avoid dynamic obstacles but also include them in the plan itself, to exploit the dynamic environment in the agent's favor. The proposed planner, Dynamic Autonomous Exploration Planner (DAEP), extends AEP to explicitly plan with respect to dynamic obstacles. To thoroughly evaluate exploration planners in such settings we propose a new enhanced benchmark suite with several dynamic environments, including large-scale outdoor environments. DAEP outperform state-of-the-art planners in dynamic and large-scale environments. DAEP is shown to be more effective at both exploration and collision avoidance.", "url": "https://arxiv.org/abs/2310.17977"}, {"metadata": {"arXiv": "2310.18112", "Date": "Fri, 27 Oct 2023 12:52:34 ", "Title": "er.autopilot 1.0: The Full Autonomous Stack for Oval Racing at High Speeds", "Authors": ["Ayoub Raji", "Danilo Caporale", "Francesco Gatti", "Andrea Giove", "Micaela Verucchi", "Davide Malatesta", "Nicola Musiu", "Alessandro Toschi", "Silviu Roberto Popitanu", "Fabio Bagni", "Massimiliano Bosi", "Alexander Liniger", "Marko Bertogna", "Daniele Morra", "Francesco Amerotti", "Luca Bartoli", "Federico Martello", "Riccardo Porta"], "Categories": "cs.RO cs.AI cs.CV cs.SY eess.SY", "Comments": ["Preprint: Accepted to Field Robotics \"Opportunities and Challenges with Autonomous Racing\" Special Issue"]}, "abstract": "The Indy Autonomous Challenge (IAC) brought together for the first time in history nine autonomous racing teams competing at unprecedented speed and in head-to-head scenario, using independently developed software on open-wheel racecars. This paper presents the complete software architecture used by team TII EuroRacing (TII-ER), covering all the modules needed to avoid static obstacles, perform active overtakes and reach speeds above 75 m/s (270 km/h). In addition to the most common modules related to perception, planning, and control, we discuss the approaches used for vehicle dynamics modelling, simulation, telemetry, and safety. Overall results and the performance of each module are described, as well as the lessons learned during the first two events of the competition on oval tracks, where the team placed respectively second and third.", "url": "https://arxiv.org/abs/2310.18112"}, {"metadata": {"arXiv": "2310.18301", "Date": "Fri, 27 Oct 2023 17:48:25 ", "Title": "Interactive Motion Planning for Autonomous Vehicles with Joint Optimization", "Authors": ["Yuxiao Chen", "Sushant Veer", "Peter Karkus", "and Marco Pavone"], "Categories": "cs.RO cs.AI cs.SY eess.SY"}, "abstract": "In highly interactive driving scenarios, the actions of one agent greatly influences those of its neighbors. Planning safe motions for autonomous vehicles in such interactive environments, therefore, requires reasoning about the impact of the ego's intended motion plan on nearby agents' behavior. Deep-learning-based models have recently achieved great success in trajectory prediction and many models in the literature allow for ego-conditioned prediction. However, leveraging ego-conditioned prediction remains challenging in downstream planning due to the complex nature of neural networks, limiting the planner structure to simple ones, e.g., sampling-based planner. Despite their ability to generate fine-grained high-quality motion plans, it is difficult for gradient-based planning algorithms, such as model predictive control (MPC), to leverage ego-conditioned prediction due to their iterative nature and need for gradient. We present Interactive Joint Planning (IJP) that bridges MPC with learned prediction models in a computationally scalable manner to provide us the best of both the worlds. In particular, IJP jointly optimizes over the behavior of the ego and the surrounding agents and leverages deep-learned prediction models as prediction priors that the join trajectory optimization tries to stay close to. Furthermore, by leveraging homotopy classes, our joint optimizer searches over diverse motion plans to avoid getting stuck at local minima. Closed-loop simulation result shows that IJP significantly outperforms the baselines that are either without joint optimization or running sampling-based planning.", "url": "https://arxiv.org/abs/2310.18301"}, {"metadata": {"arXiv": "2310.18303", "Date": "Fri, 27 Oct 2023 17:53:02 ", "Title": "Socially Cognizant Robotics for a Technology Enhanced Society", "Authors": ["Kristin J. Dana", "Clinton Andrews", "Kostas Bekris", "Jacob Feldman", "Matthew Stone", "Pernille Hemmer", "Aaron Mazzeo", "Hal Salzman", "Jingang Yi"], "Categories": "cs.RO cs.AI"}, "abstract": "Emerging applications of robotics, and concerns about their impact, require the research community to put human-centric objectives front-and-center. To meet this challenge, we advocate an interdisciplinary approach, socially cognizant robotics, which synthesizes technical and social science methods. We argue that this approach follows from the need to empower stakeholder participation (from synchronous human feedback to asynchronous societal assessment) in shaping AI-driven robot behavior at all levels, and leads to a range of novel research perspectives and problems both for improving robots' interactions with individuals and impacts on society. Drawing on these arguments, we develop best practices for socially cognizant robot design that balance traditional technology-based metrics (e.g. efficiency, precision and accuracy) with critically important, albeit challenging to measure, human and society-based metrics.", "url": "https://arxiv.org/abs/2310.18303"}, {"metadata": {"arXiv": "2310.17770", "Date": "Thu, 26 Oct 2023 20:27:16 ", "Title": "GROOViST: A Metric for Grounding Objects in Visual Storytelling", "Authors": ["Aditya K Surikuchi", "Sandro Pezzelle", "Raquel Fern\\'andez"], "Categories": "cs.AI cs.CL cs.CV cs.LG", "Comments": ["In EMNLP 2023 main conference proceedings (to appear)"]}, "abstract": "A proper evaluation of stories generated for a sequence of images -- the task commonly referred to as visual storytelling -- must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually.", "url": "https://arxiv.org/abs/2310.17770"}, {"metadata": {"arXiv": "2310.17773", "Date": "Thu, 26 Oct 2023 20:51:24 ", "Title": "Graph Convolutional Networks for Complex Traffic Scenario Classification", "Authors": ["Tobias Hoek", "Holger Caesar", "Andreas Falkov\\'en", "Tommy Johansson"], "Categories": "cs.CV cs.AI cs.LG cs.MA", "Comments": ["Netherlands Conference on Computer Vision (NCCV) 2023 camera-ready + supplementary material"], "ACM-class": "I.2; I.4; I.5"}, "abstract": "A scenario-based testing approach can reduce the time required to obtain statistically significant evidence of the safety of Automated Driving Systems (ADS). Identifying these scenarios in an automated manner is a challenging task. Most methods on scenario classification do not work for complex scenarios with diverse environments (highways, urban) and interaction with other traffic agents. This is mirrored in their approaches which model an individual vehicle in relation to its environment, but neglect the interaction between multiple vehicles (e.g. cut-ins, stationary lead vehicle). Furthermore, existing datasets lack diversity and do not have per-frame annotations to accurately learn the start and end time of a scenario. We propose a method for complex traffic scenario classification that is able to model the interaction of a vehicle with the environment, as well as other agents. We use Graph Convolutional Networks to model spatial and temporal aspects of these scenarios. Expanding the nuScenes and Argoverse 2 driving datasets, we introduce a scenario-labeled dataset, which covers different driving environments and is annotated per frame. Training our method on this dataset, we present a promising baseline for future research on per-frame complex scenario classification.", "url": "https://arxiv.org/abs/2310.17773"}, {"metadata": {"arXiv": "2310.18235", "Date": "Fri, 27 Oct 2023 16:20:10 ", "Title": "Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation", "Authors": ["Jaemin Cho", "Yushi Hu", "Roopal Garg", "Peter Anderson", "Ranjay Krishna", "Jason Baldridge", "Mohit Bansal", "Jordi Pont-Tuset", "Su Wang"], "Categories": "cs.CV cs.AI cs.CL cs.LG", "Comments": ["Project website: https://google.github.io/DSG"]}, "abstract": "Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG is an automatic, graph-based QG/A that is modularly implemented to be adaptable to any QG/A module. DSG produces atomic and unique questions organized in dependency graphs, which (i) ensure appropriate semantic coverage and (ii) sidestep inconsistent answers. With extensive experimentation and human evaluation on a range of model configurations (LLM, VQA, and T2I), we empirically demonstrate that DSG addresses the challenges noted above. Finally, we present DSG-1k, an open-sourced evaluation benchmark that includes 1,060 prompts, covering a wide range of fine-grained semantic categories with a balanced distribution. We will release the DSG-1k prompts and the corresponding DSG questions.", "url": "https://arxiv.org/abs/2310.18235"}, {"metadata": {"arXiv": "2310.17658", "Date": "Wed, 18 Oct 2023 15:24:34 ", "Title": "Is Channel Independent strategy optimal for Time Series Forecasting?", "Authors": ["Yuan Peiwen", "Zhu Changsheng"], "Categories": "cs.LG cs.AI eess.SP"}, "abstract": "There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\\mathbf{C}$hannel $\\mathbf{S}$elf-$\\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, for exmpale by over 10 times on electricity dataset, and significantly cutting training time. Second, we further propose Channel Rearrangement (CR), a method for deep models inspired by the self-clustering. CR attains competitive performance against baselines. Finally, we also discuss whether it is best to forecast the future values using the historical values of the same channel as inputs. We hope our findings and methods could inspire new solutions beyond CD/CI.", "url": "https://arxiv.org/abs/2310.17658"}, {"metadata": {"arXiv": "2310.17671", "Date": "Wed, 25 Oct 2023 09:13:12 ", "Title": "Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop", "Authors": ["Mario Picerno", "Lucas Koch", "Kevin Badalian", "Marius Wegener", "Joschka Schaub", "Charles Robert Koch", "and Jakob Andert"], "Categories": "cs.LG cs.AI"}, "abstract": "The process of developing control functions for embedded systems is resource-, time-, and data-intensive, often resulting in sub-optimal cost and solutions approaches. Reinforcement Learning (RL) has great potential for autonomously training agents to perform complex control tasks with minimal human intervention. Due to costly data generation and safety constraints, however, its application is mostly limited to purely simulated domains. To use RL effectively in embedded system function development, the generated agents must be able to handle real-world applications. In this context, this work focuses on accelerating the training process of RL agents by combining Transfer Learning (TL) and X-in-the-Loop (XiL) simulation. For the use case of transient exhaust gas re-circulation control for an internal combustion engine, use of a computationally cheap Model-in-the-Loop (MiL) simulation is made to select a suitable algorithm, fine-tune hyperparameters, and finally train candidate agents for the transfer. These pre-trained RL agents are then fine-tuned in a Hardware-in-the-Loop (HiL) system via TL. The transfer revealed the need for adjusting the reward parameters when advancing to real hardware. Further, the comparison between a purely HiL-trained and a transferred agent showed a reduction of training time by a factor of 5.9. The results emphasize the necessity to train RL agents with real hardware, and demonstrate that the maturity of the transferred policies affects both training time and performance, highlighting the strong synergies between TL and XiL simulation.", "url": "https://arxiv.org/abs/2310.17671"}, {"metadata": {"arXiv": "2310.17679", "Date": "Thu, 26 Oct 2023 10:03:12 ", "Title": "Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees", "Authors": ["Bryan Andrews", "Joseph Ramsey", "Ruben Sanchez-Romero", "Jazmin Camchong", "Erich Kummerfeld"], "Categories": "cs.LG cs.AI stat.ME"}, "abstract": "Learning graphical conditional independence structures is an important machine learning problem and a cornerstone of causal discovery. However, the accuracy and execution time of learning algorithms generally struggle to scale to problems with hundreds of highly connected variables -- for instance, recovering brain networks from fMRI data. We introduce the best order score search (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs (DAGs) in this paradigm. BOSS greedily searches over permutations of variables, using GSTs to construct and score DAGs from permutations. GSTs efficiently cache scores to eliminate redundant calculations. BOSS achieves state-of-the-art performance in accuracy and execution time, comparing favorably to a variety of combinatorial and gradient-based learning algorithms under a broad range of conditions. To demonstrate its practicality, we apply BOSS to two sets of resting-state fMRI data: simulated data with pseudo-empirical noise distributions derived from randomized empirical fMRI cortical signals and clinical data from 3T fMRI scans processed into cortical parcels. BOSS is available for use within the TETRAD project which includes Python and R wrappers.", "url": "https://arxiv.org/abs/2310.17679"}, {"metadata": {"arXiv": "2310.17722", "Date": "Thu, 26 Oct 2023 18:32:05 ", "Title": "Large Language Models as Generalizable Policies for Embodied Tasks", "Authors": ["Andrew Szot", "Max Schwarzer", "Harsh Agrawal", "Bogdan Mazoure", "Walter Talbott", "Katherine Metcalf", "Natalie Mackraz", "Devon Hjelm", "Alexander Toshev"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io.", "url": "https://arxiv.org/abs/2310.17722"}, {"metadata": {"arXiv": "2310.17729", "Date": "Thu, 26 Oct 2023 18:40:28 ", "Title": "Improving Traffic Density Forecasting in Intelligent Transportation Systems Using Gated Graph Neural Networks", "Authors": ["Razib Hayat Khan", "Jonayet Miah", "S M Yasir Arafat", "M M Mahbubul Syeed", "Duc M Ca"], "Categories": "cs.LG cs.AI cs.CV"}, "abstract": "This study delves into the application of graph neural networks in the realm of traffic forecasting, a crucial facet of intelligent transportation systems. Accurate traffic predictions are vital for functions like trip planning, traffic control, and vehicle routing in such systems. Three prominent GNN architectures Graph Convolutional Networks (Graph Sample and Aggregation) and Gated Graph Neural Networks are explored within the context of traffic prediction. Each architecture's methodology is thoroughly examined, including layer configurations, activation functions,and hyperparameters. The primary goal is to minimize prediction errors, with GGNNs emerging as the most effective choice among the three models. The research outlines outcomes for each architecture, elucidating their predictive performance through root mean squared error and mean absolute error (MAE). Hypothetical results reveal intriguing insights: GCNs display an RMSE of 9.10 and an MAE of 8.00, while GraphSAGE shows improvement with an RMSE of 8.3 and an MAE of 7.5. Gated Graph Neural Networks (GGNNs) exhibit the lowest RMSE at 9.15 and an impressive MAE of 7.1, positioning them as the frontrunner.", "url": "https://arxiv.org/abs/2310.17729"}, {"metadata": {"arXiv": "2310.17805", "Date": "Thu, 26 Oct 2023 22:40:30 ", "Title": "Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks", "Authors": ["Ryan Sullivan", "Akarsh Kumar", "Shengyi Huang", "John P. Dickerson", "Joseph Suarez"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted to NeurIPS 2023"]}, "abstract": "Most reinforcement learning methods rely heavily on dense, well-normalized environment rewards. DreamerV3 recently introduced a model-based method with a number of tricks that mitigate these limitations, achieving state-of-the-art on a wide range of benchmarks with a single set of hyperparameters. This result sparked discussion about the generality of the tricks, since they appear to be applicable to other reinforcement learning algorithms. Our work applies DreamerV3's tricks to PPO and is the first such empirical study outside of the original work. Surprisingly, we find that the tricks presented do not transfer as general improvements to PPO. We use a high quality PPO reference implementation and present extensive ablation studies totaling over 10,000 A100 hours on the Arcade Learning Environment and the DeepMind Control Suite. Though our experiments demonstrate that these tricks do not generally outperform PPO, we identify cases where they succeed and offer insight into the relationship between the implementation tricks. In particular, PPO with these tricks performs comparably to PPO on Atari games with reward clipping and significantly outperforms PPO without reward clipping.", "url": "https://arxiv.org/abs/2310.17805"}, {"metadata": {"arXiv": "2310.17852", "Date": "Fri, 27 Oct 2023 02:04:31 ", "Title": "Function Space Bayesian Pseudocoreset for Bayesian Neural Networks", "Authors": ["Balhae Kim", "Hyungi Lee", "Juho Lee"], "Categories": "cs.LG cs.AI"}, "abstract": "A Bayesian pseudocoreset is a compact synthetic dataset summarizing essential information of a large-scale dataset and thus can be used as a proxy dataset for scalable Bayesian inference. Typically, a Bayesian pseudocoreset is constructed by minimizing a divergence measure between the posterior conditioning on the pseudocoreset and the posterior conditioning on the full dataset. However, evaluating the divergence can be challenging, particularly for the models like deep neural networks having high-dimensional parameters. In this paper, we propose a novel Bayesian pseudocoreset construction method that operates on a function space. Unlike previous methods, which construct and match the coreset and full data posteriors in the space of model parameters (weights), our method constructs variational approximations to the coreset posterior on a function space and matches it to the full data posterior in the function space. By working directly on the function space, our method could bypass several challenges that may arise when working on a weight space, including limited scalability and multi-modality issue. Through various experiments, we demonstrate that the Bayesian pseudocoresets constructed from our method enjoys enhanced uncertainty quantification and better robustness across various model architectures.", "url": "https://arxiv.org/abs/2310.17852"}, {"metadata": {"arXiv": "2310.17945", "Date": "Fri, 27 Oct 2023 07:40:45 ", "Title": "A Comprehensive and Reliable Feature Attribution Method: Double-sided Remove and Reconstruct (DoRaR)", "Authors": ["Dong Qin", "George Amariucai", "Daji Qiao", "Yong Guan", "Shen Fu"], "Categories": "cs.LG cs.AI", "Comments": ["16 pages", "22 figures"]}, "abstract": "The limited transparency of the inner decision-making mechanism in deep neural networks (DNN) and other machine learning (ML) models has hindered their application in several domains. In order to tackle this issue, feature attribution methods have been developed to identify the crucial features that heavily influence decisions made by these black box models. However, many feature attribution methods have inherent downsides. For example, one category of feature attribution methods suffers from the artifacts problem, which feeds out-of-distribution masked inputs directly through the classifier that was originally trained on natural data points. Another category of feature attribution method finds explanations by using jointly trained feature selectors and predictors. While avoiding the artifacts problem, this new category suffers from the Encoding Prediction in the Explanation (EPITE) problem, in which the predictor's decisions rely not on the features, but on the masks that selects those features. As a result, the credibility of attribution results is undermined by these downsides. In this research, we introduce the Double-sided Remove and Reconstruct (DoRaR) feature attribution method based on several improvement methods that addresses these issues. By conducting thorough testing on MNIST, CIFAR10 and our own synthetic dataset, we demonstrate that the DoRaR feature attribution method can effectively bypass the above issues and can aid in training a feature selector that outperforms other state-of-the-art feature attribution methods. Our code is available at https://github.com/dxq21/DoRaR.", "url": "https://arxiv.org/abs/2310.17945"}, {"metadata": {"arXiv": "2310.17966", "Date": "Fri, 27 Oct 2023 08:30:54 ", "Title": "Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning", "Authors": ["Shenzhi Wang", "Qisen Yang", "Jiawei Gao", "Matthieu Gaetan Lin", "Hao Chen", "Liwei Wu", "Ning Jia", "Shiji Song", "Gao Huang"], "Categories": "cs.LG cs.AI", "Comments": ["NeurIPS 2023 spotlight. 24 pages", "13 figures"]}, "abstract": "Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a balance model to select a suitable policy for each state. Theoretically, we prove that state-adaptive balances are necessary for achieving a higher policy performance upper bound. Empirically, extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark. Codes are available at https://github.com/LeapLabTHU/FamO2O.", "url": "https://arxiv.org/abs/2310.17966"}, {"metadata": {"arXiv": "2310.18127", "Date": "Fri, 27 Oct 2023 13:19:19 ", "Title": "Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models", "Authors": ["Xue Yan", "Yan Song", "Xinyu Cui", "Filippos Christianos", "Haifeng Zhang", "David Henry Mguni", "Jun Wang"], "Categories": "cs.LG cs.AI cs.CL"}, "abstract": "Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical findings, leading the CoT to consider the anticipated goals. A prompt-generator policy has its own aim in our system, allowing it to adapt to the action policy and automatically root the CoT process towards outputs that lead to decisive, high-performing actions. Meanwhile, the action policy is learning how to use the CoT outputs to take specific actions. Our empirical data reveal that our system outperforms leading methods in agent learning benchmarks such as Overcooked and FourRoom.", "url": "https://arxiv.org/abs/2310.18127"}, {"metadata": {"arXiv": "2310.18144", "Date": "Fri, 27 Oct 2023 13:51:18 ", "Title": "Improving Intrinsic Exploration by Creating Stationary Objectives", "Authors": ["Roger Creus Castanyer", "Joshua Romoff", "Glen Berseth"], "Categories": "cs.LG cs.AI", "Comments": ["Under Review at ICLR 2024"]}, "abstract": "Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the agents' performance in challenging exploration problems, including sparse-reward tasks, pixel-based observations, 3D navigation, and procedurally generated environments.", "url": "https://arxiv.org/abs/2310.18144"}, {"metadata": {"arXiv": "2310.18191", "Date": "Fri, 27 Oct 2023 15:04:00 ", "Title": "Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months", "Authors": ["Fady Rezk", "Antreas Antoniou", "Henry Gouk", "Timothy Hospedales"], "Categories": "cs.LG cs.AI math.OC"}, "abstract": "We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose \"foundational\" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.", "url": "https://arxiv.org/abs/2310.18191"}, {"metadata": {"arXiv": "2310.18209", "Date": "Fri, 27 Oct 2023 15:31:42 ", "Title": "Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning", "Authors": ["Yifei Zhang", "Hao Zhu", "Jiahong Liu", "Piotr Koniusz", "Irwin King"], "Categories": "cs.LG cs.AI"}, "abstract": "Learning good self-supervised graph representations that are beneficial to downstream tasks is challenging. Among a variety of methods, contrastive learning enjoys competitive performance. The embeddings of contrastive learning are arranged on a hypersphere that enables the Cosine distance measurement in the Euclidean space. However, the underlying structure of many domains such as graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a novel contrastive learning framework to learn high-quality graph embedding. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity which are related to properties of trees, whereas in the ambient space of the hyperbolic manifold, these notions translate into imposing an isotropic ring density towards boundaries of Poincar\\'e ball. This ring density can be easily imposed by promoting the isotropic feature distribution on the tangent space of manifold. In the experiments, we demonstrate the efficacy of our proposed method across different hyperbolic graph embedding techniques in both supervised and self-supervised learning settings.", "url": "https://arxiv.org/abs/2310.18209"}, {"metadata": {"arXiv": "2310.18264", "Date": "Fri, 27 Oct 2023 16:51:41 ", "Title": "Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt", "Authors": ["Yining Ma", "Zhiguang Cao", "Yeow Meng Chee"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at NeurIPS 2023"]}, "abstract": "In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search (L2S) solver for routing problems. It learns to perform flexible k-opt exchanges based on a tailored action factorization method and a customized recurrent dual-stream decoder. As a pioneering work to circumvent the pure feasibility masking scheme and enable the autonomous exploration of both feasible and infeasible regions, we then propose the Guided Infeasible Region Exploration (GIRE) scheme, which supplements the NeuOpt policy network with feasibility-related features and leverages reward shaping to steer reinforcement learning more effectively. Additionally, we equip NeuOpt with Dynamic Data Augmentation (D2A) for more diverse searches during inference. Extensive experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only significantly outstrips existing (masking-based) L2S solvers, but also showcases superiority over the learning-to-construct (L2C) and learning-to-predict (L2P) solvers. Notably, we offer fresh perspectives on how neural solvers can handle VRP constraints. Our code is available: https://github.com/yining043/NeuOpt.", "url": "https://arxiv.org/abs/2310.18264"}, {"metadata": {"arXiv": "2310.18304", "Date": "Fri, 27 Oct 2023 17:53:53 ", "Title": "A Stability Principle for Learning under Non-Stationarity", "Authors": ["Chengpiao Huang", "Kaizheng Wang"], "Categories": "cs.LG cs.AI math.OC stat.ML", "Comments": ["47 pages", "1 figure"], "MSC-class": "68T05, 90C15"}, "abstract": "We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.", "url": "https://arxiv.org/abs/2310.18304"}, {"metadata": {"arXiv": "2310.18308", "Date": "Fri, 27 Oct 2023 17:55:32 ", "Title": "Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models", "Authors": ["Pushkal Katara", "Zhou Xian", "Katerina Fragkiadaki"], "Categories": "cs.RO cs.AI cs.LG"}, "abstract": "Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task descriptions, temporal decompositions, and corresponding python reward functions for reinforcement learning. We show Gen2Sim succeeds in learning policies for diverse long horizon tasks, where reinforcement learning with non temporally decomposed reward functions fails. Gen2Sim provides a viable path for scaling up reinforcement learning for robot manipulators in simulation, both by diversifying and expanding task and environment development, and by facilitating the discovery of reinforcement-learned behaviors through temporal task decomposition in RL. Our work contributes hundreds of simulated assets, tasks and demonstrations, taking a step towards fully autonomous robotic manipulation skill acquisition in simulation.", "url": "https://arxiv.org/abs/2310.18308"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
