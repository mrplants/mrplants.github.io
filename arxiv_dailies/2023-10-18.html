<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #papers {
            width: 50%;
            margin: auto;
        }
        .paper {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-bottom: 10px;
        }
        .dropdown {
            cursor: pointer;
            font-weight: bold;
            color: #0056b3;
            text-decoration: underline;
        }
        .dropdown:hover {
            color: #007bff;
        }
        .abstract, .link {
            display: none;
            margin-left: 20px;
            margin-top: 10px;
        }
        .link a {
            color: #0056b3;
        }
        .link a:hover {
            color: #007bff;
        }
        .footer {
            text-align: center;
            padding: 10px;
            margin-top: 20px;
            font-size: 0.8em;
            color: #999;
        }
    </style>
</head>
<body>
    <div id="papers"></div>

    <div class="footer">Thank you to arXiv for use of its open access interoperability.</div>

    <script>
        var papers = [{"metadata": {"arXiv": "2310.11142", "Date": "Tue, 17 Oct 2023 10:45:28 ", "Title": "BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference", "Authors": ["Siqi Kou", "Lei Gan", "Dequan Wang", "Chongxuan Li", "Zhijie Deng"], "Categories": "cs.CV cs.LG"}, "abstract": "Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from diffusion models based on Bayesian inference. In particular, we derive a novel uncertainty iteration principle to characterize the uncertainty dynamics in diffusion, and leverage the last-layer Laplace approximation for efficient Bayesian inference. The estimated pixel-wise uncertainty can not only be aggregated into a sample-wise metric to filter out low-fidelity images but also aids in augmenting successful generations and rectifying artifacts in failed generations in text-to-image tasks. Extensive experiments demonstrate the efficacy of BayesDiff and its promise for practical applications.", "url": "https://arxiv.org/abs/2310.11142"}, {"metadata": {"arXiv": "2310.11449", "Date": "Tue, 17 Oct 2023 17:58:00 ", "Title": "DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis", "Authors": ["Youngjoong Kwon", "Lingjie Liu", "Henry Fuchs", "Marc Habermann", "Christian Theobalt"], "Categories": "cs.CV cs.GR cs.LG"}, "abstract": "Generating controllable and photorealistic digital human avatars is a long-standing and important problem in Vision and Graphics. Recent methods have shown great progress in terms of either photorealism or inference speed while the combination of the two desired properties still remains unsolved. To this end, we propose a novel method, called DELIFFAS, which parameterizes the appearance of the human as a surface light field that is attached to a controllable and deforming human mesh model. At the core, we represent the light field around the human with a deformable two-surface parameterization, which enables fast and accurate inference of the human appearance. This allows perceptual supervision on the full image compared to previous approaches that could only supervise individual pixels or small patches due to their slow runtime. Our carefully designed human representation and supervision strategy leads to state-of-the-art synthesis results and inference time. The video results and code are available at https://vcai.mpi-inf.mpg.de/projects/DELIFFAS.", "url": "https://arxiv.org/abs/2310.11449"}, {"metadata": {"arXiv": "2310.10691", "Date": "Sun, 15 Oct 2023 14:20:09 ", "Title": "Enhancing ML model accuracy for Digital VLSI circuits using diffusion models: A study on synthetic data generation", "Authors": ["Prasha Srivastava", "Pawan Kumar", "Zia Abbas"], "Categories": "cs.LG cs.AR", "Comments": ["7 pages", "submitted to NeurIPS workshop 2023"]}, "abstract": "Generative AI has seen remarkable growth over the past few years, with diffusion models being state-of-the-art for image generation. This study investigates the use of diffusion models in generating artificial data generation for electronic circuits for enhancing the accuracy of subsequent machine learning models in tasks such as performance assessment, design, and testing when training data is usually known to be very limited. We utilize simulations in the HSPICE design environment with 22nm CMOS technology nodes to obtain representative real training data for our proposed diffusion model. Our results demonstrate the close resemblance of synthetic data using diffusion model to real data. We validate the quality of generated data, and demonstrate that data augmentation certainly effective in predictive analysis of VLSI design for digital circuits.", "url": "https://arxiv.org/abs/2310.10691"}, {"metadata": {"arXiv": "2310.10702", "Date": "Mon, 16 Oct 2023 11:46:26 ", "Title": "Transparent Anomaly Detection via Concept-based Explanations", "Authors": ["Laya Rafiee Sevyeri", "Ivaxi Sheth", "Farhood Farahnak", "Shirin Abbasinejad Enger"], "Categories": "cs.LG"}, "abstract": "Advancements in deep learning techniques have given a boost to the performance of anomaly detection. However, real-world and safety-critical applications demand a level of transparency and reasoning beyond accuracy. The task of anomaly detection (AD) focuses on finding whether a given sample follows the learned distribution. Existing methods lack the ability to reason with clear explanations for their outcomes. Hence to overcome this challenge, we propose Transparent {A}nomaly Detection {C}oncept {E}xplanations (ACE). ACE is able to provide human interpretable explanations in the form of concepts along with anomaly prediction. To the best of our knowledge, this is the first paper that proposes interpretable by-design anomaly detection. In addition to promoting transparency in AD, it allows for effective human-model interaction. Our proposed model shows either higher or comparable results to black-box uninterpretable models. We validate the performance of ACE across three realistic datasets - bird classification on CUB-200-2011, challenging histopathology slide image classification on TIL-WSI-TCGA, and gender classification on CelebA. We further demonstrate that our concept learning paradigm can be seamlessly integrated with other classification-based AD methods.", "url": "https://arxiv.org/abs/2310.10702"}, {"metadata": {"arXiv": "2310.10744", "Date": "Mon, 16 Oct 2023 18:20:44 ", "Title": "Fast Adversarial Label-Flipping Attack on Tabular Data", "Authors": ["Xinglong Chang", "Gillian Dobbie", "J\\\"org Wicker"], "Categories": "cs.LG", "Comments": ["10 pages"]}, "abstract": "Machine learning models are increasingly used in fields that require high reliability such as cybersecurity. However, these models remain vulnerable to various attacks, among which the adversarial label-flipping attack poses significant threats. In label-flipping attacks, the adversary maliciously flips a portion of training labels to compromise the machine learning model. This paper raises significant concerns as these attacks can camouflage a highly skewed dataset as an easily solvable classification problem, often misleading machine learning practitioners into lower defenses and miscalculations of potential risks. This concern amplifies in tabular data settings, where identifying true labels requires expertise, allowing malicious label-flipping attacks to easily slip under the radar. To demonstrate this risk is inherited in the adversary's objective, we propose FALFA (Fast Adversarial Label-Flipping Attack), a novel efficient attack for crafting adversarial labels. FALFA is based on transforming the adversary's objective and employs linear programming to reduce computational complexity. Using ten real-world tabular datasets, we demonstrate FALFA's superior attack potential, highlighting the need for robust defenses against such threats.", "url": "https://arxiv.org/abs/2310.10744"}, {"metadata": {"arXiv": "2310.10745", "Date": "Mon, 16 Oct 2023 18:22:02 ", "Title": "Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder", "Authors": ["Priyam Gupta", "Peter J. Schmid", "Denis Sipp", "Taraneh Sayadi", "Georgios Rigas"], "Categories": "cs.LG math.DS physics.flu-dyn stat.ML", "Comments": ["16 pages", "9 figures"]}, "abstract": "The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and stability of the Koopman operator approximation. Demonstrations showcase the technique's ability to capture regime transitions in the flow around a circular cylinder. It also provided a low dimensional approximation for chaotic Kuramoto-Sivashinsky with promising short-term predictability and robust long-term statistical performance. By bridging the gap between data-driven techniques and the mathematical foundations of Koopman theory, MZ-AE offers a promising avenue for improved understanding and prediction of complex nonlinear dynamics.", "url": "https://arxiv.org/abs/2310.10745"}, {"metadata": {"arXiv": "2310.10762", "Date": "Mon, 16 Oct 2023 18:49:59 ", "Title": "Exploring hyperelastic material model discovery for human brain cortex: multivariate analysis vs. artificial neural network approaches", "Authors": ["Jixin Hou", "Nicholas Filla", "Xianyan Chen", "Mir Jalil Razavi", "Tianming Liu", "and Xianqiao Wang"], "Categories": "cs.LG", "Comments": ["42 pages", "12 figures"]}, "abstract": "Traditional computational methods, such as the finite element analysis, have provided valuable insights into uncovering the underlying mechanisms of brain physical behaviors. However, precise predictions of brain physics require effective constitutive models to represent the intricate mechanical properties of brain tissue. In this study, we aimed to identify the most favorable constitutive material model for human brain tissue. To achieve this, we applied artificial neural network and multiple regression methods to a generalization of widely accepted classic models, and compared the results obtained from these two approaches. To evaluate the applicability and efficacy of the model, all setups were kept consistent across both methods, except for the approach to prevent potential overfitting. Our results demonstrate that artificial neural networks are capable of automatically identifying accurate constitutive models from given admissible estimators. Nonetheless, the five-term and two-term neural network models trained under single-mode and multi-mode loading scenarios, were found to be suboptimal and could be further simplified into two-term and single-term, respectively, with higher accuracy using multiple regression. Our findings highlight the importance of hyperparameters for the artificial neural network and emphasize the necessity for detailed cross-validations of regularization parameters to ensure optimal selection at a global level in the development of material constitutive models. This study validates the applicability and accuracy of artificial neural network to automatically discover constitutive material models with proper regularization as well as the benefits in model simplification without compromising accuracy for traditional multivariable regression.", "url": "https://arxiv.org/abs/2310.10762"}, {"metadata": {"arXiv": "2310.10767", "Date": "Mon, 16 Oct 2023 19:00:43 ", "Title": "Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models", "Authors": ["Tianxiang Gao", "Xiaokai Huo", "Hailiang Liu", "Hongyang Gao"], "Categories": "cs.LG stat.ML", "Comments": ["Accepted by NeurIPS 2023"]}, "abstract": "Neural networks with wide layers have attracted significant attention due to their equivalence to Gaussian processes, enabling perfect fitting of training data while maintaining generalization performance, known as benign overfitting. However, existing results mainly focus on shallow or finite-depth networks, necessitating a comprehensive analysis of wide neural networks with infinite-depth layers, such as neural ordinary differential equations (ODEs) and deep equilibrium models (DEQs). In this paper, we specifically investigate the deep equilibrium model (DEQ), an infinite-depth neural network with shared weight matrices across layers. Our analysis reveals that as the width of DEQ layers approaches infinity, it converges to a Gaussian process, establishing what is known as the Neural Network and Gaussian Process (NNGP) correspondence. Remarkably, this convergence holds even when the limits of depth and width are interchanged, which is not observed in typical infinite-depth Multilayer Perceptron (MLP) networks. Furthermore, we demonstrate that the associated Gaussian vector remains non-degenerate for any pairwise distinct input data, ensuring a strictly positive smallest eigenvalue of the corresponding kernel matrix using the NNGP kernel. These findings serve as fundamental elements for studying the training and generalization of DEQs, laying the groundwork for future research in this area.", "url": "https://arxiv.org/abs/2310.10767"}, {"metadata": {"arXiv": "2310.10773", "Date": "Mon, 16 Oct 2023 19:12:56 ", "Title": "Gotta be SAFE: A New Framework for Molecular Design", "Authors": ["Emmanuel Noutahi", "Cristian Gabellini", "Michael Craig", "Jonathan S.C Lim", "Prudencio Tossou"], "Categories": "cs.LG q-bio.BM", "Comments": ["Submitted to a workshop at Neurips 2023"]}, "abstract": "Traditional molecular string representations, such as SMILES, often pose challenges for AI-driven molecular design due to their non-sequential depiction of molecular substructures. To address this issue, we introduce Sequential Attachment-based Fragment Embedding (SAFE), a novel line notation for chemical structures. SAFE reimagines SMILES strings as an unordered sequence of interconnected fragment blocks while maintaining full compatibility with existing SMILES parsers. It streamlines complex generative tasks, including scaffold decoration, fragment linking, polymer generation, and scaffold hopping, while facilitating autoregressive generation for fragment-constrained design, thereby eliminating the need for intricate decoding or graph-based models. We demonstrate the effectiveness of SAFE by training an 87-million-parameter GPT2-like model on a dataset containing 1.1 billion SAFE representations. Through extensive experimentation, we show that our SAFE-GPT model exhibits versatile and robust optimization performance. SAFE opens up new avenues for the rapid exploration of chemical space under various constraints, promising breakthroughs in AI-driven molecular design.", "url": "https://arxiv.org/abs/2310.10773"}, {"metadata": {"arXiv": "2310.10776", "Date": "Mon, 16 Oct 2023 19:25:52 ", "Title": "Correcting model misspecification in physics-informed neural networks (PINNs)", "Authors": ["Zongren Zou", "Xuhui Meng", "George Em Karniadakis"], "Categories": "cs.LG physics.comp-ph"}, "abstract": "Data-driven discovery of governing equations in computational science has emerged as a new paradigm for obtaining accurate physical models and as a possible alternative to theoretical derivations. The recently developed physics-informed neural networks (PINNs) have also been employed to learn governing equations given data across diverse scientific disciplines. Despite the effectiveness of PINNs for discovering governing equations, the physical models encoded in PINNs may be misspecified in complex systems as some of the physical processes may not be fully understood, leading to the poor accuracy of PINN predictions. In this work, we present a general approach to correct the misspecified physical models in PINNs for discovering governing equations, given some sparse and/or noisy data. Specifically, we first encode the assumed physical models, which may be misspecified, then employ other deep neural networks (DNNs) to model the discrepancy between the imperfect models and the observational data. Due to the expressivity of DNNs, the proposed method is capable of reducing the computational errors caused by the model misspecification and thus enables the applications of PINNs in complex systems where the physical processes are not exactly known. Furthermore, we utilize the Bayesian PINNs (B-PINNs) and/or ensemble PINNs to quantify uncertainties arising from noisy and/or gappy data in the discovered governing equations. A series of numerical examples including non-Newtonian channel and cavity flows demonstrate that the added DNNs are capable of correcting the model misspecification in PINNs and thus reduce the discrepancy between the physical models and the observational data. We envision that the proposed approach will extend the applications of PINNs for discovering governing equations in problems where the physico-chemical or biological processes are not well understood.", "url": "https://arxiv.org/abs/2310.10776"}, {"metadata": {"arXiv": "2310.10791", "Date": "Mon, 16 Oct 2023 19:54:21 ", "Title": "Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs", "Authors": ["Shervin Khalafi", "Saurabh Sihag", "Alejandro Ribeiro"], "Categories": "cs.LG stat.ML"}, "abstract": "Neural tangent kernels (NTKs) provide a theoretical regime to analyze the learning and generalization behavior of over-parametrized neural networks. For a supervised learning task, the association between the eigenvectors of the NTK kernel and given data (a concept referred to as alignment in this paper) can govern the rate of convergence of gradient descent, as well as generalization to unseen data. Building upon this concept, we investigate NTKs and alignment in the context of graph neural networks (GNNs), where our analysis reveals that optimizing alignment translates to optimizing the graph representation or the graph shift operator in a GNN. Our results further establish the theoretical guarantees on the optimality of the alignment for a two-layer GNN and these guarantees are characterized by the graph shift operator being a function of the cross-covariance between the input and the output data. The theoretical insights drawn from the analysis of NTKs are validated by our experiments focused on a multi-variate time series prediction task for a publicly available dataset. Specifically, they demonstrate that GNNs with cross-covariance as the graph shift operator indeed outperform those that operate on the covariance matrix from only the input data.", "url": "https://arxiv.org/abs/2310.10791"}, {"metadata": {"arXiv": "2310.10810", "Date": "Mon, 16 Oct 2023 20:14:06 ", "Title": "Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms", "Authors": ["Alexander Bukharin", "Yan Li", "Yue Yu", "Qingru Zhang", "Zhehui Chen", "Simiao Zuo", "Chao Zhang", "Songan Zhang", "and Tuo Zhao"], "Categories": "cs.LG", "Comments": ["33 pages", "10 figures"]}, "abstract": "Multi-Agent Reinforcement Learning (MARL) has shown promising results across several domains. Despite this promise, MARL policies often lack robustness and are therefore sensitive to small changes in their environment. This presents a serious concern for the real world deployment of MARL algorithms, where the testing environment may slightly differ from the training environment. In this work we show that we can gain robustness by controlling a policy's Lipschitz constant, and under mild conditions, establish the existence of a Lipschitz and close-to-optimal policy. Based on these insights, we propose a new robust MARL framework, ERNIE, that promotes the Lipschitz continuity of the policies with respect to the state observations and actions by adversarial regularization. The ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. However, ERNIE's adversarial regularization may introduce some training instability. To reduce this instability, we reformulate adversarial regularization as a Stackelberg game. We demonstrate the effectiveness of the proposed framework with extensive experiments in traffic light control and particle environments. In addition, we extend ERNIE to mean-field MARL with a formulation based on distributionally robust optimization that outperforms its non-robust counterpart and is of independent interest. Our code is available at https://github.com/abukharin3/ERNIE.", "url": "https://arxiv.org/abs/2310.10810"}, {"metadata": {"arXiv": "2310.10818", "Date": "Mon, 16 Oct 2023 20:37:36 ", "Title": "Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning", "Authors": ["Parvin Malekzadeh", "Ming Hou", "and Konstantinos N. Plataniotis"], "Categories": "cs.LG eess.SP", "Comments": ["40 pages"], "Journal-ref": "Neurocomputing 530 (2023): 165-187", "DOI": "10.1016/j.neucom.2023.01.076"}, "abstract": "Sample efficiency is central to developing practical reinforcement learning (RL) for complex and large-scale decision-making problems. The ability to transfer and generalize knowledge gained from previous experiences to downstream tasks can significantly improve sample efficiency. Recent research indicates that successor feature (SF) RL algorithms enable knowledge generalization between tasks with different rewards but identical transition dynamics. It has recently been hypothesized that combining model-based (MB) methods with SF algorithms can alleviate the limitation of fixed transition dynamics. Furthermore, uncertainty-aware exploration is widely recognized as another appealing approach for improving sample efficiency. Putting together two ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads to an approach to the problem of sample efficient uncertainty-aware knowledge transfer across tasks with different transition dynamics or/and reward functions. In this paper, the uncertainty of the value of each action is approximated by a Kalman filter (KF)-based multiple-model adaptive estimation. This KF-based framework treats the parameters of a model as random variables. To the best of our knowledge, this is the first attempt at formulating a hybrid MB-SF algorithm capable of generalizing knowledge across large or continuous state space tasks with various transition dynamics while requiring less computation at decision time than MB methods. The number of samples required to learn the tasks was compared to recent SF and MB baselines. The results show that our algorithm generalizes its knowledge across different transition dynamics, learns downstream tasks with significantly fewer samples than starting from scratch, and outperforms existing approaches.", "url": "https://arxiv.org/abs/2310.10818"}, {"metadata": {"arXiv": "2310.10836", "Date": "Mon, 16 Oct 2023 21:18:51 ", "Title": "Gaussian processes based data augmentation and expected signature for time series classification", "Authors": ["Marco Romito and Francesco Triggiano"], "Categories": "cs.LG"}, "abstract": "The signature is a fundamental object that describes paths (that is, continuous functions from an interval to a Euclidean space). Likewise, the expected signature provides a statistical description of the law of stochastic processes. We propose a feature extraction model for time series built upon the expected signature. This is computed through a Gaussian processes based data augmentation. One of the main features is that an optimal feature extraction is learnt through the supervised task that uses the model.", "url": "https://arxiv.org/abs/2310.10836"}, {"metadata": {"arXiv": "2310.10837", "Date": "Mon, 16 Oct 2023 21:23:16 ", "Title": "Approximating Two-Layer Feedforward Networks for Efficient Transformers", "Authors": ["R\\'obert Csord\\'as", "Kazuki Irie", "J\\\"urgen Schmidhuber"], "Categories": "cs.LG cs.NE", "Comments": ["Accepted to EMNLP 2023 Findings"]}, "abstract": "How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.", "url": "https://arxiv.org/abs/2310.10837"}, {"metadata": {"arXiv": "2310.10841", "Date": "Mon, 16 Oct 2023 21:35:23 ", "Title": "A Machine Learning-based Algorithm for Automated Detection of Frequency-based Events in Recorded Time Series of Sensor Data", "Authors": ["Bahareh Medghalchi", "Andreas Vogel"], "Categories": "cs.LG math.ST stat.TH"}, "abstract": "Automated event detection has emerged as one of the fundamental practices to monitor the behavior of technical systems by means of sensor data. In the automotive industry, these methods are in high demand for tracing events in time series data. For assessing the active vehicle safety systems, a diverse range of driving scenarios is conducted. These scenarios involve the recording of the vehicle's behavior using external sensors, enabling the evaluation of operational performance. In such setting, automated detection methods not only accelerate but also standardize and objectify the evaluation by avoiding subjective, human-based appraisals in the data inspection. This work proposes a novel event detection method that allows to identify frequency-based events in time series data. To this aim, the time series data is mapped to representations in the time-frequency domain, known as scalograms. After filtering scalograms to enhance relevant parts of the signal, an object detection model is trained to detect the desired event objects in the scalograms. For the analysis of unseen time series data, events can be detected in their scalograms with the trained object detection model and are thereafter mapped back to the time series data to mark the corresponding time interval. The algorithm, evaluated on unseen datasets, achieves a precision rate of 0.97 in event detection, providing sharp time interval boundaries whose accurate indication by human visual inspection is challenging. Incorporating this method into the vehicle development process enhances the accuracy and reliability of event detection, which holds major importance for rapid testing analysis.", "url": "https://arxiv.org/abs/2310.10841"}, {"metadata": {"arXiv": "2310.10874", "Date": "Mon, 16 Oct 2023 23:01:16 ", "Title": "Religious Affiliation in the Twenty-First Century: A Machine Learning Perspective on the World Value Survey", "Authors": ["Elaheh Jafarigol", "William Keely", "Tess Hartog", "Tom Welborn", "Peyman Hekmatpour", "Theodore B. Trafalis"], "Categories": "cs.LG cs.CY", "Journal-ref": "Society, pp.1-17 (2023)", "DOI": "10.1007/s12115-023-00887-0"}, "abstract": "This paper is a quantitative analysis of the data collected globally by the World Value Survey. The data is used to study the trajectories of change in individuals' religious beliefs, values, and behaviors in societies. Utilizing random forest, we aim to identify the key factors of religiosity and classify respondents of the survey as religious and non religious using country level data. We use resampling techniques to balance the data and improve imbalanced learning performance metrics. The results of the variable importance analysis suggest that Age and Income are the most important variables in the majority of countries. The results are discussed with fundamental sociological theories regarding religion and human behavior. This study is an application of machine learning in identifying the underlying patterns in the data of 30 countries participating in the World Value Survey. The results from variable importance analysis and classification of imbalanced data provide valuable insights beneficial to theoreticians and researchers of social sciences.", "url": "https://arxiv.org/abs/2310.10874"}, {"metadata": {"arXiv": "2310.10878", "Date": "Mon, 16 Oct 2023 23:13:51 ", "Title": "Eco-Driving Control of Connected and Automated Vehicles using Neural Network based Rollout", "Authors": ["Jacob Paugh", "Zhaoxuan Zhu", "Shobhit Gupta", "Marcello Canova", "Stephanie Stockar"], "Categories": "cs.LG"}, "abstract": "Connected and autonomous vehicles have the potential to minimize energy consumption by optimizing the vehicle velocity and powertrain dynamics with Vehicle-to-Everything info en route. Existing deterministic and stochastic methods created to solve the eco-driving problem generally suffer from high computational and memory requirements, which makes online implementation challenging. This work proposes a hierarchical multi-horizon optimization framework implemented via a neural network. The neural network learns a full-route value function to account for the variability in route information and is then used to approximate the terminal cost in a receding horizon optimization. Simulations over real-world routes demonstrate that the proposed approach achieves comparable performance to a stochastic optimization solution obtained via reinforcement learning, while requiring no sophisticated training paradigm and negligible on-board memory.", "url": "https://arxiv.org/abs/2310.10878"}, {"metadata": {"arXiv": "2310.10879", "Date": "Mon, 16 Oct 2023 23:14:56 ", "Title": "BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling", "Authors": ["Raphael Ruschel", "A. S. M. Iftekhar", "B. S. Manjunath", "Suya You"], "Categories": "cs.LG cs.DC"}, "abstract": "The increasing complexity of modern deep neural network models and the expanding sizes of datasets necessitate the development of optimized and scalable training methods. In this white paper, we addressed the challenge of efficiently training neural network models using sequences of varying sizes. To address this challenge, we propose a novel training scheme that enables efficient distributed data-parallel training on sequences of different sizes with minimal overhead. By using this scheme we were able to reduce the padding amount by more than 100$x$ while not deleting a single frame, resulting in an overall increased performance on both training time and Recall in our experiments.", "url": "https://arxiv.org/abs/2310.10879"}, {"metadata": {"arXiv": "2310.10909", "Date": "Tue, 17 Oct 2023 01:05:28 ", "Title": "Heterogenous Memory Augmented Neural Networks", "Authors": ["Zihan Qiu", "Zhen Liu", "Shuicheng Yan", "Shanghang Zhang", "Jie Fu"], "Categories": "cs.LG"}, "abstract": "It has been shown that semi-parametric methods, which combine standard neural networks with non-parametric components such as external memory modules and data retrieval, are particularly helpful in data scarcity and out-of-distribution (OOD) scenarios. However, existing semi-parametric methods mostly depend on independent raw data points - this strategy is difficult to scale up due to both high computational costs and the incapacity of current attention mechanisms with a large number of tokens. In this paper, we introduce a novel heterogeneous memory augmentation approach for neural networks which, by introducing learnable memory tokens with attention mechanism, can effectively boost performance without huge computational overhead. Our general-purpose method can be seamlessly combined with various backbones (MLP, CNN, GNN, and Transformer) in a plug-and-play manner. We extensively evaluate our approach on various image and graph-based tasks under both in-distribution (ID) and OOD conditions and show its competitive performance against task-specific state-of-the-art methods. Code is available at \\url{https://github.com/qiuzh20/HMA}.", "url": "https://arxiv.org/abs/2310.10909"}, {"metadata": {"arXiv": "2310.10910", "Date": "Tue, 17 Oct 2023 01:06:59 ", "Title": "Machine Learning in the Quantum Age: Quantum vs. Classical Support Vector Machines", "Authors": ["Davut Emre Tasar", "Kutan Koruyan", "Ceren Ocal Tasar"], "Categories": "cs.LG", "Comments": ["6 Pages", "in Turkish language"]}, "abstract": "This work endeavors to juxtapose the efficacy of machine learning algorithms within classical and quantum computational paradigms. Particularly, by emphasizing on Support Vector Machines (SVM), we scrutinize the classification prowess of classical SVM and Quantum Support Vector Machines (QSVM) operational on quantum hardware over the Iris dataset. The methodology embraced encapsulates an extensive array of experiments orchestrated through the Qiskit library, alongside hyperparameter optimization. The findings unveil that in particular scenarios, QSVMs extend a level of accuracy that can vie with classical SVMs, albeit the execution times are presently protracted. Moreover, we underscore that augmenting quantum computational capacity and the magnitude of parallelism can markedly ameliorate the performance of quantum machine learning algorithms. This inquiry furnishes invaluable insights regarding the extant scenario and future potentiality of machine learning applications in the quantum epoch. Colab: https://t.ly/QKuz0", "url": "https://arxiv.org/abs/2310.10910"}, {"metadata": {"arXiv": "2310.10946", "Date": "Tue, 17 Oct 2023 02:43:22 ", "Title": "Multi-point Feedback of Bandit Convex Optimization with Hard Constraints", "Authors": ["Yasunari Hikima"], "Categories": "cs.LG"}, "abstract": "This paper studies bandit convex optimization with constraints, where the learner aims to generate a sequence of decisions under partial information of loss functions such that the cumulative loss is reduced as well as the cumulative constraint violation is simultaneously reduced. We adopt the cumulative \\textit{hard} constraint violation as the metric of constraint violation, which is defined by $\\sum_{t=1}^{T} \\max\\{g_t(\\boldsymbol{x}_t), 0\\}$. Owing to the maximum operator, a strictly feasible solution cannot cancel out the effects of violated constraints compared to the conventional metric known as \\textit{long-term} constraints violation. We present a penalty-based proximal gradient descent method that attains a sub-linear growth of both regret and cumulative hard constraint violation, in which the gradient is estimated with a two-point function evaluation. Precisely, our algorithm attains $O(d^2T^{\\max\\{c,1-c\\}})$ regret bounds and $O(d^2T^{1-\\frac{c}{2}})$ cumulative hard constraint violation bounds for convex loss functions and time-varying constraints, where $d$ is the dimensionality of the feasible region and $c\\in[\\frac{1}{2}, 1)$ is a user-determined parameter. We also extend the result for the case where the loss functions are strongly convex and show that both regret and constraint violation bounds can be further reduced.", "url": "https://arxiv.org/abs/2310.10946"}, {"metadata": {"arXiv": "2310.10948", "Date": "Tue, 17 Oct 2023 02:46:04 ", "Title": "Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control", "Authors": ["Xianyue Peng", "Hang Gao", "Hao Wang", "H. Michael Zhang"], "Categories": "cs.LG cs.MA"}, "abstract": "Over the years, reinforcement learning has emerged as a popular approach to develop signal control and vehicle platooning strategies either independently or in a hierarchical way. However, jointly controlling both in real-time to alleviate traffic congestion presents new challenges, such as the inherent physical and behavioral heterogeneity between signal control and platooning, as well as coordination between them. This paper proposes an innovative solution to tackle these challenges based on heterogeneous graph multi-agent reinforcement learning and traffic theories. Our approach involves: 1) designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow; 2) designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale. We evaluate our approach through SUMO simulation, which shows a convergent result in terms of various transportation metrics and better performance over sole signal or platooning control.", "url": "https://arxiv.org/abs/2310.10948"}, {"metadata": {"arXiv": "2310.10953", "Date": "Tue, 17 Oct 2023 02:58:49 ", "Title": "A Local Graph Limits Perspective on Sampling-Based GNNs", "Authors": ["Yeganeh Alimohammadi", "Luana Ruiz", "Amin Saberi"], "Categories": "cs.LG stat.ML"}, "abstract": "We propose a theoretical framework for training Graph Neural Networks (GNNs) on large input graphs via training on small, fixed-size sampled subgraphs. This framework is applicable to a wide range of models, including popular sampling-based GNNs, such as GraphSAGE and FastGCN. Leveraging the theory of graph local limits, we prove that, under mild assumptions, parameters learned from training sampling-based GNNs on small samples of a large input graph are within an $\\epsilon$-neighborhood of the outcome of training the same architecture on the whole graph. We derive bounds on the number of samples, the size of the graph, and the training steps required as a function of $\\epsilon$. Our results give a novel theoretical understanding for using sampling in training GNNs. They also suggest that by training GNNs on small samples of the input graph, practitioners can identify and select the best models, hyperparameters, and sampling algorithms more efficiently. We empirically illustrate our results on a node classification task on large citation graphs, observing that sampling-based GNNs trained on local subgraphs 12$\\times$ smaller than the original graph achieve comparable performance to those trained on the input graph.", "url": "https://arxiv.org/abs/2310.10953"}, {"metadata": {"arXiv": "2310.10958", "Date": "Tue, 17 Oct 2023 03:11:30 ", "Title": "Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction", "Authors": ["Hejie Ying", "Mengmeng Song", "Yaohong Tang", "Shungen Xiao", "Zimin Xiao"], "Categories": "cs.LG cs.CV"}, "abstract": "Deep neural networks (DNN) have achieved remarkable success in various fields, including computer vision and natural language processing. However, training an effective DNN model still poses challenges. This paper aims to propose a method to optimize the training effectiveness of DNN, with the goal of improving model performance. Firstly, based on the observation that the DNN parameters change in certain laws during training process, the potential of parameter prediction for improving model training efficiency and performance is discovered. Secondly, considering the magnitude of DNN model parameters, hardware limitations and characteristics of Stochastic Gradient Descent (SGD) for noise tolerance, a Parameter Linear Prediction (PLP) method is exploit to perform DNN parameter prediction. Finally, validations are carried out on some representative backbones. Experiment results show that compare to the normal training ways, under the same training conditions and epochs, by employing proposed PLP method, the optimal model is able to obtain average about 1% accuracy improvement and 0.01 top-1/top-5 error reduction for Vgg16, Resnet18 and GoogLeNet based on CIFAR-100 dataset, which shown the effectiveness of the proposed method on different DNN structures, and validated its capacity in enhancing DNN training efficiency and performance.", "url": "https://arxiv.org/abs/2310.10958"}, {"metadata": {"arXiv": "2310.10970", "Date": "Tue, 17 Oct 2023 03:31:47 ", "Title": "SD-PINN: Deep Learning based Spatially Dependent PDEs Recovery", "Authors": ["Ruixian Liu", "Peter Gerstoft"], "Categories": "cs.LG eess.SP", "Comments": ["11 pages", "15 figures"]}, "abstract": "The physics-informed neural network (PINN) is capable of recovering partial differential equation (PDE) coefficients that remain constant throughout the spatial domain directly from physical measurements. In this work, we propose a spatially dependent physics-informed neural network (SD-PINN), which enables the recovery of coefficients in spatially-dependent PDEs using a single neural network, eliminating the requirement for domain-specific physical expertise. The proposed method exhibits robustness to noise owing to the incorporation of physical constraints. It can also incorporate the low-rank assumption of the spatial variation for the PDE coefficients to recover the coefficients at locations without available measurements.", "url": "https://arxiv.org/abs/2310.10970"}, {"metadata": {"arXiv": "2310.10971", "Date": "Tue, 17 Oct 2023 03:35:27 ", "Title": "Context-Aware Meta-Learning", "Authors": ["Christopher Fifty", "Dennis Duan", "Ronald G. Junkins", "Ehsan Amid", "Jure Leskovec", "Christopher R\\'e", "Sebastian Thrun"], "Categories": "cs.LG cs.CV"}, "abstract": "Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.", "url": "https://arxiv.org/abs/2310.10971"}, {"metadata": {"arXiv": "2310.10987", "Date": "Tue, 17 Oct 2023 04:20:00 ", "Title": "Why Do Students Drop Out? University Dropout Prediction and Associated Factor Analysis Using Machine Learning Techniques", "Authors": ["Sean Kim and Eliot Yoo and Samuel Kim"], "Categories": "cs.LG cs.CY"}, "abstract": "Graduation and dropout rates have always been a serious consideration for educational institutions and students. High dropout rates negatively impact both the lives of individual students and institutions. To address this problem, this study examined university dropout prediction using academic, demographic, socioeconomic, and macroeconomic data types. Additionally, we performed associated factor analysis to analyze which type of data would be most influential on the performance of machine learning models in predicting graduation and dropout status. These features were used to train four binary classifiers to determine if students would graduate or drop out. The overall performance of the classifiers in predicting dropout status had an average ROC-AUC score of 0.935. The data type most influential to the model performance was found to be academic data, with the average ROC-AUC score dropping from 0.935 to 0.811 when excluding all academic-related features from the data set. Preliminary results indicate that a correlation does exist between data types and dropout status.", "url": "https://arxiv.org/abs/2310.10987"}, {"metadata": {"arXiv": "2310.11001", "Date": "Tue, 17 Oct 2023 05:04:53 ", "Title": "Spatially-resolved hyperlocal weather prediction and anomaly detection using IoT sensor networks and machine learning techniques", "Authors": ["Anita B. Agarwal", "Rohit Rajesh", "Nitin Arul"], "Categories": "cs.LG", "Comments": ["Submitted to IEEE Modelling Simulation & Intelligent Computing", "2023"]}, "abstract": "Accurate and timely hyperlocal weather predictions are essential for various applications, ranging from agriculture to disaster management. In this paper, we propose a novel approach that combines hyperlocal weather prediction and anomaly detection using IoT sensor networks and advanced machine learning techniques. Our approach leverages data from multiple spatially-distributed yet relatively close locations and IoT sensors to create high-resolution weather models capable of predicting short-term, localized weather conditions such as temperature, pressure, and humidity. By monitoring changes in weather parameters across these locations, our system is able to enhance the spatial resolution of predictions and effectively detect anomalies in real-time. Additionally, our system employs unsupervised learning algorithms to identify unusual weather patterns, providing timely alerts. Our findings indicate that this system has the potential to enhance decision-making.", "url": "https://arxiv.org/abs/2310.11001"}, {"metadata": {"arXiv": "2310.11009", "Date": "Tue, 17 Oct 2023 05:36:46 ", "Title": "Adaptive Pairwise Encodings for Link Prediction", "Authors": ["Harry Shomer", "Yao Ma", "Haitao Ma", "Juanhui Li", "Bo Wu", "Jiliang Tang"], "Categories": "cs.LG"}, "abstract": "Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a \"pairwise encoding\" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a new method, LPFormer, which attempts to adaptively learn the pairwise encodings for each link. LPFormer models the link factors via an attention module that learns the pairwise encoding that exists between nodes by modeling multiple factors integral to link prediction. Extensive experiments demonstrate that LPFormer can achieve SOTA performance on numerous datasets while maintaining efficiency.", "url": "https://arxiv.org/abs/2310.11009"}, {"metadata": {"arXiv": "2310.11015", "Date": "Tue, 17 Oct 2023 06:04:00 ", "Title": "Pure Exploration in Asynchronous Federated Bandits", "Authors": ["Zichen Wang", "Chuanhao Li", "Chenyu Song", "Lianghui Wang", "Quanquan Gu", "Huazheng Wang"], "Categories": "cs.LG"}, "abstract": "We study the federated pure exploration problem of multi-armed bandits and linear bandits, where $M$ agents cooperatively identify the best arm via communicating with the central server. To enhance the robustness against latency and unavailability of agents that are common in practice, we propose the first federated asynchronous multi-armed bandit and linear bandit algorithms for pure exploration with fixed confidence. Our theoretical analysis shows the proposed algorithms achieve near-optimal sample complexities and efficient communication costs in a fully asynchronous environment. Moreover, experimental results based on synthetic and real-world data empirically elucidate the effectiveness and communication cost-efficiency of the proposed algorithms.", "url": "https://arxiv.org/abs/2310.11015"}, {"metadata": {"arXiv": "2310.11025", "Date": "Tue, 17 Oct 2023 06:42:11 ", "Title": "SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning", "Authors": ["Jinsong Chen", "Gaichao Li", "John E. Hopcroft", "Kun He"], "Categories": "cs.LG cs.SI", "Comments": ["Submitted to a conference"]}, "abstract": "The emerging graph Transformers have achieved impressive performance for graph representation learning over graph neural networks (GNNs). In this work, we regard the self-attention mechanism, the core module of graph Transformers, as a two-step aggregation operation on a fully connected graph. Due to the property of generating positive attention values, the self-attention mechanism is equal to conducting a smooth operation on all nodes, preserving the low-frequency information. However, only capturing the low-frequency information is inefficient in learning complex relations of nodes on diverse graphs, such as heterophily graphs where the high-frequency information is crucial. To this end, we propose a Signed Attention-based Graph Transformer (SignGT) to adaptively capture various frequency information from the graphs. Specifically, SignGT develops a new signed self-attention mechanism (SignSA) that produces signed attention values according to the semantic relevance of node pairs. Hence, the diverse frequency information between different node pairs could be carefully preserved. Besides, SignGT proposes a structure-aware feed-forward network (SFFN) that introduces the neighborhood bias to preserve the local topology information. In this way, SignGT could learn informative node representations from both long-range dependencies and local topology information. Extensive empirical results on both node-level and graph-level tasks indicate the superiority of SignGT against state-of-the-art graph Transformers as well as advanced GNNs.", "url": "https://arxiv.org/abs/2310.11025"}, {"metadata": {"arXiv": "2310.11028", "Date": "Tue, 17 Oct 2023 06:56:57 ", "Title": "Matrix Compression via Randomized Low Rank and Low Precision Factorization", "Authors": ["Rajarshi Saha", "Varun Srivastava", "Mert Pilanci"], "Categories": "cs.LG cs.IT math.IT stat.ML", "Comments": ["Accepted to the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)"]}, "abstract": "Matrices are exceptionally useful in various fields of study as they provide a convenient framework to organize and manipulate data in a structured manner. However, modern matrices can involve billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Although prohibitively large, such matrices are often approximately low rank. We propose an algorithm that exploits this structure to obtain a low rank decomposition of any matrix $\\mathbf{A}$ as $\\mathbf{A} \\approx \\mathbf{L}\\mathbf{R}$, where $\\mathbf{L}$ and $\\mathbf{R}$ are the low rank factors. The total number of elements in $\\mathbf{L}$ and $\\mathbf{R}$ can be significantly less than that in $\\mathbf{A}$. Furthermore, the entries of $\\mathbf{L}$ and $\\mathbf{R}$ are quantized to low precision formats $--$ compressing $\\mathbf{A}$ by giving us a low rank and low precision factorization. Our algorithm first computes an approximate basis of the range space of $\\mathbf{A}$ by randomly sketching its columns, followed by a quantization of the vectors constituting this basis. It then computes approximate projections of the columns of $\\mathbf{A}$ onto this quantized basis. We derive upper bounds on the approximation error of our algorithm, and analyze the impact of target rank and quantization bit-budget. The tradeoff between compression ratio and approximation accuracy allows for flexibility in choosing these parameters based on specific application requirements. We empirically demonstrate the efficacy of our algorithm in image compression, nearest neighbor classification of image and text embeddings, and compressing the layers of LlaMa-$7$b. Our results illustrate that we can achieve compression ratios as aggressive as one bit per matrix coordinate, all while surpassing or maintaining the performance of traditional compression techniques.", "url": "https://arxiv.org/abs/2310.11028"}, {"metadata": {"arXiv": "2310.11059", "Date": "Tue, 17 Oct 2023 08:04:45 ", "Title": "Causal Feature Selection via Transfer Entropy", "Authors": ["Paolo Bonetti", "Alberto Maria Metelli", "Marcello Restelli"], "Categories": "cs.LG"}, "abstract": "Machine learning algorithms are designed to capture complex relationships between features. In this context, the high dimensionality of data often results in poor model performance, with the risk of overfitting. Feature selection, the process of selecting a subset of relevant and non-redundant features, is, therefore, an essential step to mitigate these issues. However, classical feature selection approaches do not inspect the causal relationship between selected features and target, which can lead to misleading results in real-world applications. Causal discovery, instead, aims to identify causal relationships between features with observational data. In this paper, we propose a novel methodology at the intersection between feature selection and causal discovery, focusing on time series. We introduce a new causal feature selection approach that relies on the forward and backward feature selection procedures and leverages transfer entropy to estimate the causal flow of information from the features to the target in time series. Our approach enables the selection of features not only in terms of mere model performance but also captures the causal information flow. In this context, we provide theoretical guarantees on the regression and classification errors for both the exact and the finite-sample cases. Finally, we present numerical validations on synthetic and real-world regression problems, showing results competitive w.r.t. the considered baselines.", "url": "https://arxiv.org/abs/2310.11059"}, {"metadata": {"arXiv": "2310.11077", "Date": "Tue, 17 Oct 2023 08:51:44 ", "Title": "United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit", "Authors": ["Uri Stern", "Daniel Shwartz", "Daphna Weinshall"], "Categories": "cs.LG cs.CV"}, "abstract": "Deep neural networks have become the method of choice for solving many image classification tasks, largely because they can fit very complex functions defined over raw images. The downside of such powerful learners is the danger of overfitting the training set, leading to poor generalization, which is usually avoided by regularization and \"early stopping\" of the training. In this paper, we propose a new deep network ensemble classifier that is very effective against overfit. We begin with the theoretical analysis of a regression model, whose predictions - that the variance among classifiers increases when overfit occurs - is demonstrated empirically in deep networks in common use. Guided by these results, we construct a new ensemble-based prediction method designed to combat overfit, where the prediction is determined by the most consensual prediction throughout the training. On multiple image and text classification datasets, we show that when regular ensembles suffer from overfit, our method eliminates the harmful reduction in generalization due to overfit, and often even surpasses the performance obtained by early stopping. Our method is easy to implement, and can be integrated with any training scheme and architecture, without additional prior knowledge beyond the training set. Accordingly, it is a practical and useful tool to overcome overfit.", "url": "https://arxiv.org/abs/2310.11077"}, {"metadata": {"arXiv": "2310.11083", "Date": "Tue, 17 Oct 2023 09:08:33 ", "Title": "CSG: Curriculum Representation Learning for Signed Graph", "Authors": ["Zeyu Zhang", "Jiamou Liu", "Kaiqi Zhao", "Yifei Wang", "Pengqian Han", "Xianda Zheng", "Qiqi Wang", "Zijian Zhang"], "Categories": "cs.LG"}, "abstract": "Signed graphs are valuable for modeling complex relationships with positive and negative connections, and Signed Graph Neural Networks (SGNNs) have become crucial tools for their analysis. However, prior to our work, no specific training plan existed for SGNNs, and the conventional random sampling approach did not address varying learning difficulties within the graph's structure. We proposed a curriculum-based training approach, where samples progress from easy to complex, inspired by human learning. To measure learning difficulty, we introduced a lightweight mechanism and created the Curriculum representation learning framework for Signed Graphs (CSG). This framework optimizes the order in which samples are presented to the SGNN model. Empirical validation across six real-world datasets showed impressive results, enhancing SGNN model accuracy by up to 23.7% in link sign prediction (AUC) and significantly improving stability with an up to 8.4 reduction in the standard deviation of AUC scores.", "url": "https://arxiv.org/abs/2310.11083"}, {"metadata": {"arXiv": "2310.11093", "Date": "Tue, 17 Oct 2023 09:22:20 ", "Title": "SODA: Robust Training of Test-Time Data Adaptors", "Authors": ["Zige Wang", "Yonggang Zhang", "Zhen Fang", "Long Lan", "Wenjing Yang", "Bo Han"], "Categories": "cs.LG cs.CV"}, "abstract": "Adapting models deployed to test distributions can mitigate the performance degradation caused by distribution shifts. However, privacy concerns may render model parameters inaccessible. One promising approach involves utilizing zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. Nevertheless, the data adaptor trained with ZOO typically brings restricted improvements due to the potential corruption of data features caused by the data adaptor. To address this issue, we revisit ZOO in the context of test-time data adaptation. We find that the issue directly stems from the unreliable estimation of the gradients used to optimize the data adaptor, which is inherently due to the unreliable nature of the pseudo-labels assigned to the test data. Based on this observation, we propose pseudo-label-robust data adaptation (SODA) to improve the performance of data adaptation. Specifically, SODA leverages high-confidence predicted labels as reliable labels to optimize the data adaptor with ZOO for label prediction. For data with low-confidence predictions, SODA encourages the adaptor to preserve data information to mitigate data corruption. Empirical results indicate that SODA can significantly enhance the performance of deployed models in the presence of distribution shifts without requiring access to model parameters.", "url": "https://arxiv.org/abs/2310.11093"}, {"metadata": {"arXiv": "2310.11094", "Date": "Tue, 17 Oct 2023 09:22:22 ", "Title": "Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs", "Authors": ["Uri Stern", "Daphna Weinshall"], "Categories": "cs.LG"}, "abstract": "The infrequent occurrence of overfit in deep neural networks is perplexing. On the one hand, theory predicts that as models get larger they should eventually become too specialized for a specific training set, with ensuing decrease in generalization. In contrast, empirical results in image classification indicate that increasing the training time of deep models or using bigger models almost never hurts generalization. Is it because the way we measure overfit is too limited? Here, we introduce a novel score for quantifying overfit, which monitors the forgetting rate of deep models on validation data. Presumably, this score indicates that even while generalization improves overall, there are certain regions of the data space where it deteriorates. When thus measured, we show that overfit can occur with and without a decrease in validation accuracy, and may be more common than previously appreciated. This observation may help to clarify the aforementioned confusing picture. We use our observations to construct a new ensemble method, based solely on the training history of a single network, which provides significant improvement in performance without any additional cost in training time. An extensive empirical evaluation with modern deep models shows our method's utility on multiple datasets, neural networks architectures and training schemes, both when training from scratch and when using pre-trained networks in transfer learning. Notably, our method outperforms comparable methods while being easier to implement and use, and further improves the performance of competitive networks on Imagenet by 1\\%.", "url": "https://arxiv.org/abs/2310.11094"}, {"metadata": {"arXiv": "2310.11110", "Date": "Tue, 17 Oct 2023 09:50:31 ", "Title": "Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data", "Authors": ["Nicolas Heintz", "Tom Francart", "Alexander Bertrand"], "Categories": "cs.LG eess.SP stat.ML", "Comments": ["13 pages", "7 figures"]}, "abstract": "Linear Discriminant Analysis (LDA) is one of the oldest and most popular linear methods for supervised classification problems. In this paper, we demonstrate that it is possible to compute the exact projection vector from LDA models based on unlabelled data, if some minimal prior information is available. More precisely, we show that only one of the following three pieces of information is actually sufficient to compute the LDA projection vector if only unlabelled data are available: (1) the class average of one of the two classes, (2) the difference between both class averages (up to a scaling), or (3) the class covariance matrices (up to a scaling). These theoretical results are validated in numerical experiments, demonstrating that this minimally informed Linear Discriminant Analysis (MILDA) model closely matches the performance of a supervised LDA model. Furthermore, we show that the MILDA projection vector can be computed in a closed form with a computational cost comparable to LDA and is able to quickly adapt to non-stationary data, making it well-suited to use as an adaptive classifier.", "url": "https://arxiv.org/abs/2310.11110"}, {"metadata": {"arXiv": "2310.11130", "Date": "Tue, 17 Oct 2023 10:28:00 ", "Title": "Topological Expressivity of ReLU Neural Networks", "Authors": ["Ekin Ergen", "Moritz Grillo"], "Categories": "cs.LG"}, "abstract": "We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplification. This provides a mathematically rigorous explanation why deeper networks are better equipped to handle complex and topologically rich datasets.", "url": "https://arxiv.org/abs/2310.11130"}, {"metadata": {"arXiv": "2310.11131", "Date": "Tue, 17 Oct 2023 10:28:28 ", "Title": "FROST: Towards Energy-efficient AI-on-5G Platforms -- A GPU Power Capping Evaluation", "Authors": ["Ioannis Mavromatis and Stefano De Feo and Pietro Carnelli and Robert J. Piechocki and Aftab Khan"], "Categories": "cs.LG cs.NI", "Comments": ["IEEE CSCN 2023", "Munich", "Germany"]}, "abstract": "The Open Radio Access Network (O-RAN) is a burgeoning market with projected growth in the upcoming years. RAN has the highest CAPEX impact on the network and, most importantly, consumes 73% of its total energy. That makes it an ideal target for optimisation through the integration of Machine Learning (ML). However, the energy consumption of ML is frequently overlooked in such ecosystems. Our work addresses this critical aspect by presenting FROST - Flexible Reconfiguration method with Online System Tuning - a solution for energy-aware ML pipelines that adhere to O-RAN's specifications and principles. FROST is capable of profiling the energy consumption of an ML pipeline and optimising the hardware accordingly, thereby limiting the power draw. Our findings indicate that FROST can achieve energy savings of up to 26.4% without compromising the model's accuracy or introducing significant time delays.", "url": "https://arxiv.org/abs/2310.11131"}, {"metadata": {"arXiv": "2310.11132", "Date": "Tue, 17 Oct 2023 10:29:23 ", "Title": "Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation", "Authors": ["Oana-Iuliana Popescu", "Andreas Gerhardus", "Jakob Runge"], "Categories": "cs.LG"}, "abstract": "Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables are numerical or all variables are categorical, many real-world applications involve mixed-type datasets that include numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these estimators and propose a variation of the former approach that does not treat categorical variables as numeric. Our numerical experiments show that our variant detects dependencies more robustly across different data distributions and preprocessing types.", "url": "https://arxiv.org/abs/2310.11132"}, {"metadata": {"arXiv": "2310.11138", "Date": "Tue, 17 Oct 2023 10:40:05 ", "Title": "Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control", "Authors": ["Chao Li", "Chen Gong", "Qiang He", "Xinwen Hou"], "Categories": "cs.LG"}, "abstract": "The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \\textbf{T}rajectories-awar\\textbf{E} \\textbf{E}nsemble exploratio\\textbf{N} (TEEN). The primary goal of TEEN is to maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble policy compared to using sub-policies alone but also improves the performance over ensemble RL algorithms. On average, TEEN outperforms the baseline ensemble DRL algorithms by 41\\% in performance on the tested representative environments.", "url": "https://arxiv.org/abs/2310.11138"}, {"metadata": {"arXiv": "2310.11186", "Date": "Tue, 17 Oct 2023 12:07:14 ", "Title": "Efficiently Visualizing Large Graphs", "Authors": ["Xinyu Li", "Yao Xiao", "Yuchen Zhou"], "Categories": "cs.LG"}, "abstract": "Most existing graph visualization methods based on dimension reduction are limited to relatively small graphs due to performance issues. In this work, we propose a novel dimension reduction method for graph visualization, called t-Distributed Stochastic Graph Neighbor Embedding (t-SGNE). t-SGNE is specifically designed to visualize cluster structures in the graph. As a variant of the standard t-SNE method, t-SGNE avoids the time-consuming computations of pairwise similarity. Instead, it uses the neighbor structures of the graph to reduce the time complexity from quadratic to linear, thus supporting larger graphs. In addition, to suit t-SGNE, we combined Laplacian Eigenmaps with the shortest path algorithm in graphs to form the graph embedding algorithm ShortestPath Laplacian Eigenmaps Embedding (SPLEE). Performing SPLEE to obtain a high-dimensional embedding of the large-scale graph and then using t-SGNE to reduce its dimension for visualization, we are able to visualize graphs with up to 300K nodes and 1M edges within 5 minutes and achieve approximately 10% improvement in visualization quality. Codes and data are available at https://github.com/Charlie-XIAO/embedding-visualization-test.", "url": "https://arxiv.org/abs/2310.11186"}, {"metadata": {"arXiv": "2310.11188", "Date": "Tue, 17 Oct 2023 12:08:15 ", "Title": "A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback", "Authors": ["Yandi Li", "Jianxiong Guo"], "Categories": "cs.LG", "Comments": ["This is an extended version of \"A Modified EXP3 in Adversarial Bandits with Multi-User Delayed Feedback\" published in COCOON 2023"]}, "abstract": "For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\\mathcal{O}(\\sqrt{TM^2\\ln{N}(N\\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algorithm named AMUD-EXP3 is proposed with a sublinear regret with respect to $T$. Finally, extensive experiments are conducted to indicate the correctness and effectiveness of our algorithms.", "url": "https://arxiv.org/abs/2310.11188"}, {"metadata": {"arXiv": "2310.11203", "Date": "Tue, 17 Oct 2023 12:29:29 ", "Title": "Federated Learning with Nonvacuous Generalisation Bounds", "Authors": ["Pierre Jobic and Maxime Haddouche and Benjamin Guedj"], "Categories": "cs.LG stat.ML"}, "abstract": "We introduce a novel strategy to train randomised predictors in federated learning, where each node of the network aims at preserving its privacy by releasing a local predictor but keeping secret its training dataset with respect to the other nodes. We then build a global randomised predictor which inherits the properties of the local private predictors in the sense of a PAC-Bayesian generalisation bound. We consider the synchronous case where all nodes share the same training objective (derived from a generalisation bound), and the asynchronous case where each node may have its own personalised training objective. We show through a series of numerical experiments that our approach achieves a comparable predictive performance to that of the batch approach where all datasets are shared across nodes. Moreover the predictors are supported by numerically nonvacuous generalisation bounds while preserving privacy for each node. We explicitly compute the increment on predictive performance and generalisation bounds between batch and federated settings, highlighting the price to pay to preserve privacy.", "url": "https://arxiv.org/abs/2310.11203"}, {"metadata": {"arXiv": "2310.11232", "Date": "Tue, 17 Oct 2023 13:03:49 ", "Title": "Learning to Sample Better", "Authors": ["Michael S. Albergo and Eric Vanden-Eijnden"], "Categories": "cs.LG stat.ML", "Comments": ["Les Houches 2022 Summer School on Statistical Physics and Machine Learning"]}, "abstract": "These lecture notes provide an introduction to recent advances in generative modeling methods based on the dynamical transportation of measures, by means of which samples from a simple base measure are mapped to samples from a target measure of interest. Special emphasis is put on the applications of these methods to Monte-Carlo (MC) sampling techniques, such as importance sampling and Markov Chain Monte-Carlo (MCMC) schemes. In this context, it is shown how the maps can be learned variationally using data generated by MC sampling, and how they can in turn be used to improve such sampling in a positive feedback loop.", "url": "https://arxiv.org/abs/2310.11232"}, {"metadata": {"arXiv": "2310.11248", "Date": "Tue, 17 Oct 2023 13:18:01 ", "Title": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion", "Authors": ["Yangruibo Ding", "Zijian Wang", "Wasi Uddin Ahmad", "Hantian Ding", "Ming Tan", "Nihal Jain", "Murali Krishna Ramanathan", "Ramesh Nallapati", "Parminder Bhatia", "Dan Roth", "Bing Xiang"], "Categories": "cs.LG cs.CL cs.SE", "Comments": ["To appear at NeurIPS 2023 (Datasets and Benchmarks Track)"]}, "abstract": "Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly. To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file. Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.", "url": "https://arxiv.org/abs/2310.11248"}, {"metadata": {"arXiv": "2310.11281", "Date": "Tue, 17 Oct 2023 14:04:22 ", "Title": "Self-supervision meets kernel graph neural models: From architecture to augmentations", "Authors": ["Jiawang Dan", "Ruofan Wu", "Yunpeng Liu", "Baokun Wang", "Changhua Meng", "Tengfei Liu", "Tianyi Zhang", "Ningtao Wang", "Xing Fu", "Qi Li", "Weiqiang Wang"], "Categories": "cs.LG"}, "abstract": "Graph representation learning has now become the de facto standard when handling graph-structured data, with the framework of message-passing graph neural networks (MPNN) being the most prevailing algorithmic tool. Despite its popularity, the family of MPNNs suffers from several drawbacks such as transparency and expressivity. Recently, the idea of designing neural models on graphs using the theory of graph kernels has emerged as a more transparent as well as sometimes more expressive alternative to MPNNs known as kernel graph neural networks (KGNNs). Developments on KGNNs are currently a nascent field of research, leaving several challenges from algorithmic design and adaptation to other learning paradigms such as self-supervised learning. In this paper, we improve the design and learning of KGNNs. Firstly, we extend the algorithmic formulation of KGNNs by allowing a more flexible graph-level similarity definition that encompasses former proposals like random walk graph kernel, as well as providing a smoother optimization objective that alleviates the need of introducing combinatorial learning procedures. Secondly, we enhance KGNNs through the lens of self-supervision via developing a novel structure-preserving graph data augmentation method called latent graph augmentation (LGA). Finally, we perform extensive empirical evaluations to demonstrate the efficacy of our proposed mechanisms. Experimental results over benchmark datasets suggest that our proposed model achieves competitive performance that is comparable to or sometimes outperforming state-of-the-art graph representation learning frameworks with or without self-supervision on graph classification tasks. Comparisons against other previously established graph data augmentation methods verify that the proposed LGA augmentation scheme captures better semantics of graph-level invariance.", "url": "https://arxiv.org/abs/2310.11281"}, {"metadata": {"arXiv": "2310.11287", "Date": "Tue, 17 Oct 2023 14:09:45 ", "Title": "Evaluating the Impact of Humanitarian Aid on Food Security", "Authors": ["Jordi Cerd\\`a-Bautista", "Jos\\'e Mar\\'ia T\\'arraga", "Vasileios Sitokonstantinou and Gustau Camps-Valls"], "Categories": "cs.LG", "Comments": ["3 pages", "1 figure", "3 tables"]}, "abstract": "In the face of climate change-induced droughts, vulnerable regions encounter severe threats to food security, demanding urgent humanitarian assistance. This paper introduces a causal inference framework for the Horn of Africa, aiming to assess the impact of cash-based interventions on food crises. Our contributions encompass identifying causal relationships within the food security system, harmonizing a comprehensive database, and estimating the causal effect of humanitarian interventions on malnutrition. Our results revealed no significant effects, likely due to limited sample size, suboptimal data quality, and an imperfect causal graph resulting from our limited understanding of multidisciplinary systems like food security. This underscores the need to enhance data collection and refine causal models with domain experts for more effective future interventions and policies, improving transparency and accountability in humanitarian aid.", "url": "https://arxiv.org/abs/2310.11287"}, {"metadata": {"arXiv": "2310.11291", "Date": "Tue, 17 Oct 2023 14:15:57 ", "Title": "An Automatic Learning Rate Schedule Algorithm for Achieving Faster Convergence and Steeper Descent", "Authors": ["Zhao Song", "Chiwun Yang"], "Categories": "cs.LG"}, "abstract": "The delta-bar-delta algorithm is recognized as a learning rate adaptation technique that enhances the convergence speed of the training process in optimization by dynamically scheduling the learning rate based on the difference between the current and previous weight updates. While this algorithm has demonstrated strong competitiveness in full data optimization when compared to other state-of-the-art algorithms like Adam and SGD, it may encounter convergence issues in mini-batch optimization scenarios due to the presence of noisy gradients. In this study, we thoroughly investigate the convergence behavior of the delta-bar-delta algorithm in real-world neural network optimization. To address any potential convergence challenges, we propose a novel approach called RDBD (Regrettable Delta-Bar-Delta). Our approach allows for prompt correction of biased learning rate adjustments and ensures the convergence of the optimization process. Furthermore, we demonstrate that RDBD can be seamlessly integrated with any optimization algorithm and significantly improve the convergence speed. By conducting extensive experiments and evaluations, we validate the effectiveness and efficiency of our proposed RDBD approach. The results showcase its capability to overcome convergence issues in mini-batch optimization and its potential to enhance the convergence speed of various optimization algorithms. This research contributes to the advancement of optimization techniques in neural network training, providing practitioners with a reliable automatic learning rate scheduler for achieving faster convergence and improved optimization outcomes.", "url": "https://arxiv.org/abs/2310.11291"}, {"metadata": {"arXiv": "2310.11311", "Date": "Tue, 17 Oct 2023 14:34:58 ", "Title": "Elucidating The Design Space of Classifier-Guided Diffusion Generation", "Authors": ["Jiajun Ma", "Tianyang Hu", "Wenjia Wang and Jiacheng Sun"], "Categories": "cs.LG stat.ML"}, "abstract": "Guidance in conditional diffusion generation is of great importance for sample quality and controllability. However, existing guidance schemes are to be desired. On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data, which is time-consuming and unable to adapt to new conditions. On the other hand, training-free methods such as universal guidance, though more flexible, have yet to demonstrate comparable performance. In this work, through a comprehensive investigation into the design space, we show that it is possible to achieve significant performance improvements over existing guidance schemes by leveraging off-the-shelf classifiers in a training-free fashion, enjoying the best of both worlds. Employing calibration as a general guideline, we propose several pre-conditioning techniques to better exploit pretrained off-the-shelf classifiers for guiding diffusion generation. Extensive experiments on ImageNet validate our proposed method, showing that state-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (up to 20%) using off-the-shelf classifiers with barely any extra computational cost. With the proliferation of publicly available pretrained classifiers, our proposed approach has great potential and can be readily scaled up to text-to-image generation tasks. The code is available at https://github.com/AlexMaOLS/EluCD/tree/main.", "url": "https://arxiv.org/abs/2310.11311"}, {"metadata": {"arXiv": "2310.11335", "Date": "Tue, 17 Oct 2023 15:13:33 ", "Title": "Non-ergodicity in reinforcement learning: robustness via ergodicity transformations", "Authors": ["Dominik Baumann and Erfaun Noorani and James Price and Ole Peters and Colm Connaughton and Thomas B. Sch\\\"on"], "Categories": "cs.LG"}, "abstract": "Envisioned application areas for reinforcement learning (RL) include autonomous driving, precision agriculture, and finance, which all require RL agents to make decisions in the real world. A significant challenge hindering the adoption of RL methods in these domains is the non-robustness of conventional algorithms. In this paper, we argue that a fundamental issue contributing to this lack of robustness lies in the focus on the expected value of the return as the sole \"correct\" optimization objective. The expected value is the average over the statistical ensemble of infinitely many trajectories. For non-ergodic returns, this average differs from the average over a single but infinitely long trajectory. Consequently, optimizing the expected value can lead to policies that yield exceptionally high returns with probability zero but almost surely result in catastrophic outcomes. This problem can be circumvented by transforming the time series of collected returns into one with ergodic increments. This transformation enables learning robust policies by optimizing the long-term return for individual agents rather than the average across infinitely many trajectories. We propose an algorithm for learning ergodicity transformations from data and demonstrate its effectiveness in an instructive, non-ergodic environment and on standard RL benchmarks.", "url": "https://arxiv.org/abs/2310.11335"}, {"metadata": {"arXiv": "2310.11366", "Date": "Tue, 17 Oct 2023 16:04:33 ", "Title": "Lie Group Decompositions for Equivariant Neural Networks", "Authors": ["Mircea Mironenco", "Patrick Forr\\'e"], "Categories": "cs.LG stat.ML"}, "abstract": "Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective. Further limitations are encountered when $G$ is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \\text{GL}^{+}(n, \\mathbb{R})$ and $G = \\text{SL}(n, \\mathbb{R})$, as well as their representation as affine transformations $\\mathbb{R}^{n} \\rtimes G$. Invariant integration as well as a global parametrization is realized by decomposing the `larger` groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals.", "url": "https://arxiv.org/abs/2310.11366"}, {"metadata": {"arXiv": "2310.11389", "Date": "Tue, 17 Oct 2023 16:35:39 ", "Title": "VaR\\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds", "Authors": ["Sanjay Bhat", "Prashanth L.A. and Gugan Thoppe"], "Categories": "cs.LG stat.ML"}, "abstract": "We tackle the problem of estimating the Value-at-Risk (VaR) and the Conditional Value-at-Risk (CVaR) of the infinite-horizon discounted cost within a Markov cost process. First, we derive a minimax lower bound of $\\Omega(1/\\sqrt{n})$ that holds both in an expected and in a probabilistic sense. Then, using a finite-horizon truncation scheme, we derive an upper bound for the error in CVaR estimation, which matches our lower bound up to constant factors. Finally, we discuss an extension of our estimation scheme that covers more general risk measures satisfying a certain continuity criterion, e.g., spectral risk measures, utility-based shortfall risk. To the best of our knowledge, our work is the first to provide lower and upper bounds on the estimation error for any risk measure within Markovian settings. We remark that our lower bounds also extend to the infinite-horizon discounted costs' mean. Even in that case, our result $\\Omega(1/\\sqrt{n}) $ improves upon the existing result $\\Omega(1/n)$[13].", "url": "https://arxiv.org/abs/2310.11389"}, {"metadata": {"arXiv": "2310.11401", "Date": "Tue, 17 Oct 2023 17:10:56 ", "Title": "Enhancing Group Fairness in Online Settings Using Oblique Decision Forests", "Authors": ["Somnath Basu Roy Chowdhury", "Nicholas Monath", "Ahmad Beirami", "Rahul Kidambi", "Avinava Dubey", "Amr Ahmed", "Snigdha Chaturvedi"], "Categories": "cs.LG", "Comments": ["Work in progress"]}, "abstract": "Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of oblique decision trees, to make fair decisions in online settings. The hierarchical tree structure of Aranyani enables parameter isolation and allows us to efficiently compute the fairness gradients using aggregate statistics of previous decisions, eliminating the need for additional storage and forward/backward passes. We also present an efficient framework to train Aranyani and theoretically analyze several of its properties. We conduct empirical evaluations on 5 publicly available benchmarks (including vision and language datasets) to show that Aranyani achieves a better accuracy-fairness trade-off compared to baseline approaches.", "url": "https://arxiv.org/abs/2310.11401"}, {"metadata": {"arXiv": "2310.11407", "Date": "Tue, 17 Oct 2023 17:14:07 ", "Title": "Group-blind optimal transport to group parity and its constrained variants", "Authors": ["Quan Zhou", "Jakub Marecek"], "Categories": "cs.LG math.OC"}, "abstract": "Fairness holds a pivotal role in the realm of machine learning, particularly when it comes to addressing groups categorised by sensitive attributes, e.g., gender, race. Prevailing algorithms in fair learning predominantly hinge on accessibility or estimations of these sensitive attributes, at least in the training process. We design a single group-blind projection map that aligns the feature distributions of both groups in the source data, achieving (demographic) group parity, without requiring values of the protected attribute for individual samples in the computation of the map, as well as its use. Instead, our approach utilises the feature distributions of the privileged and unprivileged groups in a boarder population and the essential assumption that the source data are unbiased representation of the population. We present numerical results on synthetic data and real data.", "url": "https://arxiv.org/abs/2310.11407"}, {"metadata": {"arXiv": "2310.11428", "Date": "Tue, 17 Oct 2023 17:39:40 ", "Title": "Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression", "Authors": ["Adam Block", "Dylan J. Foster", "Akshay Krishnamurthy", "Max Simchowitz", "Cyril Zhang"], "Categories": "cs.LG math.OC stat.ML"}, "abstract": "This work studies training instabilities of behavior cloning with deep neural networks. We observe that minibatch SGD updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss. We empirically disentangle the statistical and computational causes of these oscillations, and find them to stem from the chaotic propagation of minibatch SGD noise through unstable closed-loop dynamics. While SGD noise is benign in the single-step action prediction objective, it results in catastrophic error accumulation over long horizons, an effect we term gradient variance amplification (GVA). We show that many standard mitigation techniques do not alleviate GVA, but find an exponential moving average (EMA) of iterates to be surprisingly effective at doing so. We illustrate the generality of this phenomenon by showing the existence of GVA and its amelioration by EMA in both continuous control and autoregressive language generation. Finally, we provide theoretical vignettes that highlight the benefits of EMA in alleviating GVA and shed light on the extent to which classical convex models can help in understanding the benefits of iterate averaging in deep learning.", "url": "https://arxiv.org/abs/2310.11428"}, {"metadata": {"arXiv": "2310.10943", "Date": "Tue, 17 Oct 2023 02:40:27 ", "Title": "Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning", "Authors": ["Yunlong Song", "Angel Romero", "Matthias Mueller", "Vladlen Koltun", "Davide Scaramuzza"], "Categories": "cs.RO cs.LG", "Journal-ref": "Science Robotics, 2023", "DOI": "10.1126/scirobotics.adg1462"}, "abstract": "A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain randomization to cope with model uncertainty, allowing the discovery of more robust control responses. Our findings allowed us to push an agile drone to its maximum performance, achieving a peak acceleration greater than 12 times the gravitational acceleration and a peak velocity of 108 kilometers per hour. Our policy achieved superhuman control within minutes of training on a standard workstation. This work presents a milestone in agile robotics and sheds light on the role of RL and OC in robot control.", "url": "https://arxiv.org/abs/2310.10943"}, {"metadata": {"arXiv": "2310.10856", "Date": "Mon, 16 Oct 2023 22:10:47 ", "Title": "Joint Optimization of Traffic Signal Control and Vehicle Routing in Signalized Road Networks using Multi-Agent Deep Reinforcement Learning", "Authors": ["Xianyue Peng", "Hang Gao", "Gengyue Han", "Hao Wang", "Michael Zhang"], "Categories": "eess.SY cs.LG cs.MA cs.SY"}, "abstract": "Urban traffic congestion is a critical predicament that plagues modern road networks. To alleviate this issue and enhance traffic efficiency, traffic signal control and vehicle routing have proven to be effective measures. In this paper, we propose a joint optimization approach for traffic signal control and vehicle routing in signalized road networks. The objective is to enhance network performance by simultaneously controlling signal timings and route choices using Multi-Agent Deep Reinforcement Learning (MADRL). Signal control agents (SAs) are employed to establish signal timings at intersections, whereas vehicle routing agents (RAs) are responsible for selecting vehicle routes. By establishing relevance between agents and enabling them to share observations and rewards, interaction and cooperation among agents are fostered, which enhances individual training. The Multi-Agent Advantage Actor-Critic algorithm is used to handle multi-agent environments, and Deep Neural Network (DNN) structures are designed to facilitate the algorithm's convergence. Notably, our work is the first to utilize MADRL in determining the optimal joint policy for signal control and vehicle routing. Numerical experiments conducted on the modified Sioux network demonstrate that our integration of signal control and vehicle routing outperforms controlling signal timings or vehicles' routes alone in enhancing traffic efficiency.", "url": "https://arxiv.org/abs/2310.10856"}, {"metadata": {"arXiv": "2310.11029", "Date": "Tue, 17 Oct 2023 06:59:31 ", "Title": "Core Building Blocks: Next Gen Geo Spatial GPT Application", "Authors": ["Ashley Fernandez", "Swaraj Dube"], "Categories": "cs.AI"}, "abstract": "This paper proposes MapGPT which is a novel approach that integrates the capabilities of language models, specifically large language models (LLMs), with spatial data processing techniques. This paper introduces MapGPT, which aims to bridge the gap between natural language understanding and spatial data analysis by highlighting the relevant core building blocks. By combining the strengths of LLMs and geospatial analysis, MapGPT enables more accurate and contextually aware responses to location-based queries. The proposed methodology highlights building LLMs on spatial and textual data, utilizing tokenization and vector representations specific to spatial information. The paper also explores the challenges associated with generating spatial vector representations. Furthermore, the study discusses the potential of computational capabilities within MapGPT, allowing users to perform geospatial computations and obtain visualized outputs. Overall, this research paper presents the building blocks and methodology of MapGPT, highlighting its potential to enhance spatial data understanding and generation in natural language processing applications.", "url": "https://arxiv.org/abs/2310.11029"}, {"metadata": {"arXiv": "2310.11154", "Date": "Tue, 17 Oct 2023 11:21:23 ", "Title": "Causal discovery using dynamically requested knowledge", "Authors": ["Neville K Kitson and Anthony C Constantinou"], "Categories": "cs.AI"}, "abstract": "Causal Bayesian Networks (CBNs) are an important tool for reasoning under uncertainty in complex real-world systems. Determining the graphical structure of a CBN remains a key challenge and is undertaken either by eliciting it from humans, using machine learning to learn it from data, or using a combination of these two approaches. In the latter case, human knowledge is generally provided to the algorithm before it starts, but here we investigate a novel approach where the structure learning algorithm itself dynamically identifies and requests knowledge for relationships that the algorithm identifies as uncertain during structure learning. We integrate this approach into the Tabu structure learning algorithm and show that it offers considerable gains in structural accuracy, which are generally larger than those offered by existing approaches for integrating knowledge. We suggest that a variant which requests only arc orientation information may be particularly useful where the practitioner has little preexisting knowledge of the causal relationships. As well as offering improved accuracy, the approach can use human expertise more effectively and contributes to making the structure learning process more transparent.", "url": "https://arxiv.org/abs/2310.11154"}, {"metadata": {"arXiv": "2310.11161", "Date": "Tue, 17 Oct 2023 11:28:30 ", "Title": "Accurate prediction of international trade flows: Leveraging knowledge graphs and their embeddings", "Authors": ["Diego Rincon-Yanez", "Chahinez Ounoughi", "Bassem Sellami", "Tarmo Kalvet", "Marek Tiits", "Sabrina Senatore", "Sadok Ben Yahia"], "Categories": "cs.AI cs.SC", "Comments": ["Accepted on Journal of King Saud University Computer and Information Sciences"]}, "abstract": "Knowledge representation (KR) is vital in designing symbolic notations to represent real-world facts and facilitate automated decision-making tasks. Knowledge graphs (KGs) have emerged so far as a popular form of KR, offering a contextual and human-like representation of knowledge. In international economics, KGs have proven valuable in capturing complex interactions between commodities, companies, and countries. By putting the gravity model, which is a common economic framework, into the process of building KGs, important factors that affect trade relationships can be taken into account, making it possible to predict international trade patterns. This paper proposes an approach that leverages Knowledge Graph embeddings for modeling international trade, focusing on link prediction using embeddings. Thus, valuable insights are offered to policymakers, businesses, and economists, enabling them to anticipate the effects of changes in the international trade system. Moreover, the integration of traditional machine learning methods with KG embeddings, such as decision trees and graph neural networks are also explored. The research findings demonstrate the potential for improving prediction accuracy and provide insights into embedding explainability in knowledge representation. The paper also presents a comprehensive analysis of the influence of embedding methods on other intelligent algorithms.", "url": "https://arxiv.org/abs/2310.11161"}, {"metadata": {"arXiv": "2310.11246", "Date": "Tue, 17 Oct 2023 13:13:30 ", "Title": "Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs", "Authors": ["Yao Xu", "Shizhu He", "Cunguang Wang", "Li Cai", "Kang Liu", "Jun Zhao"], "Categories": "cs.AI", "Comments": ["Accepted by EMNLP 2023 findings"]}, "abstract": "Complex Query Answering (CQA) is a challenge task of Knowledge Graph (KG). Due to the incompleteness of KGs, query embedding (QE) methods have been proposed to encode queries and entities into the same embedding space, and treat logical operators as neural set operators to obtain answers. However, these methods train KG embeddings and neural set operators concurrently on both simple (one-hop) and complex (multi-hop and logical) queries, which causes performance degradation on simple queries and low training efficiency. In this paper, we propose Query to Triple (Q2T), a novel approach that decouples the training for simple and complex queries. Q2T divides the training into two stages: (1) Pre-training a neural link predictor on simple queries to predict tail entities based on the head entity and relation. (2) Training a query encoder on complex queries to encode diverse complex queries into a unified triple form that can be efficiently solved by the pretrained neural link predictor. Our proposed Q2T is not only efficient to train, but also modular, thus easily adaptable to various neural link predictors that have been studied well. Extensive experiments demonstrate that, even without explicit modeling for neural set operators, Q2T still achieves state-of-the-art performance on diverse complex queries over three public benchmarks.", "url": "https://arxiv.org/abs/2310.11246"}, {"metadata": {"arXiv": "2310.11249", "Date": "Tue, 17 Oct 2023 13:18:02 ", "Title": "Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R&D Cycle", "Authors": ["Xu Yang", "Xiao Yang", "Weiqing Liu", "Jinhui Li", "Peng Yu", "Zeqi Ye", "Jiang Bian"], "Categories": "cs.AI q-fin.GN", "Comments": ["29 pages", "11 figures"]}, "abstract": "In the wake of relentless digital transformation, data-driven solutions are emerging as powerful tools to address multifarious industrial tasks such as forecasting, anomaly detection, planning, and even complex decision-making. Although data-centric R&D has been pivotal in harnessing these solutions, it often comes with significant costs in terms of human, computational, and time resources. This paper delves into the potential of large language models (LLMs) to expedite the evolution cycle of data-centric R&D. Assessing the foundational elements of data-centric R&D, including heterogeneous task-related data, multi-facet domain knowledge, and diverse computing-functional tools, we explore how well LLMs can understand domain-specific requirements, generate professional ideas, utilize domain-specific tools to conduct experiments, interpret results, and incorporate knowledge from past endeavors to tackle new challenges. We take quantitative investment research as a typical example of industrial data-centric R&D scenario and verified our proposed framework upon our full-stack open-sourced quantitative research platform Qlib and obtained promising results which shed light on our vision of automatic evolving of industrial data-centric R&D cycle.", "url": "https://arxiv.org/abs/2310.11249"}, {"metadata": {"arXiv": "2310.11334", "Date": "Tue, 17 Oct 2023 15:12:56 ", "Title": "Agent-Specific Effects", "Authors": ["Stelios Triantafyllou", "Aleksa Sukovic", "Debmalya Mandal", "Goran Radanovic"], "Categories": "cs.AI"}, "abstract": "Establishing causal relationships between actions and outcomes is fundamental for accountable multi-agent decision-making. However, interpreting and quantifying agents' contributions to such relationships pose significant challenges. These challenges are particularly prominent in the context of multi-agent sequential decision-making, where the causal effect of an agent's action on the outcome depends on how the other agents respond to that action. In this paper, our objective is to present a systematic approach for attributing the causal effects of agents' actions to the influence they exert on other agents. Focusing on multi-agent Markov decision processes, we introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents. We then turn to the counterfactual counterpart of ASE (cf-ASE), provide a sufficient set of conditions for identifying cf-ASE, and propose a practical sampling-based algorithm for estimating it. Finally, we experimentally evaluate the utility of cf-ASE through a simulation-based testbed, which includes a sepsis management environment.", "url": "https://arxiv.org/abs/2310.11334"}, {"metadata": {"arXiv": "2310.10708", "Date": "Mon, 16 Oct 2023 17:04:51 ", "Title": "Automated Natural Language Explanation of Deep Visual Neurons with Large Models", "Authors": ["Chenxu Zhao", "Wei Qian", "Yucheng Shi", "Mengdi Huai", "Ninghao Liu"], "Categories": "cs.CV cs.AI"}, "abstract": "Deep neural networks have exhibited remarkable performance across a wide range of real-world tasks. However, comprehending the underlying reasons for their effectiveness remains a challenging problem. Interpreting deep neural networks through examining neurons offers distinct advantages when it comes to exploring the inner workings of neural networks. Previous research has indicated that specific neurons within deep vision networks possess semantic meaning and play pivotal roles in model performance. Nonetheless, the current methods for generating neuron semantics heavily rely on human intervention, which hampers their scalability and applicability. To address this limitation, this paper proposes a novel post-hoc framework for generating semantic explanations of neurons with large foundation models, without requiring human intervention or prior knowledge. Our framework is designed to be compatible with various model architectures and datasets, facilitating automated and scalable neuron interpretation. Experiments are conducted with both qualitative and quantitative analysis to verify the effectiveness of our proposed approach.", "url": "https://arxiv.org/abs/2310.10708"}, {"metadata": {"arXiv": "2310.10765", "Date": "Mon, 16 Oct 2023 18:59:31 ", "Title": "BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys", "Authors": ["Yu Gu", "Jianwei Yang", "Naoto Usuyama", "Chunyuan Li", "Sheng Zhang", "Matthew P. Lungren", "Jianfeng Gao", "Hoifung Poon"], "Categories": "cs.CV cs.AI cs.CL"}, "abstract": "Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion model for counterfactual biomedical image generation. Given the relative scarcity of image time series data, we introduce a two-stage curriculum that first pretrains the denoising network using the much more abundant single image-report pairs (with dummy prior image), and then continues training using the counterfactual triples. Experiments using the standard MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive battery of tests on counterfactual medical image generation, BiomedJourney substantially outperforms prior state-of-the-art methods in instruction image editing and medical image generation such as InstructPix2Pix and RoentGen. To facilitate future study in counterfactual medical generation, we plan to release our instruction-learning code and pretrained models.", "url": "https://arxiv.org/abs/2310.10765"}, {"metadata": {"arXiv": "2310.11117", "Date": "Tue, 17 Oct 2023 10:04:47 ", "Title": "USDC: Unified Static and Dynamic Compression for Visual Transformer", "Authors": ["Huan Yuan", "Chao Liao", "Jianchao Tan", "Peng Yao", "Jiyuan Jia", "Bin Chen", "Chengru Song", "Di Zhang"], "Categories": "cs.CV cs.AI", "Comments": ["This paper was actually finished in 2021"]}, "abstract": "Visual Transformers have achieved great success in almost all vision tasks, such as classification, detection, and so on. However, the model complexity and the inference speed of the visual transformers hinder their deployments in industrial products. Various model compression techniques focus on directly compressing the visual transformers into a smaller one while maintaining the model performance, however, the performance drops dramatically when the compression ratio is large. Furthermore, several dynamic network techniques have also been applied to dynamically compress the visual transformers to obtain input-adaptive efficient sub-structures during the inference stage, which can achieve a better trade-off between the compression ratio and the model performance. The upper bound of memory of dynamic models is not reduced in the practical deployment since the whole original visual transformer model and the additional control gating modules should be loaded onto devices together for inference. To alleviate two disadvantages of two categories of methods, we propose to unify the static compression and dynamic compression techniques jointly to obtain an input-adaptive compressed model, which can further better balance the total compression ratios and the model performances. Moreover, in practical deployment, the batch sizes of the training and inference stage are usually different, which will cause the model inference performance to be worse than the model training performance, which is not touched by all previous dynamic network papers. We propose a sub-group gates augmentation technique to solve this performance drop problem. Extensive experiments demonstrate the superiority of our method on various baseline visual transformers such as DeiT, T2T-ViT, and so on.", "url": "https://arxiv.org/abs/2310.11117"}, {"metadata": {"arXiv": "2310.11173", "Date": "Tue, 17 Oct 2023 11:41:38 ", "Title": "Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models", "Authors": ["Shuo Wang", "Yan Zhu", "Xiaoyuan Luo", "Zhiwei Yang", "Yizhe Zhang", "Peiyao Fu", "Manning Wang", "Zhijian Song", "Quanlin Li", "Pinghong Zhou", "Yike Guo"], "Categories": "cs.CV cs.AI"}, "abstract": "The development of artificial intelligence systems for colonoscopy analysis often necessitates expert-annotated image datasets. However, limitations in dataset size and diversity impede model performance and generalisation. Image-text colonoscopy records from routine clinical practice, comprising millions of images and text reports, serve as a valuable data source, though annotating them is labour-intensive. Here we leverage recent advancements in large language and vision models and propose EndoKED, a data mining paradigm for deep knowledge extraction and distillation. EndoKED automates the transformation of raw colonoscopy records into image datasets with pixel-level annotation. We validate EndoKED using multi-centre datasets of raw colonoscopy records (~1 million images), demonstrating its superior performance in training polyp detection and segmentation models. Furthermore, the EndoKED pre-trained vision backbone enables data-efficient and generalisable learning for optical biopsy, achieving expert-level performance in both retrospective and prospective validation.", "url": "https://arxiv.org/abs/2310.11173"}, {"metadata": {"arXiv": "2310.11178", "Date": "Tue, 17 Oct 2023 11:53:32 ", "Title": "FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus", "Authors": ["Xueyang Kang", "Fengze Han", "Abdur Fayjie", "Dong Gong"], "Categories": "cs.CV cs.AI eess.IV", "Comments": ["20 pages", "18 figures", "journal paper"], "ACM-class": "I.4.9; I.2.10"}, "abstract": "Depth estimation from focal stacks is a fundamental computer vision problem that aims to infer depth from focus/defocus cues in the image stacks. Most existing methods tackle this problem by applying convolutional neural networks (CNNs) with 2D or 3D convolutions over a set of fixed stack images to learn features across images and stacks. Their performance is restricted due to the local properties of the CNNs, and they are constrained to process a fixed number of stacks consistent in train and inference, limiting the generalization to the arbitrary length of stacks. To handle the above limitations, we develop a novel Transformer-based network, FocDepthFormer, composed mainly of a Transformer with an LSTM module and a CNN decoder. The self-attention in Transformer enables learning more informative features via an implicit non-local cross reference. The LSTM module is learned to integrate the representations across the stack with arbitrary images. To directly capture the low-level features of various degrees of focus/defocus, we propose to use multi-scale convolutional kernels in an early-stage encoder. Benefiting from the design with LSTM, our FocDepthFormer can be pre-trained with abundant monocular RGB depth estimation data for visual pattern capturing, alleviating the demand for the hard-to-collect focal stack data. Extensive experiments on various focal stack benchmark datasets show that our model outperforms the state-of-the-art models on multiple metrics.", "url": "https://arxiv.org/abs/2310.11178"}, {"metadata": {"arXiv": "2310.11316", "Date": "Tue, 17 Oct 2023 14:48:02 ", "Title": "MonoSKD: General Distillation Framework for Monocular 3D Object Detection via Spearman Correlation Coefficient", "Authors": ["Sen Wang", "Jin Zheng"], "Categories": "cs.CV cs.AI", "Comments": ["Accepted by ECAI 2023"]}, "abstract": "Monocular 3D object detection is an inherently ill-posed problem, as it is challenging to predict accurate 3D localization from a single image. Existing monocular 3D detection knowledge distillation methods usually project the LiDAR onto the image plane and train the teacher network accordingly. Transferring LiDAR-based model knowledge to RGB-based models is more complex, so a general distillation strategy is needed. To alleviate cross-modal prob-lem, we propose MonoSKD, a novel Knowledge Distillation framework for Monocular 3D detection based on Spearman correlation coefficient, to learn the relative correlation between cross-modal features. Considering the large gap between these features, strict alignment of features may mislead the training, so we propose a looser Spearman loss. Furthermore, by selecting appropriate distillation locations and removing redundant modules, our scheme saves more GPU resources and trains faster than existing methods. Extensive experiments are performed to verify the effectiveness of our framework on the challenging KITTI 3D object detection benchmark. Our method achieves state-of-the-art performance until submission with no additional inference computational cost. Our codes are available at https://github.com/Senwang98/MonoSKD", "url": "https://arxiv.org/abs/2310.11316"}, {"metadata": {"arXiv": "2310.11333", "Date": "Tue, 17 Oct 2023 15:12:11 ", "Title": "Key Point-based Orientation Estimation of Strawberries for Robotic Fruit Picking", "Authors": ["Justin Le Lou\\\"edec and Grzegorz Cielniak"], "Categories": "cs.CV cs.AI", "Journal-ref": "Computer Vision Systems. ICVS 2023. Lecture Notes in Computer Science, vol 14253", "DOI": "10.1007/978-3-031-44137-0_13"}, "abstract": "Selective robotic harvesting is a promising technological solution to address labour shortages which are affecting modern agriculture in many parts of the world. For an accurate and efficient picking process, a robotic harvester requires the precise location and orientation of the fruit to effectively plan the trajectory of the end effector. The current methods for estimating fruit orientation employ either complete 3D information which typically requires registration from multiple views or rely on fully-supervised learning techniques, which require difficult-to-obtain manual annotation of the reference orientation. In this paper, we introduce a novel key-point-based fruit orientation estimation method allowing for the prediction of 3D orientation from 2D images directly. The proposed technique can work without full 3D orientation annotations but can also exploit such information for improved accuracy. We evaluate our work on two separate datasets of strawberry images obtained from real-world data collection scenarios. Our proposed method achieves state-of-the-art performance with an average error as low as $8^{\\circ}$, improving predictions by $\\sim30\\%$ compared to previous work presented in~\\cite{wagner2021efficient}. Furthermore, our method is suited for real-time robotic applications with fast inference times of $\\sim30$ms.", "url": "https://arxiv.org/abs/2310.11333"}, {"metadata": {"arXiv": "2310.11392", "Date": "Tue, 17 Oct 2023 16:45:47 ", "Title": "Towards Automatic Satellite Images Captions Generation Using Large Language Models", "Authors": ["Yingxu He and Qiqi Sun"], "Categories": "cs.CV cs.AI"}, "abstract": "Automatic image captioning is a promising technique for conveying visual information using natural language. It can benefit various tasks in satellite remote sensing, such as environmental monitoring, resource management, disaster management, etc. However, one of the main challenges in this domain is the lack of large-scale image-caption datasets, as they require a lot of human expertise and effort to create. Recent research on large language models (LLMs) has demonstrated their impressive performance in natural language understanding and generation tasks. Nonetheless, most of them cannot handle images (GPT-3.5, Falcon, Claude, etc.), while conventional captioning models pre-trained on general ground-view images often fail to produce detailed and accurate captions for aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we propose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to automatically collect captions for remote sensing images by guiding LLMs to describe their object annotations. We also present a benchmark model that adapts the pre-trained generative image2text model (GIT) to generate high-quality captions for remote-sensing images. Our evaluation demonstrates the effectiveness of our approach for collecting captions for remote sensing images.", "url": "https://arxiv.org/abs/2310.11392"}, {"metadata": {"arXiv": "2310.11441", "Date": "Tue, 17 Oct 2023 17:51:31 ", "Title": "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V", "Authors": ["Jianwei Yang", "Hao Zhang", "Feng Li", "Xueyan Zou", "Chunyuan Li", "Jianfeng Gao"], "Categories": "cs.CV cs.AI cs.CL cs.HC"}, "abstract": "We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a zero-shot setting.", "url": "https://arxiv.org/abs/2310.11441"}, {"metadata": {"arXiv": "2310.10863", "Date": "Mon, 16 Oct 2023 22:23:18 ", "Title": "Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments", "Authors": ["Krishna Suresh", "Aditya Rauniyar", "Micah Corah", "Sebastian Scherer"], "Categories": "cs.RO cs.AI", "Comments": ["Submitted to ICRA'24; 7 pages", "8 figures", "2 tables"]}, "abstract": "Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance, we plan in five test environments with complex multiple-actor behaviors. Compared with a formation planner, our sequential planner generates 14% greater view reward over the actors for three scenarios and comparable performance to formation planning on two others. We also observe near identical performance of sequential planning both with and without inter-robot collision constraints. Overall, we demonstrate effective coordination of teams of aerial robots for filming groups that may split, merge, or spread apart and in environments cluttered with obstacles that may cause collisions or occlusions.", "url": "https://arxiv.org/abs/2310.10863"}, {"metadata": {"arXiv": "2310.11075", "Date": "Tue, 17 Oct 2023 08:46:56 ", "Title": "Sim-to-Real Transfer of Adaptive Control Parameters for AUV Stabilization under Current Disturbance", "Authors": ["Thomas Chaffre", "Jonathan Wheare", "Andrew Lammas", "Paulo Santos", "Gilles Le Chenadec", "Karl Sammut", "Benoit Clement"], "Categories": "cs.RO cs.AI cs.SY eess.SY"}, "abstract": "Learning-based adaptive control methods hold the premise of enabling autonomous agents to reduce the effect of process variations with minimal human intervention. However, its application to autonomous underwater vehicles (AUVs) has so far been restricted due to 1) unknown dynamics under the form of sea current disturbance that we can not model properly nor measure due to limited sensor capability and 2) the nonlinearity of AUVs tasks where the controller response at some operating points must be overly conservative in order to satisfy the specification at other operating points. Deep Reinforcement Learning (DRL) can alleviates these limitations by training general-purpose neural network policies, but applications of DRL algorithms to AUVs have been restricted to simulated environments, due to their inherent high sample complexity and distribution shift problem. This paper presents a novel approach, merging the Maximum Entropy Deep Reinforcement Learning framework with a classic model-based control architecture, to formulate an adaptive controller. Within this framework, we introduce a Sim-to-Real transfer strategy comprising the following components: a bio-inspired experience replay mechanism, an enhanced domain randomisation technique, and an evaluation protocol executed on a physical platform. Our experimental assessments demonstrate that this method effectively learns proficient policies from suboptimal simulated models of the AUV, resulting in control performance 3 times higher when transferred to a real-world vehicle, compared to its model-based nonadaptive but optimal counterpart.", "url": "https://arxiv.org/abs/2310.11075"}, {"metadata": {"arXiv": "2310.11305", "Date": "Tue, 17 Oct 2023 14:29:25 ", "Title": "MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello, and Atari Games", "Authors": ["Ti-Rong Wu", "Hung Guei", "Po-Wei Huang", "Pei-Chiun Peng", "Ting Han Wei", "Chung-Chin Shih", "Yun-Jui Tsai"], "Categories": "cs.AI cs.LG"}, "abstract": "This paper presents MiniZero, a zero-knowledge learning framework that supports four state-of-the-art algorithms, including AlphaZero, MuZero, Gumbel AlphaZero, and Gumbel MuZero. While these algorithms have demonstrated super-human performance in many games, it remains unclear which among them is most suitable or efficient for specific tasks. Through MiniZero, we systematically evaluate the performance of each algorithm in two board games, 9x9 Go and 8x8 Othello, as well as 57 Atari games. Our empirical findings are summarized as follows. For two board games, using more simulations generally results in higher performance. However, the choice of AlphaZero and MuZero may differ based on game properties. For Atari games, both MuZero and Gumbel MuZero are worth considering. Since each game has unique characteristics, different algorithms and simulations yield varying results. In addition, we introduce an approach, called progressive simulation, which progressively increases the simulation budget during training to allocate computation more efficiently. Our empirical results demonstrate that progressive simulation achieves significantly superior performance in two board games. By making our framework and trained models publicly available, this paper contributes a benchmark for future research on zero-knowledge learning algorithms, assisting researchers in algorithm selection and comparison against these zero-knowledge learning baselines.", "url": "https://arxiv.org/abs/2310.11305"}, {"metadata": {"arXiv": "2310.11341", "Date": "Tue, 17 Oct 2023 15:24:02 ", "Title": "Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning", "Authors": ["Shruthi Gowda", "Bahram Zonooz", "Elahe Arani"], "Categories": "cs.CV cs.AI cs.LG", "Comments": ["Published in Transactions on Machine Learning Research (TMLR)"]}, "abstract": "Artificial neural networks (ANNs) exhibit a narrow scope of expertise on stationary independent data. However, the data in the real world is continuous and dynamic, and ANNs must adapt to novel scenarios while also retaining the learned knowledge to become lifelong learners. The ability of humans to excel at these tasks can be attributed to multiple factors ranging from cognitive computational structures, cognitive biases, and the multi-memory systems in the brain. We incorporate key concepts from each of these to design a novel framework, Dual Cognitive Architecture (DUCA), which includes multiple sub-systems, implicit and explicit knowledge representation dichotomy, inductive bias, and a multi-memory system. The inductive bias learner within DUCA is instrumental in encoding shape information, effectively countering the tendency of ANNs to learn local textures. Simultaneously, the inclusion of a semantic memory submodule facilitates the gradual consolidation of knowledge, replicating the dynamics observed in fast and slow learning systems, reminiscent of the principles underpinning the complementary learning system in human cognition. DUCA shows improvement across different settings and datasets, and it also exhibits reduced task recency bias, without the need for extra information. To further test the versatility of lifelong learning methods on a challenging distribution shift, we introduce a novel domain-incremental dataset DN4IL. In addition to improving performance on existing benchmarks, DUCA also demonstrates superior performance on this complex dataset.", "url": "https://arxiv.org/abs/2310.11341"}, {"metadata": {"arXiv": "2310.10685", "Date": "Sat, 14 Oct 2023 12:13:41 ", "Title": "PS-AAS: Portfolio Selection for Automated Algorithm Selection in Black-Box Optimization", "Authors": ["Ana Kostovska", "Gjorgjina Cenikj", "Diederick Vermetten", "Anja Jankovic", "Ana Nikolikj", "Urban Skvorc", "Peter Korosec", "Carola Doerr", "Tome Eftimov"], "Categories": "cs.LG cs.AI cs.NE", "Comments": ["Proc. of International Conference on Automated Machine Learning (AutoML 2023)"]}, "abstract": "The performance of automated algorithm selection (AAS) strongly depends on the portfolio of algorithms to choose from. Selecting the portfolio is a non-trivial task that requires balancing the trade-off between the higher flexibility of large portfolios with the increased complexity of the AAS task. In practice, probably the most common way to choose the algorithms for the portfolio is a greedy selection of the algorithms that perform well in some reference tasks of interest. We set out in this work to investigate alternative, data-driven portfolio selection techniques. Our proposed method creates algorithm behavior meta-representations, constructs a graph from a set of algorithms based on their meta-representation similarity, and applies a graph algorithm to select a final portfolio of diverse, representative, and non-redundant algorithms. We evaluate two distinct meta-representation techniques (SHAP and performance2vec) for selecting complementary portfolios from a total of 324 different variants of CMA-ES for the task of optimizing the BBOB single-objective problems in dimensionalities 5 and 30 with different cut-off budgets. We test two types of portfolios: one related to overall algorithm behavior and the `personalized' one (related to algorithm behavior per each problem separately). We observe that the approach built on the performance2vec-based representations favors small portfolios with negligible error in the AAS task relative to the virtual best solver from the selected portfolio, whereas the portfolios built from the SHAP-based representations gain from higher flexibility at the cost of decreased performance of the AAS. Across most considered scenarios, personalized portfolios yield comparable or slightly better performance than the classical greedy approach. They outperform the full portfolio in all scenarios.", "url": "https://arxiv.org/abs/2310.10685"}, {"metadata": {"arXiv": "2310.10692", "Date": "Sun, 15 Oct 2023 14:57:14 ", "Title": "ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors", "Authors": ["Julien Pourcel", "C\\'edric Colas", "Pierre-Yves Oudeyer", "Laetitia Teodorescu"], "Categories": "cs.LG cs.AI"}, "abstract": "Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to explore that abstract semantic space, slowly discovering a diversity of solvable programming puzzles in any given run. Across a set of experiments, we show that ACES discovers a richer diversity of puzzles than existing diversity-maximizing algorithms as measured across a range of diversity metrics. We further study whether and in which conditions this diversity can translate into the successful training of puzzle solving models.", "url": "https://arxiv.org/abs/2310.10692"}, {"metadata": {"arXiv": "2310.10696", "Date": "Mon, 16 Oct 2023 04:20:52 ", "Title": "Robust Collaborative Filtering to Popularity Distribution Shift", "Authors": ["An Zhang", "Wenchang Ma", "Jingnan Zheng", "Xiang Wang", "Tat-seng Chua"], "Categories": "cs.LG cs.AI", "Journal-ref": "Transactions on Information Systems 2023", "DOI": "10.1145/3627159"}, "abstract": "In leading collaborative filtering (CF) models, representations of users and items are prone to learn popularity bias in the training data as shortcuts. The popularity shortcut tricks are good for in-distribution (ID) performance but poorly generalized to out-of-distribution (OOD) data, i.e., when popularity distribution of test data shifts w.r.t. the training one. To close the gap, debiasing strategies try to assess the shortcut degrees and mitigate them from the representations. However, there exist two deficiencies: (1) when measuring the shortcut degrees, most strategies only use statistical metrics on a single aspect (i.e., item frequency on item and user frequency on user aspect), failing to accommodate the compositional degree of a user-item pair; (2) when mitigating shortcuts, many strategies assume that the test distribution is known in advance. This results in low-quality debiased representations. Worse still, these strategies achieve OOD generalizability with a sacrifice on ID performance. In this work, we present a simple yet effective debiasing strategy, PopGo, which quantifies and reduces the interaction-wise popularity shortcut without any assumptions on the test data. It first learns a shortcut model, which yields a shortcut degree of a user-item pair based on their popularity representations. Then, it trains the CF model by adjusting the predictions with the interaction-wise shortcut degrees. By taking both causal- and information-theoretical looks at PopGo, we can justify why it encourages the CF model to capture the critical popularity-agnostic features while leaving the spurious popularity-relevant patterns out. We use PopGo to debias two high-performing CF models (MF, LightGCN) on four benchmark datasets. On both ID and OOD test sets, PopGo achieves significant gains over the state-of-the-art debiasing strategies (e.g., DICE, MACR).", "url": "https://arxiv.org/abs/2310.10696"}, {"metadata": {"arXiv": "2310.10699", "Date": "Mon, 16 Oct 2023 06:16:47 ", "Title": "Reusing Pretrained Models by Multi-linear Operators for Efficient Training", "Authors": ["Yu Pan", "Ye Yuan", "Yichun Yin", "Zenglin Xu", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted in NeurIPS 2023"]}, "abstract": "Training large models from scratch usually costs a substantial amount of resources. Towards this problem, recent studies such as bert2BERT and LiGO have reused small pretrained models to initialize a large model (termed the ``target model''), leading to a considerable acceleration in training. Despite the successes of these previous studies, they grew pretrained models by mapping partial weights only, ignoring potential correlations across the entire model. As we show in this paper, there are inter- and intra-interactions among the weights of both the pretrained and the target models. As a result, the partial mapping may not capture the complete information and lead to inadequate growth. In this paper, we propose a method that linearly correlates each weight of the target model to all the weights of the pretrained model to further enhance acceleration ability. We utilize multi-linear operators to reduce computational and spacial complexity, enabling acceptable resource requirements. Experiments demonstrate that our method can save 76\\% computational costs on DeiT-base transferred from DeiT-small, which outperforms bert2BERT by +12.0\\% and LiGO by +20.7\\%, respectively.", "url": "https://arxiv.org/abs/2310.10699"}, {"metadata": {"arXiv": "2310.10705", "Date": "Mon, 16 Oct 2023 14:46:45 ", "Title": "Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations", "Authors": ["Kamal Taha"], "Categories": "cs.LG cs.AI"}, "abstract": "This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous empirical and experimental evaluation to rank these varying techniques. For the empirical evaluation, we assess techniques based on a set of four criteria. The experimental evaluation ranks the algorithms employing the same sub-techniques, techniques, sub-categories, and categories. This integration of a multi-layered taxonomy, empirical evaluations, and comparative experiments provides a detailed and holistic understanding of ML techniques and algorithms for identifying wafer defects. This approach guides researchers towards making more informed decisions in their work. Additionally, the paper illuminates the future prospects of ML techniques for wafer defect identification, underscoring potential advancements and opportunities for further research in this field", "url": "https://arxiv.org/abs/2310.10705"}, {"metadata": {"arXiv": "2310.10833", "Date": "Mon, 16 Oct 2023 21:14:50 ", "Title": "Proper Laplacian Representation Learning", "Authors": ["Diego Gomez", "Michael Bowling", "Marlos C. Machado"], "Categories": "cs.LG cs.AI"}, "abstract": "The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.", "url": "https://arxiv.org/abs/2310.10833"}, {"metadata": {"arXiv": "2310.10899", "Date": "Tue, 17 Oct 2023 00:12:19 ", "Title": "Instilling Inductive Biases with Subnetworks", "Authors": ["Enyan Zhang", "Michael A. Lepori", "Ellie Pavlick"], "Categories": "cs.LG cs.AI", "Comments": ["9 pages", "5 figures"]}, "abstract": "Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to adopt a specific, generalizable solution to a modular arithmetic task. Second, we demonstrate that Subtask Induction successfully induces a human-like shape bias while increasing data efficiency for convolutional and transformer-based image classification models.", "url": "https://arxiv.org/abs/2310.10899"}, {"metadata": {"arXiv": "2310.10908", "Date": "Tue, 17 Oct 2023 01:02:32 ", "Title": "Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?", "Authors": ["Zihan Qiu", "Zeyu Huang", "Jie Fu"], "Categories": "cs.LG cs.AI"}, "abstract": "Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \\textbf{E}mergent $\\textbf{M}$ixture-$\\textbf{o}$f-$\\textbf{E}$xperts (EMoE). Without introducing additional parameters, EMoE can be seen as the modular counterpart of the original model and can be effortlessly incorporated into downstream tuning. Extensive experiments (we tune 1785 models) on various downstream tasks (vision and language) and models (22M to1.5B) demonstrate that EMoE effectively boosts in-domain and out-of-domain generalization abilities. Further analysis and ablation study suggest that EMoE mitigates negative knowledge transfer and is robust to various configurations. Code is available at \\url{https://github.com/qiuzh20/EMoE}", "url": "https://arxiv.org/abs/2310.10908"}, {"metadata": {"arXiv": "2310.10998", "Date": "Tue, 17 Oct 2023 05:03:00 ", "Title": "Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation", "Authors": ["Xinyi Gao", "Wentao Zhang", "Junliang Yu", "Yingxia Shao", "Quoc Viet Hung Nguyen", "Bin Cui", "Hongzhi Yin"], "Categories": "cs.LG cs.AI", "Comments": ["arXiv admin note: substantial text overlap with arXiv:2211.00495"]}, "abstract": "Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse array of applications. However, the sheer size of large-scale graphs presents a significant challenge to real-time inference with GNNs. Although existing Scalable GNNs leverage linear propagation to preprocess the features and accelerate the training and inference procedure, these methods still suffer from scalability issues when making inferences on unseen nodes, as the feature preprocessing requires the graph to be known and fixed. To further accelerate Scalable GNNs inference in this inductive setting, we propose an online propagation framework and two novel node-adaptive propagation methods that can customize the optimal propagation depth for each node based on its topological information and thereby avoid redundant feature propagation. The trade-off between accuracy and latency can be flexibly managed through simple hyper-parameters to accommodate various latency constraints. Moreover, to compensate for the inference accuracy loss caused by the potential early termination of propagation, we further propose Inception Distillation to exploit the multi-scale receptive field information within graphs. The rigorous and comprehensive experimental study on public datasets with varying scales and characteristics demonstrates that the proposed inference acceleration framework outperforms existing state-of-the-art graph inference acceleration methods in terms of accuracy and efficiency. Particularly, the superiority of our approach is notable on datasets with larger scales, yielding a 75x inference speedup on the largest Ogbn-products dataset.", "url": "https://arxiv.org/abs/2310.10998"}, {"metadata": {"arXiv": "2310.11011", "Date": "Tue, 17 Oct 2023 05:45:32 ", "Title": "From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling", "Authors": ["Aneesh Komanduri", "Xintao Wu", "Yongkai Wu", "Feng Chen"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual generation methods. We focus on fundamental theory, formulations, drawbacks, datasets, metrics, and applications of causal generative models in fairness, privacy, out-of-distribution generalization, and precision medicine. We also discuss open problems and fruitful research directions for future work in the field.", "url": "https://arxiv.org/abs/2310.11011"}, {"metadata": {"arXiv": "2310.11022", "Date": "Tue, 17 Oct 2023 06:29:09 ", "Title": "Compatible Transformer for Irregularly Sampled Multivariate Time Series", "Authors": ["Yuxi Wei", "Juntong Peng", "Tong He", "Chenxin Xu", "Jian Zhang", "Shirui Pan", "Siheng Chen"], "Categories": "cs.LG cs.AI", "Comments": ["Accepted at the IEEE International Conference on Data Mining (ICDM) 2023 as short paper"]}, "abstract": "To analyze multivariate time series, most previous methods assume regular subsampling of time series, where the interval between adjacent measurements and the number of samples remain unchanged. Practically, data collection systems could produce irregularly sampled time series due to sensor failures and interventions. However, existing methods designed for regularly sampled multivariate time series cannot directly handle irregularity owing to misalignment along both temporal and variate dimensions. To fill this gap, we propose Compatible Transformer (CoFormer), a transformer-based encoder to achieve comprehensive temporal-interaction feature learning for each individual sample in irregular multivariate time series. In CoFormer, we view each sample as a unique variate-time point and leverage intra-variate/inter-variate attentions to learn sample-wise temporal/interaction features based on intra-variate/inter-variate neighbors. With CoFormer as the core, we can analyze irregularly sampled multivariate time series for many downstream tasks, including classification and prediction. We conduct extensive experiments on 3 real-world datasets and validate that the proposed CoFormer significantly and consistently outperforms existing methods.", "url": "https://arxiv.org/abs/2310.11022"}, {"metadata": {"arXiv": "2310.11046", "Date": "Tue, 17 Oct 2023 07:25:59 ", "Title": "Fast Graph Condensation with Structure-based Neural Tangent Kernel", "Authors": ["Lin Wang", "Wenqi Fan", "Jiatong Li", "Yao Ma", "Qing Li"], "Categories": "cs.LG cs.AI", "Comments": ["15 pages", "6 figures", "5 tables"], "MSC-class": "68T01", "ACM-class": "I.2.0"}, "abstract": "The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and serves as the kernel function in KRR paradigm. Comprehensive experiments demonstrate the effectiveness of our proposed model in accelerating graph condensation while maintaining high prediction performance.", "url": "https://arxiv.org/abs/2310.11046"}, {"metadata": {"arXiv": "2310.11048", "Date": "Tue, 17 Oct 2023 07:32:59 ", "Title": "Understanding Contrastive Learning via Distributionally Robust Optimization", "Authors": ["Junkang Wu", "Jiawei Chen", "Jiancan Wu", "Wentao Shi", "Xiang Wang", "Xiangnan He"], "Categories": "cs.LG cs.AI"}, "abstract": "This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (\\eg labels). However, existing theories fall short in providing explanations for this phenomenon. We bridge this research gap by analyzing CL through the lens of distributionally robust optimization (DRO), yielding several key insights: (1) CL essentially conducts DRO over the negative sampling distribution, thus enabling robust performance across a variety of potential distributions and demonstrating robustness to sampling bias; (2) The design of the temperature $\\tau$ is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set; (3) A theoretical connection is established between DRO and mutual information, thus presenting fresh evidence for ``InfoNCE as an estimate of MI'' and a new estimation approach for $\\phi$-divergence-based generalized mutual information. We also identify CL's potential shortcomings, including over-conservatism and sensitivity to outliers, and introduce a novel Adjusted InfoNCE loss (ADNCE) to mitigate these issues. It refines potential distribution, improving performance and accelerating convergence. Extensive experiments on various domains (image, sentence, and graphs) validate the effectiveness of the proposal. The code is available at \\url{https://github.com/junkangwu/ADNCE}.", "url": "https://arxiv.org/abs/2310.11048"}, {"metadata": {"arXiv": "2310.11082", "Date": "Tue, 17 Oct 2023 09:06:41 ", "Title": "Multi-omics Sampling-based Graph Transformer for Synthetic Lethality Prediction", "Authors": ["Xusheng Zhao", "Hao Liu", "Qiong Dai", "Hao Peng", "Xu Bai", "Huailiang Peng"], "Categories": "cs.LG cs.AI q-bio.QM"}, "abstract": "Synthetic lethality (SL) prediction is used to identify if the co-mutation of two genes results in cell death. The prevalent strategy is to abstract SL prediction as an edge classification task on gene nodes within SL data and achieve it through graph neural networks (GNNs). However, GNNs suffer from limitations in their message passing mechanisms, including over-smoothing and over-squashing issues. Moreover, harnessing the information of non-SL gene relationships within large-scale multi-omics data to facilitate SL prediction poses a non-trivial challenge. To tackle these issues, we propose a new multi-omics sampling-based graph transformer for SL prediction (MSGT-SL). Concretely, we introduce a shallow multi-view GNN to acquire local structural patterns from both SL and multi-omics data. Further, we input gene features that encode multi-view information into the standard self-attention to capture long-range dependencies. Notably, starting with batch genes from SL data, we adopt parallel random walk sampling across multiple omics gene graphs encompassing them. Such sampling effectively and modestly incorporates genes from omics in a structure-aware manner before using self-attention. We showcase the effectiveness of MSGT-SL on real-world SL tasks, demonstrating the empirical benefits gained from the graph transformer and multi-omics data.", "url": "https://arxiv.org/abs/2310.11082"}, {"metadata": {"arXiv": "2310.11087", "Date": "Tue, 17 Oct 2023 09:13:10 ", "Title": "Feature Pyramid biLSTM: Using Smartphone Sensors for Transportation Mode Detection", "Authors": ["Qinrui Tang", "Hao Cheng"], "Categories": "cs.LG cs.AI eess.SP"}, "abstract": "The widespread utilization of smartphones has provided extensive availability to Inertial Measurement Units, providing a wide range of sensory data that can be advantageous for the detection of transportation modes. The objective of this study is to propose a novel end-to-end approach to effectively explore a reduced amount of sensory data collected from a smartphone to achieve accurate mode detection in common daily traveling activities. Our approach, called Feature Pyramid biLSTM (FPbiLSTM), is characterized by its ability to reduce the number of sensors required and processing demands, resulting in a more efficient modeling process without sacrificing the quality of the outcomes than the other current models. FPbiLSTM extends an existing CNN biLSTM model with the Feature Pyramid Network, leveraging the advantages of both shallow layer richness and deeper layer feature resilience for capturing temporal moving patterns in various transportation modes. It exhibits an excellent performance by employing the data collected from only three out of seven sensors, i.e. accelerometers, gyroscopes, and magnetometers, in the 2018 Sussex-Huawei Locomotion (SHL) challenge dataset, attaining a noteworthy accuracy of 95.1% and an F1-score of 94.7% in detecting eight different transportation modes.", "url": "https://arxiv.org/abs/2310.11087"}, {"metadata": {"arXiv": "2310.11102", "Date": "Tue, 17 Oct 2023 09:34:34 ", "Title": "HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning", "Authors": ["Yulan Hu", "Zhirui Yang", "Sheng Ouyang", "Yong Liu"], "Categories": "cs.LG cs.AI"}, "abstract": "Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard negative samples for contrastive learning, utilizing the power of variational inference. Additionally, we present a dynamic mask strategy to ensure effective and stable learning. Moreover, we propose an enhanced scaled cosine error as the criterion for better attribute reconstruction. As an initial step in combining generative and contrastive SSL, HGCVAE achieves remarkable results compared to various state-of-the-art baselines, confirming its superiority.", "url": "https://arxiv.org/abs/2310.11102"}, {"metadata": {"arXiv": "2310.11169", "Date": "Tue, 17 Oct 2023 11:37:40 ", "Title": "MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection", "Authors": ["Chaoyue Ding", "Shiliang Sun", "Jing Zhao"], "Categories": "cs.LG cs.AI", "Comments": ["Information Fusion 2023 accepted"], "ACM-class": "I.5.4", "DOI": "10.1016/j.inffus.2022.08.011"}, "abstract": "Multimodal time series (MTS) anomaly detection is crucial for maintaining the safety and stability of working devices (e.g., water treatment system and spacecraft), whose data are characterized by multivariate time series with diverse modalities. Although recent deep learning methods show great potential in anomaly detection, they do not explicitly capture spatial-temporal relationships between univariate time series of different modalities, resulting in more false negatives and false positives. In this paper, we propose a multimodal spatial-temporal graph attention network (MST-GAT) to tackle this problem. MST-GAT first employs a multimodal graph attention network (M-GAT) and a temporal convolution network to capture the spatial-temporal correlation in multimodal time series. Specifically, M-GAT uses a multi-head attention module and two relational attention modules (i.e., intra- and inter-modal attention) to model modal correlations explicitly. Furthermore, MST-GAT optimizes the reconstruction and prediction modules simultaneously. Experimental results on four multimodal benchmarks demonstrate that MST-GAT outperforms the state-of-the-art baselines. Further analysis indicates that MST-GAT strengthens the interpretability of detected anomalies by locating the most anomalous univariate time series.", "url": "https://arxiv.org/abs/2310.11169"}, {"metadata": {"arXiv": "2310.11211", "Date": "Tue, 17 Oct 2023 12:40:53 ", "Title": "Understanding Fairness Surrogate Functions in Algorithmic Fairness", "Authors": ["Wei Yao", "Zhanke Zhou", "Zhicong Li", "Bo Han", "Yong Liu"], "Categories": "cs.LG cs.AI", "Comments": ["8 pages"]}, "abstract": "It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The \"gap\" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the \"gap\" motivate us that the unbounded surrogate functions will be affected by the points far from the decision boundary, which is the large margin points issue investigated in this paper. To address it, we propose the general sigmoid surrogate with a rigorous and reliable fairness guarantee. Interestingly, the theory also provides insights into two important issues that deal with the large margin points as well as obtaining a more balanced dataset are beneficial to fairness. Furthermore, we elaborate a novel and general algorithm called Balanced Surrogate, which iteratively reduces the \"gap\" to improve fairness. Finally, we provide empirical evidence showing that our methods achieve better fairness performance in three real-world datasets.", "url": "https://arxiv.org/abs/2310.11211"}, {"metadata": {"arXiv": "2310.11439", "Date": "Tue, 17 Oct 2023 17:50:22 ", "Title": "Understanding deep neural networks through the lens of their non-linearity", "Authors": ["Quentin Bouniot", "Ievgen Redko", "Anton Mallasto", "Charlotte Laclau", "Karol Arndt", "Oliver Struckmeier", "Markus Heinonen", "Ville Kyrki", "Samuel Kaski"], "Categories": "cs.LG cs.AI stat.ML"}, "abstract": "The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.", "url": "https://arxiv.org/abs/2310.11439"}, {"metadata": {"arXiv": "2310.11450", "Date": "Tue, 17 Oct 2023 17:58:19 ", "Title": "Explaining Deep Neural Networks for Bearing Fault Detection with Vibration Concepts", "Authors": ["Thomas Decker", "Michael Lebacher and Volker Tresp"], "Categories": "cs.LG cs.AI", "Comments": ["2023 IEEE 21st International Conference on Industrial Informatics (INDIN)"], "DOI": "10.1109/INDIN51400.2023.10218170"}, "abstract": "Concept-based explanation methods, such as Concept Activation Vectors, are potent means to quantify how abstract or high-level characteristics of input data influence the predictions of complex deep neural networks. However, applying them to industrial prediction problems is challenging as it is not immediately clear how to define and access appropriate concepts for individual use cases and specific data types. In this work, we investigate how to leverage established concept-based explanation techniques in the context of bearing fault detection with deep neural networks trained on vibration signals. Since bearings are prevalent in almost every rotating equipment, ensuring the reliability of intransparent fault detection models is crucial to prevent costly repairs and downtimes of industrial machinery. Our evaluations demonstrate that explaining opaque models in terms of vibration concepts enables human-comprehensible and intuitive insights about their inner workings, but the underlying assumptions need to be carefully validated first.", "url": "https://arxiv.org/abs/2310.11450"}];

        var papersDiv = document.getElementById("papers");

        papers.forEach(function(paper, index) {
            var paperDiv = document.createElement("div");
            paperDiv.className = "paper";

            var titleDiv = document.createElement("div");
            titleDiv.innerText = (index + 1) + ". " + paper.metadata.Title;
            titleDiv.className = "dropdown";

            var abstractDiv = document.createElement("div");
            abstractDiv.innerText = paper.abstract;
            abstractDiv.className = "abstract";

            var linkDiv = document.createElement("div");
            linkDiv.innerHTML = '<a href="' + paper.url + '" target="_blank">Link to paper</a>';
            linkDiv.className = "link";

            titleDiv.addEventListener("click", function() {
                var display = abstractDiv.style.display;
                abstractDiv.style.display = display === "block" ? "none" : "block";
                linkDiv.style.display = abstractDiv.style.display;
            });

            paperDiv.appendChild(titleDiv);
            paperDiv.appendChild(abstractDiv);
            paperDiv.appendChild(linkDiv);
            papersDiv.appendChild(paperDiv);
        });
    </script>
</body>
</html>
